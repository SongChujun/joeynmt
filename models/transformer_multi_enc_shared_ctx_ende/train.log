2020-06-09 16:43:08,719 Hello! This is Joey-NMT.
2020-06-09 16:43:16,461 Total params: 50536449
2020-06-09 16:43:16,466 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder_2.0.layer_norm.bias', 'encoder_2.0.layer_norm.weight', 'encoder_2.0.layers.0.feed_forward.layer_norm.bias', 'encoder_2.0.layers.0.feed_forward.layer_norm.weight', 'encoder_2.0.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.0.layer_norm.bias', 'encoder_2.0.layers.0.layer_norm.weight', 'encoder_2.0.layers.0.src_src_att.k_layer.bias', 'encoder_2.0.layers.0.src_src_att.k_layer.weight', 'encoder_2.0.layers.0.src_src_att.output_layer.bias', 'encoder_2.0.layers.0.src_src_att.output_layer.weight', 'encoder_2.0.layers.0.src_src_att.q_layer.bias', 'encoder_2.0.layers.0.src_src_att.q_layer.weight', 'encoder_2.0.layers.0.src_src_att.v_layer.bias', 'encoder_2.0.layers.0.src_src_att.v_layer.weight', 'encoder_2.0.layers.1.feed_forward.layer_norm.bias', 'encoder_2.0.layers.1.feed_forward.layer_norm.weight', 'encoder_2.0.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.1.layer_norm.bias', 'encoder_2.0.layers.1.layer_norm.weight', 'encoder_2.0.layers.1.src_src_att.k_layer.bias', 'encoder_2.0.layers.1.src_src_att.k_layer.weight', 'encoder_2.0.layers.1.src_src_att.output_layer.bias', 'encoder_2.0.layers.1.src_src_att.output_layer.weight', 'encoder_2.0.layers.1.src_src_att.q_layer.bias', 'encoder_2.0.layers.1.src_src_att.q_layer.weight', 'encoder_2.0.layers.1.src_src_att.v_layer.bias', 'encoder_2.0.layers.1.src_src_att.v_layer.weight', 'encoder_2.0.layers.2.feed_forward.layer_norm.bias', 'encoder_2.0.layers.2.feed_forward.layer_norm.weight', 'encoder_2.0.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.2.layer_norm.bias', 'encoder_2.0.layers.2.layer_norm.weight', 'encoder_2.0.layers.2.src_src_att.k_layer.bias', 'encoder_2.0.layers.2.src_src_att.k_layer.weight', 'encoder_2.0.layers.2.src_src_att.output_layer.bias', 'encoder_2.0.layers.2.src_src_att.output_layer.weight', 'encoder_2.0.layers.2.src_src_att.q_layer.bias', 'encoder_2.0.layers.2.src_src_att.q_layer.weight', 'encoder_2.0.layers.2.src_src_att.v_layer.bias', 'encoder_2.0.layers.2.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-09 16:43:22,085 cfg.name                           : transformer_multi_enc_shared_ctx_ende
2020-06-09 16:43:22,085 cfg.data.src                       : en
2020-06-09 16:43:22,085 cfg.data.trg                       : de
2020-06-09 16:43:22,085 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-09 16:43:22,085 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-09 16:43:22,085 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-09 16:43:22,085 cfg.data.level                     : bpe
2020-06-09 16:43:22,085 cfg.data.lowercase                 : True
2020-06-09 16:43:22,085 cfg.data.max_sent_length           : 100
2020-06-09 16:43:22,085 cfg.testing.beam_size              : 5
2020-06-09 16:43:22,085 cfg.testing.alpha                  : 1.0
2020-06-09 16:43:22,085 cfg.training.random_seed           : 42
2020-06-09 16:43:22,085 cfg.training.optimizer             : adam
2020-06-09 16:43:22,085 cfg.training.normalization         : tokens
2020-06-09 16:43:22,085 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-09 16:43:22,085 cfg.training.scheduling            : plateau
2020-06-09 16:43:22,085 cfg.training.patience              : 8
2020-06-09 16:43:22,086 cfg.training.decrease_factor       : 0.7
2020-06-09 16:43:22,086 cfg.training.loss                  : crossentropy
2020-06-09 16:43:22,086 cfg.training.learning_rate         : 0.0002
2020-06-09 16:43:22,086 cfg.training.learning_rate_min     : 1e-08
2020-06-09 16:43:22,086 cfg.training.weight_decay          : 0.0
2020-06-09 16:43:22,086 cfg.training.label_smoothing       : 0.1
2020-06-09 16:43:22,086 cfg.training.batch_size            : 4096
2020-06-09 16:43:22,086 cfg.training.batch_type            : token
2020-06-09 16:43:22,086 cfg.training.eval_batch_size       : 3600
2020-06-09 16:43:22,086 cfg.training.eval_batch_type       : token
2020-06-09 16:43:22,086 cfg.training.batch_multiplier      : 1
2020-06-09 16:43:22,086 cfg.training.early_stopping_metric : ppl
2020-06-09 16:43:22,086 cfg.training.epochs                : 100
2020-06-09 16:43:22,086 cfg.training.validation_freq       : 1000
2020-06-09 16:43:22,086 cfg.training.logging_freq          : 100
2020-06-09 16:43:22,086 cfg.training.eval_metric           : bleu
2020-06-09 16:43:22,086 cfg.training.model_dir             : models/transformer_multi_enc_shared_ctx_ende
2020-06-09 16:43:22,086 cfg.training.overwrite             : True
2020-06-09 16:43:22,086 cfg.training.shuffle               : True
2020-06-09 16:43:22,086 cfg.training.use_cuda              : True
2020-06-09 16:43:22,086 cfg.training.max_output_length     : 100
2020-06-09 16:43:22,086 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-09 16:43:22,086 cfg.training.keep_last_ckpts       : 3
2020-06-09 16:43:22,086 cfg.model.initializer              : xavier
2020-06-09 16:43:22,086 cfg.model.bias_initializer         : zeros
2020-06-09 16:43:22,086 cfg.model.init_gain                : 1.0
2020-06-09 16:43:22,086 cfg.model.embed_initializer        : xavier
2020-06-09 16:43:22,087 cfg.model.embed_init_gain          : 1.0
2020-06-09 16:43:22,087 cfg.model.tied_embeddings          : False
2020-06-09 16:43:22,087 cfg.model.tied_softmax             : True
2020-06-09 16:43:22,087 cfg.model.encoder.type             : transformer
2020-06-09 16:43:22,087 cfg.model.encoder.num_layers       : 3
2020-06-09 16:43:22,087 cfg.model.encoder.num_heads        : 8
2020-06-09 16:43:22,087 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-09 16:43:22,087 cfg.model.encoder.embeddings.scale : True
2020-06-09 16:43:22,087 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-09 16:43:22,087 cfg.model.encoder.hidden_size      : 512
2020-06-09 16:43:22,087 cfg.model.encoder.ff_size          : 2048
2020-06-09 16:43:22,087 cfg.model.encoder.dropout          : 0.1
2020-06-09 16:43:22,087 cfg.model.encoder.freeze           : False
2020-06-09 16:43:22,087 cfg.model.encoder.multi_encoder    : True
2020-06-09 16:43:22,087 cfg.model.encoder.share_ctx_encoder : True
2020-06-09 16:43:22,087 cfg.model.decoder.type             : transformer
2020-06-09 16:43:22,087 cfg.model.decoder.num_layers       : 6
2020-06-09 16:43:22,087 cfg.model.decoder.num_heads        : 8
2020-06-09 16:43:22,087 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-09 16:43:22,087 cfg.model.decoder.embeddings.scale : True
2020-06-09 16:43:22,087 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-09 16:43:22,087 cfg.model.decoder.hidden_size      : 512
2020-06-09 16:43:22,087 cfg.model.decoder.ff_size          : 2048
2020-06-09 16:43:22,087 cfg.model.decoder.dropout          : 0.1
2020-06-09 16:43:22,087 cfg.model.decoder.freeze           : False
2020-06-09 16:43:22,087 Data set sizes: 
	train 9765,
	valid 1524,
	test 1190
2020-06-09 16:43:22,088 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-09 16:43:22,088 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-09 16:43:22,088 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-09 16:43:22,088 Number of Src words (types): 4559
2020-06-09 16:43:22,088 Number of Trg words (types): 5874
2020-06-09 16:43:22,088 Model(
	encoder=TransformerEncoder(num_layers=2, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4559),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5874))
2020-06-09 16:43:22,103 EPOCH 1
2020-06-09 16:43:42,309 Epoch   1: total training loss 291.77
2020-06-09 16:43:42,310 EPOCH 2
2020-06-09 16:43:58,408 Epoch   2 Step:      100 Batch Loss:     4.595460 Tokens per Sec:     6857, Lr: 0.000200
2020-06-09 16:44:01,620 Epoch   2: total training loss 261.84
2020-06-09 16:44:01,620 EPOCH 3
2020-06-09 16:44:20,566 Epoch   3: total training loss 228.42
2020-06-09 16:44:20,568 EPOCH 4
2020-06-09 16:44:33,104 Epoch   4 Step:      200 Batch Loss:     3.089890 Tokens per Sec:     6796, Lr: 0.000200
2020-06-09 16:44:39,674 Epoch   4: total training loss 219.11
2020-06-09 16:44:39,675 EPOCH 5
2020-06-09 16:44:59,039 Epoch   5: total training loss 200.16
2020-06-09 16:44:59,040 EPOCH 6
2020-06-09 16:45:07,767 Epoch   6 Step:      300 Batch Loss:     3.318379 Tokens per Sec:     6845, Lr: 0.000200
2020-06-09 16:45:18,065 Epoch   6: total training loss 177.39
2020-06-09 16:45:18,067 EPOCH 7
2020-06-09 16:45:37,123 Epoch   7: total training loss 157.36
2020-06-09 16:45:37,124 EPOCH 8
2020-06-09 16:45:42,498 Epoch   8 Step:      400 Batch Loss:     2.687974 Tokens per Sec:     6979, Lr: 0.000200
2020-06-09 16:45:56,316 Epoch   8: total training loss 146.85
2020-06-09 16:45:56,318 EPOCH 9
2020-06-09 16:46:15,462 Epoch   9: total training loss 129.07
2020-06-09 16:46:15,463 EPOCH 10
2020-06-09 16:46:16,837 Epoch  10 Step:      500 Batch Loss:     2.766216 Tokens per Sec:     7393, Lr: 0.000200
2020-06-09 16:46:34,607 Epoch  10: total training loss 117.45
2020-06-09 16:46:34,608 EPOCH 11
2020-06-09 16:46:51,960 Epoch  11 Step:      600 Batch Loss:     3.249125 Tokens per Sec:     6843, Lr: 0.000200
2020-06-09 16:46:53,775 Epoch  11: total training loss 112.12
2020-06-09 16:46:53,775 EPOCH 12
2020-06-09 16:47:12,833 Epoch  12: total training loss 101.58
2020-06-09 16:47:12,834 EPOCH 13
2020-06-09 16:47:26,517 Epoch  13 Step:      700 Batch Loss:     3.380038 Tokens per Sec:     6872, Lr: 0.000200
2020-06-09 16:47:31,764 Epoch  13: total training loss 92.08
2020-06-09 16:47:31,765 EPOCH 14
2020-06-09 16:47:50,870 Epoch  14: total training loss 84.68
2020-06-09 16:47:50,871 EPOCH 15
2020-06-09 16:48:00,740 Epoch  15 Step:      800 Batch Loss:     2.046888 Tokens per Sec:     6865, Lr: 0.000200
2020-06-09 16:48:10,101 Epoch  15: total training loss 79.39
2020-06-09 16:48:10,102 EPOCH 16
2020-06-09 16:48:29,348 Epoch  16: total training loss 71.46
2020-06-09 16:48:29,349 EPOCH 17
2020-06-09 16:48:36,003 Epoch  17 Step:      900 Batch Loss:     0.614374 Tokens per Sec:     6463, Lr: 0.000200
2020-06-09 16:48:48,577 Epoch  17: total training loss 70.98
2020-06-09 16:48:48,578 EPOCH 18
2020-06-09 16:49:07,530 Epoch  18: total training loss 62.36
2020-06-09 16:49:07,531 EPOCH 19
2020-06-09 16:49:09,960 Epoch  19 Step:     1000 Batch Loss:     0.672430 Tokens per Sec:     8087, Lr: 0.000200
2020-06-09 16:49:42,789 Hooray! New best validation result [ppl]!
2020-06-09 16:49:42,790 Saving new checkpoint.
2020-06-09 16:49:50,986 Example #0
2020-06-09 16:49:50,987 	Raw source:     ['hello', '.']
2020-06-09 16:49:50,987 	Raw hypothesis: ['hallo', '.']
2020-06-09 16:49:50,987 	Source:     hello .
2020-06-09 16:49:50,987 	Reference:  hallo ,
2020-06-09 16:49:50,987 	Hypothesis: hallo .
2020-06-09 16:49:50,987 Example #1
2020-06-09 16:49:50,987 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 16:49:50,987 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 16:49:50,987 	Source:     hi , how can i help you ?
2020-06-09 16:49:50,987 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 16:49:50,987 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 16:49:50,987 Example #2
2020-06-09 16:49:50,987 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 16:49:50,987 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 16:49:50,987 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 16:49:50,987 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 16:49:50,987 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 16:49:50,987 Example #3
2020-06-09 16:49:50,987 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 16:49:50,987 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 16:49:50,987 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 16:49:50,987 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 16:49:50,987 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 16:49:50,987 Validation result (greedy) at epoch  19, step     1000: bleu:  32.99, loss: 45770.8633, ppl:   8.7579, duration: 41.0268s
2020-06-09 16:50:07,877 Epoch  19: total training loss 55.86
2020-06-09 16:50:07,878 EPOCH 20
2020-06-09 16:50:26,369 Epoch  20 Step:     1100 Batch Loss:     1.076506 Tokens per Sec:     6770, Lr: 0.000200
2020-06-09 16:50:27,120 Epoch  20: total training loss 53.24
2020-06-09 16:50:27,120 EPOCH 21
2020-06-09 16:50:46,411 Epoch  21: total training loss 46.74
2020-06-09 16:50:46,412 EPOCH 22
2020-06-09 16:51:02,025 Epoch  22 Step:     1200 Batch Loss:     1.457047 Tokens per Sec:     6759, Lr: 0.000200
2020-06-09 16:51:05,517 Epoch  22: total training loss 43.16
2020-06-09 16:51:05,518 EPOCH 23
2020-06-09 16:51:25,102 Epoch  23: total training loss 42.29
2020-06-09 16:51:25,103 EPOCH 24
2020-06-09 16:51:37,066 Epoch  24 Step:     1300 Batch Loss:     0.584611 Tokens per Sec:     6700, Lr: 0.000200
2020-06-09 16:51:44,252 Epoch  24: total training loss 39.34
2020-06-09 16:51:44,253 EPOCH 25
2020-06-09 16:52:03,576 Epoch  25: total training loss 36.39
2020-06-09 16:52:03,577 EPOCH 26
2020-06-09 16:52:11,432 Epoch  26 Step:     1400 Batch Loss:     0.647504 Tokens per Sec:     6859, Lr: 0.000200
2020-06-09 16:52:22,715 Epoch  26: total training loss 32.49
2020-06-09 16:52:22,716 EPOCH 27
2020-06-09 16:52:42,073 Epoch  27: total training loss 27.72
2020-06-09 16:52:42,074 EPOCH 28
2020-06-09 16:52:46,990 Epoch  28 Step:     1500 Batch Loss:     0.478502 Tokens per Sec:     6675, Lr: 0.000200
2020-06-09 16:53:01,486 Epoch  28: total training loss 26.19
2020-06-09 16:53:01,487 EPOCH 29
2020-06-09 16:53:20,910 Epoch  29: total training loss 24.51
2020-06-09 16:53:20,911 EPOCH 30
2020-06-09 16:53:22,095 Epoch  30 Step:     1600 Batch Loss:     0.466582 Tokens per Sec:     8905, Lr: 0.000200
2020-06-09 16:53:40,125 Epoch  30: total training loss 23.25
2020-06-09 16:53:40,126 EPOCH 31
2020-06-09 16:53:57,126 Epoch  31 Step:     1700 Batch Loss:     0.248634 Tokens per Sec:     6846, Lr: 0.000200
2020-06-09 16:53:59,176 Epoch  31: total training loss 22.26
2020-06-09 16:53:59,176 EPOCH 32
2020-06-09 16:54:18,095 Epoch  32: total training loss 20.03
2020-06-09 16:54:18,096 EPOCH 33
2020-06-09 16:54:31,338 Epoch  33 Step:     1800 Batch Loss:     0.338274 Tokens per Sec:     7023, Lr: 0.000200
2020-06-09 16:54:37,175 Epoch  33: total training loss 18.49
2020-06-09 16:54:37,176 EPOCH 34
2020-06-09 16:54:55,986 Epoch  34: total training loss 17.30
2020-06-09 16:54:55,987 EPOCH 35
2020-06-09 16:55:05,566 Epoch  35 Step:     1900 Batch Loss:     0.249881 Tokens per Sec:     7074, Lr: 0.000200
2020-06-09 16:55:14,729 Epoch  35: total training loss 15.64
2020-06-09 16:55:14,730 EPOCH 36
2020-06-09 16:55:34,092 Epoch  36: total training loss 14.81
2020-06-09 16:55:34,093 EPOCH 37
2020-06-09 16:55:40,129 Epoch  37 Step:     2000 Batch Loss:     0.354266 Tokens per Sec:     7137, Lr: 0.000200
2020-06-09 16:56:03,030 Example #0
2020-06-09 16:56:03,031 	Raw source:     ['hello', '.']
2020-06-09 16:56:03,031 	Raw hypothesis: ['hallo', '.']
2020-06-09 16:56:03,031 	Source:     hello .
2020-06-09 16:56:03,031 	Reference:  hallo ,
2020-06-09 16:56:03,031 	Hypothesis: hallo .
2020-06-09 16:56:03,031 Example #1
2020-06-09 16:56:03,031 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 16:56:03,032 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 16:56:03,032 	Source:     hi , how can i help you ?
2020-06-09 16:56:03,032 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 16:56:03,032 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 16:56:03,032 Example #2
2020-06-09 16:56:03,032 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 16:56:03,032 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 16:56:03,032 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 16:56:03,032 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 16:56:03,033 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 16:56:03,033 Example #3
2020-06-09 16:56:03,033 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 16:56:03,033 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 16:56:03,033 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 16:56:03,033 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 16:56:03,033 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 16:56:03,033 Validation result (greedy) at epoch  37, step     2000: bleu:  39.28, loss: 46291.3867, ppl:   8.9767, duration: 22.9029s
2020-06-09 16:56:16,289 Epoch  37: total training loss 14.44
2020-06-09 16:56:16,290 EPOCH 38
2020-06-09 16:56:35,714 Epoch  38: total training loss 12.81
2020-06-09 16:56:35,715 EPOCH 39
2020-06-09 16:56:39,100 Epoch  39 Step:     2100 Batch Loss:     0.142602 Tokens per Sec:     6275, Lr: 0.000200
2020-06-09 16:56:54,792 Epoch  39: total training loss 12.50
2020-06-09 16:56:54,793 EPOCH 40
2020-06-09 16:57:13,175 Epoch  40 Step:     2200 Batch Loss:     0.206635 Tokens per Sec:     6850, Lr: 0.000200
2020-06-09 16:57:13,952 Epoch  40: total training loss 13.94
2020-06-09 16:57:13,953 EPOCH 41
2020-06-09 16:57:33,028 Epoch  41: total training loss 12.20
2020-06-09 16:57:33,029 EPOCH 42
2020-06-09 16:57:48,191 Epoch  42 Step:     2300 Batch Loss:     0.132898 Tokens per Sec:     6784, Lr: 0.000200
2020-06-09 16:57:52,245 Epoch  42: total training loss 11.09
2020-06-09 16:57:52,245 EPOCH 43
2020-06-09 16:58:11,618 Epoch  43: total training loss 11.20
2020-06-09 16:58:11,619 EPOCH 44
2020-06-09 16:58:22,935 Epoch  44 Step:     2400 Batch Loss:     0.286621 Tokens per Sec:     7168, Lr: 0.000200
2020-06-09 16:58:30,738 Epoch  44: total training loss 11.60
2020-06-09 16:58:30,739 EPOCH 45
2020-06-09 16:58:49,471 Epoch  45: total training loss 11.00
2020-06-09 16:58:49,472 EPOCH 46
2020-06-09 16:58:57,569 Epoch  46 Step:     2500 Batch Loss:     0.188363 Tokens per Sec:     6926, Lr: 0.000200
2020-06-09 16:59:08,810 Epoch  46: total training loss 10.33
2020-06-09 16:59:08,810 EPOCH 47
2020-06-09 16:59:28,009 Epoch  47: total training loss 9.51
2020-06-09 16:59:28,010 EPOCH 48
2020-06-09 16:59:32,560 Epoch  48 Step:     2600 Batch Loss:     0.263316 Tokens per Sec:     6655, Lr: 0.000200
2020-06-09 16:59:46,894 Epoch  48: total training loss 10.25
2020-06-09 16:59:46,895 EPOCH 49
2020-06-09 17:00:06,163 Epoch  49: total training loss 8.85
2020-06-09 17:00:06,165 EPOCH 50
2020-06-09 17:00:07,502 Epoch  50 Step:     2700 Batch Loss:     0.162697 Tokens per Sec:     7966, Lr: 0.000200
2020-06-09 17:00:25,579 Epoch  50: total training loss 9.18
2020-06-09 17:00:25,580 EPOCH 51
2020-06-09 17:00:42,280 Epoch  51 Step:     2800 Batch Loss:     0.159372 Tokens per Sec:     6887, Lr: 0.000200
2020-06-09 17:00:44,546 Epoch  51: total training loss 8.49
2020-06-09 17:00:44,547 EPOCH 52
2020-06-09 17:01:03,628 Epoch  52: total training loss 8.06
2020-06-09 17:01:03,629 EPOCH 53
2020-06-09 17:01:17,132 Epoch  53 Step:     2900 Batch Loss:     0.147537 Tokens per Sec:     7057, Lr: 0.000200
2020-06-09 17:01:22,784 Epoch  53: total training loss 7.92
2020-06-09 17:01:22,785 EPOCH 54
2020-06-09 17:01:41,996 Epoch  54: total training loss 7.62
2020-06-09 17:01:41,997 EPOCH 55
2020-06-09 17:01:53,183 Epoch  55 Step:     3000 Batch Loss:     0.138371 Tokens per Sec:     6662, Lr: 0.000200
2020-06-09 17:02:15,675 Example #0
2020-06-09 17:02:15,676 	Raw source:     ['hello', '.']
2020-06-09 17:02:15,677 	Raw hypothesis: ['hallo', '.']
2020-06-09 17:02:15,677 	Source:     hello .
2020-06-09 17:02:15,677 	Reference:  hallo ,
2020-06-09 17:02:15,677 	Hypothesis: hallo .
2020-06-09 17:02:15,677 Example #1
2020-06-09 17:02:15,677 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 17:02:15,677 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 17:02:15,677 	Source:     hi , how can i help you ?
2020-06-09 17:02:15,677 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 17:02:15,677 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 17:02:15,677 Example #2
2020-06-09 17:02:15,677 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 17:02:15,677 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 17:02:15,677 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 17:02:15,677 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 17:02:15,677 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 17:02:15,677 Example #3
2020-06-09 17:02:15,678 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 17:02:15,678 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 17:02:15,678 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 17:02:15,678 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 17:02:15,678 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 17:02:15,678 Validation result (greedy) at epoch  55, step     3000: bleu:  40.04, loss: 46506.2695, ppl:   9.0686, duration: 22.4927s
2020-06-09 17:02:23,685 Epoch  55: total training loss 7.20
2020-06-09 17:02:23,686 EPOCH 56
2020-06-09 17:02:42,736 Epoch  56: total training loss 7.24
2020-06-09 17:02:42,737 EPOCH 57
2020-06-09 17:02:50,408 Epoch  57 Step:     3100 Batch Loss:     0.133198 Tokens per Sec:     6639, Lr: 0.000200
2020-06-09 17:03:01,807 Epoch  57: total training loss 7.32
2020-06-09 17:03:01,808 EPOCH 58
2020-06-09 17:03:20,964 Epoch  58: total training loss 7.16
2020-06-09 17:03:20,965 EPOCH 59
2020-06-09 17:03:25,103 Epoch  59 Step:     3200 Batch Loss:     0.106752 Tokens per Sec:     6798, Lr: 0.000200
2020-06-09 17:03:40,002 Epoch  59: total training loss 7.06
2020-06-09 17:03:40,003 EPOCH 60
2020-06-09 17:03:59,066 Epoch  60: total training loss 6.94
2020-06-09 17:03:59,067 EPOCH 61
2020-06-09 17:03:59,570 Epoch  61 Step:     3300 Batch Loss:     0.125683 Tokens per Sec:     3173, Lr: 0.000200
2020-06-09 17:04:18,213 Epoch  61: total training loss 6.93
2020-06-09 17:04:18,213 EPOCH 62
2020-06-09 17:04:34,573 Epoch  62 Step:     3400 Batch Loss:     0.086035 Tokens per Sec:     6641, Lr: 0.000200
2020-06-09 17:04:37,483 Epoch  62: total training loss 6.64
2020-06-09 17:04:37,484 EPOCH 63
2020-06-09 17:04:56,362 Epoch  63: total training loss 6.66
2020-06-09 17:04:56,363 EPOCH 64
2020-06-09 17:05:09,266 Epoch  64 Step:     3500 Batch Loss:     0.114457 Tokens per Sec:     6817, Lr: 0.000200
2020-06-09 17:05:15,669 Epoch  64: total training loss 6.59
2020-06-09 17:05:15,670 EPOCH 65
2020-06-09 17:05:34,788 Epoch  65: total training loss 6.31
2020-06-09 17:05:34,790 EPOCH 66
2020-06-09 17:05:45,310 Epoch  66 Step:     3600 Batch Loss:     0.109843 Tokens per Sec:     6507, Lr: 0.000200
2020-06-09 17:05:53,982 Epoch  66: total training loss 6.21
2020-06-09 17:05:53,983 EPOCH 67
2020-06-09 17:06:13,737 Epoch  67: total training loss 6.05
2020-06-09 17:06:13,738 EPOCH 68
2020-06-09 17:06:21,111 Epoch  68 Step:     3700 Batch Loss:     0.107178 Tokens per Sec:     6643, Lr: 0.000200
2020-06-09 17:06:33,062 Epoch  68: total training loss 6.06
2020-06-09 17:06:33,062 EPOCH 69
2020-06-09 17:06:52,509 Epoch  69: total training loss 6.05
2020-06-09 17:06:52,510 EPOCH 70
2020-06-09 17:06:56,268 Epoch  70 Step:     3800 Batch Loss:     0.100751 Tokens per Sec:     7589, Lr: 0.000200
2020-06-09 17:07:11,603 Epoch  70: total training loss 5.95
2020-06-09 17:07:11,603 EPOCH 71
2020-06-09 17:07:30,981 Epoch  71: total training loss 6.59
2020-06-09 17:07:30,982 EPOCH 72
2020-06-09 17:07:31,319 Epoch  72 Step:     3900 Batch Loss:     0.116667 Tokens per Sec:     7324, Lr: 0.000200
2020-06-09 17:07:50,195 Epoch  72: total training loss 6.76
2020-06-09 17:07:50,196 EPOCH 73
2020-06-09 17:08:06,801 Epoch  73 Step:     4000 Batch Loss:     0.119804 Tokens per Sec:     6609, Lr: 0.000200
2020-06-09 17:08:27,304 Example #0
2020-06-09 17:08:27,305 	Raw source:     ['hello', '.']
2020-06-09 17:08:27,305 	Raw hypothesis: ['hallo', '!']
2020-06-09 17:08:27,305 	Source:     hello .
2020-06-09 17:08:27,305 	Reference:  hallo ,
2020-06-09 17:08:27,305 	Hypothesis: hallo !
2020-06-09 17:08:27,305 Example #1
2020-06-09 17:08:27,305 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 17:08:27,305 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 17:08:27,305 	Source:     hi , how can i help you ?
2020-06-09 17:08:27,305 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 17:08:27,305 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 17:08:27,305 Example #2
2020-06-09 17:08:27,305 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 17:08:27,305 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 17:08:27,305 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 17:08:27,305 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 17:08:27,305 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 17:08:27,305 Example #3
2020-06-09 17:08:27,305 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 17:08:27,305 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 17:08:27,305 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 17:08:27,305 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 17:08:27,306 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 17:08:27,306 Validation result (greedy) at epoch  73, step     4000: bleu:  39.89, loss: 47038.8359, ppl:   9.3005, duration: 20.5026s
2020-06-09 17:08:30,021 Epoch  73: total training loss 6.28
2020-06-09 17:08:30,021 EPOCH 74
2020-06-09 17:08:49,125 Epoch  74: total training loss 5.95
2020-06-09 17:08:49,125 EPOCH 75
2020-06-09 17:09:01,966 Epoch  75 Step:     4100 Batch Loss:     0.101609 Tokens per Sec:     6966, Lr: 0.000200
2020-06-09 17:09:08,186 Epoch  75: total training loss 5.84
2020-06-09 17:09:08,186 EPOCH 76
2020-06-09 17:09:27,221 Epoch  76: total training loss 6.57
2020-06-09 17:09:27,222 EPOCH 77
2020-06-09 17:09:36,420 Epoch  77 Step:     4200 Batch Loss:     0.098984 Tokens per Sec:     7041, Lr: 0.000200
2020-06-09 17:09:46,357 Epoch  77: total training loss 5.79
2020-06-09 17:09:46,357 EPOCH 78
2020-06-09 17:10:05,413 Epoch  78: total training loss 5.58
2020-06-09 17:10:05,414 EPOCH 79
2020-06-09 17:10:11,827 Epoch  79 Step:     4300 Batch Loss:     0.090148 Tokens per Sec:     6851, Lr: 0.000200
2020-06-09 17:10:24,498 Epoch  79: total training loss 5.31
2020-06-09 17:10:24,499 EPOCH 80
2020-06-09 17:10:43,806 Epoch  80: total training loss 5.46
2020-06-09 17:10:43,807 EPOCH 81
2020-06-09 17:10:46,745 Epoch  81 Step:     4400 Batch Loss:     0.105767 Tokens per Sec:     6640, Lr: 0.000200
2020-06-09 17:11:03,000 Epoch  81: total training loss 5.40
2020-06-09 17:11:03,001 EPOCH 82
2020-06-09 17:11:21,402 Epoch  82 Step:     4500 Batch Loss:     0.097144 Tokens per Sec:     6798, Lr: 0.000200
2020-06-09 17:11:22,082 Epoch  82: total training loss 5.26
2020-06-09 17:11:22,083 EPOCH 83
2020-06-09 17:11:41,252 Epoch  83: total training loss 5.35
2020-06-09 17:11:41,253 EPOCH 84
2020-06-09 17:11:56,802 Epoch  84 Step:     4600 Batch Loss:     0.113893 Tokens per Sec:     6861, Lr: 0.000200
2020-06-09 17:12:00,556 Epoch  84: total training loss 5.18
2020-06-09 17:12:00,556 EPOCH 85
2020-06-09 17:12:19,845 Epoch  85: total training loss 5.36
2020-06-09 17:12:19,846 EPOCH 86
2020-06-09 17:12:31,380 Epoch  86 Step:     4700 Batch Loss:     0.110823 Tokens per Sec:     7180, Lr: 0.000200
2020-06-09 17:12:39,030 Epoch  86: total training loss 5.42
2020-06-09 17:12:39,031 EPOCH 87
2020-06-09 17:12:57,948 Epoch  87: total training loss 5.37
2020-06-09 17:12:57,949 EPOCH 88
2020-06-09 17:13:06,448 Epoch  88 Step:     4800 Batch Loss:     0.081146 Tokens per Sec:     6761, Lr: 0.000200
2020-06-09 17:13:16,835 Epoch  88: total training loss 5.27
2020-06-09 17:13:16,836 EPOCH 89
2020-06-09 17:13:36,058 Epoch  89: total training loss 4.99
2020-06-09 17:13:36,059 EPOCH 90
2020-06-09 17:13:41,596 Epoch  90 Step:     4900 Batch Loss:     0.094828 Tokens per Sec:     6102, Lr: 0.000200
2020-06-09 17:13:55,096 Epoch  90: total training loss 5.06
2020-06-09 17:13:55,097 EPOCH 91
2020-06-09 17:14:14,175 Epoch  91: total training loss 4.75
2020-06-09 17:14:14,176 EPOCH 92
2020-06-09 17:14:16,484 Epoch  92 Step:     5000 Batch Loss:     0.078430 Tokens per Sec:     5681, Lr: 0.000200
2020-06-09 17:14:39,399 Example #0
2020-06-09 17:14:39,400 	Raw source:     ['hello', '.']
2020-06-09 17:14:39,400 	Raw hypothesis: ['hallo', '.']
2020-06-09 17:14:39,401 	Source:     hello .
2020-06-09 17:14:39,401 	Reference:  hallo ,
2020-06-09 17:14:39,401 	Hypothesis: hallo .
2020-06-09 17:14:39,401 Example #1
2020-06-09 17:14:39,401 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 17:14:39,401 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 17:14:39,401 	Source:     hi , how can i help you ?
2020-06-09 17:14:39,401 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 17:14:39,401 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 17:14:39,402 Example #2
2020-06-09 17:14:39,402 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 17:14:39,402 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 17:14:39,402 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 17:14:39,402 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 17:14:39,402 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 17:14:39,402 Example #3
2020-06-09 17:14:39,402 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 17:14:39,403 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 17:14:39,403 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 17:14:39,403 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 17:14:39,403 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 17:14:39,403 Validation result (greedy) at epoch  92, step     5000: bleu:  39.26, loss: 47078.2539, ppl:   9.3179, duration: 22.9180s
2020-06-09 17:14:56,938 Epoch  92: total training loss 5.06
2020-06-09 17:14:56,939 EPOCH 93
2020-06-09 17:15:14,552 Epoch  93 Step:     5100 Batch Loss:     0.085732 Tokens per Sec:     6861, Lr: 0.000200
2020-06-09 17:15:16,065 Epoch  93: total training loss 4.76
2020-06-09 17:15:16,066 EPOCH 94
2020-06-09 17:15:35,075 Epoch  94: total training loss 4.65
2020-06-09 17:15:35,076 EPOCH 95
2020-06-09 17:15:49,472 Epoch  95 Step:     5200 Batch Loss:     0.078577 Tokens per Sec:     6996, Lr: 0.000200
2020-06-09 17:15:54,237 Epoch  95: total training loss 4.77
2020-06-09 17:15:54,237 EPOCH 96
2020-06-09 17:16:13,281 Epoch  96: total training loss 4.81
2020-06-09 17:16:13,282 EPOCH 97
2020-06-09 17:16:24,886 Epoch  97 Step:     5300 Batch Loss:     0.090312 Tokens per Sec:     6563, Lr: 0.000200
2020-06-09 17:16:32,590 Epoch  97: total training loss 4.81
2020-06-09 17:16:32,591 EPOCH 98
2020-06-09 17:16:51,592 Epoch  98: total training loss 4.65
2020-06-09 17:16:51,593 EPOCH 99
2020-06-09 17:16:59,829 Epoch  99 Step:     5400 Batch Loss:     0.069430 Tokens per Sec:     6613, Lr: 0.000200
2020-06-09 17:17:10,628 Epoch  99: total training loss 4.69
2020-06-09 17:17:10,629 EPOCH 100
2020-06-09 17:17:29,758 Epoch 100: total training loss 4.68
2020-06-09 17:17:29,760 Training ended after 100 epochs.
2020-06-09 17:17:29,760 Best validation result (greedy) at step     1000:   8.76 ppl.
2020-06-09 17:17:58,835  dev bleu:  35.66 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-09 17:17:58,842 Translations saved to: models/transformer_multi_enc_shared_ctx_ende/00001000.hyps.dev
2020-06-09 17:18:19,826 test bleu:  33.21 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-09 17:18:19,832 Translations saved to: models/transformer_multi_enc_shared_ctx_ende/00001000.hyps.test
