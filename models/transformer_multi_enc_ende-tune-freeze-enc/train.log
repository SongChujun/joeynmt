2020-07-02 04:36:47,987 Hello! This is Joey-NMT.
2020-07-02 04:37:06,000 Total params: 48183809
2020-07-02 04:37:06,004 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-07-02 04:37:15,049 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-02 04:37:22,075 Reset optimizer.
2020-07-02 04:37:22,076 Reset scheduler.
2020-07-02 04:37:22,076 Reset tracking of the best checkpoint.
2020-07-02 04:37:22,082 cfg.name                           : transformer
2020-07-02 04:37:22,082 cfg.data.src                       : en
2020-07-02 04:37:22,082 cfg.data.trg                       : de
2020-07-02 04:37:22,082 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-02 04:37:22,082 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-02 04:37:22,083 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-02 04:37:22,083 cfg.data.level                     : bpe
2020-07-02 04:37:22,083 cfg.data.lowercase                 : False
2020-07-02 04:37:22,083 cfg.data.max_sent_length           : 100
2020-07-02 04:37:22,083 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-02 04:37:22,083 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-02 04:37:22,083 cfg.testing.beam_size              : 5
2020-07-02 04:37:22,083 cfg.testing.alpha                  : 1.0
2020-07-02 04:37:22,083 cfg.training.random_seed           : 42
2020-07-02 04:37:22,083 cfg.training.optimizer             : adam
2020-07-02 04:37:22,083 cfg.training.normalization         : tokens
2020-07-02 04:37:22,083 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-02 04:37:22,083 cfg.training.scheduling            : plateau
2020-07-02 04:37:22,083 cfg.training.patience              : 8
2020-07-02 04:37:22,083 cfg.training.decrease_factor       : 0.7
2020-07-02 04:37:22,083 cfg.training.loss                  : crossentropy
2020-07-02 04:37:22,083 cfg.training.learning_rate         : 0.0002
2020-07-02 04:37:22,083 cfg.training.learning_rate_min     : 1e-08
2020-07-02 04:37:22,083 cfg.training.weight_decay          : 0.0
2020-07-02 04:37:22,083 cfg.training.label_smoothing       : 0.1
2020-07-02 04:37:22,083 cfg.training.batch_size            : 2048
2020-07-02 04:37:22,083 cfg.training.batch_type            : token
2020-07-02 04:37:22,083 cfg.training.batch_multiplier      : 1
2020-07-02 04:37:22,084 cfg.training.early_stopping_metric : ppl
2020-07-02 04:37:22,084 cfg.training.epochs                : 100
2020-07-02 04:37:22,084 cfg.training.validation_freq       : 1000
2020-07-02 04:37:22,084 cfg.training.logging_freq          : 100
2020-07-02 04:37:22,084 cfg.training.eval_metric           : bleu
2020-07-02 04:37:22,084 cfg.training.model_dir             : models/transformer_multi_enc_ende-tune-freeze-enc
2020-07-02 04:37:22,084 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-02 04:37:22,084 cfg.training.reset_best_ckpt       : True
2020-07-02 04:37:22,084 cfg.training.reset_scheduler       : True
2020-07-02 04:37:22,084 cfg.training.reset_optimizer       : True
2020-07-02 04:37:22,084 cfg.training.overwrite             : False
2020-07-02 04:37:22,084 cfg.training.shuffle               : True
2020-07-02 04:37:22,084 cfg.training.use_cuda              : True
2020-07-02 04:37:22,084 cfg.training.max_output_length     : 100
2020-07-02 04:37:22,084 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-02 04:37:22,084 cfg.training.keep_last_ckpts       : 3
2020-07-02 04:37:22,084 cfg.model.initializer              : xavier
2020-07-02 04:37:22,084 cfg.model.bias_initializer         : zeros
2020-07-02 04:37:22,084 cfg.model.init_gain                : 1.0
2020-07-02 04:37:22,084 cfg.model.embed_initializer        : xavier
2020-07-02 04:37:22,084 cfg.model.embed_init_gain          : 1.0
2020-07-02 04:37:22,084 cfg.model.tied_embeddings          : True
2020-07-02 04:37:22,084 cfg.model.tied_softmax             : True
2020-07-02 04:37:22,084 cfg.model.encoder.type             : transformer
2020-07-02 04:37:22,084 cfg.model.encoder.num_layers       : 6
2020-07-02 04:37:22,084 cfg.model.encoder.num_heads        : 8
2020-07-02 04:37:22,085 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-02 04:37:22,085 cfg.model.encoder.embeddings.scale : True
2020-07-02 04:37:22,085 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-02 04:37:22,085 cfg.model.encoder.hidden_size      : 512
2020-07-02 04:37:22,085 cfg.model.encoder.ff_size          : 2048
2020-07-02 04:37:22,085 cfg.model.encoder.dropout          : 0.1
2020-07-02 04:37:22,085 cfg.model.encoder.freeze           : True
2020-07-02 04:37:22,085 cfg.model.encoder.multi_encoder    : True
2020-07-02 04:37:22,085 cfg.model.decoder.type             : transformer
2020-07-02 04:37:22,085 cfg.model.decoder.num_layers       : 6
2020-07-02 04:37:22,085 cfg.model.decoder.num_heads        : 8
2020-07-02 04:37:22,085 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-02 04:37:22,085 cfg.model.decoder.embeddings.scale : True
2020-07-02 04:37:22,085 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-02 04:37:22,085 cfg.model.decoder.hidden_size      : 512
2020-07-02 04:37:22,085 cfg.model.decoder.ff_size          : 2048
2020-07-02 04:37:22,085 cfg.model.decoder.dropout          : 0.1
2020-07-02 04:37:22,085 Data set sizes: 
	train 9771,
	valid 1522,
	test 1160
2020-07-02 04:37:22,085 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-02 04:37:22,085 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-02 04:37:22,085 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-02 04:37:22,085 Number of Src words (types): 36628
2020-07-02 04:37:22,085 Number of Trg words (types): 36628
2020-07-02 04:37:22,086 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-02 04:37:22,113 EPOCH 1
2020-07-02 04:37:47,077 Epoch   1 Step:  1360100 Batch Loss:     1.825191 Tokens per Sec:     4922, Lr: 0.000200
2020-07-02 04:37:53,697 Epoch   1: total training loss 446.40
2020-07-02 04:37:53,697 EPOCH 2
