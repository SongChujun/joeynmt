2020-06-18 17:25:03,891 Hello! This is Joey-NMT.
2020-06-18 17:25:10,080 Total params: 24182785
2020-06-18 17:25:10,082 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-06-18 17:25:12,193 Loading model from models/transformer_iwslt14_deen_bpe/best.ckpt
2020-06-18 17:25:12,318 Reset optimizer.
2020-06-18 17:25:12,319 Reset scheduler.
2020-06-18 17:25:12,319 Reset tracking of the best checkpoint.
2020-06-18 17:25:12,325 cfg.name                           : iwslt14-deen-bpe-transformer-multi_enc-tune
2020-06-18 17:25:12,325 cfg.data.src                       : de
2020-06-18 17:25:12,325 cfg.data.trg                       : en
2020-06-18 17:25:12,325 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.iwslt14-deen-bpe
2020-06-18 17:25:12,325 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.iwslt14-deen-bpe
2020-06-18 17:25:12,325 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.iwslt14-deen-bpe
2020-06-18 17:25:12,325 cfg.data.level                     : bpe
2020-06-18 17:25:12,325 cfg.data.lowercase                 : True
2020-06-18 17:25:12,325 cfg.data.max_sent_length           : 62
2020-06-18 17:25:12,325 cfg.data.src_vocab                 : models/transformer_iwslt14_deen_bpe/src_vocab.txt
2020-06-18 17:25:12,325 cfg.data.trg_vocab                 : models/transformer_iwslt14_deen_bpe/trg_vocab.txt
2020-06-18 17:25:12,325 cfg.testing.beam_size              : 5
2020-06-18 17:25:12,325 cfg.testing.alpha                  : 1.0
2020-06-18 17:25:12,325 cfg.training.random_seed           : 42
2020-06-18 17:25:12,325 cfg.training.optimizer             : adam
2020-06-18 17:25:12,325 cfg.training.normalization         : tokens
2020-06-18 17:25:12,325 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-18 17:25:12,325 cfg.training.scheduling            : plateau
2020-06-18 17:25:12,325 cfg.training.patience              : 5
2020-06-18 17:25:12,325 cfg.training.decrease_factor       : 0.7
2020-06-18 17:25:12,325 cfg.training.loss                  : crossentropy
2020-06-18 17:25:12,325 cfg.training.learning_rate         : 0.0003
2020-06-18 17:25:12,326 cfg.training.learning_rate_min     : 1e-08
2020-06-18 17:25:12,326 cfg.training.weight_decay          : 0.0
2020-06-18 17:25:12,326 cfg.training.label_smoothing       : 0.1
2020-06-18 17:25:12,326 cfg.training.batch_size            : 4096
2020-06-18 17:25:12,326 cfg.training.batch_type            : token
2020-06-18 17:25:12,326 cfg.training.early_stopping_metric : eval_metric
2020-06-18 17:25:12,326 cfg.training.epochs                : 100
2020-06-18 17:25:12,326 cfg.training.validation_freq       : 1000
2020-06-18 17:25:12,326 cfg.training.logging_freq          : 100
2020-06-18 17:25:12,326 cfg.training.eval_metric           : bleu
2020-06-18 17:25:12,326 cfg.training.model_dir             : models/transformer_iwslt14_deen_bpe-multi_enc-tune
2020-06-18 17:25:12,326 cfg.training.load_model            : models/transformer_iwslt14_deen_bpe/best.ckpt
2020-06-18 17:25:12,326 cfg.training.reset_best_ckpt       : True
2020-06-18 17:25:12,326 cfg.training.reset_scheduler       : True
2020-06-18 17:25:12,326 cfg.training.reset_optimizer       : True
2020-06-18 17:25:12,326 cfg.training.overwrite             : False
2020-06-18 17:25:12,326 cfg.training.shuffle               : True
2020-06-18 17:25:12,326 cfg.training.use_cuda              : True
2020-06-18 17:25:12,326 cfg.training.keep_last_ckpts       : 5
2020-06-18 17:25:12,326 cfg.model.initializer              : xavier
2020-06-18 17:25:12,326 cfg.model.embed_initializer        : xavier
2020-06-18 17:25:12,326 cfg.model.embed_init_gain          : 1.0
2020-06-18 17:25:12,326 cfg.model.init_gain                : 1.0
2020-06-18 17:25:12,326 cfg.model.bias_initializer         : zeros
2020-06-18 17:25:12,326 cfg.model.tied_embeddings          : True
2020-06-18 17:25:12,326 cfg.model.tied_softmax             : True
2020-06-18 17:25:12,326 cfg.model.encoder.type             : transformer
2020-06-18 17:25:12,326 cfg.model.encoder.num_layers       : 6
2020-06-18 17:25:12,326 cfg.model.encoder.num_heads        : 4
2020-06-18 17:25:12,326 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-18 17:25:12,326 cfg.model.encoder.embeddings.scale : True
2020-06-18 17:25:12,326 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-18 17:25:12,326 cfg.model.encoder.hidden_size      : 256
2020-06-18 17:25:12,326 cfg.model.encoder.ff_size          : 1024
2020-06-18 17:25:12,326 cfg.model.encoder.dropout          : 0.3
2020-06-18 17:25:12,326 cfg.model.encoder.multi_encoder    : True
2020-06-18 17:25:12,326 cfg.model.decoder.type             : transformer
2020-06-18 17:25:12,326 cfg.model.decoder.num_layers       : 6
2020-06-18 17:25:12,326 cfg.model.decoder.num_heads        : 4
2020-06-18 17:25:12,326 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-18 17:25:12,327 cfg.model.decoder.embeddings.scale : True
2020-06-18 17:25:12,327 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-18 17:25:12,327 cfg.model.decoder.hidden_size      : 256
2020-06-18 17:25:12,327 cfg.model.decoder.ff_size          : 1024
2020-06-18 17:25:12,327 cfg.model.decoder.dropout          : 0.3
2020-06-18 17:25:12,327 Data set sizes: 
	train 9688,
	valid 1510,
	test 1153
2020-06-18 17:25:12,327 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-18 17:25:12,327 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) in (9) und
2020-06-18 17:25:12,327 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) in (9) und
2020-06-18 17:25:12,327 Number of Src words (types): 31716
2020-06-18 17:25:12,327 Number of Trg words (types): 31716
2020-06-18 17:25:12,327 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=256, vocab_size=31716),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=31716))
2020-06-18 17:25:12,362 EPOCH 1
2020-06-18 17:25:24,645 Epoch   1: total training loss 158.32
2020-06-18 17:25:24,646 EPOCH 2
2020-06-18 17:25:34,826 Epoch   2 Step:    32100 Batch Loss:     2.283125 Tokens per Sec:    11762, Lr: 0.000300
2020-06-18 17:25:36,327 Epoch   2: total training loss 99.39
2020-06-18 17:25:36,327 EPOCH 3
2020-06-18 17:25:48,109 Epoch   3: total training loss 82.55
2020-06-18 17:25:48,110 EPOCH 4
2020-06-18 17:25:57,296 Epoch   4 Step:    32200 Batch Loss:     0.547989 Tokens per Sec:    11147, Lr: 0.000300
2020-06-18 17:26:00,002 Epoch   4: total training loss 72.33
2020-06-18 17:26:00,003 EPOCH 5
2020-06-18 17:26:11,958 Epoch   5: total training loss 64.76
2020-06-18 17:26:11,959 EPOCH 6
2020-06-18 17:26:19,619 Epoch   6 Step:    32300 Batch Loss:     1.391460 Tokens per Sec:    11579, Lr: 0.000300
2020-06-18 17:26:23,896 Epoch   6: total training loss 59.34
2020-06-18 17:26:23,896 EPOCH 7
2020-06-18 17:26:35,916 Epoch   7: total training loss 56.72
2020-06-18 17:26:35,916 EPOCH 8
2020-06-18 17:26:41,746 Epoch   8 Step:    32400 Batch Loss:     1.130745 Tokens per Sec:    11917, Lr: 0.000300
2020-06-18 17:26:47,676 Epoch   8: total training loss 51.27
2020-06-18 17:26:47,677 EPOCH 9
2020-06-18 17:26:59,474 Epoch   9: total training loss 46.86
2020-06-18 17:26:59,475 EPOCH 10
2020-06-18 17:27:04,107 Epoch  10 Step:    32500 Batch Loss:     0.728840 Tokens per Sec:    12691, Lr: 0.000300
2020-06-18 17:27:11,439 Epoch  10: total training loss 45.01
2020-06-18 17:27:11,440 EPOCH 11
2020-06-18 17:27:23,404 Epoch  11: total training loss 43.05
2020-06-18 17:27:23,404 EPOCH 12
2020-06-18 17:27:26,995 Epoch  12 Step:    32600 Batch Loss:     0.627293 Tokens per Sec:    11823, Lr: 0.000300
2020-06-18 17:27:35,369 Epoch  12: total training loss 40.75
2020-06-18 17:27:35,369 EPOCH 13
2020-06-18 17:27:47,488 Epoch  13: total training loss 38.86
2020-06-18 17:27:47,488 EPOCH 14
2020-06-18 17:27:50,077 Epoch  14 Step:    32700 Batch Loss:     0.573057 Tokens per Sec:    10068, Lr: 0.000300
2020-06-18 17:27:59,495 Epoch  14: total training loss 37.25
2020-06-18 17:27:59,496 EPOCH 15
2020-06-18 17:28:11,436 Epoch  15: total training loss 35.84
2020-06-18 17:28:11,436 EPOCH 16
2020-06-18 17:28:12,394 Epoch  16 Step:    32800 Batch Loss:     0.542342 Tokens per Sec:    10955, Lr: 0.000300
2020-06-18 17:28:23,480 Epoch  16: total training loss 34.29
2020-06-18 17:28:23,480 EPOCH 17
2020-06-18 17:28:35,155 Epoch  17 Step:    32900 Batch Loss:     0.454406 Tokens per Sec:    11258, Lr: 0.000300
2020-06-18 17:28:35,571 Epoch  17: total training loss 33.11
2020-06-18 17:28:35,571 EPOCH 18
2020-06-18 17:28:47,889 Epoch  18: total training loss 33.17
2020-06-18 17:28:47,890 EPOCH 19
2020-06-18 17:28:58,022 Epoch  19 Step:    33000 Batch Loss:     0.459557 Tokens per Sec:    11341, Lr: 0.000300
2020-06-18 17:29:11,594 Hooray! New best validation result [eval_metric]!
2020-06-18 17:29:11,594 Saving new checkpoint.
2020-06-18 17:29:14,900 Example #0
2020-06-18 17:29:14,901 	Raw source:     ['hallo', ',']
2020-06-18 17:29:14,901 	Raw hypothesis: ['hi', '.']
2020-06-18 17:29:14,901 	Source:     hallo ,
2020-06-18 17:29:14,901 	Reference:  hello .
2020-06-18 17:29:14,901 	Hypothesis: hi .
2020-06-18 17:29:14,901 Example #1
2020-06-18 17:29:14,901 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 17:29:14,901 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 17:29:14,901 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 17:29:14,901 	Reference:  hi , how can i help you ?
2020-06-18 17:29:14,901 	Hypothesis: hi , how can i help you ?
2020-06-18 17:29:14,901 Example #2
2020-06-18 17:29:14,901 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 17:29:14,901 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 17:29:14,901 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 17:29:14,901 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 17:29:14,901 	Hypothesis: hi , i &apos;m looking for a restaurant at the arden fair mall in san francisco , california .
2020-06-18 17:29:14,901 Validation result (greedy) at epoch  19, step    33000: bleu:  55.85, loss: 20777.8633, ppl:   2.6434, duration: 16.8786s
2020-06-18 17:29:17,076 Epoch  19: total training loss 32.20
2020-06-18 17:29:17,076 EPOCH 20
2020-06-18 17:29:29,226 Epoch  20: total training loss 30.06
2020-06-18 17:29:29,226 EPOCH 21
2020-06-18 17:29:37,987 Epoch  21 Step:    33100 Batch Loss:     0.506860 Tokens per Sec:    11230, Lr: 0.000300
2020-06-18 17:29:41,415 Epoch  21: total training loss 29.03
2020-06-18 17:29:41,415 EPOCH 22
2020-06-18 17:29:53,795 Epoch  22: total training loss 27.82
2020-06-18 17:29:53,796 EPOCH 23
2020-06-18 17:30:01,474 Epoch  23 Step:    33200 Batch Loss:     0.587087 Tokens per Sec:    10787, Lr: 0.000300
2020-06-18 17:30:06,403 Epoch  23: total training loss 26.96
2020-06-18 17:30:06,404 EPOCH 24
2020-06-18 17:30:18,821 Epoch  24: total training loss 26.31
2020-06-18 17:30:18,821 EPOCH 25
2020-06-18 17:30:25,493 Epoch  25 Step:    33300 Batch Loss:     0.523357 Tokens per Sec:    10160, Lr: 0.000300
2020-06-18 17:30:31,276 Epoch  25: total training loss 25.22
2020-06-18 17:30:31,277 EPOCH 26
2020-06-18 17:30:43,626 Epoch  26: total training loss 24.64
2020-06-18 17:30:43,627 EPOCH 27
2020-06-18 17:30:47,981 Epoch  27 Step:    33400 Batch Loss:     0.496437 Tokens per Sec:    12084, Lr: 0.000300
2020-06-18 17:30:56,062 Epoch  27: total training loss 23.93
2020-06-18 17:30:56,063 EPOCH 28
2020-06-18 17:31:08,720 Epoch  28: total training loss 23.17
2020-06-18 17:31:08,720 EPOCH 29
2020-06-18 17:31:12,027 Epoch  29 Step:    33500 Batch Loss:     0.404645 Tokens per Sec:    11011, Lr: 0.000300
2020-06-18 17:31:21,301 Epoch  29: total training loss 22.64
2020-06-18 17:31:21,301 EPOCH 30
2020-06-18 17:31:33,806 Epoch  30: total training loss 22.78
2020-06-18 17:31:33,807 EPOCH 31
2020-06-18 17:31:35,575 Epoch  31 Step:    33600 Batch Loss:     0.442019 Tokens per Sec:    10915, Lr: 0.000300
2020-06-18 17:31:46,346 Epoch  31: total training loss 22.00
2020-06-18 17:31:46,346 EPOCH 32
2020-06-18 17:31:58,943 Epoch  32: total training loss 21.20
2020-06-18 17:31:58,943 EPOCH 33
2020-06-18 17:31:59,430 Epoch  33 Step:    33700 Batch Loss:     0.259032 Tokens per Sec:     8758, Lr: 0.000300
2020-06-18 17:32:11,954 Epoch  33: total training loss 20.57
2020-06-18 17:32:11,955 EPOCH 34
2020-06-18 17:32:23,948 Epoch  34 Step:    33800 Batch Loss:     0.317839 Tokens per Sec:    10607, Lr: 0.000300
2020-06-18 17:32:24,987 Epoch  34: total training loss 20.02
2020-06-18 17:32:24,987 EPOCH 35
2020-06-18 17:32:37,294 Epoch  35: total training loss 19.02
2020-06-18 17:32:37,294 EPOCH 36
2020-06-18 17:32:47,570 Epoch  36 Step:    33900 Batch Loss:     0.367076 Tokens per Sec:    10811, Lr: 0.000300
2020-06-18 17:32:49,913 Epoch  36: total training loss 19.43
2020-06-18 17:32:49,914 EPOCH 37
2020-06-18 17:33:02,419 Epoch  37: total training loss 18.55
2020-06-18 17:33:02,419 EPOCH 38
2020-06-18 17:33:11,099 Epoch  38 Step:    34000 Batch Loss:     0.367646 Tokens per Sec:    10985, Lr: 0.000300
2020-06-18 17:33:24,487 Hooray! New best validation result [eval_metric]!
2020-06-18 17:33:24,488 Saving new checkpoint.
2020-06-18 17:33:28,028 Example #0
2020-06-18 17:33:28,028 	Raw source:     ['hallo', ',']
2020-06-18 17:33:28,028 	Raw hypothesis: ['hi', 'there', '?']
2020-06-18 17:33:28,029 	Source:     hallo ,
2020-06-18 17:33:28,029 	Reference:  hello .
2020-06-18 17:33:28,029 	Hypothesis: hi there ?
2020-06-18 17:33:28,029 Example #1
2020-06-18 17:33:28,029 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 17:33:28,029 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 17:33:28,029 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 17:33:28,029 	Reference:  hi , how can i help you ?
2020-06-18 17:33:28,029 	Hypothesis: hi , how can i help you ?
2020-06-18 17:33:28,029 Example #2
2020-06-18 17:33:28,029 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 17:33:28,029 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 17:33:28,030 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 17:33:28,030 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 17:33:28,030 	Hypothesis: hi , i was looking for a restaurant at the arden fair mall in san francisco , california .
2020-06-18 17:33:28,030 Validation result (greedy) at epoch  38, step    34000: bleu:  56.97, loss: 21417.6328, ppl:   2.7237, duration: 16.9308s
2020-06-18 17:33:31,775 Epoch  38: total training loss 18.17
2020-06-18 17:33:31,776 EPOCH 39
2020-06-18 17:33:43,966 Epoch  39: total training loss 17.85
2020-06-18 17:33:43,967 EPOCH 40
2020-06-18 17:33:51,025 Epoch  40 Step:    34100 Batch Loss:     0.302113 Tokens per Sec:    11313, Lr: 0.000300
2020-06-18 17:33:56,205 Epoch  40: total training loss 17.32
2020-06-18 17:33:56,205 EPOCH 41
2020-06-18 17:34:08,641 Epoch  41: total training loss 17.31
2020-06-18 17:34:08,641 EPOCH 42
2020-06-18 17:34:13,977 Epoch  42 Step:    34200 Batch Loss:     0.302982 Tokens per Sec:    12013, Lr: 0.000300
2020-06-18 17:34:20,957 Epoch  42: total training loss 16.86
2020-06-18 17:34:20,958 EPOCH 43
2020-06-18 17:34:33,501 Epoch  43: total training loss 16.30
2020-06-18 17:34:33,502 EPOCH 44
2020-06-18 17:34:37,578 Epoch  44 Step:    34300 Batch Loss:     0.298984 Tokens per Sec:    11805, Lr: 0.000300
2020-06-18 17:34:46,013 Epoch  44: total training loss 16.15
2020-06-18 17:34:46,013 EPOCH 45
2020-06-18 17:34:58,479 Epoch  45: total training loss 16.09
2020-06-18 17:34:58,480 EPOCH 46
2020-06-18 17:35:00,923 Epoch  46 Step:    34400 Batch Loss:     0.276109 Tokens per Sec:    11665, Lr: 0.000300
2020-06-18 17:35:10,980 Epoch  46: total training loss 15.38
2020-06-18 17:35:10,981 EPOCH 47
2020-06-18 17:35:23,363 Epoch  47: total training loss 15.12
2020-06-18 17:35:23,363 EPOCH 48
2020-06-18 17:35:24,502 Epoch  48 Step:    34500 Batch Loss:     0.271889 Tokens per Sec:    11379, Lr: 0.000300
2020-06-18 17:35:35,885 Epoch  48: total training loss 15.04
2020-06-18 17:35:35,885 EPOCH 49
2020-06-18 17:35:47,736 Epoch  49 Step:    34600 Batch Loss:     0.321387 Tokens per Sec:    11164, Lr: 0.000300
2020-06-18 17:35:48,332 Epoch  49: total training loss 14.69
2020-06-18 17:35:48,333 EPOCH 50
2020-06-18 17:36:00,880 Epoch  50: total training loss 14.48
2020-06-18 17:36:00,880 EPOCH 51
2020-06-18 17:36:11,182 Epoch  51 Step:    34700 Batch Loss:     0.278647 Tokens per Sec:    11198, Lr: 0.000300
2020-06-18 17:36:13,325 Epoch  51: total training loss 14.15
2020-06-18 17:36:13,325 EPOCH 52
2020-06-18 17:36:25,837 Epoch  52: total training loss 13.99
2020-06-18 17:36:25,838 EPOCH 53
2020-06-18 17:36:34,722 Epoch  53 Step:    34800 Batch Loss:     0.232151 Tokens per Sec:    10692, Lr: 0.000300
2020-06-18 17:36:38,367 Epoch  53: total training loss 13.92
2020-06-18 17:36:38,367 EPOCH 54
2020-06-18 17:36:50,694 Epoch  54: total training loss 13.37
2020-06-18 17:36:50,695 EPOCH 55
2020-06-18 17:36:57,443 Epoch  55 Step:    34900 Batch Loss:     0.246677 Tokens per Sec:    11647, Lr: 0.000300
2020-06-18 17:37:03,155 Epoch  55: total training loss 13.34
2020-06-18 17:37:03,155 EPOCH 56
2020-06-18 17:37:15,742 Epoch  56: total training loss 12.99
2020-06-18 17:37:15,743 EPOCH 57
2020-06-18 17:37:21,115 Epoch  57 Step:    35000 Batch Loss:     0.240719 Tokens per Sec:    11682, Lr: 0.000300
2020-06-18 17:37:33,398 Hooray! New best validation result [eval_metric]!
2020-06-18 17:37:33,398 Saving new checkpoint.
2020-06-18 17:37:36,706 Example #0
2020-06-18 17:37:36,706 	Raw source:     ['hallo', ',']
2020-06-18 17:37:36,706 	Raw hypothesis: ['hi', 'there', '?']
2020-06-18 17:37:36,706 	Source:     hallo ,
2020-06-18 17:37:36,706 	Reference:  hello .
2020-06-18 17:37:36,706 	Hypothesis: hi there ?
2020-06-18 17:37:36,706 Example #1
2020-06-18 17:37:36,707 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 17:37:36,707 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 17:37:36,707 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 17:37:36,707 	Reference:  hi , how can i help you ?
2020-06-18 17:37:36,707 	Hypothesis: hi , how can i help you ?
2020-06-18 17:37:36,707 Example #2
2020-06-18 17:37:36,707 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 17:37:36,707 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 17:37:36,707 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 17:37:36,707 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 17:37:36,707 	Hypothesis: hi , i was looking for a restaurant at the arden fair mall in san francisco , california .
2020-06-18 17:37:36,707 Validation result (greedy) at epoch  57, step    35000: bleu:  57.71, loss: 22673.0000, ppl:   2.8885, duration: 15.5924s
2020-06-18 17:37:43,753 Epoch  57: total training loss 12.74
2020-06-18 17:37:43,754 EPOCH 58
2020-06-18 17:37:56,057 Epoch  58: total training loss 12.46
2020-06-18 17:37:56,057 EPOCH 59
2020-06-18 17:38:00,095 Epoch  59 Step:    35100 Batch Loss:     0.226386 Tokens per Sec:    11615, Lr: 0.000300
2020-06-18 17:38:08,456 Epoch  59: total training loss 12.28
2020-06-18 17:38:08,457 EPOCH 60
2020-06-18 17:38:20,962 Epoch  60: total training loss 12.22
2020-06-18 17:38:20,963 EPOCH 61
2020-06-18 17:38:23,560 Epoch  61 Step:    35200 Batch Loss:     0.226908 Tokens per Sec:    11258, Lr: 0.000300
2020-06-18 17:38:33,391 Epoch  61: total training loss 11.91
2020-06-18 17:38:33,392 EPOCH 62
2020-06-18 17:38:45,901 Epoch  62: total training loss 11.68
2020-06-18 17:38:45,901 EPOCH 63
2020-06-18 17:38:47,208 Epoch  63 Step:    35300 Batch Loss:     0.215516 Tokens per Sec:    10385, Lr: 0.000300
2020-06-18 17:38:58,351 Epoch  63: total training loss 11.59
2020-06-18 17:38:58,352 EPOCH 64
2020-06-18 17:39:10,426 Epoch  64 Step:    35400 Batch Loss:     0.206152 Tokens per Sec:    10995, Lr: 0.000300
2020-06-18 17:39:10,985 Epoch  64: total training loss 11.55
2020-06-18 17:39:10,985 EPOCH 65
2020-06-18 17:39:23,372 Epoch  65: total training loss 11.50
2020-06-18 17:39:23,373 EPOCH 66
2020-06-18 17:39:33,944 Epoch  66 Step:    35500 Batch Loss:     0.221654 Tokens per Sec:    11112, Lr: 0.000300
2020-06-18 17:39:35,767 Epoch  66: total training loss 11.22
2020-06-18 17:39:35,767 EPOCH 67
2020-06-18 17:39:48,384 Epoch  67: total training loss 11.03
2020-06-18 17:39:48,385 EPOCH 68
2020-06-18 17:39:57,317 Epoch  68 Step:    35600 Batch Loss:     0.223513 Tokens per Sec:    11166, Lr: 0.000300
2020-06-18 17:40:00,815 Epoch  68: total training loss 10.84
2020-06-18 17:40:00,815 EPOCH 69
2020-06-18 17:40:13,344 Epoch  69: total training loss 10.83
2020-06-18 17:40:13,345 EPOCH 70
2020-06-18 17:40:20,652 Epoch  70 Step:    35700 Batch Loss:     0.209559 Tokens per Sec:    11044, Lr: 0.000300
2020-06-18 17:40:25,757 Epoch  70: total training loss 10.53
2020-06-18 17:40:25,757 EPOCH 71
2020-06-18 17:40:38,288 Epoch  71: total training loss 10.35
2020-06-18 17:40:38,289 EPOCH 72
2020-06-18 17:40:44,087 Epoch  72 Step:    35800 Batch Loss:     0.195731 Tokens per Sec:    10924, Lr: 0.000300
2020-06-18 17:40:50,844 Epoch  72: total training loss 10.25
2020-06-18 17:40:50,845 EPOCH 73
2020-06-18 17:41:03,300 Epoch  73: total training loss 10.02
2020-06-18 17:41:03,301 EPOCH 74
2020-06-18 17:41:07,858 Epoch  74 Step:    35900 Batch Loss:     0.167494 Tokens per Sec:    10308, Lr: 0.000300
2020-06-18 17:41:15,694 Epoch  74: total training loss 9.96
2020-06-18 17:41:15,695 EPOCH 75
2020-06-18 17:41:28,103 Epoch  75: total training loss 9.67
2020-06-18 17:41:28,104 EPOCH 76
2020-06-18 17:41:31,242 Epoch  76 Step:    36000 Batch Loss:     0.169969 Tokens per Sec:    11426, Lr: 0.000300
2020-06-18 17:41:43,820 Example #0
2020-06-18 17:41:43,820 	Raw source:     ['hallo', ',']
2020-06-18 17:41:43,820 	Raw hypothesis: ['hello', '?']
2020-06-18 17:41:43,820 	Source:     hallo ,
2020-06-18 17:41:43,820 	Reference:  hello .
2020-06-18 17:41:43,820 	Hypothesis: hello ?
2020-06-18 17:41:43,821 Example #1
2020-06-18 17:41:43,821 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 17:41:43,821 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 17:41:43,821 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 17:41:43,821 	Reference:  hi , how can i help you ?
2020-06-18 17:41:43,821 	Hypothesis: hi , how can i help you ?
2020-06-18 17:41:43,821 Example #2
2020-06-18 17:41:43,821 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 17:41:43,821 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'san', 'francisco', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 17:41:43,821 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 17:41:43,821 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 17:41:43,821 	Hypothesis: hi , i was looking for a restaurant in san francisco mall in san francisco , california .
2020-06-18 17:41:43,821 Validation result (greedy) at epoch  76, step    36000: bleu:  57.03, loss: 23688.4844, ppl:   3.0290, duration: 12.5780s
2020-06-18 17:41:53,157 Epoch  76: total training loss 9.77
2020-06-18 17:41:53,157 EPOCH 77
2020-06-18 17:42:05,600 Epoch  77: total training loss 9.81
2020-06-18 17:42:05,601 EPOCH 78
2020-06-18 17:42:07,083 Epoch  78 Step:    36100 Batch Loss:     0.184776 Tokens per Sec:    12811, Lr: 0.000300
2020-06-18 17:42:17,954 Epoch  78: total training loss 9.58
2020-06-18 17:42:17,954 EPOCH 79
2020-06-18 17:42:30,514 Epoch  79 Step:    36200 Batch Loss:     0.169940 Tokens per Sec:    10909, Lr: 0.000300
2020-06-18 17:42:30,515 Epoch  79: total training loss 9.56
2020-06-18 17:42:30,515 EPOCH 80
2020-06-18 17:42:42,922 Epoch  80: total training loss 9.33
2020-06-18 17:42:42,923 EPOCH 81
2020-06-18 17:42:54,006 Epoch  81 Step:    36300 Batch Loss:     0.177649 Tokens per Sec:    10922, Lr: 0.000300
2020-06-18 17:42:55,437 Epoch  81: total training loss 9.41
2020-06-18 17:42:55,437 EPOCH 82
2020-06-18 17:43:07,826 Epoch  82: total training loss 9.39
2020-06-18 17:43:07,827 EPOCH 83
2020-06-18 17:43:17,041 Epoch  83 Step:    36400 Batch Loss:     0.186860 Tokens per Sec:    11034, Lr: 0.000300
2020-06-18 17:43:20,349 Epoch  83: total training loss 9.32
2020-06-18 17:43:20,349 EPOCH 84
2020-06-18 17:43:33,169 Epoch  84: total training loss 8.92
2020-06-18 17:43:33,170 EPOCH 85
2020-06-18 17:43:41,137 Epoch  85 Step:    36500 Batch Loss:     0.153546 Tokens per Sec:    11013, Lr: 0.000300
2020-06-18 17:43:45,560 Epoch  85: total training loss 8.80
2020-06-18 17:43:45,561 EPOCH 86
2020-06-18 17:43:57,885 Epoch  86: total training loss 8.92
2020-06-18 17:43:57,886 EPOCH 87
2020-06-18 17:44:04,279 Epoch  87 Step:    36600 Batch Loss:     0.168824 Tokens per Sec:    11288, Lr: 0.000300
2020-06-18 17:44:10,449 Epoch  87: total training loss 8.52
2020-06-18 17:44:10,450 EPOCH 88
2020-06-18 17:44:22,836 Epoch  88: total training loss 8.47
2020-06-18 17:44:22,837 EPOCH 89
2020-06-18 17:44:27,715 Epoch  89 Step:    36700 Batch Loss:     0.148587 Tokens per Sec:    10979, Lr: 0.000300
2020-06-18 17:44:35,364 Epoch  89: total training loss 8.66
2020-06-18 17:44:35,364 EPOCH 90
2020-06-18 17:44:47,679 Epoch  90: total training loss 8.39
2020-06-18 17:44:47,680 EPOCH 91
2020-06-18 17:44:51,322 Epoch  91 Step:    36800 Batch Loss:     0.163901 Tokens per Sec:     9791, Lr: 0.000300
2020-06-18 17:45:00,041 Epoch  91: total training loss 8.35
2020-06-18 17:45:00,041 EPOCH 92
2020-06-18 17:45:12,562 Epoch  92: total training loss 8.25
2020-06-18 17:45:12,562 EPOCH 93
2020-06-18 17:45:14,270 Epoch  93 Step:    36900 Batch Loss:     0.140476 Tokens per Sec:    12945, Lr: 0.000300
2020-06-18 17:45:24,948 Epoch  93: total training loss 8.25
2020-06-18 17:45:24,948 EPOCH 94
2020-06-18 17:45:37,490 Epoch  94: total training loss 8.17
2020-06-18 17:45:37,490 EPOCH 95
2020-06-18 17:45:37,988 Epoch  95 Step:    37000 Batch Loss:     0.136770 Tokens per Sec:    10394, Lr: 0.000300
2020-06-18 17:45:50,416 Example #0
2020-06-18 17:45:50,416 	Raw source:     ['hallo', ',']
2020-06-18 17:45:50,416 	Raw hypothesis: ['hello', '?']
2020-06-18 17:45:50,417 	Source:     hallo ,
2020-06-18 17:45:50,417 	Reference:  hello .
2020-06-18 17:45:50,417 	Hypothesis: hello ?
2020-06-18 17:45:50,417 Example #1
2020-06-18 17:45:50,417 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 17:45:50,417 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 17:45:50,417 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 17:45:50,417 	Reference:  hi , how can i help you ?
2020-06-18 17:45:50,417 	Hypothesis: hi , how can i help you ?
2020-06-18 17:45:50,417 Example #2
2020-06-18 17:45:50,417 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 17:45:50,417 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'san', 'francisco', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 17:45:50,417 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 17:45:50,417 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 17:45:50,417 	Hypothesis: hi , i was looking for a restaurant in san francisco mall in san francisco , california .
2020-06-18 17:45:50,417 Validation result (greedy) at epoch  95, step    37000: bleu:  57.47, loss: 24357.8496, ppl:   3.1254, duration: 12.4286s
2020-06-18 17:46:02,496 Epoch  95: total training loss 8.12
2020-06-18 17:46:02,497 EPOCH 96
2020-06-18 17:46:13,833 Epoch  96 Step:    37100 Batch Loss:     0.151732 Tokens per Sec:    10941, Lr: 0.000300
2020-06-18 17:46:14,923 Epoch  96: total training loss 8.01
2020-06-18 17:46:14,923 EPOCH 97
2020-06-18 17:46:27,408 Epoch  97: total training loss 7.95
2020-06-18 17:46:27,408 EPOCH 98
2020-06-18 17:46:37,417 Epoch  98 Step:    37200 Batch Loss:     0.141411 Tokens per Sec:    10836, Lr: 0.000300
2020-06-18 17:46:39,824 Epoch  98: total training loss 7.96
2020-06-18 17:46:39,825 EPOCH 99
2020-06-18 17:46:52,315 Epoch  99: total training loss 7.79
2020-06-18 17:46:52,316 EPOCH 100
2020-06-18 17:47:00,810 Epoch 100 Step:    37300 Batch Loss:     0.154585 Tokens per Sec:    10958, Lr: 0.000300
2020-06-18 17:47:04,715 Epoch 100: total training loss 7.65
2020-06-18 17:47:04,716 Training ended after 100 epochs.
2020-06-18 17:47:04,716 Best validation result (greedy) at step    35000:  57.71 eval_metric.
2020-06-18 17:47:39,296  dev bleu:  58.26 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-18 17:47:39,348 Translations saved to: models/transformer_iwslt14_deen_bpe-multi_enc-tune/00035000.hyps.dev
2020-06-18 17:47:47,936 test bleu:  53.40 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-18 17:47:47,940 Translations saved to: models/transformer_iwslt14_deen_bpe-multi_enc-tune/00035000.hyps.test
