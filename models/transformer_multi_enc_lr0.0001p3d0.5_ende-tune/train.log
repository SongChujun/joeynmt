2020-07-01 15:15:14,574 Hello! This is Joey-NMT.
2020-07-01 15:15:19,537 Total params: 82862081
2020-07-01 15:15:19,539 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-07-01 15:15:21,875 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-01 15:15:22,171 Reset optimizer.
2020-07-01 15:15:22,172 Reset scheduler.
2020-07-01 15:15:22,172 Reset tracking of the best checkpoint.
2020-07-01 15:15:22,177 cfg.name                           : transformer
2020-07-01 15:15:22,177 cfg.data.src                       : en
2020-07-01 15:15:22,177 cfg.data.trg                       : de
2020-07-01 15:15:22,177 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-01 15:15:22,177 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-01 15:15:22,177 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-01 15:15:22,177 cfg.data.level                     : bpe
2020-07-01 15:15:22,177 cfg.data.lowercase                 : False
2020-07-01 15:15:22,177 cfg.data.max_sent_length           : 100
2020-07-01 15:15:22,177 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-01 15:15:22,177 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-01 15:15:22,177 cfg.testing.beam_size              : 5
2020-07-01 15:15:22,177 cfg.testing.alpha                  : 1.0
2020-07-01 15:15:22,177 cfg.training.random_seed           : 42
2020-07-01 15:15:22,177 cfg.training.optimizer             : adam
2020-07-01 15:15:22,177 cfg.training.normalization         : tokens
2020-07-01 15:15:22,177 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-01 15:15:22,177 cfg.training.scheduling            : plateau
2020-07-01 15:15:22,177 cfg.training.patience              : 3
2020-07-01 15:15:22,177 cfg.training.decrease_factor       : 0.5
2020-07-01 15:15:22,177 cfg.training.loss                  : crossentropy
2020-07-01 15:15:22,177 cfg.training.learning_rate         : 0.0001
2020-07-01 15:15:22,177 cfg.training.learning_rate_min     : 1e-08
2020-07-01 15:15:22,177 cfg.training.weight_decay          : 0.0
2020-07-01 15:15:22,177 cfg.training.label_smoothing       : 0.1
2020-07-01 15:15:22,177 cfg.training.batch_size            : 2048
2020-07-01 15:15:22,177 cfg.training.batch_type            : token
2020-07-01 15:15:22,178 cfg.training.eval_batch_size       : 3600
2020-07-01 15:15:22,178 cfg.training.eval_batch_type       : token
2020-07-01 15:15:22,178 cfg.training.batch_multiplier      : 1
2020-07-01 15:15:22,178 cfg.training.early_stopping_metric : ppl
2020-07-01 15:15:22,178 cfg.training.epochs                : 100
2020-07-01 15:15:22,178 cfg.training.validation_freq       : 1000
2020-07-01 15:15:22,178 cfg.training.logging_freq          : 100
2020-07-01 15:15:22,178 cfg.training.eval_metric           : bleu
2020-07-01 15:15:22,178 cfg.training.model_dir             : models/transformer_multi_enc_lr0.0001p3d0.5_ende-tune
2020-07-01 15:15:22,178 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-01 15:15:22,178 cfg.training.reset_best_ckpt       : True
2020-07-01 15:15:22,178 cfg.training.reset_scheduler       : True
2020-07-01 15:15:22,178 cfg.training.reset_optimizer       : True
2020-07-01 15:15:22,178 cfg.training.overwrite             : False
2020-07-01 15:15:22,178 cfg.training.shuffle               : True
2020-07-01 15:15:22,178 cfg.training.use_cuda              : True
2020-07-01 15:15:22,178 cfg.training.max_output_length     : 100
2020-07-01 15:15:22,178 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-01 15:15:22,178 cfg.training.keep_last_ckpts       : 3
2020-07-01 15:15:22,178 cfg.model.initializer              : xavier
2020-07-01 15:15:22,178 cfg.model.bias_initializer         : zeros
2020-07-01 15:15:22,178 cfg.model.init_gain                : 1.0
2020-07-01 15:15:22,178 cfg.model.embed_initializer        : xavier
2020-07-01 15:15:22,178 cfg.model.embed_init_gain          : 1.0
2020-07-01 15:15:22,178 cfg.model.tied_embeddings          : True
2020-07-01 15:15:22,178 cfg.model.tied_softmax             : True
2020-07-01 15:15:22,178 cfg.model.encoder.type             : transformer
2020-07-01 15:15:22,178 cfg.model.encoder.num_layers       : 6
2020-07-01 15:15:22,178 cfg.model.encoder.num_heads        : 8
2020-07-01 15:15:22,178 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-01 15:15:22,178 cfg.model.encoder.embeddings.scale : True
2020-07-01 15:15:22,178 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-01 15:15:22,178 cfg.model.encoder.hidden_size      : 512
2020-07-01 15:15:22,178 cfg.model.encoder.ff_size          : 2048
2020-07-01 15:15:22,178 cfg.model.encoder.dropout          : 0.1
2020-07-01 15:15:22,178 cfg.model.encoder.multi_encoder    : True
2020-07-01 15:15:22,178 cfg.model.decoder.type             : transformer
2020-07-01 15:15:22,178 cfg.model.decoder.num_layers       : 6
2020-07-01 15:15:22,178 cfg.model.decoder.num_heads        : 8
2020-07-01 15:15:22,178 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-01 15:15:22,179 cfg.model.decoder.embeddings.scale : True
2020-07-01 15:15:22,179 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-01 15:15:22,179 cfg.model.decoder.hidden_size      : 512
2020-07-01 15:15:22,179 cfg.model.decoder.ff_size          : 2048
2020-07-01 15:15:22,179 cfg.model.decoder.dropout          : 0.1
2020-07-01 15:15:22,179 Data set sizes: 
	train 9771,
	valid 1522,
	test 1160
2020-07-01 15:15:22,179 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-01 15:15:22,179 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-01 15:15:22,179 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-01 15:15:22,179 Number of Src words (types): 36628
2020-07-01 15:15:22,179 Number of Trg words (types): 36628
2020-07-01 15:15:22,179 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-01 15:15:22,206 EPOCH 1
2020-07-01 15:15:45,291 Epoch   1 Step:  1360100 Batch Loss:     1.990676 Tokens per Sec:     5322, Lr: 0.000100
2020-07-01 15:15:51,399 Epoch   1: total training loss 496.76
2020-07-01 15:15:51,399 EPOCH 2
2020-07-01 15:16:08,636 Epoch   2 Step:  1360200 Batch Loss:     2.489625 Tokens per Sec:     5195, Lr: 0.000100
2020-07-01 15:16:20,669 Epoch   2: total training loss 197.33
2020-07-01 15:16:20,669 EPOCH 3
2020-07-01 15:16:31,578 Epoch   3 Step:  1360300 Batch Loss:     0.839243 Tokens per Sec:     5315, Lr: 0.000100
2020-07-01 15:16:50,170 Epoch   3: total training loss 154.43
2020-07-01 15:16:50,170 EPOCH 4
2020-07-01 15:16:54,999 Epoch   4 Step:  1360400 Batch Loss:     1.297393 Tokens per Sec:     5256, Lr: 0.000100
2020-07-01 15:17:18,351 Epoch   4 Step:  1360500 Batch Loss:     0.805546 Tokens per Sec:     5110, Lr: 0.000100
2020-07-01 15:17:19,914 Epoch   4: total training loss 135.14
2020-07-01 15:17:19,914 EPOCH 5
2020-07-01 15:17:41,654 Epoch   5 Step:  1360600 Batch Loss:     0.734634 Tokens per Sec:     5133, Lr: 0.000100
2020-07-01 15:17:50,450 Epoch   5: total training loss 122.89
2020-07-01 15:17:50,450 EPOCH 6
2020-07-01 15:18:06,162 Epoch   6 Step:  1360700 Batch Loss:     0.814509 Tokens per Sec:     4851, Lr: 0.000100
2020-07-01 15:18:21,296 Epoch   6: total training loss 109.44
2020-07-01 15:18:21,297 EPOCH 7
2020-07-01 15:18:30,213 Epoch   7 Step:  1360800 Batch Loss:     0.735048 Tokens per Sec:     5191, Lr: 0.000100
2020-07-01 15:18:52,005 Epoch   7: total training loss 103.77
2020-07-01 15:18:52,005 EPOCH 8
2020-07-01 15:18:54,283 Epoch   8 Step:  1360900 Batch Loss:     0.845975 Tokens per Sec:     5345, Lr: 0.000100
2020-07-01 15:19:17,889 Epoch   8 Step:  1361000 Batch Loss:     0.673081 Tokens per Sec:     5086, Lr: 0.000100
2020-07-01 15:19:57,731 Hooray! New best validation result [ppl]!
2020-07-01 15:19:57,732 Saving new checkpoint.
2020-07-01 15:20:08,615 Example #0
2020-07-01 15:20:08,615 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:20:08,615 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:20:08,615 	Source:     Hello .
2020-07-01 15:20:08,615 	Reference:  Hallo ,
2020-07-01 15:20:08,615 	Hypothesis: Hallo .
2020-07-01 15:20:08,616 Example #1
2020-07-01 15:20:08,616 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:20:08,616 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:20:08,616 	Source:     Hi , how can I help you ?
2020-07-01 15:20:08,616 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:20:08,616 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:20:08,616 Example #2
2020-07-01 15:20:08,616 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:20:08,616 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:20:08,616 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:20:08,616 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:20:08,616 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:20:08,616 Example #3
2020-07-01 15:20:08,616 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:20:08,616 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:20:08,616 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:20:08,616 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:20:08,616 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:20:08,616 Validation result (greedy) at epoch   8, step  1361000: bleu:  52.98, loss: 19086.7754, ppl:   2.2103, duration: 50.7261s
2020-07-01 15:20:12,664 Epoch   8: total training loss 98.14
2020-07-01 15:20:12,665 EPOCH 9
2020-07-01 15:20:32,019 Epoch   9 Step:  1361100 Batch Loss:     0.930602 Tokens per Sec:     5101, Lr: 0.000100
2020-07-01 15:20:42,957 Epoch   9: total training loss 93.58
2020-07-01 15:20:42,958 EPOCH 10
2020-07-01 15:20:55,582 Epoch  10 Step:  1361200 Batch Loss:     0.889599 Tokens per Sec:     5041, Lr: 0.000100
2020-07-01 15:21:13,176 Epoch  10: total training loss 86.57
2020-07-01 15:21:13,176 EPOCH 11
2020-07-01 15:21:18,801 Epoch  11 Step:  1361300 Batch Loss:     0.241673 Tokens per Sec:     5150, Lr: 0.000100
2020-07-01 15:21:42,869 Epoch  11 Step:  1361400 Batch Loss:     0.546940 Tokens per Sec:     5067, Lr: 0.000100
2020-07-01 15:21:43,401 Epoch  11: total training loss 81.69
2020-07-01 15:21:43,402 EPOCH 12
2020-07-01 15:22:06,966 Epoch  12 Step:  1361500 Batch Loss:     0.814178 Tokens per Sec:     5002, Lr: 0.000100
2020-07-01 15:22:13,815 Epoch  12: total training loss 77.95
2020-07-01 15:22:13,815 EPOCH 13
2020-07-01 15:22:30,675 Epoch  13 Step:  1361600 Batch Loss:     0.519237 Tokens per Sec:     5085, Lr: 0.000100
2020-07-01 15:22:44,014 Epoch  13: total training loss 73.77
2020-07-01 15:22:44,015 EPOCH 14
2020-07-01 15:22:54,614 Epoch  14 Step:  1361700 Batch Loss:     0.517196 Tokens per Sec:     5161, Lr: 0.000100
2020-07-01 15:23:14,078 Epoch  14: total training loss 69.65
2020-07-01 15:23:14,079 EPOCH 15
2020-07-01 15:23:18,387 Epoch  15 Step:  1361800 Batch Loss:     0.651509 Tokens per Sec:     4897, Lr: 0.000100
2020-07-01 15:23:42,105 Epoch  15 Step:  1361900 Batch Loss:     0.345402 Tokens per Sec:     5069, Lr: 0.000100
2020-07-01 15:23:44,268 Epoch  15: total training loss 67.52
2020-07-01 15:23:44,268 EPOCH 16
2020-07-01 15:24:05,803 Epoch  16 Step:  1362000 Batch Loss:     0.398135 Tokens per Sec:     5066, Lr: 0.000100
2020-07-01 15:24:48,714 Hooray! New best validation result [ppl]!
2020-07-01 15:24:48,714 Saving new checkpoint.
2020-07-01 15:24:59,425 Example #0
2020-07-01 15:24:59,425 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:24:59,425 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:24:59,425 	Source:     Hello .
2020-07-01 15:24:59,425 	Reference:  Hallo ,
2020-07-01 15:24:59,425 	Hypothesis: Hallo .
2020-07-01 15:24:59,425 Example #1
2020-07-01 15:24:59,425 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:24:59,426 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:24:59,426 	Source:     Hi , how can I help you ?
2020-07-01 15:24:59,426 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:24:59,426 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:24:59,426 Example #2
2020-07-01 15:24:59,426 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:24:59,426 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:24:59,426 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:24:59,426 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:24:59,426 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:24:59,426 Example #3
2020-07-01 15:24:59,426 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:24:59,426 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:24:59,426 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:24:59,426 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:24:59,426 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:24:59,427 Validation result (greedy) at epoch  16, step  1362000: bleu:  54.57, loss: 17393.0020, ppl:   2.0601, duration: 53.6221s
2020-07-01 15:25:07,998 Epoch  16: total training loss 64.12
2020-07-01 15:25:07,998 EPOCH 17
2020-07-01 15:25:22,661 Epoch  17 Step:  1362100 Batch Loss:     0.376765 Tokens per Sec:     5216, Lr: 0.000100
2020-07-01 15:25:38,204 Epoch  17: total training loss 62.52
2020-07-01 15:25:38,204 EPOCH 18
2020-07-01 15:25:46,298 Epoch  18 Step:  1362200 Batch Loss:     0.463846 Tokens per Sec:     5241, Lr: 0.000100
2020-07-01 15:26:08,377 Epoch  18: total training loss 59.08
2020-07-01 15:26:08,378 EPOCH 19
2020-07-01 15:26:10,010 Epoch  19 Step:  1362300 Batch Loss:     0.479199 Tokens per Sec:     5252, Lr: 0.000100
2020-07-01 15:26:33,476 Epoch  19 Step:  1362400 Batch Loss:     0.556331 Tokens per Sec:     5071, Lr: 0.000100
2020-07-01 15:26:38,608 Epoch  19: total training loss 57.44
2020-07-01 15:26:38,608 EPOCH 20
2020-07-01 15:26:57,154 Epoch  20 Step:  1362500 Batch Loss:     0.443323 Tokens per Sec:     5042, Lr: 0.000100
2020-07-01 15:27:08,817 Epoch  20: total training loss 54.30
2020-07-01 15:27:08,817 EPOCH 21
2020-07-01 15:27:21,205 Epoch  21 Step:  1362600 Batch Loss:     0.358531 Tokens per Sec:     5030, Lr: 0.000100
2020-07-01 15:27:39,060 Epoch  21: total training loss 51.55
2020-07-01 15:27:39,061 EPOCH 22
2020-07-01 15:27:44,864 Epoch  22 Step:  1362700 Batch Loss:     0.178093 Tokens per Sec:     5287, Lr: 0.000100
2020-07-01 15:28:08,700 Epoch  22 Step:  1362800 Batch Loss:     0.341509 Tokens per Sec:     5050, Lr: 0.000100
2020-07-01 15:28:09,168 Epoch  22: total training loss 49.63
2020-07-01 15:28:09,168 EPOCH 23
2020-07-01 15:28:32,221 Epoch  23 Step:  1362900 Batch Loss:     0.468134 Tokens per Sec:     5163, Lr: 0.000100
2020-07-01 15:28:38,924 Epoch  23: total training loss 48.05
2020-07-01 15:28:38,925 EPOCH 24
2020-07-01 15:28:55,542 Epoch  24 Step:  1363000 Batch Loss:     0.383275 Tokens per Sec:     5005, Lr: 0.000100
2020-07-01 15:29:45,048 Example #0
2020-07-01 15:29:45,048 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:29:45,049 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:29:45,049 	Source:     Hello .
2020-07-01 15:29:45,049 	Reference:  Hallo ,
2020-07-01 15:29:45,049 	Hypothesis: Hallo .
2020-07-01 15:29:45,049 Example #1
2020-07-01 15:29:45,049 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:29:45,049 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:29:45,049 	Source:     Hi , how can I help you ?
2020-07-01 15:29:45,049 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:29:45,049 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:29:45,049 Example #2
2020-07-01 15:29:45,049 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:29:45,049 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:29:45,049 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:29:45,049 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:29:45,049 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:29:45,049 Example #3
2020-07-01 15:29:45,049 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:29:45,049 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:29:45,049 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:29:45,049 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:29:45,049 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:29:45,049 Validation result (greedy) at epoch  24, step  1363000: bleu:  54.27, loss: 17594.4727, ppl:   2.0774, duration: 49.5064s
2020-07-01 15:29:58,728 Epoch  24: total training loss 45.73
2020-07-01 15:29:58,729 EPOCH 25
2020-07-01 15:30:08,899 Epoch  25 Step:  1363100 Batch Loss:     0.334037 Tokens per Sec:     5087, Lr: 0.000100
2020-07-01 15:30:28,945 Epoch  25: total training loss 44.28
2020-07-01 15:30:28,946 EPOCH 26
2020-07-01 15:30:32,571 Epoch  26 Step:  1363200 Batch Loss:     0.376297 Tokens per Sec:     5059, Lr: 0.000100
2020-07-01 15:30:56,117 Epoch  26 Step:  1363300 Batch Loss:     0.307852 Tokens per Sec:     5169, Lr: 0.000100
2020-07-01 15:30:59,094 Epoch  26: total training loss 42.20
2020-07-01 15:30:59,095 EPOCH 27
2020-07-01 15:31:19,530 Epoch  27 Step:  1363400 Batch Loss:     0.400511 Tokens per Sec:     5232, Lr: 0.000100
2020-07-01 15:31:28,987 Epoch  27: total training loss 39.92
2020-07-01 15:31:28,987 EPOCH 28
2020-07-01 15:31:43,580 Epoch  28 Step:  1363500 Batch Loss:     0.315409 Tokens per Sec:     5120, Lr: 0.000100
2020-07-01 15:31:58,534 Epoch  28: total training loss 38.43
2020-07-01 15:31:58,534 EPOCH 29
2020-07-01 15:32:07,093 Epoch  29 Step:  1363600 Batch Loss:     0.317025 Tokens per Sec:     4918, Lr: 0.000100
2020-07-01 15:32:28,496 Epoch  29: total training loss 36.72
2020-07-01 15:32:28,497 EPOCH 30
2020-07-01 15:32:30,357 Epoch  30 Step:  1363700 Batch Loss:     0.288836 Tokens per Sec:     4333, Lr: 0.000100
2020-07-01 15:32:54,318 Epoch  30 Step:  1363800 Batch Loss:     0.261822 Tokens per Sec:     5134, Lr: 0.000100
2020-07-01 15:32:58,776 Epoch  30: total training loss 35.22
2020-07-01 15:32:58,777 EPOCH 31
2020-07-01 15:33:17,776 Epoch  31 Step:  1363900 Batch Loss:     0.297588 Tokens per Sec:     5236, Lr: 0.000100
2020-07-01 15:33:29,035 Epoch  31: total training loss 33.67
2020-07-01 15:33:29,036 EPOCH 32
2020-07-01 15:33:42,017 Epoch  32 Step:  1364000 Batch Loss:     0.298998 Tokens per Sec:     5037, Lr: 0.000100
2020-07-01 15:34:39,649 Example #0
2020-07-01 15:34:39,649 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:34:39,649 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:34:39,649 	Source:     Hello .
2020-07-01 15:34:39,649 	Reference:  Hallo ,
2020-07-01 15:34:39,649 	Hypothesis: Hallo .
2020-07-01 15:34:39,649 Example #1
2020-07-01 15:34:39,649 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:34:39,649 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:34:39,649 	Source:     Hi , how can I help you ?
2020-07-01 15:34:39,649 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:34:39,649 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:34:39,650 Example #2
2020-07-01 15:34:39,650 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:34:39,650 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:34:39,650 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:34:39,650 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:34:39,650 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:34:39,650 Example #3
2020-07-01 15:34:39,650 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:34:39,650 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:34:39,650 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:34:39,650 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:34:39,650 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:34:39,650 Validation result (greedy) at epoch  32, step  1364000: bleu:  54.09, loss: 18559.4902, ppl:   2.1624, duration: 57.6321s
2020-07-01 15:34:56,922 Epoch  32: total training loss 32.14
2020-07-01 15:34:56,922 EPOCH 33
2020-07-01 15:35:03,227 Epoch  33 Step:  1364100 Batch Loss:     0.229974 Tokens per Sec:     5294, Lr: 0.000100
2020-07-01 15:35:26,807 Epoch  33 Step:  1364200 Batch Loss:     0.202273 Tokens per Sec:     5039, Lr: 0.000100
2020-07-01 15:35:27,050 Epoch  33: total training loss 31.44
2020-07-01 15:35:27,051 EPOCH 34
2020-07-01 15:35:50,634 Epoch  34 Step:  1364300 Batch Loss:     0.203903 Tokens per Sec:     5089, Lr: 0.000100
2020-07-01 15:35:56,651 Epoch  34: total training loss 30.32
2020-07-01 15:35:56,651 EPOCH 35
2020-07-01 15:36:13,292 Epoch  35 Step:  1364400 Batch Loss:     0.205849 Tokens per Sec:     5327, Lr: 0.000100
2020-07-01 15:36:25,573 Epoch  35: total training loss 28.23
2020-07-01 15:36:25,574 EPOCH 36
2020-07-01 15:36:36,140 Epoch  36 Step:  1364500 Batch Loss:     0.201027 Tokens per Sec:     5453, Lr: 0.000100
2020-07-01 15:36:54,594 Epoch  36: total training loss 27.21
2020-07-01 15:36:54,595 EPOCH 37
2020-07-01 15:36:58,585 Epoch  37 Step:  1364600 Batch Loss:     0.184201 Tokens per Sec:     5824, Lr: 0.000100
2020-07-01 15:37:21,454 Epoch  37 Step:  1364700 Batch Loss:     0.238758 Tokens per Sec:     5279, Lr: 0.000100
2020-07-01 15:37:23,714 Epoch  37: total training loss 26.12
2020-07-01 15:37:23,714 EPOCH 38
2020-07-01 15:37:44,344 Epoch  38 Step:  1364800 Batch Loss:     0.190433 Tokens per Sec:     5349, Lr: 0.000100
2020-07-01 15:37:52,812 Epoch  38: total training loss 24.63
2020-07-01 15:37:52,813 EPOCH 39
2020-07-01 15:38:07,732 Epoch  39 Step:  1364900 Batch Loss:     0.245545 Tokens per Sec:     5193, Lr: 0.000100
2020-07-01 15:38:22,142 Epoch  39: total training loss 23.95
2020-07-01 15:38:22,143 EPOCH 40
2020-07-01 15:38:30,144 Epoch  40 Step:  1365000 Batch Loss:     0.145025 Tokens per Sec:     5393, Lr: 0.000100
2020-07-01 15:39:22,340 Example #0
2020-07-01 15:39:22,340 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:39:22,340 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:39:22,340 	Source:     Hello .
2020-07-01 15:39:22,340 	Reference:  Hallo ,
2020-07-01 15:39:22,340 	Hypothesis: Hallo .
2020-07-01 15:39:22,340 Example #1
2020-07-01 15:39:22,340 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:39:22,341 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:39:22,341 	Source:     Hi , how can I help you ?
2020-07-01 15:39:22,341 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:39:22,341 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:39:22,341 Example #2
2020-07-01 15:39:22,341 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:39:22,341 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:39:22,341 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:39:22,341 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:39:22,341 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:39:22,341 Example #3
2020-07-01 15:39:22,341 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:39:22,341 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:39:22,341 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:39:22,341 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:39:22,341 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:39:22,341 Validation result (greedy) at epoch  40, step  1365000: bleu:  53.15, loss: 19190.4863, ppl:   2.2199, duration: 52.1959s
2020-07-01 15:39:43,998 Epoch  40: total training loss 22.89
2020-07-01 15:39:43,998 EPOCH 41
2020-07-01 15:39:46,067 Epoch  41 Step:  1365100 Batch Loss:     0.134650 Tokens per Sec:     5155, Lr: 0.000100
2020-07-01 15:40:09,728 Epoch  41 Step:  1365200 Batch Loss:     0.185708 Tokens per Sec:     5099, Lr: 0.000100
2020-07-01 15:40:14,116 Epoch  41: total training loss 21.88
2020-07-01 15:40:14,117 EPOCH 42
2020-07-01 15:40:33,181 Epoch  42 Step:  1365300 Batch Loss:     0.147804 Tokens per Sec:     5020, Lr: 0.000100
2020-07-01 15:40:44,108 Epoch  42: total training loss 21.54
2020-07-01 15:40:44,108 EPOCH 43
2020-07-01 15:40:56,103 Epoch  43 Step:  1365400 Batch Loss:     0.217387 Tokens per Sec:     5315, Lr: 0.000100
2020-07-01 15:41:14,091 Epoch  43: total training loss 20.14
2020-07-01 15:41:14,092 EPOCH 44
2020-07-01 15:41:20,069 Epoch  44 Step:  1365500 Batch Loss:     0.140572 Tokens per Sec:     4969, Lr: 0.000100
2020-07-01 15:41:43,881 Epoch  44 Step:  1365600 Batch Loss:     0.181969 Tokens per Sec:     5052, Lr: 0.000100
2020-07-01 15:41:44,610 Epoch  44: total training loss 20.06
2020-07-01 15:41:44,610 EPOCH 45
2020-07-01 15:42:07,681 Epoch  45 Step:  1365700 Batch Loss:     0.163071 Tokens per Sec:     5054, Lr: 0.000100
2020-07-01 15:42:14,890 Epoch  45: total training loss 18.96
2020-07-01 15:42:14,890 EPOCH 46
2020-07-01 15:42:32,126 Epoch  46 Step:  1365800 Batch Loss:     0.136612 Tokens per Sec:     5010, Lr: 0.000100
2020-07-01 15:42:45,511 Epoch  46: total training loss 17.99
2020-07-01 15:42:45,512 EPOCH 47
2020-07-01 15:42:55,980 Epoch  47 Step:  1365900 Batch Loss:     0.114016 Tokens per Sec:     4971, Lr: 0.000100
2020-07-01 15:43:15,676 Epoch  47: total training loss 17.58
2020-07-01 15:43:15,677 EPOCH 48
2020-07-01 15:43:19,631 Epoch  48 Step:  1366000 Batch Loss:     0.109408 Tokens per Sec:     4560, Lr: 0.000100
2020-07-01 15:44:08,833 Example #0
2020-07-01 15:44:08,833 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:44:08,833 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:44:08,833 	Source:     Hello .
2020-07-01 15:44:08,833 	Reference:  Hallo ,
2020-07-01 15:44:08,833 	Hypothesis: Hallo .
2020-07-01 15:44:08,833 Example #1
2020-07-01 15:44:08,833 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:44:08,833 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:44:08,833 	Source:     Hi , how can I help you ?
2020-07-01 15:44:08,834 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:44:08,834 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:44:08,834 Example #2
2020-07-01 15:44:08,834 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:44:08,834 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:44:08,834 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:44:08,834 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:44:08,834 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:44:08,834 Example #3
2020-07-01 15:44:08,834 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:44:08,834 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:44:08,834 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:44:08,834 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:44:08,834 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:44:08,834 Validation result (greedy) at epoch  48, step  1366000: bleu:  53.44, loss: 19908.3594, ppl:   2.2871, duration: 49.2031s
2020-07-01 15:44:32,778 Epoch  48 Step:  1366100 Batch Loss:     0.140979 Tokens per Sec:     5092, Lr: 0.000050
2020-07-01 15:44:35,279 Epoch  48: total training loss 16.43
2020-07-01 15:44:35,279 EPOCH 49
2020-07-01 15:44:57,093 Epoch  49 Step:  1366200 Batch Loss:     0.117661 Tokens per Sec:     4844, Lr: 0.000050
2020-07-01 15:45:05,930 Epoch  49: total training loss 15.49
2020-07-01 15:45:05,930 EPOCH 50
2020-07-01 15:45:21,091 Epoch  50 Step:  1366300 Batch Loss:     0.128946 Tokens per Sec:     4854, Lr: 0.000050
2020-07-01 15:45:36,699 Epoch  50: total training loss 15.00
2020-07-01 15:45:36,700 EPOCH 51
2020-07-01 15:45:44,773 Epoch  51 Step:  1366400 Batch Loss:     0.149114 Tokens per Sec:     4843, Lr: 0.000050
2020-07-01 15:46:07,097 Epoch  51: total training loss 14.89
2020-07-01 15:46:07,097 EPOCH 52
2020-07-01 15:46:08,156 Epoch  52 Step:  1366500 Batch Loss:     0.123090 Tokens per Sec:     5703, Lr: 0.000050
2020-07-01 15:46:32,239 Epoch  52 Step:  1366600 Batch Loss:     0.135617 Tokens per Sec:     5029, Lr: 0.000050
2020-07-01 15:46:37,525 Epoch  52: total training loss 14.54
2020-07-01 15:46:37,526 EPOCH 53
2020-07-01 15:46:56,234 Epoch  53 Step:  1366700 Batch Loss:     0.098883 Tokens per Sec:     5109, Lr: 0.000050
2020-07-01 15:47:07,923 Epoch  53: total training loss 14.16
2020-07-01 15:47:07,924 EPOCH 54
2020-07-01 15:47:20,281 Epoch  54 Step:  1366800 Batch Loss:     0.140492 Tokens per Sec:     4959, Lr: 0.000050
2020-07-01 15:47:38,528 Epoch  54: total training loss 13.71
2020-07-01 15:47:38,529 EPOCH 55
2020-07-01 15:47:44,296 Epoch  55 Step:  1366900 Batch Loss:     0.144098 Tokens per Sec:     5316, Lr: 0.000050
2020-07-01 15:48:08,597 Epoch  55 Step:  1367000 Batch Loss:     0.140815 Tokens per Sec:     4951, Lr: 0.000050
2020-07-01 15:48:56,575 Example #0
2020-07-01 15:48:56,576 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:48:56,576 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:48:56,576 	Source:     Hello .
2020-07-01 15:48:56,576 	Reference:  Hallo ,
2020-07-01 15:48:56,576 	Hypothesis: Hallo .
2020-07-01 15:48:56,576 Example #1
2020-07-01 15:48:56,576 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:48:56,576 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:48:56,576 	Source:     Hi , how can I help you ?
2020-07-01 15:48:56,576 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:48:56,576 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:48:56,577 Example #2
2020-07-01 15:48:56,577 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:48:56,577 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:48:56,577 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:48:56,577 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:48:56,577 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:48:56,577 Example #3
2020-07-01 15:48:56,577 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:48:56,577 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:48:56,577 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:48:56,577 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:48:56,577 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:48:56,577 Validation result (greedy) at epoch  55, step  1367000: bleu:  53.12, loss: 20331.1074, ppl:   2.3276, duration: 47.9789s
2020-07-01 15:48:56,981 Epoch  55: total training loss 13.72
2020-07-01 15:48:56,981 EPOCH 56
2020-07-01 15:49:20,776 Epoch  56 Step:  1367100 Batch Loss:     0.111203 Tokens per Sec:     5072, Lr: 0.000050
2020-07-01 15:49:27,494 Epoch  56: total training loss 13.32
2020-07-01 15:49:27,495 EPOCH 57
2020-07-01 15:49:44,341 Epoch  57 Step:  1367200 Batch Loss:     0.046009 Tokens per Sec:     5214, Lr: 0.000050
2020-07-01 15:49:57,543 Epoch  57: total training loss 13.18
2020-07-01 15:49:57,543 EPOCH 58
2020-07-01 15:50:08,145 Epoch  58 Step:  1367300 Batch Loss:     0.119198 Tokens per Sec:     5088, Lr: 0.000050
2020-07-01 15:50:28,065 Epoch  58: total training loss 12.99
2020-07-01 15:50:28,066 EPOCH 59
2020-07-01 15:50:32,383 Epoch  59 Step:  1367400 Batch Loss:     0.130730 Tokens per Sec:     5350, Lr: 0.000050
2020-07-01 15:50:56,859 Epoch  59 Step:  1367500 Batch Loss:     0.105613 Tokens per Sec:     4866, Lr: 0.000050
2020-07-01 15:50:58,685 Epoch  59: total training loss 12.94
2020-07-01 15:50:58,686 EPOCH 60
2020-07-01 15:51:20,235 Epoch  60 Step:  1367600 Batch Loss:     0.097035 Tokens per Sec:     5117, Lr: 0.000050
2020-07-01 15:51:28,569 Epoch  60: total training loss 12.70
2020-07-01 15:51:28,570 EPOCH 61
2020-07-01 15:51:44,271 Epoch  61 Step:  1367700 Batch Loss:     0.134018 Tokens per Sec:     5044, Lr: 0.000050
2020-07-01 15:51:59,095 Epoch  61: total training loss 12.27
2020-07-01 15:51:59,095 EPOCH 62
2020-07-01 15:52:08,189 Epoch  62 Step:  1367800 Batch Loss:     0.097223 Tokens per Sec:     5030, Lr: 0.000050
2020-07-01 15:52:29,465 Epoch  62: total training loss 12.31
2020-07-01 15:52:29,466 EPOCH 63
2020-07-01 15:52:31,801 Epoch  63 Step:  1367900 Batch Loss:     0.078299 Tokens per Sec:     5659, Lr: 0.000050
2020-07-01 15:52:55,878 Epoch  63 Step:  1368000 Batch Loss:     0.104051 Tokens per Sec:     5018, Lr: 0.000050
2020-07-01 15:53:42,899 Example #0
2020-07-01 15:53:42,900 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:53:42,900 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:53:42,900 	Source:     Hello .
2020-07-01 15:53:42,900 	Reference:  Hallo ,
2020-07-01 15:53:42,900 	Hypothesis: Hallo .
2020-07-01 15:53:42,900 Example #1
2020-07-01 15:53:42,900 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:53:42,900 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:53:42,900 	Source:     Hi , how can I help you ?
2020-07-01 15:53:42,900 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:53:42,900 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:53:42,900 Example #2
2020-07-01 15:53:42,900 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:53:42,901 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:53:42,901 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:53:42,901 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:53:42,901 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:53:42,901 Example #3
2020-07-01 15:53:42,901 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:53:42,901 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:53:42,901 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:53:42,901 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:53:42,901 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:53:42,901 Validation result (greedy) at epoch  63, step  1368000: bleu:  53.76, loss: 20555.1953, ppl:   2.3494, duration: 47.0222s
2020-07-01 15:53:46,909 Epoch  63: total training loss 12.07
2020-07-01 15:53:46,909 EPOCH 64
2020-07-01 15:54:06,782 Epoch  64 Step:  1368100 Batch Loss:     0.079528 Tokens per Sec:     5038, Lr: 0.000050
2020-07-01 15:54:17,380 Epoch  64: total training loss 11.81
2020-07-01 15:54:17,381 EPOCH 65
2020-07-01 15:54:30,954 Epoch  65 Step:  1368200 Batch Loss:     0.084723 Tokens per Sec:     4944, Lr: 0.000050
2020-07-01 15:54:47,170 Epoch  65: total training loss 11.67
2020-07-01 15:54:47,171 EPOCH 66
2020-07-01 15:54:53,858 Epoch  66 Step:  1368300 Batch Loss:     0.113829 Tokens per Sec:     5256, Lr: 0.000050
2020-07-01 15:55:17,286 Epoch  66: total training loss 11.75
2020-07-01 15:55:17,287 EPOCH 67
2020-07-01 15:55:17,516 Epoch  67 Step:  1368400 Batch Loss:     0.094314 Tokens per Sec:     6148, Lr: 0.000050
2020-07-01 15:55:41,673 Epoch  67 Step:  1368500 Batch Loss:     0.097990 Tokens per Sec:     4957, Lr: 0.000050
2020-07-01 15:55:47,635 Epoch  67: total training loss 11.46
2020-07-01 15:55:47,636 EPOCH 68
2020-07-01 15:56:05,387 Epoch  68 Step:  1368600 Batch Loss:     0.092170 Tokens per Sec:     5060, Lr: 0.000050
2020-07-01 15:56:17,976 Epoch  68: total training loss 11.12
2020-07-01 15:56:17,976 EPOCH 69
2020-07-01 15:56:29,328 Epoch  69 Step:  1368700 Batch Loss:     0.058109 Tokens per Sec:     4894, Lr: 0.000050
2020-07-01 15:56:48,475 Epoch  69: total training loss 11.09
2020-07-01 15:56:48,475 EPOCH 70
2020-07-01 15:56:53,158 Epoch  70 Step:  1368800 Batch Loss:     0.082937 Tokens per Sec:     4772, Lr: 0.000050
2020-07-01 15:57:17,098 Epoch  70 Step:  1368900 Batch Loss:     0.069802 Tokens per Sec:     5108, Lr: 0.000050
2020-07-01 15:57:18,747 Epoch  70: total training loss 10.89
2020-07-01 15:57:18,748 EPOCH 71
2020-07-01 15:57:40,672 Epoch  71 Step:  1369000 Batch Loss:     0.080706 Tokens per Sec:     5051, Lr: 0.000050
2020-07-01 15:58:28,994 Example #0
2020-07-01 15:58:28,995 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:58:28,995 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:58:28,995 	Source:     Hello .
2020-07-01 15:58:28,995 	Reference:  Hallo ,
2020-07-01 15:58:28,995 	Hypothesis: Hallo .
2020-07-01 15:58:28,995 Example #1
2020-07-01 15:58:28,995 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:58:28,995 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:58:28,995 	Source:     Hi , how can I help you ?
2020-07-01 15:58:28,995 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:58:28,995 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:58:28,995 Example #2
2020-07-01 15:58:28,995 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:58:28,995 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:58:28,995 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:58:28,995 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:58:28,995 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:58:28,995 Example #3
2020-07-01 15:58:28,995 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:58:28,995 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:58:28,996 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:58:28,996 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:58:28,996 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:58:28,996 Validation result (greedy) at epoch  71, step  1369000: bleu:  53.24, loss: 20934.9102, ppl:   2.3867, duration: 48.3228s
2020-07-01 15:58:37,262 Epoch  71: total training loss 10.76
2020-07-01 15:58:37,263 EPOCH 72
2020-07-01 15:58:52,620 Epoch  72 Step:  1369100 Batch Loss:     0.087341 Tokens per Sec:     5087, Lr: 0.000050
2020-07-01 15:59:07,601 Epoch  72: total training loss 10.83
2020-07-01 15:59:07,602 EPOCH 73
2020-07-01 15:59:16,141 Epoch  73 Step:  1369200 Batch Loss:     0.049954 Tokens per Sec:     5049, Lr: 0.000050
2020-07-01 15:59:38,048 Epoch  73: total training loss 10.63
2020-07-01 15:59:38,049 EPOCH 74
2020-07-01 15:59:40,063 Epoch  74 Step:  1369300 Batch Loss:     0.083778 Tokens per Sec:     5020, Lr: 0.000050
2020-07-01 16:00:04,308 Epoch  74 Step:  1369400 Batch Loss:     0.062780 Tokens per Sec:     5075, Lr: 0.000050
2020-07-01 16:00:08,228 Epoch  74: total training loss 10.16
2020-07-01 16:00:08,228 EPOCH 75
2020-07-01 16:00:28,632 Epoch  75 Step:  1369500 Batch Loss:     0.088023 Tokens per Sec:     4942, Lr: 0.000050
2020-07-01 16:00:38,691 Epoch  75: total training loss 10.07
2020-07-01 16:00:38,692 EPOCH 76
2020-07-01 16:00:52,429 Epoch  76 Step:  1369600 Batch Loss:     0.075657 Tokens per Sec:     4985, Lr: 0.000050
2020-07-01 16:01:09,399 Epoch  76: total training loss 10.24
2020-07-01 16:01:09,399 EPOCH 77
2020-07-01 16:01:16,878 Epoch  77 Step:  1369700 Batch Loss:     0.088098 Tokens per Sec:     4836, Lr: 0.000050
2020-07-01 16:01:39,829 Epoch  77: total training loss 10.00
2020-07-01 16:01:39,829 EPOCH 78
2020-07-01 16:01:40,307 Epoch  78 Step:  1369800 Batch Loss:     0.068900 Tokens per Sec:     5316, Lr: 0.000050
2020-07-01 16:02:03,636 Epoch  78 Step:  1369900 Batch Loss:     0.082958 Tokens per Sec:     5104, Lr: 0.000050
2020-07-01 16:02:09,828 Epoch  78: total training loss 9.85
2020-07-01 16:02:09,828 EPOCH 79
2020-07-01 16:02:27,605 Epoch  79 Step:  1370000 Batch Loss:     0.077811 Tokens per Sec:     5015, Lr: 0.000050
2020-07-01 16:03:10,993 Example #0
2020-07-01 16:03:10,993 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 16:03:10,993 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 16:03:10,993 	Source:     Hello .
2020-07-01 16:03:10,994 	Reference:  Hallo ,
2020-07-01 16:03:10,994 	Hypothesis: Hallo .
2020-07-01 16:03:10,994 Example #1
2020-07-01 16:03:10,994 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 16:03:10,994 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 16:03:10,994 	Source:     Hi , how can I help you ?
2020-07-01 16:03:10,994 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 16:03:10,994 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 16:03:10,994 Example #2
2020-07-01 16:03:10,994 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 16:03:10,994 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 16:03:10,994 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 16:03:10,994 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 16:03:10,994 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 16:03:10,994 Example #3
2020-07-01 16:03:10,994 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 16:03:10,994 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 16:03:10,994 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 16:03:10,994 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 16:03:10,994 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 16:03:10,994 Validation result (greedy) at epoch  79, step  1370000: bleu:  53.76, loss: 21261.4258, ppl:   2.4194, duration: 43.3878s
2020-07-01 16:03:23,551 Epoch  79: total training loss 9.76
2020-07-01 16:03:23,551 EPOCH 80
2020-07-01 16:03:34,921 Epoch  80 Step:  1370100 Batch Loss:     0.068917 Tokens per Sec:     5076, Lr: 0.000025
2020-07-01 16:03:54,046 Epoch  80: total training loss 9.27
2020-07-01 16:03:54,047 EPOCH 81
2020-07-01 16:03:58,775 Epoch  81 Step:  1370200 Batch Loss:     0.085517 Tokens per Sec:     5207, Lr: 0.000025
2020-07-01 16:04:22,866 Epoch  81 Step:  1370300 Batch Loss:     0.051665 Tokens per Sec:     4913, Lr: 0.000025
2020-07-01 16:04:24,562 Epoch  81: total training loss 9.24
2020-07-01 16:04:24,562 EPOCH 82
2020-07-01 16:04:46,783 Epoch  82 Step:  1370400 Batch Loss:     0.053375 Tokens per Sec:     4946, Lr: 0.000025
2020-07-01 16:04:54,829 Epoch  82: total training loss 9.05
2020-07-01 16:04:54,829 EPOCH 83
2020-07-01 16:05:10,056 Epoch  83 Step:  1370500 Batch Loss:     0.050819 Tokens per Sec:     5168, Lr: 0.000025
2020-07-01 16:05:25,119 Epoch  83: total training loss 9.02
2020-07-01 16:05:25,120 EPOCH 84
2020-07-01 16:05:33,745 Epoch  84 Step:  1370600 Batch Loss:     0.053948 Tokens per Sec:     5192, Lr: 0.000025
2020-07-01 16:05:54,980 Epoch  84: total training loss 8.83
2020-07-01 16:05:54,981 EPOCH 85
2020-07-01 16:05:57,662 Epoch  85 Step:  1370700 Batch Loss:     0.068805 Tokens per Sec:     5015, Lr: 0.000025
2020-07-01 16:06:21,457 Epoch  85 Step:  1370800 Batch Loss:     0.073669 Tokens per Sec:     5162, Lr: 0.000025
2020-07-01 16:06:25,195 Epoch  85: total training loss 8.77
2020-07-01 16:06:25,195 EPOCH 86
2020-07-01 16:06:45,091 Epoch  86 Step:  1370900 Batch Loss:     0.075815 Tokens per Sec:     5103, Lr: 0.000025
2020-07-01 16:06:55,316 Epoch  86: total training loss 8.84
2020-07-01 16:06:55,317 EPOCH 87
2020-07-01 16:07:08,361 Epoch  87 Step:  1371000 Batch Loss:     0.092582 Tokens per Sec:     5144, Lr: 0.000025
2020-07-01 16:07:54,815 Example #0
2020-07-01 16:07:54,816 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 16:07:54,816 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 16:07:54,816 	Source:     Hello .
2020-07-01 16:07:54,816 	Reference:  Hallo ,
2020-07-01 16:07:54,816 	Hypothesis: Hallo .
2020-07-01 16:07:54,816 Example #1
2020-07-01 16:07:54,816 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 16:07:54,816 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 16:07:54,816 	Source:     Hi , how can I help you ?
2020-07-01 16:07:54,817 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 16:07:54,817 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 16:07:54,817 Example #2
2020-07-01 16:07:54,817 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 16:07:54,817 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 16:07:54,817 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 16:07:54,817 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 16:07:54,817 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 16:07:54,817 Example #3
2020-07-01 16:07:54,817 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 16:07:54,817 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 16:07:54,817 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 16:07:54,817 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 16:07:54,817 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 16:07:54,817 Validation result (greedy) at epoch  87, step  1371000: bleu:  53.54, loss: 21259.1660, ppl:   2.4191, duration: 46.4555s
2020-07-01 16:08:11,881 Epoch  87: total training loss 8.79
2020-07-01 16:08:11,881 EPOCH 88
2020-07-01 16:08:18,916 Epoch  88 Step:  1371100 Batch Loss:     0.056318 Tokens per Sec:     5036, Lr: 0.000025
2020-07-01 16:08:42,332 Epoch  88: total training loss 8.65
2020-07-01 16:08:42,333 EPOCH 89
2020-07-01 16:08:42,719 Epoch  89 Step:  1371200 Batch Loss:     0.068714 Tokens per Sec:     6219, Lr: 0.000025
2020-07-01 16:09:06,528 Epoch  89 Step:  1371300 Batch Loss:     0.083349 Tokens per Sec:     5082, Lr: 0.000025
2020-07-01 16:09:12,445 Epoch  89: total training loss 8.57
2020-07-01 16:09:12,446 EPOCH 90
2020-07-01 16:09:30,096 Epoch  90 Step:  1371400 Batch Loss:     0.058299 Tokens per Sec:     5153, Lr: 0.000025
2020-07-01 16:09:42,624 Epoch  90: total training loss 8.53
2020-07-01 16:09:42,624 EPOCH 91
2020-07-01 16:09:53,752 Epoch  91 Step:  1371500 Batch Loss:     0.078755 Tokens per Sec:     5044, Lr: 0.000025
2020-07-01 16:10:12,684 Epoch  91: total training loss 8.75
2020-07-01 16:10:12,684 EPOCH 92
2020-07-01 16:10:17,068 Epoch  92 Step:  1371600 Batch Loss:     0.051573 Tokens per Sec:     4776, Lr: 0.000025
2020-07-01 16:10:40,793 Epoch  92 Step:  1371700 Batch Loss:     0.092014 Tokens per Sec:     5125, Lr: 0.000025
2020-07-01 16:10:42,920 Epoch  92: total training loss 8.37
2020-07-01 16:10:42,920 EPOCH 93
2020-07-01 16:11:04,146 Epoch  93 Step:  1371800 Batch Loss:     0.067389 Tokens per Sec:     5120, Lr: 0.000025
2020-07-01 16:11:13,323 Epoch  93: total training loss 8.53
2020-07-01 16:11:13,324 EPOCH 94
2020-07-01 16:11:28,067 Epoch  94 Step:  1371900 Batch Loss:     0.067350 Tokens per Sec:     4945, Lr: 0.000025
2020-07-01 16:11:44,099 Epoch  94: total training loss 8.33
2020-07-01 16:11:44,100 EPOCH 95
2020-07-01 16:11:52,502 Epoch  95 Step:  1372000 Batch Loss:     0.063489 Tokens per Sec:     4838, Lr: 0.000025
2020-07-01 16:12:38,626 Example #0
2020-07-01 16:12:38,626 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 16:12:38,626 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 16:12:38,626 	Source:     Hello .
2020-07-01 16:12:38,626 	Reference:  Hallo ,
2020-07-01 16:12:38,626 	Hypothesis: Hallo .
2020-07-01 16:12:38,626 Example #1
2020-07-01 16:12:38,626 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 16:12:38,626 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 16:12:38,626 	Source:     Hi , how can I help you ?
2020-07-01 16:12:38,626 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 16:12:38,626 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 16:12:38,626 Example #2
2020-07-01 16:12:38,626 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 16:12:38,626 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 16:12:38,626 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 16:12:38,626 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 16:12:38,627 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 16:12:38,627 Example #3
2020-07-01 16:12:38,627 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 16:12:38,627 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 16:12:38,627 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 16:12:38,627 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 16:12:38,627 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 16:12:38,627 Validation result (greedy) at epoch  95, step  1372000: bleu:  53.71, loss: 21460.7441, ppl:   2.4395, duration: 46.1243s
2020-07-01 16:13:00,844 Epoch  95: total training loss 8.14
2020-07-01 16:13:00,845 EPOCH 96
2020-07-01 16:13:02,466 Epoch  96 Step:  1372100 Batch Loss:     0.075436 Tokens per Sec:     6031, Lr: 0.000025
2020-07-01 16:13:26,294 Epoch  96 Step:  1372200 Batch Loss:     0.059886 Tokens per Sec:     5054, Lr: 0.000025
2020-07-01 16:13:30,956 Epoch  96: total training loss 8.12
2020-07-01 16:13:30,957 EPOCH 97
2020-07-01 16:13:49,754 Epoch  97 Step:  1372300 Batch Loss:     0.064670 Tokens per Sec:     5181, Lr: 0.000025
2020-07-01 16:14:00,767 Epoch  97: total training loss 8.20
2020-07-01 16:14:00,767 EPOCH 98
2020-07-01 16:14:13,398 Epoch  98 Step:  1372400 Batch Loss:     0.078788 Tokens per Sec:     5374, Lr: 0.000025
2020-07-01 16:14:30,556 Epoch  98: total training loss 8.09
2020-07-01 16:14:30,557 EPOCH 99
2020-07-01 16:14:36,454 Epoch  99 Step:  1372500 Batch Loss:     0.065691 Tokens per Sec:     5211, Lr: 0.000025
2020-07-01 16:15:00,133 Epoch  99 Step:  1372600 Batch Loss:     0.066903 Tokens per Sec:     5135, Lr: 0.000025
2020-07-01 16:15:00,425 Epoch  99: total training loss 8.00
2020-07-01 16:15:00,425 EPOCH 100
2020-07-01 16:15:23,978 Epoch 100 Step:  1372700 Batch Loss:     0.062843 Tokens per Sec:     5113, Lr: 0.000025
2020-07-01 16:15:30,366 Epoch 100: total training loss 7.99
2020-07-01 16:15:30,367 Training ended after 100 epochs.
2020-07-01 16:15:30,367 Best validation result (greedy) at step  1362000:   2.06 ppl.
2020-07-01 16:16:25,951  dev bleu:  55.62 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 16:16:25,956 Translations saved to: models/transformer_multi_enc_lr0.0001p3d0.5_ende-tune/01362000.hyps.dev
2020-07-01 16:16:48,849 test bleu:  51.61 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 16:16:48,855 Translations saved to: models/transformer_multi_enc_lr0.0001p3d0.5_ende-tune/01362000.hyps.test
