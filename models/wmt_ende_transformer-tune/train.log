2020-07-01 11:13:54,071 Hello! This is Joey-NMT.
2020-07-01 11:14:04,006 Total params: 62894080
2020-07-01 11:14:04,008 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-07-01 11:14:06,281 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-01 11:14:06,601 Reset optimizer.
2020-07-01 11:14:06,601 Reset scheduler.
2020-07-01 11:14:06,601 Reset tracking of the best checkpoint.
2020-07-01 11:14:06,606 cfg.name                           : transformer
2020-07-01 11:14:06,607 cfg.data.src                       : en
2020-07-01 11:14:06,607 cfg.data.trg                       : de
2020-07-01 11:14:06,607 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-01 11:14:06,607 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-01 11:14:06,607 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-01 11:14:06,607 cfg.data.level                     : bpe
2020-07-01 11:14:06,607 cfg.data.lowercase                 : False
2020-07-01 11:14:06,607 cfg.data.max_sent_length           : 100
2020-07-01 11:14:06,607 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-01 11:14:06,607 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-01 11:14:06,607 cfg.testing.beam_size              : 5
2020-07-01 11:14:06,607 cfg.testing.alpha                  : 1.0
2020-07-01 11:14:06,607 cfg.training.random_seed           : 42
2020-07-01 11:14:06,607 cfg.training.optimizer             : adam
2020-07-01 11:14:06,607 cfg.training.normalization         : tokens
2020-07-01 11:14:06,607 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-01 11:14:06,607 cfg.training.scheduling            : plateau
2020-07-01 11:14:06,607 cfg.training.patience              : 8
2020-07-01 11:14:06,607 cfg.training.decrease_factor       : 0.7
2020-07-01 11:14:06,607 cfg.training.loss                  : crossentropy
2020-07-01 11:14:06,607 cfg.training.learning_rate         : 0.0002
2020-07-01 11:14:06,607 cfg.training.learning_rate_min     : 1e-08
2020-07-01 11:14:06,607 cfg.training.weight_decay          : 0.0
2020-07-01 11:14:06,607 cfg.training.label_smoothing       : 0.1
2020-07-01 11:14:06,607 cfg.training.batch_size            : 4096
2020-07-01 11:14:06,607 cfg.training.batch_type            : token
2020-07-01 11:14:06,607 cfg.training.batch_multiplier      : 1
2020-07-01 11:14:06,607 cfg.training.early_stopping_metric : ppl
2020-07-01 11:14:06,607 cfg.training.epochs                : 100
2020-07-01 11:14:06,607 cfg.training.validation_freq       : 1000
2020-07-01 11:14:06,607 cfg.training.logging_freq          : 100
2020-07-01 11:14:06,607 cfg.training.eval_metric           : bleu
2020-07-01 11:14:06,607 cfg.training.model_dir             : models/wmt_ende_transformer-tune
2020-07-01 11:14:06,607 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-01 11:14:06,607 cfg.training.reset_best_ckpt       : True
2020-07-01 11:14:06,607 cfg.training.reset_scheduler       : True
2020-07-01 11:14:06,607 cfg.training.reset_optimizer       : True
2020-07-01 11:14:06,607 cfg.training.overwrite             : False
2020-07-01 11:14:06,607 cfg.training.shuffle               : True
2020-07-01 11:14:06,608 cfg.training.use_cuda              : True
2020-07-01 11:14:06,608 cfg.training.max_output_length     : 100
2020-07-01 11:14:06,608 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-01 11:14:06,608 cfg.training.keep_last_ckpts       : 3
2020-07-01 11:14:06,608 cfg.model.initializer              : xavier
2020-07-01 11:14:06,608 cfg.model.bias_initializer         : zeros
2020-07-01 11:14:06,608 cfg.model.init_gain                : 1.0
2020-07-01 11:14:06,608 cfg.model.embed_initializer        : xavier
2020-07-01 11:14:06,608 cfg.model.embed_init_gain          : 1.0
2020-07-01 11:14:06,608 cfg.model.tied_embeddings          : True
2020-07-01 11:14:06,608 cfg.model.tied_softmax             : True
2020-07-01 11:14:06,608 cfg.model.encoder.type             : transformer
2020-07-01 11:14:06,608 cfg.model.encoder.num_layers       : 6
2020-07-01 11:14:06,608 cfg.model.encoder.num_heads        : 8
2020-07-01 11:14:06,608 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-01 11:14:06,608 cfg.model.encoder.embeddings.scale : True
2020-07-01 11:14:06,608 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-01 11:14:06,608 cfg.model.encoder.hidden_size      : 512
2020-07-01 11:14:06,608 cfg.model.encoder.ff_size          : 2048
2020-07-01 11:14:06,608 cfg.model.encoder.dropout          : 0.1
2020-07-01 11:14:06,608 cfg.model.decoder.type             : transformer
2020-07-01 11:14:06,608 cfg.model.decoder.num_layers       : 6
2020-07-01 11:14:06,608 cfg.model.decoder.num_heads        : 8
2020-07-01 11:14:06,608 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-01 11:14:06,608 cfg.model.decoder.embeddings.scale : True
2020-07-01 11:14:06,608 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-01 11:14:06,608 cfg.model.decoder.hidden_size      : 512
2020-07-01 11:14:06,608 cfg.model.decoder.ff_size          : 2048
2020-07-01 11:14:06,608 cfg.model.decoder.dropout          : 0.1
2020-07-01 11:14:06,608 Data set sizes: 
	train 9771,
	valid 1525,
	test 1165
2020-07-01 11:14:06,608 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-01 11:14:06,608 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-01 11:14:06,608 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-01 11:14:06,608 Number of Src words (types): 36628
2020-07-01 11:14:06,608 Number of Trg words (types): 36628
2020-07-01 11:14:06,609 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-01 11:14:06,635 EPOCH 1
2020-07-01 11:14:20,192 Epoch   1: total training loss 67.64
2020-07-01 11:14:20,193 EPOCH 2
2020-07-01 11:14:27,010 Epoch   2 Step:  1360100 Batch Loss:     0.637896 Tokens per Sec:    11720, Lr: 0.000200
2020-07-01 11:14:33,551 Epoch   2: total training loss 50.71
2020-07-01 11:14:33,552 EPOCH 3
2020-07-01 11:14:47,390 Epoch   3 Step:  1360200 Batch Loss:     0.623896 Tokens per Sec:    10758, Lr: 0.000200
2020-07-01 11:14:47,856 Epoch   3: total training loss 44.49
2020-07-01 11:14:47,856 EPOCH 4
2020-07-01 11:15:04,107 Epoch   4: total training loss 39.07
2020-07-01 11:15:04,107 EPOCH 5
2020-07-01 11:15:11,129 Epoch   5 Step:  1360300 Batch Loss:     0.350995 Tokens per Sec:     9478, Lr: 0.000200
2020-07-01 11:15:20,105 Epoch   5: total training loss 35.34
2020-07-01 11:15:20,105 EPOCH 6
2020-07-01 11:15:34,649 Epoch   6 Step:  1360400 Batch Loss:     0.402520 Tokens per Sec:     9534, Lr: 0.000200
2020-07-01 11:15:36,240 Epoch   6: total training loss 31.72
2020-07-01 11:15:36,240 EPOCH 7
2020-07-01 11:15:52,769 Epoch   7: total training loss 28.93
2020-07-01 11:15:52,769 EPOCH 8
2020-07-01 11:15:58,963 Epoch   8 Step:  1360500 Batch Loss:     0.415768 Tokens per Sec:     9584, Lr: 0.000200
2020-07-01 11:16:09,682 Epoch   8: total training loss 26.17
2020-07-01 11:16:09,683 EPOCH 9
2020-07-01 11:16:23,831 Epoch   9 Step:  1360600 Batch Loss:     0.375826 Tokens per Sec:     9402, Lr: 0.000200
2020-07-01 11:16:26,043 Epoch   9: total training loss 24.34
2020-07-01 11:16:26,043 EPOCH 10
2020-07-01 11:16:42,201 Epoch  10: total training loss 22.65
2020-07-01 11:16:42,202 EPOCH 11
2020-07-01 11:16:47,898 Epoch  11 Step:  1360700 Batch Loss:     0.268605 Tokens per Sec:     9789, Lr: 0.000200
2020-07-01 11:16:58,615 Epoch  11: total training loss 21.24
2020-07-01 11:16:58,615 EPOCH 12
2020-07-01 11:17:12,414 Epoch  12 Step:  1360800 Batch Loss:     0.309510 Tokens per Sec:     9412, Lr: 0.000200
2020-07-01 11:17:15,013 Epoch  12: total training loss 20.16
2020-07-01 11:17:15,013 EPOCH 13
2020-07-01 11:17:31,352 Epoch  13: total training loss 18.73
2020-07-01 11:17:31,353 EPOCH 14
2020-07-01 11:17:36,564 Epoch  14 Step:  1360900 Batch Loss:     0.260730 Tokens per Sec:     9383, Lr: 0.000200
2020-07-01 11:17:47,989 Epoch  14: total training loss 17.84
2020-07-01 11:17:47,990 EPOCH 15
2020-07-01 11:18:00,676 Epoch  15 Step:  1361000 Batch Loss:     0.213651 Tokens per Sec:     9728, Lr: 0.000200
2020-07-01 11:19:04,246 Hooray! New best validation result [ppl]!
2020-07-01 11:19:04,246 Saving new checkpoint.
2020-07-01 11:19:12,370 Example #0
2020-07-01 11:19:12,371 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:19:12,371 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:19:12,371 	Source:     Hello .
2020-07-01 11:19:12,371 	Reference:  Hallo ,
2020-07-01 11:19:12,371 	Hypothesis: Hallo .
2020-07-01 11:19:12,371 Example #1
2020-07-01 11:19:12,371 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:19:12,371 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:19:12,371 	Source:     Hi , how can I help you ?
2020-07-01 11:19:12,371 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:19:12,371 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:19:12,371 Example #2
2020-07-01 11:19:12,371 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:19:12,371 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:19:12,371 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:19:12,371 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:19:12,371 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:19:12,371 Example #3
2020-07-01 11:19:12,371 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:19:12,371 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:19:12,371 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:19:12,371 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:19:12,371 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:19:12,371 Validation result (greedy) at epoch  15, step  1361000: bleu:  55.40, loss: 16195.4385, ppl:   1.9424, duration: 71.6942s
2020-07-01 11:19:15,647 Epoch  15: total training loss 17.16
2020-07-01 11:19:15,647 EPOCH 16
2020-07-01 11:19:31,192 Epoch  16: total training loss 15.69
2020-07-01 11:19:31,193 EPOCH 17
2020-07-01 11:19:35,412 Epoch  17 Step:  1361100 Batch Loss:     0.219030 Tokens per Sec:     9316, Lr: 0.000200
2020-07-01 11:19:46,903 Epoch  17: total training loss 15.07
2020-07-01 11:19:46,903 EPOCH 18
2020-07-01 11:19:58,891 Epoch  18 Step:  1361200 Batch Loss:     0.213065 Tokens per Sec:     9860, Lr: 0.000200
2020-07-01 11:20:02,756 Epoch  18: total training loss 14.53
2020-07-01 11:20:02,756 EPOCH 19
2020-07-01 11:20:18,924 Epoch  19: total training loss 13.73
2020-07-01 11:20:18,924 EPOCH 20
2020-07-01 11:20:22,398 Epoch  20 Step:  1361300 Batch Loss:     0.180164 Tokens per Sec:     9651, Lr: 0.000200
2020-07-01 11:20:34,756 Epoch  20: total training loss 13.12
2020-07-01 11:20:34,757 EPOCH 21
2020-07-01 11:20:45,987 Epoch  21 Step:  1361400 Batch Loss:     0.213568 Tokens per Sec:     9730, Lr: 0.000200
2020-07-01 11:20:50,736 Epoch  21: total training loss 12.35
2020-07-01 11:20:50,737 EPOCH 22
2020-07-01 11:21:06,772 Epoch  22: total training loss 11.93
2020-07-01 11:21:06,773 EPOCH 23
2020-07-01 11:21:09,902 Epoch  23 Step:  1361500 Batch Loss:     0.172118 Tokens per Sec:     9374, Lr: 0.000200
2020-07-01 11:21:22,605 Epoch  23: total training loss 11.39
2020-07-01 11:21:22,606 EPOCH 24
2020-07-01 11:21:33,722 Epoch  24 Step:  1361600 Batch Loss:     0.154419 Tokens per Sec:     9414, Lr: 0.000200
2020-07-01 11:21:38,860 Epoch  24: total training loss 11.12
2020-07-01 11:21:38,861 EPOCH 25
2020-07-01 11:21:54,244 Epoch  25: total training loss 10.54
2020-07-01 11:21:54,245 EPOCH 26
2020-07-01 11:21:56,727 Epoch  26 Step:  1361700 Batch Loss:     0.139323 Tokens per Sec:    10007, Lr: 0.000200
2020-07-01 11:22:09,583 Epoch  26: total training loss 10.25
2020-07-01 11:22:09,584 EPOCH 27
2020-07-01 11:22:19,751 Epoch  27 Step:  1361800 Batch Loss:     0.148820 Tokens per Sec:     9652, Lr: 0.000200
2020-07-01 11:22:25,395 Epoch  27: total training loss 10.06
2020-07-01 11:22:25,395 EPOCH 28
2020-07-01 11:22:41,332 Epoch  28: total training loss 9.82
2020-07-01 11:22:41,333 EPOCH 29
2020-07-01 11:22:42,739 Epoch  29 Step:  1361900 Batch Loss:     0.133901 Tokens per Sec:     8598, Lr: 0.000200
2020-07-01 11:22:57,236 Epoch  29: total training loss 9.35
2020-07-01 11:22:57,236 EPOCH 30
2020-07-01 11:23:05,890 Epoch  30 Step:  1362000 Batch Loss:     0.135401 Tokens per Sec:    10142, Lr: 0.000200
2020-07-01 11:24:05,310 Example #0
2020-07-01 11:24:05,311 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:24:05,311 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:24:05,311 	Source:     Hello .
2020-07-01 11:24:05,311 	Reference:  Hallo ,
2020-07-01 11:24:05,311 	Hypothesis: Hallo .
2020-07-01 11:24:05,311 Example #1
2020-07-01 11:24:05,311 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:24:05,311 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:24:05,311 	Source:     Hi , how can I help you ?
2020-07-01 11:24:05,311 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:24:05,311 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:24:05,311 Example #2
2020-07-01 11:24:05,311 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:24:05,311 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:24:05,311 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:24:05,311 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:24:05,311 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:24:05,311 Example #3
2020-07-01 11:24:05,311 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:24:05,311 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:24:05,311 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:24:05,311 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:24:05,311 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:24:05,311 Validation result (greedy) at epoch  30, step  1362000: bleu:  53.80, loss: 18182.3184, ppl:   2.1073, duration: 59.4201s
2020-07-01 11:24:12,235 Epoch  30: total training loss 8.87
2020-07-01 11:24:12,236 EPOCH 31
2020-07-01 11:24:27,964 Epoch  31: total training loss 8.76
2020-07-01 11:24:27,964 EPOCH 32
2020-07-01 11:24:28,592 Epoch  32 Step:  1362100 Batch Loss:     0.099867 Tokens per Sec:    10420, Lr: 0.000200
2020-07-01 11:24:43,049 Epoch  32: total training loss 8.36
2020-07-01 11:24:43,049 EPOCH 33
2020-07-01 11:24:51,080 Epoch  33 Step:  1362200 Batch Loss:     0.124973 Tokens per Sec:    10202, Lr: 0.000200
2020-07-01 11:24:58,511 Epoch  33: total training loss 8.20
2020-07-01 11:24:58,511 EPOCH 34
2020-07-01 11:25:14,536 Epoch  34: total training loss 7.83
2020-07-01 11:25:14,537 EPOCH 35
2020-07-01 11:25:14,786 Epoch  35 Step:  1362300 Batch Loss:     0.105560 Tokens per Sec:     8267, Lr: 0.000200
2020-07-01 11:25:29,918 Epoch  35: total training loss 7.46
2020-07-01 11:25:29,919 EPOCH 36
2020-07-01 11:25:37,819 Epoch  36 Step:  1362400 Batch Loss:     0.117121 Tokens per Sec:    10233, Lr: 0.000200
2020-07-01 11:25:45,237 Epoch  36: total training loss 7.64
2020-07-01 11:25:45,238 EPOCH 37
2020-07-01 11:26:00,786 Epoch  37 Step:  1362500 Batch Loss:     0.123398 Tokens per Sec:     9699, Lr: 0.000200
2020-07-01 11:26:01,030 Epoch  37: total training loss 7.28
2020-07-01 11:26:01,030 EPOCH 38
2020-07-01 11:26:17,064 Epoch  38: total training loss 7.22
2020-07-01 11:26:17,065 EPOCH 39
2020-07-01 11:26:24,418 Epoch  39 Step:  1362600 Batch Loss:     0.100186 Tokens per Sec:     9976, Lr: 0.000200
2020-07-01 11:26:32,928 Epoch  39: total training loss 6.86
2020-07-01 11:26:32,928 EPOCH 40
2020-07-01 11:26:47,916 Epoch  40 Step:  1362700 Batch Loss:     0.099116 Tokens per Sec:     9652, Lr: 0.000200
2020-07-01 11:26:48,874 Epoch  40: total training loss 6.85
2020-07-01 11:26:48,875 EPOCH 41
2020-07-01 11:27:04,759 Epoch  41: total training loss 6.64
2020-07-01 11:27:04,759 EPOCH 42
2020-07-01 11:27:11,165 Epoch  42 Step:  1362800 Batch Loss:     0.094912 Tokens per Sec:     9969, Lr: 0.000200
2020-07-01 11:27:20,570 Epoch  42: total training loss 6.50
2020-07-01 11:27:20,571 EPOCH 43
2020-07-01 11:27:34,744 Epoch  43 Step:  1362900 Batch Loss:     0.106837 Tokens per Sec:     9623, Lr: 0.000200
2020-07-01 11:27:36,510 Epoch  43: total training loss 6.28
2020-07-01 11:27:36,511 EPOCH 44
2020-07-01 11:27:52,772 Epoch  44: total training loss 6.12
2020-07-01 11:27:52,773 EPOCH 45
2020-07-01 11:27:58,483 Epoch  45 Step:  1363000 Batch Loss:     0.086548 Tokens per Sec:     9890, Lr: 0.000200
2020-07-01 11:28:56,098 Example #0
2020-07-01 11:28:56,100 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:28:56,100 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:28:56,100 	Source:     Hello .
2020-07-01 11:28:56,100 	Reference:  Hallo ,
2020-07-01 11:28:56,100 	Hypothesis: Hallo .
2020-07-01 11:28:56,100 Example #1
2020-07-01 11:28:56,100 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:28:56,100 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:28:56,100 	Source:     Hi , how can I help you ?
2020-07-01 11:28:56,100 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:28:56,100 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:28:56,100 Example #2
2020-07-01 11:28:56,100 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:28:56,100 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:28:56,100 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:28:56,100 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:28:56,100 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:28:56,100 Example #3
2020-07-01 11:28:56,100 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:28:56,100 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:28:56,100 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:28:56,100 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:28:56,100 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:28:56,100 Validation result (greedy) at epoch  45, step  1363000: bleu:  53.64, loss: 19982.1211, ppl:   2.2686, duration: 57.6159s
2020-07-01 11:29:06,176 Epoch  45: total training loss 5.95
2020-07-01 11:29:06,176 EPOCH 46
2020-07-01 11:29:19,966 Epoch  46 Step:  1363100 Batch Loss:     0.079606 Tokens per Sec:     9541, Lr: 0.000200
2020-07-01 11:29:22,180 Epoch  46: total training loss 5.85
2020-07-01 11:29:22,181 EPOCH 47
2020-07-01 11:29:38,345 Epoch  47: total training loss 5.84
2020-07-01 11:29:38,346 EPOCH 48
2020-07-01 11:29:43,844 Epoch  48 Step:  1363200 Batch Loss:     0.072912 Tokens per Sec:     9438, Lr: 0.000200
2020-07-01 11:29:54,165 Epoch  48: total training loss 5.68
2020-07-01 11:29:54,166 EPOCH 49
2020-07-01 11:30:06,955 Epoch  49 Step:  1363300 Batch Loss:     0.080133 Tokens per Sec:    10021, Lr: 0.000200
2020-07-01 11:30:09,595 Epoch  49: total training loss 5.62
2020-07-01 11:30:09,595 EPOCH 50
2020-07-01 11:30:25,325 Epoch  50: total training loss 5.55
2020-07-01 11:30:25,326 EPOCH 51
2020-07-01 11:30:30,126 Epoch  51 Step:  1363400 Batch Loss:     0.074295 Tokens per Sec:     9517, Lr: 0.000200
2020-07-01 11:30:41,339 Epoch  51: total training loss 5.41
2020-07-01 11:30:41,340 EPOCH 52
2020-07-01 11:30:53,498 Epoch  52 Step:  1363500 Batch Loss:     0.068766 Tokens per Sec:     9603, Lr: 0.000200
2020-07-01 11:30:57,423 Epoch  52: total training loss 5.40
2020-07-01 11:30:57,424 EPOCH 53
2020-07-01 11:31:13,589 Epoch  53: total training loss 5.29
2020-07-01 11:31:13,590 EPOCH 54
2020-07-01 11:31:16,799 Epoch  54 Step:  1363600 Batch Loss:     0.073984 Tokens per Sec:     9681, Lr: 0.000200
2020-07-01 11:31:29,064 Epoch  54: total training loss 5.19
2020-07-01 11:31:29,065 EPOCH 55
2020-07-01 11:31:39,885 Epoch  55 Step:  1363700 Batch Loss:     0.068570 Tokens per Sec:     9647, Lr: 0.000200
2020-07-01 11:31:45,091 Epoch  55: total training loss 5.08
2020-07-01 11:31:45,092 EPOCH 56
2020-07-01 11:32:00,631 Epoch  56: total training loss 5.30
2020-07-01 11:32:00,632 EPOCH 57
2020-07-01 11:32:02,694 Epoch  57 Step:  1363800 Batch Loss:     0.073428 Tokens per Sec:     9942, Lr: 0.000200
2020-07-01 11:32:16,640 Epoch  57: total training loss 5.95
2020-07-01 11:32:16,641 EPOCH 58
2020-07-01 11:32:26,018 Epoch  58 Step:  1363900 Batch Loss:     0.235698 Tokens per Sec:     9877, Lr: 0.000200
2020-07-01 11:32:32,456 Epoch  58: total training loss 5.41
2020-07-01 11:32:32,456 EPOCH 59
2020-07-01 11:32:48,272 Epoch  59: total training loss 4.94
2020-07-01 11:32:48,272 EPOCH 60
2020-07-01 11:32:49,630 Epoch  60 Step:  1364000 Batch Loss:     0.077774 Tokens per Sec:     9631, Lr: 0.000200
2020-07-01 11:33:46,791 Example #0
2020-07-01 11:33:46,792 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:33:46,792 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:33:46,792 	Source:     Hello .
2020-07-01 11:33:46,792 	Reference:  Hallo ,
2020-07-01 11:33:46,792 	Hypothesis: Hallo .
2020-07-01 11:33:46,792 Example #1
2020-07-01 11:33:46,792 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:33:46,792 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:33:46,792 	Source:     Hi , how can I help you ?
2020-07-01 11:33:46,792 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:33:46,792 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:33:46,792 Example #2
2020-07-01 11:33:46,792 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:33:46,793 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:33:46,793 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:33:46,793 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:33:46,793 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:33:46,793 Example #3
2020-07-01 11:33:46,793 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:33:46,793 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:33:46,793 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:33:46,793 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:33:46,793 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:33:46,793 Validation result (greedy) at epoch  60, step  1364000: bleu:  53.61, loss: 21182.3750, ppl:   2.3830, duration: 57.1627s
2020-07-01 11:34:01,826 Epoch  60: total training loss 4.85
2020-07-01 11:34:01,826 EPOCH 61
2020-07-01 11:34:10,831 Epoch  61 Step:  1364100 Batch Loss:     0.059488 Tokens per Sec:     9598, Lr: 0.000200
2020-07-01 11:34:17,775 Epoch  61: total training loss 4.70
2020-07-01 11:34:17,775 EPOCH 62
2020-07-01 11:34:33,632 Epoch  62: total training loss 4.65
2020-07-01 11:34:33,632 EPOCH 63
2020-07-01 11:34:34,108 Epoch  63 Step:  1364200 Batch Loss:     0.070279 Tokens per Sec:     6599, Lr: 0.000200
2020-07-01 11:34:48,996 Epoch  63: total training loss 4.52
2020-07-01 11:34:48,997 EPOCH 64
2020-07-01 11:34:57,312 Epoch  64 Step:  1364300 Batch Loss:     0.073598 Tokens per Sec:     9181, Lr: 0.000200
2020-07-01 11:35:05,114 Epoch  64: total training loss 4.48
2020-07-01 11:35:05,115 EPOCH 65
2020-07-01 11:35:20,456 Epoch  65 Step:  1364400 Batch Loss:     0.059377 Tokens per Sec:     9865, Lr: 0.000200
2020-07-01 11:35:20,684 Epoch  65: total training loss 4.45
2020-07-01 11:35:20,684 EPOCH 66
2020-07-01 11:35:36,647 Epoch  66: total training loss 4.35
2020-07-01 11:35:36,647 EPOCH 67
2020-07-01 11:35:44,035 Epoch  67 Step:  1364500 Batch Loss:     0.059285 Tokens per Sec:     9710, Lr: 0.000200
2020-07-01 11:35:52,634 Epoch  67: total training loss 4.27
2020-07-01 11:35:52,635 EPOCH 68
2020-07-01 11:36:07,966 Epoch  68 Step:  1364600 Batch Loss:     0.059921 Tokens per Sec:     9615, Lr: 0.000200
2020-07-01 11:36:08,570 Epoch  68: total training loss 4.20
2020-07-01 11:36:08,570 EPOCH 69
2020-07-01 11:36:24,174 Epoch  69: total training loss 4.12
2020-07-01 11:36:24,175 EPOCH 70
2020-07-01 11:36:31,374 Epoch  70 Step:  1364700 Batch Loss:     0.050628 Tokens per Sec:     9500, Lr: 0.000200
2020-07-01 11:36:40,119 Epoch  70: total training loss 4.16
2020-07-01 11:36:40,119 EPOCH 71
2020-07-01 11:36:54,691 Epoch  71 Step:  1364800 Batch Loss:     0.056429 Tokens per Sec:     9716, Lr: 0.000200
2020-07-01 11:36:55,913 Epoch  71: total training loss 4.05
2020-07-01 11:36:55,913 EPOCH 72
2020-07-01 11:37:11,936 Epoch  72: total training loss 4.06
2020-07-01 11:37:11,936 EPOCH 73
2020-07-01 11:37:18,311 Epoch  73 Step:  1364900 Batch Loss:     0.053871 Tokens per Sec:     9668, Lr: 0.000200
2020-07-01 11:37:27,889 Epoch  73: total training loss 4.07
2020-07-01 11:37:27,890 EPOCH 74
2020-07-01 11:37:41,807 Epoch  74 Step:  1365000 Batch Loss:     0.052590 Tokens per Sec:     9735, Lr: 0.000200
2020-07-01 11:38:38,464 Example #0
2020-07-01 11:38:38,465 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:38:38,465 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:38:38,465 	Source:     Hello .
2020-07-01 11:38:38,465 	Reference:  Hallo ,
2020-07-01 11:38:38,465 	Hypothesis: Hallo .
2020-07-01 11:38:38,465 Example #1
2020-07-01 11:38:38,465 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:38:38,465 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:38:38,465 	Source:     Hi , how can I help you ?
2020-07-01 11:38:38,465 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:38:38,465 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:38:38,465 Example #2
2020-07-01 11:38:38,465 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:38:38,466 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:38:38,466 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:38:38,466 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:38:38,466 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:38:38,466 Example #3
2020-07-01 11:38:38,466 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:38:38,466 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:38:38,466 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:38:38,466 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:38:38,466 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:38:38,466 Validation result (greedy) at epoch  74, step  1365000: bleu:  53.31, loss: 22096.5840, ppl:   2.4741, duration: 56.6582s
2020-07-01 11:38:40,383 Epoch  74: total training loss 3.93
2020-07-01 11:38:40,384 EPOCH 75
2020-07-01 11:38:56,439 Epoch  75: total training loss 3.87
2020-07-01 11:38:56,440 EPOCH 76
2020-07-01 11:39:02,320 Epoch  76 Step:  1365100 Batch Loss:     0.055407 Tokens per Sec:     9476, Lr: 0.000200
2020-07-01 11:39:12,342 Epoch  76: total training loss 3.89
2020-07-01 11:39:12,342 EPOCH 77
2020-07-01 11:39:26,258 Epoch  77 Step:  1365200 Batch Loss:     0.057079 Tokens per Sec:     9353, Lr: 0.000200
2020-07-01 11:39:28,660 Epoch  77: total training loss 3.90
2020-07-01 11:39:28,660 EPOCH 78
2020-07-01 11:39:44,584 Epoch  78: total training loss 3.88
2020-07-01 11:39:44,585 EPOCH 79
2020-07-01 11:39:49,771 Epoch  79 Step:  1365300 Batch Loss:     0.062887 Tokens per Sec:     9650, Lr: 0.000200
2020-07-01 11:40:00,128 Epoch  79: total training loss 3.81
2020-07-01 11:40:00,128 EPOCH 80
2020-07-01 11:40:13,428 Epoch  80 Step:  1365400 Batch Loss:     0.049645 Tokens per Sec:     9344, Lr: 0.000200
2020-07-01 11:40:16,397 Epoch  80: total training loss 3.74
2020-07-01 11:40:16,398 EPOCH 81
2020-07-01 11:40:32,251 Epoch  81: total training loss 3.69
2020-07-01 11:40:32,251 EPOCH 82
2020-07-01 11:40:36,757 Epoch  82 Step:  1365500 Batch Loss:     0.051428 Tokens per Sec:     9514, Lr: 0.000200
2020-07-01 11:40:48,049 Epoch  82: total training loss 3.69
2020-07-01 11:40:48,050 EPOCH 83
2020-07-01 11:41:00,479 Epoch  83 Step:  1365600 Batch Loss:     0.051545 Tokens per Sec:     9681, Lr: 0.000200
2020-07-01 11:41:04,020 Epoch  83: total training loss 3.63
2020-07-01 11:41:04,020 EPOCH 84
2020-07-01 11:41:19,915 Epoch  84: total training loss 3.62
2020-07-01 11:41:19,916 EPOCH 85
2020-07-01 11:41:24,022 Epoch  85 Step:  1365700 Batch Loss:     0.068542 Tokens per Sec:     9790, Lr: 0.000200
2020-07-01 11:41:35,736 Epoch  85: total training loss 3.57
2020-07-01 11:41:35,736 EPOCH 86
2020-07-01 11:41:47,713 Epoch  86 Step:  1365800 Batch Loss:     0.071047 Tokens per Sec:     9675, Lr: 0.000200
2020-07-01 11:41:51,601 Epoch  86: total training loss 3.59
2020-07-01 11:41:51,601 EPOCH 87
2020-07-01 11:42:07,037 Epoch  87: total training loss 3.49
2020-07-01 11:42:07,037 EPOCH 88
2020-07-01 11:42:10,584 Epoch  88 Step:  1365900 Batch Loss:     0.053945 Tokens per Sec:     9994, Lr: 0.000200
2020-07-01 11:42:23,039 Epoch  88: total training loss 3.56
2020-07-01 11:42:23,040 EPOCH 89
2020-07-01 11:42:34,413 Epoch  89 Step:  1366000 Batch Loss:     0.057621 Tokens per Sec:     9670, Lr: 0.000200
2020-07-01 11:43:29,445 Example #0
2020-07-01 11:43:29,445 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:43:29,445 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:43:29,445 	Source:     Hello .
2020-07-01 11:43:29,446 	Reference:  Hallo ,
2020-07-01 11:43:29,446 	Hypothesis: Hallo .
2020-07-01 11:43:29,446 Example #1
2020-07-01 11:43:29,446 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:43:29,446 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:43:29,446 	Source:     Hi , how can I help you ?
2020-07-01 11:43:29,446 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:43:29,446 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:43:29,446 Example #2
2020-07-01 11:43:29,446 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:43:29,446 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:43:29,446 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:43:29,446 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:43:29,446 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:43:29,446 Example #3
2020-07-01 11:43:29,446 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:43:29,446 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:43:29,446 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:43:29,446 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:43:29,446 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:43:29,446 Validation result (greedy) at epoch  89, step  1366000: bleu:  53.22, loss: 22911.0000, ppl:   2.5580, duration: 55.0318s
2020-07-01 11:43:34,171 Epoch  89: total training loss 3.56
2020-07-01 11:43:34,172 EPOCH 90
2020-07-01 11:43:50,358 Epoch  90: total training loss 3.46
2020-07-01 11:43:50,359 EPOCH 91
2020-07-01 11:43:53,510 Epoch  91 Step:  1366100 Batch Loss:     0.046244 Tokens per Sec:     9598, Lr: 0.000200
2020-07-01 11:44:06,136 Epoch  91: total training loss 3.39
2020-07-01 11:44:06,137 EPOCH 92
2020-07-01 11:44:16,955 Epoch  92 Step:  1366200 Batch Loss:     0.042227 Tokens per Sec:     9675, Lr: 0.000200
2020-07-01 11:44:21,985 Epoch  92: total training loss 3.41
2020-07-01 11:44:21,985 EPOCH 93
2020-07-01 11:44:38,150 Epoch  93: total training loss 3.39
2020-07-01 11:44:38,150 EPOCH 94
2020-07-01 11:44:40,727 Epoch  94 Step:  1366300 Batch Loss:     0.041841 Tokens per Sec:     9549, Lr: 0.000200
2020-07-01 11:44:54,205 Epoch  94: total training loss 3.45
2020-07-01 11:44:54,206 EPOCH 95
2020-07-01 11:45:04,336 Epoch  95 Step:  1366400 Batch Loss:     0.044234 Tokens per Sec:     9516, Lr: 0.000200
2020-07-01 11:45:10,433 Epoch  95: total training loss 3.34
2020-07-01 11:45:10,434 EPOCH 96
2020-07-01 11:45:26,288 Epoch  96: total training loss 3.32
2020-07-01 11:45:26,289 EPOCH 97
2020-07-01 11:45:28,002 Epoch  97 Step:  1366500 Batch Loss:     0.045016 Tokens per Sec:     9410, Lr: 0.000200
2020-07-01 11:45:42,424 Epoch  97: total training loss 3.28
2020-07-01 11:45:42,425 EPOCH 98
2020-07-01 11:45:51,169 Epoch  98 Step:  1366600 Batch Loss:     0.039869 Tokens per Sec:     9896, Lr: 0.000200
2020-07-01 11:45:58,052 Epoch  98: total training loss 3.32
2020-07-01 11:45:58,052 EPOCH 99
2020-07-01 11:46:14,024 Epoch  99: total training loss 3.23
2020-07-01 11:46:14,025 EPOCH 100
2020-07-01 11:46:14,952 Epoch 100 Step:  1366700 Batch Loss:     0.079474 Tokens per Sec:     8419, Lr: 0.000200
2020-07-01 11:46:30,110 Epoch 100: total training loss 3.25
2020-07-01 11:46:30,110 Training ended after 100 epochs.
2020-07-01 11:46:30,110 Best validation result (greedy) at step  1361000:   1.94 ppl.
2020-07-01 11:47:19,588  dev bleu:  55.05 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 11:47:19,592 Translations saved to: models/wmt_ende_transformer-tune/01361000.hyps.dev
2020-07-01 11:47:47,281 test bleu:  51.46 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 11:47:47,285 Translations saved to: models/wmt_ende_transformer-tune/01361000.hyps.test
