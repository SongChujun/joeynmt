2020-06-18 19:38:07,857 Hello! This is Joey-NMT.
2020-06-18 19:38:13,933 Total params: 20233985
2020-06-18 19:38:13,935 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-06-18 19:38:16,035 Loading model from models/transformer_iwslt14_deen_bpe/best.ckpt
2020-06-18 19:38:16,144 Reset optimizer.
2020-06-18 19:38:16,144 Reset scheduler.
2020-06-18 19:38:16,144 Reset tracking of the best checkpoint.
2020-06-18 19:38:16,150 cfg.name                           : iwslt14-deen-bpe-transformer-multi_enc_shared-tune
2020-06-18 19:38:16,150 cfg.data.src                       : de
2020-06-18 19:38:16,151 cfg.data.trg                       : en
2020-06-18 19:38:16,151 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.iwslt14-deen-bpe
2020-06-18 19:38:16,151 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.iwslt14-deen-bpe
2020-06-18 19:38:16,151 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.iwslt14-deen-bpe
2020-06-18 19:38:16,151 cfg.data.level                     : bpe
2020-06-18 19:38:16,151 cfg.data.lowercase                 : True
2020-06-18 19:38:16,151 cfg.data.max_sent_length           : 62
2020-06-18 19:38:16,151 cfg.data.src_vocab                 : models/transformer_iwslt14_deen_bpe/src_vocab.txt
2020-06-18 19:38:16,151 cfg.data.trg_vocab                 : models/transformer_iwslt14_deen_bpe/trg_vocab.txt
2020-06-18 19:38:16,151 cfg.testing.beam_size              : 5
2020-06-18 19:38:16,151 cfg.testing.alpha                  : 1.0
2020-06-18 19:38:16,151 cfg.training.random_seed           : 42
2020-06-18 19:38:16,151 cfg.training.optimizer             : adam
2020-06-18 19:38:16,151 cfg.training.normalization         : tokens
2020-06-18 19:38:16,152 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-18 19:38:16,152 cfg.training.scheduling            : plateau
2020-06-18 19:38:16,152 cfg.training.patience              : 5
2020-06-18 19:38:16,152 cfg.training.decrease_factor       : 0.7
2020-06-18 19:38:16,152 cfg.training.loss                  : crossentropy
2020-06-18 19:38:16,152 cfg.training.learning_rate         : 0.0003
2020-06-18 19:38:16,152 cfg.training.learning_rate_min     : 1e-08
2020-06-18 19:38:16,152 cfg.training.weight_decay          : 0.0
2020-06-18 19:38:16,152 cfg.training.label_smoothing       : 0.1
2020-06-18 19:38:16,152 cfg.training.batch_size            : 4096
2020-06-18 19:38:16,152 cfg.training.batch_type            : token
2020-06-18 19:38:16,152 cfg.training.early_stopping_metric : eval_metric
2020-06-18 19:38:16,152 cfg.training.epochs                : 100
2020-06-18 19:38:16,152 cfg.training.validation_freq       : 1000
2020-06-18 19:38:16,152 cfg.training.logging_freq          : 100
2020-06-18 19:38:16,152 cfg.training.eval_metric           : bleu
2020-06-18 19:38:16,152 cfg.training.model_dir             : models/transformer_iwslt14_deen_bpe-multi_enc_shared-tune
2020-06-18 19:38:16,152 cfg.training.load_model            : models/transformer_iwslt14_deen_bpe/best.ckpt
2020-06-18 19:38:16,152 cfg.training.reset_best_ckpt       : True
2020-06-18 19:38:16,152 cfg.training.reset_scheduler       : True
2020-06-18 19:38:16,152 cfg.training.reset_optimizer       : True
2020-06-18 19:38:16,152 cfg.training.overwrite             : False
2020-06-18 19:38:16,152 cfg.training.shuffle               : True
2020-06-18 19:38:16,152 cfg.training.use_cuda              : True
2020-06-18 19:38:16,152 cfg.training.keep_last_ckpts       : 5
2020-06-18 19:38:16,152 cfg.model.initializer              : xavier
2020-06-18 19:38:16,152 cfg.model.embed_initializer        : xavier
2020-06-18 19:38:16,152 cfg.model.embed_init_gain          : 1.0
2020-06-18 19:38:16,152 cfg.model.init_gain                : 1.0
2020-06-18 19:38:16,152 cfg.model.bias_initializer         : zeros
2020-06-18 19:38:16,152 cfg.model.tied_embeddings          : True
2020-06-18 19:38:16,152 cfg.model.tied_softmax             : True
2020-06-18 19:38:16,152 cfg.model.encoder.type             : transformer
2020-06-18 19:38:16,152 cfg.model.encoder.num_layers       : 6
2020-06-18 19:38:16,152 cfg.model.encoder.num_heads        : 4
2020-06-18 19:38:16,152 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-18 19:38:16,152 cfg.model.encoder.embeddings.scale : True
2020-06-18 19:38:16,152 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-18 19:38:16,152 cfg.model.encoder.hidden_size      : 256
2020-06-18 19:38:16,152 cfg.model.encoder.ff_size          : 1024
2020-06-18 19:38:16,152 cfg.model.encoder.dropout          : 0.3
2020-06-18 19:38:16,153 cfg.model.encoder.multi_encoder    : True
2020-06-18 19:38:16,153 cfg.model.encoder.share_encoder    : True
2020-06-18 19:38:16,153 cfg.model.decoder.type             : transformer
2020-06-18 19:38:16,153 cfg.model.decoder.num_layers       : 6
2020-06-18 19:38:16,153 cfg.model.decoder.num_heads        : 4
2020-06-18 19:38:16,153 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-18 19:38:16,153 cfg.model.decoder.embeddings.scale : True
2020-06-18 19:38:16,153 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-18 19:38:16,153 cfg.model.decoder.hidden_size      : 256
2020-06-18 19:38:16,153 cfg.model.decoder.ff_size          : 1024
2020-06-18 19:38:16,153 cfg.model.decoder.dropout          : 0.3
2020-06-18 19:38:16,153 Data set sizes: 
	train 9688,
	valid 1510,
	test 1153
2020-06-18 19:38:16,153 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-18 19:38:16,153 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) in (9) und
2020-06-18 19:38:16,153 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) in (9) und
2020-06-18 19:38:16,153 Number of Src words (types): 31716
2020-06-18 19:38:16,153 Number of Trg words (types): 31716
2020-06-18 19:38:16,153 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=256, vocab_size=31716),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=31716))
2020-06-18 19:38:16,178 EPOCH 1
2020-06-18 19:38:28,156 Epoch   1: total training loss 153.90
2020-06-18 19:38:28,157 EPOCH 2
2020-06-18 19:38:38,070 Epoch   2 Step:    32100 Batch Loss:     2.229270 Tokens per Sec:    12079, Lr: 0.000300
2020-06-18 19:38:39,530 Epoch   2: total training loss 96.51
2020-06-18 19:38:39,530 EPOCH 3
2020-06-18 19:38:51,003 Epoch   3: total training loss 79.96
2020-06-18 19:38:51,003 EPOCH 4
2020-06-18 19:38:59,940 Epoch   4 Step:    32200 Batch Loss:     0.522506 Tokens per Sec:    11458, Lr: 0.000300
2020-06-18 19:39:02,569 Epoch   4: total training loss 69.82
2020-06-18 19:39:02,569 EPOCH 5
2020-06-18 19:39:14,212 Epoch   5: total training loss 62.57
2020-06-18 19:39:14,212 EPOCH 6
2020-06-18 19:39:21,682 Epoch   6 Step:    32300 Batch Loss:     1.328463 Tokens per Sec:    11874, Lr: 0.000300
2020-06-18 19:39:25,841 Epoch   6: total training loss 57.37
2020-06-18 19:39:25,842 EPOCH 7
2020-06-18 19:39:37,538 Epoch   7: total training loss 55.29
2020-06-18 19:39:37,538 EPOCH 8
2020-06-18 19:39:43,216 Epoch   8 Step:    32400 Batch Loss:     1.091336 Tokens per Sec:    12236, Lr: 0.000300
2020-06-18 19:39:48,993 Epoch   8: total training loss 49.90
2020-06-18 19:39:48,994 EPOCH 9
2020-06-18 19:40:00,475 Epoch   9: total training loss 45.59
2020-06-18 19:40:00,476 EPOCH 10
2020-06-18 19:40:04,972 Epoch  10 Step:    32500 Batch Loss:     0.705858 Tokens per Sec:    13074, Lr: 0.000300
2020-06-18 19:40:12,129 Epoch  10: total training loss 43.64
2020-06-18 19:40:12,130 EPOCH 11
2020-06-18 19:40:23,774 Epoch  11: total training loss 41.82
2020-06-18 19:40:23,775 EPOCH 12
2020-06-18 19:40:27,272 Epoch  12 Step:    32600 Batch Loss:     0.595448 Tokens per Sec:    12140, Lr: 0.000300
2020-06-18 19:40:35,435 Epoch  12: total training loss 39.77
2020-06-18 19:40:35,435 EPOCH 13
2020-06-18 19:40:47,219 Epoch  13: total training loss 37.79
2020-06-18 19:40:47,219 EPOCH 14
2020-06-18 19:40:49,750 Epoch  14 Step:    32700 Batch Loss:     0.542545 Tokens per Sec:    10301, Lr: 0.000300
2020-06-18 19:40:58,908 Epoch  14: total training loss 36.27
2020-06-18 19:40:58,908 EPOCH 15
2020-06-18 19:41:10,535 Epoch  15: total training loss 34.85
2020-06-18 19:41:10,535 EPOCH 16
2020-06-18 19:41:11,470 Epoch  16 Step:    32800 Batch Loss:     0.521163 Tokens per Sec:    11229, Lr: 0.000300
2020-06-18 19:41:22,258 Epoch  16: total training loss 33.57
2020-06-18 19:41:22,259 EPOCH 17
2020-06-18 19:41:33,609 Epoch  17 Step:    32900 Batch Loss:     0.450092 Tokens per Sec:    11580, Lr: 0.000300
2020-06-18 19:41:34,017 Epoch  17: total training loss 32.28
2020-06-18 19:41:34,017 EPOCH 18
2020-06-18 19:41:45,985 Epoch  18: total training loss 32.05
2020-06-18 19:41:45,986 EPOCH 19
2020-06-18 19:41:55,848 Epoch  19 Step:    33000 Batch Loss:     0.428467 Tokens per Sec:    11652, Lr: 0.000300
2020-06-18 19:42:08,921 Hooray! New best validation result [eval_metric]!
2020-06-18 19:42:08,922 Saving new checkpoint.
2020-06-18 19:42:11,767 Example #0
2020-06-18 19:42:11,767 	Raw source:     ['hallo', ',']
2020-06-18 19:42:11,767 	Raw hypothesis: ['hi', '.']
2020-06-18 19:42:11,767 	Source:     hallo ,
2020-06-18 19:42:11,767 	Reference:  hello .
2020-06-18 19:42:11,767 	Hypothesis: hi .
2020-06-18 19:42:11,767 Example #1
2020-06-18 19:42:11,767 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 19:42:11,767 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 19:42:11,767 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 19:42:11,767 	Reference:  hi , how can i help you ?
2020-06-18 19:42:11,767 	Hypothesis: hi , how can i help you ?
2020-06-18 19:42:11,767 Example #2
2020-06-18 19:42:11,767 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 19:42:11,767 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 19:42:11,767 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 19:42:11,768 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 19:42:11,768 	Hypothesis: hi , i &apos;m looking for a restaurant at arden fair mall in san francisco , california .
2020-06-18 19:42:11,768 Validation result (greedy) at epoch  19, step    33000: bleu:  55.60, loss: 20848.3809, ppl:   2.6521, duration: 15.9196s
2020-06-18 19:42:14,015 Epoch  19: total training loss 30.78
2020-06-18 19:42:14,015 EPOCH 20
2020-06-18 19:42:26,496 Epoch  20: total training loss 28.98
2020-06-18 19:42:26,497 EPOCH 21
2020-06-18 19:42:35,557 Epoch  21 Step:    33100 Batch Loss:     0.492659 Tokens per Sec:    10858, Lr: 0.000300
2020-06-18 19:42:39,099 Epoch  21: total training loss 28.17
2020-06-18 19:42:39,099 EPOCH 22
2020-06-18 19:42:51,431 Epoch  22: total training loss 26.97
2020-06-18 19:42:51,432 EPOCH 23
2020-06-18 19:42:58,832 Epoch  23 Step:    33200 Batch Loss:     0.572077 Tokens per Sec:    11192, Lr: 0.000300
2020-06-18 19:43:03,607 Epoch  23: total training loss 26.19
2020-06-18 19:43:03,607 EPOCH 24
2020-06-18 19:43:15,665 Epoch  24: total training loss 25.51
2020-06-18 19:43:15,666 EPOCH 25
2020-06-18 19:43:22,303 Epoch  25 Step:    33300 Batch Loss:     0.506038 Tokens per Sec:    10215, Lr: 0.000300
2020-06-18 19:43:27,920 Epoch  25: total training loss 24.46
2020-06-18 19:43:27,920 EPOCH 26
2020-06-18 19:43:39,926 Epoch  26: total training loss 23.89
2020-06-18 19:43:39,926 EPOCH 27
2020-06-18 19:43:44,267 Epoch  27 Step:    33400 Batch Loss:     0.487403 Tokens per Sec:    12121, Lr: 0.000300
2020-06-18 19:43:52,321 Epoch  27: total training loss 23.15
2020-06-18 19:43:52,322 EPOCH 28
2020-06-18 19:44:04,663 Epoch  28: total training loss 22.37
2020-06-18 19:44:04,663 EPOCH 29
2020-06-18 19:44:07,861 Epoch  29 Step:    33500 Batch Loss:     0.389442 Tokens per Sec:    11383, Lr: 0.000300
2020-06-18 19:44:16,878 Epoch  29: total training loss 21.89
2020-06-18 19:44:16,878 EPOCH 30
2020-06-18 19:44:29,008 Epoch  30: total training loss 21.70
2020-06-18 19:44:29,008 EPOCH 31
2020-06-18 19:44:30,772 Epoch  31 Step:    33600 Batch Loss:     0.417434 Tokens per Sec:    10948, Lr: 0.000300
2020-06-18 19:44:41,268 Epoch  31: total training loss 20.86
2020-06-18 19:44:41,269 EPOCH 32
2020-06-18 19:44:53,490 Epoch  32: total training loss 20.23
2020-06-18 19:44:53,491 EPOCH 33
2020-06-18 19:44:53,947 Epoch  33 Step:    33700 Batch Loss:     0.242668 Tokens per Sec:     9344, Lr: 0.000300
2020-06-18 19:45:05,687 Epoch  33: total training loss 19.88
2020-06-18 19:45:05,688 EPOCH 34
2020-06-18 19:45:16,956 Epoch  34 Step:    33800 Batch Loss:     0.303527 Tokens per Sec:    11290, Lr: 0.000300
2020-06-18 19:45:17,969 Epoch  34: total training loss 19.31
2020-06-18 19:45:17,970 EPOCH 35
2020-06-18 19:45:30,034 Epoch  35: total training loss 18.30
2020-06-18 19:45:30,035 EPOCH 36
2020-06-18 19:45:40,026 Epoch  36 Step:    33900 Batch Loss:     0.347040 Tokens per Sec:    11119, Lr: 0.000300
2020-06-18 19:45:42,293 Epoch  36: total training loss 18.56
2020-06-18 19:45:42,293 EPOCH 37
2020-06-18 19:45:54,477 Epoch  37: total training loss 17.86
2020-06-18 19:45:54,478 EPOCH 38
2020-06-18 19:46:02,928 Epoch  38 Step:    34000 Batch Loss:     0.361233 Tokens per Sec:    11282, Lr: 0.000300
2020-06-18 19:46:15,217 Hooray! New best validation result [eval_metric]!
2020-06-18 19:46:15,217 Saving new checkpoint.
2020-06-18 19:46:18,101 Example #0
2020-06-18 19:46:18,101 	Raw source:     ['hallo', ',']
2020-06-18 19:46:18,101 	Raw hypothesis: ['hi', '.']
2020-06-18 19:46:18,101 	Source:     hallo ,
2020-06-18 19:46:18,101 	Reference:  hello .
2020-06-18 19:46:18,101 	Hypothesis: hi .
2020-06-18 19:46:18,101 Example #1
2020-06-18 19:46:18,101 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 19:46:18,101 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 19:46:18,101 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 19:46:18,101 	Reference:  hi , how can i help you ?
2020-06-18 19:46:18,101 	Hypothesis: hi , how can i help you ?
2020-06-18 19:46:18,101 Example #2
2020-06-18 19:46:18,101 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 19:46:18,101 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 19:46:18,101 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 19:46:18,101 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 19:46:18,101 	Hypothesis: hi , i was looking for a restaurant at the arden fair mall in san francisco , california .
2020-06-18 19:46:18,102 Validation result (greedy) at epoch  38, step    34000: bleu:  56.85, loss: 21733.0117, ppl:   2.7642, duration: 15.1725s
2020-06-18 19:46:21,755 Epoch  38: total training loss 17.29
2020-06-18 19:46:21,756 EPOCH 39
2020-06-18 19:46:33,657 Epoch  39: total training loss 17.05
2020-06-18 19:46:33,657 EPOCH 40
2020-06-18 19:46:40,563 Epoch  40 Step:    34100 Batch Loss:     0.292252 Tokens per Sec:    11562, Lr: 0.000300
2020-06-18 19:46:45,602 Epoch  40: total training loss 16.71
2020-06-18 19:46:45,602 EPOCH 41
2020-06-18 19:46:57,730 Epoch  41: total training loss 16.73
2020-06-18 19:46:57,730 EPOCH 42
2020-06-18 19:47:02,933 Epoch  42 Step:    34200 Batch Loss:     0.279487 Tokens per Sec:    12320, Lr: 0.000300
2020-06-18 19:47:09,743 Epoch  42: total training loss 16.29
2020-06-18 19:47:09,744 EPOCH 43
2020-06-18 19:47:21,960 Epoch  43: total training loss 15.70
2020-06-18 19:47:21,961 EPOCH 44
2020-06-18 19:47:25,942 Epoch  44 Step:    34300 Batch Loss:     0.282295 Tokens per Sec:    12087, Lr: 0.000300
2020-06-18 19:47:34,122 Epoch  44: total training loss 15.61
2020-06-18 19:47:34,123 EPOCH 45
2020-06-18 19:47:46,239 Epoch  45: total training loss 15.35
2020-06-18 19:47:46,239 EPOCH 46
2020-06-18 19:47:48,611 Epoch  46 Step:    34400 Batch Loss:     0.258293 Tokens per Sec:    12018, Lr: 0.000300
2020-06-18 19:47:58,423 Epoch  46: total training loss 14.73
2020-06-18 19:47:58,424 EPOCH 47
2020-06-18 19:48:11,081 Epoch  47: total training loss 14.48
2020-06-18 19:48:11,082 EPOCH 48
2020-06-18 19:48:12,249 Epoch  48 Step:    34500 Batch Loss:     0.258845 Tokens per Sec:    11101, Lr: 0.000300
2020-06-18 19:48:23,495 Epoch  48: total training loss 14.41
2020-06-18 19:48:23,496 EPOCH 49
2020-06-18 19:48:35,042 Epoch  49 Step:    34600 Batch Loss:     0.313471 Tokens per Sec:    11459, Lr: 0.000300
2020-06-18 19:48:35,626 Epoch  49: total training loss 13.97
2020-06-18 19:48:35,626 EPOCH 50
2020-06-18 19:48:47,881 Epoch  50: total training loss 13.80
2020-06-18 19:48:47,881 EPOCH 51
2020-06-18 19:48:57,945 Epoch  51 Step:    34700 Batch Loss:     0.273567 Tokens per Sec:    11462, Lr: 0.000300
2020-06-18 19:49:00,025 Epoch  51: total training loss 13.51
2020-06-18 19:49:00,025 EPOCH 52
2020-06-18 19:49:12,182 Epoch  52: total training loss 13.35
2020-06-18 19:49:12,183 EPOCH 53
2020-06-18 19:49:20,849 Epoch  53 Step:    34800 Batch Loss:     0.211265 Tokens per Sec:    10962, Lr: 0.000300
2020-06-18 19:49:24,404 Epoch  53: total training loss 13.34
2020-06-18 19:49:24,405 EPOCH 54
2020-06-18 19:49:36,920 Epoch  54: total training loss 12.84
2020-06-18 19:49:36,921 EPOCH 55
2020-06-18 19:49:43,712 Epoch  55 Step:    34900 Batch Loss:     0.234194 Tokens per Sec:    11573, Lr: 0.000300
2020-06-18 19:49:49,407 Epoch  55: total training loss 12.66
2020-06-18 19:49:49,408 EPOCH 56
2020-06-18 19:50:01,591 Epoch  56: total training loss 12.30
2020-06-18 19:50:01,591 EPOCH 57
2020-06-18 19:50:06,801 Epoch  57 Step:    35000 Batch Loss:     0.237179 Tokens per Sec:    12047, Lr: 0.000300
2020-06-18 19:50:18,993 Hooray! New best validation result [eval_metric]!
2020-06-18 19:50:18,994 Saving new checkpoint.
2020-06-18 19:50:21,895 Example #0
2020-06-18 19:50:21,895 	Raw source:     ['hallo', ',']
2020-06-18 19:50:21,895 	Raw hypothesis: ['hello', '?']
2020-06-18 19:50:21,895 	Source:     hallo ,
2020-06-18 19:50:21,895 	Reference:  hello .
2020-06-18 19:50:21,895 	Hypothesis: hello ?
2020-06-18 19:50:21,895 Example #1
2020-06-18 19:50:21,895 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 19:50:21,895 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 19:50:21,895 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 19:50:21,895 	Reference:  hi , how can i help you ?
2020-06-18 19:50:21,895 	Hypothesis: hi , how can i help you ?
2020-06-18 19:50:21,895 Example #2
2020-06-18 19:50:21,895 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 19:50:21,895 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 19:50:21,895 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 19:50:21,895 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 19:50:21,895 	Hypothesis: hi , i was looking for a restaurant at the arden fair mall in san francisco , california .
2020-06-18 19:50:21,895 Validation result (greedy) at epoch  57, step    35000: bleu:  56.96, loss: 23039.3887, ppl:   2.9384, duration: 15.0944s
2020-06-18 19:50:29,174 Epoch  57: total training loss 12.15
2020-06-18 19:50:29,175 EPOCH 58
2020-06-18 19:50:41,861 Epoch  58: total training loss 11.87
2020-06-18 19:50:41,861 EPOCH 59
2020-06-18 19:50:46,015 Epoch  59 Step:    35100 Batch Loss:     0.221032 Tokens per Sec:    11290, Lr: 0.000300
2020-06-18 19:50:54,261 Epoch  59: total training loss 11.70
2020-06-18 19:50:54,262 EPOCH 60
2020-06-18 19:51:06,442 Epoch  60: total training loss 11.71
2020-06-18 19:51:06,443 EPOCH 61
2020-06-18 19:51:08,948 Epoch  61 Step:    35200 Batch Loss:     0.214266 Tokens per Sec:    11672, Lr: 0.000300
2020-06-18 19:51:18,521 Epoch  61: total training loss 11.21
2020-06-18 19:51:18,521 EPOCH 62
2020-06-18 19:51:30,671 Epoch  62: total training loss 10.95
2020-06-18 19:51:30,672 EPOCH 63
2020-06-18 19:51:31,962 Epoch  63 Step:    35300 Batch Loss:     0.209949 Tokens per Sec:    10514, Lr: 0.000300
2020-06-18 19:51:42,863 Epoch  63: total training loss 10.98
2020-06-18 19:51:42,864 EPOCH 64
2020-06-18 19:51:54,611 Epoch  64 Step:    35400 Batch Loss:     0.191736 Tokens per Sec:    11301, Lr: 0.000300
2020-06-18 19:51:55,151 Epoch  64: total training loss 10.98
2020-06-18 19:51:55,151 EPOCH 65
2020-06-18 19:52:07,246 Epoch  65: total training loss 11.32
2020-06-18 19:52:07,246 EPOCH 66
2020-06-18 19:52:17,489 Epoch  66 Step:    35500 Batch Loss:     0.191264 Tokens per Sec:    11467, Lr: 0.000300
2020-06-18 19:52:19,263 Epoch  66: total training loss 10.52
2020-06-18 19:52:19,263 EPOCH 67
2020-06-18 19:52:31,579 Epoch  67: total training loss 10.46
2020-06-18 19:52:31,580 EPOCH 68
2020-06-18 19:52:40,306 Epoch  68 Step:    35600 Batch Loss:     0.205920 Tokens per Sec:    11430, Lr: 0.000300
2020-06-18 19:52:43,746 Epoch  68: total training loss 10.14
2020-06-18 19:52:43,746 EPOCH 69
2020-06-18 19:52:55,912 Epoch  69: total training loss 10.17
2020-06-18 19:52:55,913 EPOCH 70
2020-06-18 19:53:03,016 Epoch  70 Step:    35700 Batch Loss:     0.201488 Tokens per Sec:    11360, Lr: 0.000300
2020-06-18 19:53:07,990 Epoch  70: total training loss 9.88
2020-06-18 19:53:07,991 EPOCH 71
2020-06-18 19:53:20,230 Epoch  71: total training loss 9.67
2020-06-18 19:53:20,230 EPOCH 72
2020-06-18 19:53:25,892 Epoch  72 Step:    35800 Batch Loss:     0.186905 Tokens per Sec:    11186, Lr: 0.000300
2020-06-18 19:53:32,447 Epoch  72: total training loss 9.56
2020-06-18 19:53:32,447 EPOCH 73
2020-06-18 19:53:44,560 Epoch  73: total training loss 9.34
2020-06-18 19:53:44,561 EPOCH 74
2020-06-18 19:53:49,002 Epoch  74 Step:    35900 Batch Loss:     0.142882 Tokens per Sec:    10577, Lr: 0.000300
2020-06-18 19:53:56,608 Epoch  74: total training loss 9.28
2020-06-18 19:53:56,609 EPOCH 75
2020-06-18 19:54:08,767 Epoch  75: total training loss 8.92
2020-06-18 19:54:08,768 EPOCH 76
2020-06-18 19:54:11,828 Epoch  76 Step:    36000 Batch Loss:     0.109274 Tokens per Sec:    11716, Lr: 0.000300
2020-06-18 19:54:24,181 Hooray! New best validation result [eval_metric]!
2020-06-18 19:54:24,181 Saving new checkpoint.
2020-06-18 19:54:26,942 Example #0
2020-06-18 19:54:26,942 	Raw source:     ['hallo', ',']
2020-06-18 19:54:26,942 	Raw hypothesis: ['hello', '.']
2020-06-18 19:54:26,942 	Source:     hallo ,
2020-06-18 19:54:26,942 	Reference:  hello .
2020-06-18 19:54:26,943 	Hypothesis: hello .
2020-06-18 19:54:26,943 Example #1
2020-06-18 19:54:26,943 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 19:54:26,943 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 19:54:26,943 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 19:54:26,943 	Reference:  hi , how can i help you ?
2020-06-18 19:54:26,943 	Hypothesis: hi , how can i help you ?
2020-06-18 19:54:26,943 Example #2
2020-06-18 19:54:26,943 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 19:54:26,943 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 19:54:26,943 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 19:54:26,943 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 19:54:26,943 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-18 19:54:26,943 Validation result (greedy) at epoch  76, step    36000: bleu:  56.97, loss: 24198.1094, ppl:   3.1021, duration: 15.1143s
2020-06-18 19:54:35,923 Epoch  76: total training loss 9.22
2020-06-18 19:54:35,923 EPOCH 77
2020-06-18 19:54:47,924 Epoch  77: total training loss 9.08
2020-06-18 19:54:47,924 EPOCH 78
2020-06-18 19:54:49,354 Epoch  78 Step:    36100 Batch Loss:     0.188186 Tokens per Sec:    13274, Lr: 0.000300
2020-06-18 19:54:59,962 Epoch  78: total training loss 8.88
2020-06-18 19:54:59,963 EPOCH 79
2020-06-18 19:55:12,149 Epoch  79 Step:    36200 Batch Loss:     0.161737 Tokens per Sec:    11244, Lr: 0.000300
2020-06-18 19:55:12,150 Epoch  79: total training loss 8.79
2020-06-18 19:55:12,150 EPOCH 80
2020-06-18 19:55:24,259 Epoch  80: total training loss 8.58
2020-06-18 19:55:24,260 EPOCH 81
2020-06-18 19:55:35,001 Epoch  81 Step:    36300 Batch Loss:     0.110952 Tokens per Sec:    11270, Lr: 0.000300
2020-06-18 19:55:36,406 Epoch  81: total training loss 8.61
2020-06-18 19:55:36,406 EPOCH 82
2020-06-18 19:55:48,480 Epoch  82: total training loss 8.63
2020-06-18 19:55:48,481 EPOCH 83
2020-06-18 19:55:57,461 Epoch  83 Step:    36400 Batch Loss:     0.169583 Tokens per Sec:    11319, Lr: 0.000300
2020-06-18 19:56:00,653 Epoch  83: total training loss 8.46
2020-06-18 19:56:00,653 EPOCH 84
2020-06-18 19:56:12,835 Epoch  84: total training loss 8.15
2020-06-18 19:56:12,835 EPOCH 85
2020-06-18 19:56:20,578 Epoch  85 Step:    36500 Batch Loss:     0.150292 Tokens per Sec:    11331, Lr: 0.000300
2020-06-18 19:56:24,890 Epoch  85: total training loss 8.13
2020-06-18 19:56:24,890 EPOCH 86
2020-06-18 19:56:36,970 Epoch  86: total training loss 8.32
2020-06-18 19:56:36,971 EPOCH 87
2020-06-18 19:56:43,174 Epoch  87 Step:    36600 Batch Loss:     0.158977 Tokens per Sec:    11633, Lr: 0.000300
2020-06-18 19:56:49,170 Epoch  87: total training loss 8.01
2020-06-18 19:56:49,170 EPOCH 88
2020-06-18 19:57:01,225 Epoch  88: total training loss 7.75
2020-06-18 19:57:01,225 EPOCH 89
2020-06-18 19:57:05,961 Epoch  89 Step:    36700 Batch Loss:     0.142172 Tokens per Sec:    11307, Lr: 0.000300
2020-06-18 19:57:13,385 Epoch  89: total training loss 7.89
2020-06-18 19:57:13,386 EPOCH 90
2020-06-18 19:57:25,415 Epoch  90: total training loss 7.80
2020-06-18 19:57:25,416 EPOCH 91
2020-06-18 19:57:28,974 Epoch  91 Step:    36800 Batch Loss:     0.097058 Tokens per Sec:    10020, Lr: 0.000300
2020-06-18 19:57:37,477 Epoch  91: total training loss 7.77
2020-06-18 19:57:37,477 EPOCH 92
2020-06-18 19:57:49,714 Epoch  92: total training loss 7.45
2020-06-18 19:57:49,715 EPOCH 93
2020-06-18 19:57:51,348 Epoch  93 Step:    36900 Batch Loss:     0.130987 Tokens per Sec:    13538, Lr: 0.000300
2020-06-18 19:58:01,786 Epoch  93: total training loss 7.39
2020-06-18 19:58:01,786 EPOCH 94
2020-06-18 19:58:13,998 Epoch  94: total training loss 7.30
2020-06-18 19:58:13,999 EPOCH 95
2020-06-18 19:58:14,485 Epoch  95 Step:    37000 Batch Loss:     0.125981 Tokens per Sec:    10641, Lr: 0.000300
2020-06-18 19:58:26,834 Hooray! New best validation result [eval_metric]!
2020-06-18 19:58:26,835 Saving new checkpoint.
2020-06-18 19:58:29,643 Example #0
2020-06-18 19:58:29,643 	Raw source:     ['hallo', ',']
2020-06-18 19:58:29,643 	Raw hypothesis: ['hello', '.']
2020-06-18 19:58:29,643 	Source:     hallo ,
2020-06-18 19:58:29,643 	Reference:  hello .
2020-06-18 19:58:29,643 	Hypothesis: hello .
2020-06-18 19:58:29,643 Example #1
2020-06-18 19:58:29,643 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-18 19:58:29,643 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-18 19:58:29,643 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-18 19:58:29,643 	Reference:  hi , how can i help you ?
2020-06-18 19:58:29,643 	Hypothesis: hi , how can i help you ?
2020-06-18 19:58:29,643 Example #2
2020-06-18 19:58:29,643 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-18 19:58:29,643 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'san', 'francisco', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-18 19:58:29,643 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-18 19:58:29,643 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-18 19:58:29,643 	Hypothesis: hi , i was looking for a restaurant in san francisco fair mall in san francisco , california .
2020-06-18 19:58:29,644 Validation result (greedy) at epoch  95, step    37000: bleu:  57.20, loss: 24804.8750, ppl:   3.1914, duration: 15.1582s
2020-06-18 19:58:41,293 Epoch  95: total training loss 7.41
2020-06-18 19:58:41,294 EPOCH 96
2020-06-18 19:58:52,251 Epoch  96 Step:    37100 Batch Loss:     0.150145 Tokens per Sec:    11319, Lr: 0.000300
2020-06-18 19:58:53,302 Epoch  96: total training loss 7.60
2020-06-18 19:58:53,302 EPOCH 97
2020-06-18 19:59:05,527 Epoch  97: total training loss 7.12
2020-06-18 19:59:05,527 EPOCH 98
2020-06-18 19:59:15,298 Epoch  98 Step:    37200 Batch Loss:     0.138123 Tokens per Sec:    11100, Lr: 0.000300
2020-06-18 19:59:17,635 Epoch  98: total training loss 7.18
2020-06-18 19:59:17,636 EPOCH 99
2020-06-18 19:59:29,823 Epoch  99: total training loss 6.92
2020-06-18 19:59:29,824 EPOCH 100
2020-06-18 19:59:38,032 Epoch 100 Step:    37300 Batch Loss:     0.122679 Tokens per Sec:    11339, Lr: 0.000300
2020-06-18 19:59:41,841 Epoch 100: total training loss 6.80
2020-06-18 19:59:41,842 Training ended after 100 epochs.
2020-06-18 19:59:41,842 Best validation result (greedy) at step    37000:  57.20 eval_metric.
2020-06-18 20:00:13,247  dev bleu:  57.58 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-18 20:00:13,252 Translations saved to: models/transformer_iwslt14_deen_bpe-multi_enc_shared-tune/00037000.hyps.dev
2020-06-18 20:00:21,779 test bleu:  52.91 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-18 20:00:21,784 Translations saved to: models/transformer_iwslt14_deen_bpe-multi_enc_shared-tune/00037000.hyps.test
