data:
  dev: chatnmt/official_split_line_by_line/wmt17bpe/dev.tags.bpe.wmt-ende-best
  level: bpe
  lowercase: false
  max_sent_length: 100
  src: en
  src_vocab: models/wmt_ende_transformer/src_vocab.txt
  test: chatnmt/official_split_line_by_line/wmt17bpe/test.tags.bpe.wmt-ende-best
  train: chatnmt/official_split_line_by_line/wmt17bpe/train.news.tags.bpe.wmt-ende-best
  trg: de
  trg_vocab: models/wmt_ende_transformer/trg_vocab.txt
model:
  bias_initializer: zeros
  decoder:
    dropout: 0.1
    embeddings:
      dropout: 0.0
      embedding_dim: 512
      scale: true
    ff_size: 512
    freeze: false
    hidden_size: 256
    num_heads: 2
    num_layers: 6
    type: transformer
  embed_init_gain: 1.0
  embed_initializer: xavier
  encoder:
    dropout: 0.2
    embeddings:
      dropout: 0.0
      embedding_dim: 512
      scale: true
    ff_size: 512
    freeze: false
    hidden_size: 512
    multi_encoder: true
    num_heads: 8
    num_layers: 6
    type: transformer
  init_gain: 1.0
  initializer: xavier
  tied_embeddings: true
  tied_softmax: true
name: transformer
testing:
  alpha: 1.0
  beam_size: 5
training:
  adam_betas:
  - 0.9
  - 0.999
  batch_multiplier: 1
  batch_size: 512
  batch_type: token
  decrease_factor: 0.7
  early_stopping_metric: ppl
  epochs: 150
  eval_metric: bleu
  keep_last_ckpts: 3
  label_smoothing: 0.1
  learning_rate: 0.0002
  learning_rate_min: 1.0e-08
  logging_freq: 100
  loss: crossentropy
  max_output_length: 100
  model_dir: models/viznet/8_2_0.2_512_256_512
  normalization: tokens
  optimizer: adam
  overwrite: true
  patience: 8
  print_valid_sents:
  - 0
  - 1
  - 2
  - 3
  random_seed: 42
  scheduling: plateau
  shuffle: true
  use_cuda: true
  validation_freq: 750
  weight_decay: 0.0
