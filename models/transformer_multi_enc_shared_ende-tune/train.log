2020-06-08 08:14:35,380 Hello! This is Joey-NMT.
2020-06-08 08:14:43,125 Total params: 67100161
2020-06-08 08:14:43,127 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-06-08 08:14:47,639 Loading model from models/wmt_ende_transformer/best.ckpt
2020-06-08 08:14:48,250 Reset optimizer.
2020-06-08 08:14:48,250 Reset scheduler.
2020-06-08 08:14:48,250 Reset tracking of the best checkpoint.
2020-06-08 08:14:48,260 cfg.name                           : transformer
2020-06-08 08:14:48,260 cfg.data.src                       : en
2020-06-08 08:14:48,260 cfg.data.trg                       : de
2020-06-08 08:14:48,260 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-06-08 08:14:48,260 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-06-08 08:14:48,260 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-06-08 08:14:48,260 cfg.data.level                     : bpe
2020-06-08 08:14:48,260 cfg.data.lowercase                 : False
2020-06-08 08:14:48,261 cfg.data.max_sent_length           : 100
2020-06-08 08:14:48,261 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-06-08 08:14:48,261 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-06-08 08:14:48,261 cfg.testing.beam_size              : 5
2020-06-08 08:14:48,261 cfg.testing.alpha                  : 1.0
2020-06-08 08:14:48,261 cfg.training.random_seed           : 42
2020-06-08 08:14:48,261 cfg.training.optimizer             : adam
2020-06-08 08:14:48,261 cfg.training.normalization         : tokens
2020-06-08 08:14:48,261 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-08 08:14:48,261 cfg.training.scheduling            : plateau
2020-06-08 08:14:48,261 cfg.training.patience              : 8
2020-06-08 08:14:48,261 cfg.training.decrease_factor       : 0.7
2020-06-08 08:14:48,261 cfg.training.loss                  : crossentropy
2020-06-08 08:14:48,261 cfg.training.learning_rate         : 0.0002
2020-06-08 08:14:48,261 cfg.training.learning_rate_min     : 1e-08
2020-06-08 08:14:48,261 cfg.training.weight_decay          : 0.0
2020-06-08 08:14:48,261 cfg.training.label_smoothing       : 0.1
2020-06-08 08:14:48,262 cfg.training.batch_size            : 4096
2020-06-08 08:14:48,262 cfg.training.batch_type            : token
2020-06-08 08:14:48,262 cfg.training.eval_batch_size       : 3600
2020-06-08 08:14:48,262 cfg.training.eval_batch_type       : token
2020-06-08 08:14:48,262 cfg.training.batch_multiplier      : 1
2020-06-08 08:14:48,262 cfg.training.early_stopping_metric : ppl
2020-06-08 08:14:48,262 cfg.training.epochs                : 100
2020-06-08 08:14:48,262 cfg.training.validation_freq       : 1000
2020-06-08 08:14:48,262 cfg.training.logging_freq          : 100
2020-06-08 08:14:48,262 cfg.training.eval_metric           : bleu
2020-06-08 08:14:48,262 cfg.training.model_dir             : models/transformer_multi_enc_shared_ende-tune
2020-06-08 08:14:48,263 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-06-08 08:14:48,263 cfg.training.reset_best_ckpt       : True
2020-06-08 08:14:48,263 cfg.training.reset_scheduler       : True
2020-06-08 08:14:48,263 cfg.training.reset_optimizer       : True
2020-06-08 08:14:48,263 cfg.training.overwrite             : False
2020-06-08 08:14:48,263 cfg.training.shuffle               : True
2020-06-08 08:14:48,263 cfg.training.use_cuda              : True
2020-06-08 08:14:48,263 cfg.training.max_output_length     : 100
2020-06-08 08:14:48,263 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-08 08:14:48,263 cfg.training.keep_last_ckpts       : 3
2020-06-08 08:14:48,263 cfg.model.initializer              : xavier
2020-06-08 08:14:48,264 cfg.model.bias_initializer         : zeros
2020-06-08 08:14:48,264 cfg.model.init_gain                : 1.0
2020-06-08 08:14:48,264 cfg.model.embed_initializer        : xavier
2020-06-08 08:14:48,264 cfg.model.embed_init_gain          : 1.0
2020-06-08 08:14:48,264 cfg.model.tied_embeddings          : True
2020-06-08 08:14:48,264 cfg.model.tied_softmax             : True
2020-06-08 08:14:48,264 cfg.model.encoder.type             : transformer
2020-06-08 08:14:48,264 cfg.model.encoder.num_layers       : 6
2020-06-08 08:14:48,264 cfg.model.encoder.num_heads        : 8
2020-06-08 08:14:48,264 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-08 08:14:48,264 cfg.model.encoder.embeddings.scale : True
2020-06-08 08:14:48,265 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-08 08:14:48,265 cfg.model.encoder.hidden_size      : 512
2020-06-08 08:14:48,265 cfg.model.encoder.ff_size          : 2048
2020-06-08 08:14:48,265 cfg.model.encoder.dropout          : 0.1
2020-06-08 08:14:48,265 cfg.model.encoder.multi_encoder    : True
2020-06-08 08:14:48,265 cfg.model.encoder.share_encoder    : True
2020-06-08 08:14:48,265 cfg.model.decoder.type             : transformer
2020-06-08 08:14:48,265 cfg.model.decoder.num_layers       : 6
2020-06-08 08:14:48,265 cfg.model.decoder.num_heads        : 8
2020-06-08 08:14:48,265 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-08 08:14:48,265 cfg.model.decoder.embeddings.scale : True
2020-06-08 08:14:48,265 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-08 08:14:48,266 cfg.model.decoder.hidden_size      : 512
2020-06-08 08:14:48,266 cfg.model.decoder.ff_size          : 2048
2020-06-08 08:14:48,266 cfg.model.decoder.dropout          : 0.1
2020-06-08 08:14:48,266 Data set sizes: 
	train 9747,
	valid 1523,
	test 1186
2020-06-08 08:14:48,266 First training example:
	[SRC] H@@ i there@@ ! How can I hel@@ p@@ ?
	[TRG] Hal@@ lo@@ ! Wie kann ich hel@@ fen@@ ?
2020-06-08 08:14:48,266 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-08 08:14:48,266 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-08 08:14:48,266 Number of Src words (types): 36628
2020-06-08 08:14:48,267 Number of Trg words (types): 36628
2020-06-08 08:14:48,267 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-06-08 08:14:48,308 EPOCH 1
2020-06-08 08:15:18,014 Epoch   1: total training loss 312.06
2020-06-08 08:15:18,015 EPOCH 2
2020-06-08 08:15:32,811 Epoch   2 Step:  1360100 Batch Loss:     2.664258 Tokens per Sec:     5876, Lr: 0.000200
2020-06-08 08:15:46,697 Epoch   2: total training loss 140.82
2020-06-08 08:15:46,698 EPOCH 3
2020-06-08 08:16:15,453 Epoch   3: total training loss 98.21
2020-06-08 08:16:15,454 EPOCH 4
2020-06-08 08:16:17,700 Epoch   4 Step:  1360200 Batch Loss:     1.007157 Tokens per Sec:     5317, Lr: 0.000200
2020-06-08 08:16:44,176 Epoch   4: total training loss 80.89
2020-06-08 08:16:44,177 EPOCH 5
2020-06-08 08:17:01,843 Epoch   5 Step:  1360300 Batch Loss:     1.201306 Tokens per Sec:     5752, Lr: 0.000200
2020-06-08 08:17:13,363 Epoch   5: total training loss 71.29
2020-06-08 08:17:13,364 EPOCH 6
2020-06-08 08:17:42,389 Epoch   6: total training loss 62.68
2020-06-08 08:17:42,390 EPOCH 7
2020-06-08 08:17:46,584 Epoch   7 Step:  1360400 Batch Loss:     1.056654 Tokens per Sec:     6151, Lr: 0.000200
2020-06-08 08:18:11,370 Epoch   7: total training loss 58.90
2020-06-08 08:18:11,371 EPOCH 8
2020-06-08 08:18:30,270 Epoch   8 Step:  1360500 Batch Loss:     0.969877 Tokens per Sec:     5810, Lr: 0.000200
2020-06-08 08:18:39,573 Epoch   8: total training loss 53.41
2020-06-08 08:18:39,574 EPOCH 9
2020-06-08 08:19:08,948 Epoch   9: total training loss 49.36
2020-06-08 08:19:08,949 EPOCH 10
2020-06-08 08:19:14,598 Epoch  10 Step:  1360600 Batch Loss:     0.560998 Tokens per Sec:     5391, Lr: 0.000200
2020-06-08 08:19:37,824 Epoch  10: total training loss 45.53
2020-06-08 08:19:37,825 EPOCH 11
2020-06-08 08:19:59,056 Epoch  11 Step:  1360700 Batch Loss:     0.767166 Tokens per Sec:     5584, Lr: 0.000200
2020-06-08 08:20:06,994 Epoch  11: total training loss 42.29
2020-06-08 08:20:06,995 EPOCH 12
2020-06-08 08:20:35,446 Epoch  12: total training loss 40.23
2020-06-08 08:20:35,446 EPOCH 13
2020-06-08 08:20:42,685 Epoch  13 Step:  1360800 Batch Loss:     0.558945 Tokens per Sec:     5498, Lr: 0.000200
2020-06-08 08:21:04,413 Epoch  13: total training loss 39.05
2020-06-08 08:21:04,414 EPOCH 14
2020-06-08 08:21:26,947 Epoch  14 Step:  1360900 Batch Loss:     0.575289 Tokens per Sec:     5478, Lr: 0.000200
2020-06-08 08:21:33,496 Epoch  14: total training loss 36.57
2020-06-08 08:21:33,497 EPOCH 15
2020-06-08 08:22:02,197 Epoch  15: total training loss 33.67
2020-06-08 08:22:02,198 EPOCH 16
2020-06-08 08:22:10,366 Epoch  16 Step:  1361000 Batch Loss:     0.582578 Tokens per Sec:     5756, Lr: 0.000200
2020-06-08 08:22:52,144 Hooray! New best validation result [ppl]!
2020-06-08 08:22:52,145 Saving new checkpoint.
2020-06-08 08:23:02,078 Example #0
2020-06-08 08:23:02,079 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-08 08:23:02,079 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-08 08:23:02,079 	Source:     Hello.
2020-06-08 08:23:02,079 	Reference:  Hallo,
2020-06-08 08:23:02,079 	Hypothesis: Hallo.
2020-06-08 08:23:02,079 Example #1
2020-06-08 08:23:02,079 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-08 08:23:02,079 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-08 08:23:02,079 	Source:     Hi, how can I help you?
2020-06-08 08:23:02,079 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:23:02,079 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:23:02,079 Example #2
2020-06-08 08:23:02,079 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-08 08:23:02,079 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-08 08:23:02,079 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-08 08:23:02,080 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-08 08:23:02,080 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-08 08:23:02,080 Example #3
2020-06-08 08:23:02,080 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-08 08:23:02,080 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-08 08:23:02,080 	Source:     Ok, what type of restaurant are you looking for?
2020-06-08 08:23:02,080 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-08 08:23:02,080 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-08 08:23:02,080 Validation result (greedy) at epoch  16, step  1361000: bleu:  44.41, loss: 20527.9219, ppl:   2.2294, duration: 51.7120s
2020-06-08 08:23:22,692 Epoch  16: total training loss 31.78
2020-06-08 08:23:22,697 EPOCH 17
2020-06-08 08:23:46,915 Epoch  17 Step:  1361100 Batch Loss:     0.463708 Tokens per Sec:     5647, Lr: 0.000200
2020-06-08 08:23:51,607 Epoch  17: total training loss 30.53
2020-06-08 08:23:51,607 EPOCH 18
2020-06-08 08:24:20,400 Epoch  18: total training loss 28.42
2020-06-08 08:24:20,401 EPOCH 19
2020-06-08 08:24:31,997 Epoch  19 Step:  1361200 Batch Loss:     0.492065 Tokens per Sec:     5554, Lr: 0.000200
2020-06-08 08:24:49,341 Epoch  19: total training loss 27.71
2020-06-08 08:24:49,341 EPOCH 20
2020-06-08 08:25:16,233 Epoch  20 Step:  1361300 Batch Loss:     0.462500 Tokens per Sec:     5571, Lr: 0.000200
2020-06-08 08:25:18,293 Epoch  20: total training loss 26.27
2020-06-08 08:25:18,294 EPOCH 21
2020-06-08 08:25:47,611 Epoch  21: total training loss 25.78
2020-06-08 08:25:47,612 EPOCH 22
2020-06-08 08:26:00,650 Epoch  22 Step:  1361400 Batch Loss:     0.401843 Tokens per Sec:     5672, Lr: 0.000200
2020-06-08 08:26:16,341 Epoch  22: total training loss 24.13
2020-06-08 08:26:16,342 EPOCH 23
2020-06-08 08:26:44,641 Epoch  23 Step:  1361500 Batch Loss:     0.362898 Tokens per Sec:     5559, Lr: 0.000200
2020-06-08 08:26:45,439 Epoch  23: total training loss 23.41
2020-06-08 08:26:45,439 EPOCH 24
2020-06-08 08:27:14,627 Epoch  24: total training loss 22.41
2020-06-08 08:27:14,628 EPOCH 25
2020-06-08 08:27:29,671 Epoch  25 Step:  1361600 Batch Loss:     0.270279 Tokens per Sec:     5315, Lr: 0.000200
2020-06-08 08:27:43,987 Epoch  25: total training loss 21.22
2020-06-08 08:27:43,988 EPOCH 26
2020-06-08 08:28:12,744 Epoch  26: total training loss 20.20
2020-06-08 08:28:12,745 EPOCH 27
2020-06-08 08:28:14,119 Epoch  27 Step:  1361700 Batch Loss:     0.219364 Tokens per Sec:     3085, Lr: 0.000200
2020-06-08 08:28:41,663 Epoch  27: total training loss 19.52
2020-06-08 08:28:41,664 EPOCH 28
2020-06-08 08:28:57,095 Epoch  28 Step:  1361800 Batch Loss:     0.302051 Tokens per Sec:     5750, Lr: 0.000200
2020-06-08 08:29:10,244 Epoch  28: total training loss 19.05
2020-06-08 08:29:10,245 EPOCH 29
2020-06-08 08:29:39,035 Epoch  29: total training loss 17.85
2020-06-08 08:29:39,036 EPOCH 30
2020-06-08 08:29:41,093 Epoch  30 Step:  1361900 Batch Loss:     0.300919 Tokens per Sec:     6283, Lr: 0.000200
2020-06-08 08:30:08,105 Epoch  30: total training loss 17.25
2020-06-08 08:30:08,106 EPOCH 31
2020-06-08 08:30:25,153 Epoch  31 Step:  1362000 Batch Loss:     0.299284 Tokens per Sec:     5687, Lr: 0.000200
2020-06-08 08:31:07,311 Example #0
2020-06-08 08:31:07,312 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-08 08:31:07,312 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-08 08:31:07,312 	Source:     Hello.
2020-06-08 08:31:07,312 	Reference:  Hallo,
2020-06-08 08:31:07,313 	Hypothesis: Hallo.
2020-06-08 08:31:07,313 Example #1
2020-06-08 08:31:07,313 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-08 08:31:07,313 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-08 08:31:07,313 	Source:     Hi, how can I help you?
2020-06-08 08:31:07,313 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:31:07,313 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:31:07,313 Example #2
2020-06-08 08:31:07,314 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-08 08:31:07,314 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-08 08:31:07,314 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-08 08:31:07,314 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-08 08:31:07,314 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-08 08:31:07,314 Example #3
2020-06-08 08:31:07,314 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-08 08:31:07,314 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-08 08:31:07,314 	Source:     Ok, what type of restaurant are you looking for?
2020-06-08 08:31:07,315 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-08 08:31:07,315 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-08 08:31:07,315 Validation result (greedy) at epoch  31, step  1362000: bleu:  45.14, loss: 20868.9766, ppl:   2.2593, duration: 42.1601s
2020-06-08 08:31:18,977 Epoch  31: total training loss 16.75
2020-06-08 08:31:18,978 EPOCH 32
2020-06-08 08:31:47,713 Epoch  32: total training loss 15.57
2020-06-08 08:31:47,714 EPOCH 33
2020-06-08 08:31:52,178 Epoch  33 Step:  1362100 Batch Loss:     0.268035 Tokens per Sec:     4935, Lr: 0.000200
2020-06-08 08:32:16,493 Epoch  33: total training loss 15.34
2020-06-08 08:32:16,494 EPOCH 34
2020-06-08 08:32:34,597 Epoch  34 Step:  1362200 Batch Loss:     0.223161 Tokens per Sec:     5815, Lr: 0.000200
2020-06-08 08:32:45,380 Epoch  34: total training loss 14.78
2020-06-08 08:32:45,381 EPOCH 35
2020-06-08 08:33:14,151 Epoch  35: total training loss 14.07
2020-06-08 08:33:14,152 EPOCH 36
2020-06-08 08:33:19,659 Epoch  36 Step:  1362300 Batch Loss:     0.216336 Tokens per Sec:     5201, Lr: 0.000200
2020-06-08 08:33:43,200 Epoch  36: total training loss 13.70
2020-06-08 08:33:43,201 EPOCH 37
2020-06-08 08:34:03,151 Epoch  37 Step:  1362400 Batch Loss:     0.196042 Tokens per Sec:     5701, Lr: 0.000200
2020-06-08 08:34:11,983 Epoch  37: total training loss 13.45
2020-06-08 08:34:11,984 EPOCH 38
2020-06-08 08:34:40,228 Epoch  38: total training loss 12.61
2020-06-08 08:34:40,229 EPOCH 39
2020-06-08 08:34:47,409 Epoch  39 Step:  1362500 Batch Loss:     0.218145 Tokens per Sec:     5387, Lr: 0.000200
2020-06-08 08:35:09,216 Epoch  39: total training loss 12.27
2020-06-08 08:35:09,217 EPOCH 40
2020-06-08 08:35:30,972 Epoch  40 Step:  1362600 Batch Loss:     0.209200 Tokens per Sec:     5750, Lr: 0.000200
2020-06-08 08:35:37,980 Epoch  40: total training loss 11.72
2020-06-08 08:35:37,981 EPOCH 41
2020-06-08 08:36:06,547 Epoch  41: total training loss 11.30
2020-06-08 08:36:06,548 EPOCH 42
2020-06-08 08:36:15,044 Epoch  42 Step:  1362700 Batch Loss:     0.193295 Tokens per Sec:     5618, Lr: 0.000200
2020-06-08 08:36:35,031 Epoch  42: total training loss 10.83
2020-06-08 08:36:35,032 EPOCH 43
2020-06-08 08:36:58,310 Epoch  43 Step:  1362800 Batch Loss:     0.181325 Tokens per Sec:     5804, Lr: 0.000200
2020-06-08 08:37:03,614 Epoch  43: total training loss 10.66
2020-06-08 08:37:03,615 EPOCH 44
2020-06-08 08:37:32,512 Epoch  44: total training loss 10.40
2020-06-08 08:37:32,513 EPOCH 45
2020-06-08 08:37:43,912 Epoch  45 Step:  1362900 Batch Loss:     0.147566 Tokens per Sec:     5302, Lr: 0.000200
2020-06-08 08:38:01,501 Epoch  45: total training loss 10.54
2020-06-08 08:38:01,502 EPOCH 46
2020-06-08 08:38:26,816 Epoch  46 Step:  1363000 Batch Loss:     0.133231 Tokens per Sec:     5763, Lr: 0.000200
2020-06-08 08:39:09,921 Example #0
2020-06-08 08:39:09,921 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-08 08:39:09,921 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-08 08:39:09,921 	Source:     Hello.
2020-06-08 08:39:09,921 	Reference:  Hallo,
2020-06-08 08:39:09,921 	Hypothesis: Hallo.
2020-06-08 08:39:09,921 Example #1
2020-06-08 08:39:09,922 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-08 08:39:09,922 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-08 08:39:09,922 	Source:     Hi, how can I help you?
2020-06-08 08:39:09,922 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:39:09,922 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:39:09,922 Example #2
2020-06-08 08:39:09,922 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-08 08:39:09,922 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-08 08:39:09,922 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-08 08:39:09,922 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-08 08:39:09,922 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-08 08:39:09,922 Example #3
2020-06-08 08:39:09,922 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-08 08:39:09,922 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-08 08:39:09,922 	Source:     Ok, what type of restaurant are you looking for?
2020-06-08 08:39:09,922 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-08 08:39:09,922 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-08 08:39:09,922 Validation result (greedy) at epoch  46, step  1363000: bleu:  44.99, loss: 21972.6348, ppl:   2.3588, duration: 43.1040s
2020-06-08 08:39:13,055 Epoch  46: total training loss 10.04
2020-06-08 08:39:13,056 EPOCH 47
2020-06-08 08:39:42,092 Epoch  47: total training loss 9.44
2020-06-08 08:39:42,093 EPOCH 48
2020-06-08 08:39:55,729 Epoch  48 Step:  1363100 Batch Loss:     0.066574 Tokens per Sec:     5171, Lr: 0.000200
2020-06-08 08:40:11,268 Epoch  48: total training loss 9.34
2020-06-08 08:40:11,269 EPOCH 49
2020-06-08 08:40:38,200 Epoch  49 Step:  1363200 Batch Loss:     0.144783 Tokens per Sec:     5742, Lr: 0.000200
2020-06-08 08:40:39,428 Epoch  49: total training loss 9.16
2020-06-08 08:40:39,429 EPOCH 50
2020-06-08 08:41:08,434 Epoch  50: total training loss 8.72
2020-06-08 08:41:08,435 EPOCH 51
2020-06-08 08:41:22,857 Epoch  51 Step:  1363300 Batch Loss:     0.100499 Tokens per Sec:     5396, Lr: 0.000200
2020-06-08 08:41:37,403 Epoch  51: total training loss 8.45
2020-06-08 08:41:37,404 EPOCH 52
2020-06-08 08:42:05,900 Epoch  52: total training loss 8.27
2020-06-08 08:42:05,900 EPOCH 53
2020-06-08 08:42:06,282 Epoch  53 Step:  1363400 Batch Loss:     0.138504 Tokens per Sec:     7764, Lr: 0.000200
2020-06-08 08:42:34,319 Epoch  53: total training loss 8.28
2020-06-08 08:42:34,320 EPOCH 54
2020-06-08 08:42:49,450 Epoch  54 Step:  1363500 Batch Loss:     0.155638 Tokens per Sec:     5764, Lr: 0.000200
2020-06-08 08:43:03,246 Epoch  54: total training loss 8.78
2020-06-08 08:43:03,247 EPOCH 55
2020-06-08 08:43:31,804 Epoch  55: total training loss 7.62
2020-06-08 08:43:31,804 EPOCH 56
2020-06-08 08:43:34,119 Epoch  56 Step:  1363600 Batch Loss:     0.138548 Tokens per Sec:     5497, Lr: 0.000200
2020-06-08 08:44:00,525 Epoch  56: total training loss 7.45
2020-06-08 08:44:00,526 EPOCH 57
2020-06-08 08:44:19,084 Epoch  57 Step:  1363700 Batch Loss:     0.109378 Tokens per Sec:     5574, Lr: 0.000200
2020-06-08 08:44:29,344 Epoch  57: total training loss 7.12
2020-06-08 08:44:29,345 EPOCH 58
2020-06-08 08:44:58,275 Epoch  58: total training loss 7.14
2020-06-08 08:44:58,276 EPOCH 59
2020-06-08 08:45:02,596 Epoch  59 Step:  1363800 Batch Loss:     0.103712 Tokens per Sec:     5556, Lr: 0.000200
2020-06-08 08:45:26,908 Epoch  59: total training loss 7.05
2020-06-08 08:45:26,909 EPOCH 60
2020-06-08 08:45:46,649 Epoch  60 Step:  1363900 Batch Loss:     0.122895 Tokens per Sec:     5634, Lr: 0.000200
2020-06-08 08:45:55,339 Epoch  60: total training loss 6.83
2020-06-08 08:45:55,340 EPOCH 61
2020-06-08 08:46:24,388 Epoch  61: total training loss 6.53
2020-06-08 08:46:24,389 EPOCH 62
2020-06-08 08:46:30,799 Epoch  62 Step:  1364000 Batch Loss:     0.103762 Tokens per Sec:     5753, Lr: 0.000200
2020-06-08 08:47:13,262 Example #0
2020-06-08 08:47:13,263 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-08 08:47:13,263 	Raw hypothesis: ['Hal@@', 'lo@@', ',']
2020-06-08 08:47:13,263 	Source:     Hello.
2020-06-08 08:47:13,263 	Reference:  Hallo,
2020-06-08 08:47:13,263 	Hypothesis: Hallo,
2020-06-08 08:47:13,263 Example #1
2020-06-08 08:47:13,263 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-08 08:47:13,263 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-08 08:47:13,264 	Source:     Hi, how can I help you?
2020-06-08 08:47:13,264 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:47:13,264 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:47:13,264 Example #2
2020-06-08 08:47:13,264 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-08 08:47:13,264 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-08 08:47:13,264 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-08 08:47:13,264 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-08 08:47:13,264 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-08 08:47:13,265 Example #3
2020-06-08 08:47:13,265 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-08 08:47:13,265 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-08 08:47:13,265 	Source:     Ok, what type of restaurant are you looking for?
2020-06-08 08:47:13,265 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-08 08:47:13,265 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-08 08:47:13,265 Validation result (greedy) at epoch  62, step  1364000: bleu:  45.24, loss: 22718.5449, ppl:   2.4286, duration: 42.4651s
2020-06-08 08:47:35,896 Epoch  62: total training loss 6.41
2020-06-08 08:47:35,897 EPOCH 63
2020-06-08 08:47:57,660 Epoch  63 Step:  1364100 Batch Loss:     0.097330 Tokens per Sec:     5706, Lr: 0.000200
2020-06-08 08:48:04,851 Epoch  63: total training loss 6.30
2020-06-08 08:48:04,852 EPOCH 64
2020-06-08 08:48:33,788 Epoch  64: total training loss 6.01
2020-06-08 08:48:33,789 EPOCH 65
2020-06-08 08:48:42,133 Epoch  65 Step:  1364200 Batch Loss:     0.087017 Tokens per Sec:     5965, Lr: 0.000200
2020-06-08 08:49:02,764 Epoch  65: total training loss 6.10
2020-06-08 08:49:02,765 EPOCH 66
2020-06-08 08:49:26,533 Epoch  66 Step:  1364300 Batch Loss:     0.113631 Tokens per Sec:     5753, Lr: 0.000200
2020-06-08 08:49:31,723 Epoch  66: total training loss 6.02
2020-06-08 08:49:31,724 EPOCH 67
2020-06-08 08:50:00,860 Epoch  67: total training loss 5.82
2020-06-08 08:50:00,861 EPOCH 68
2020-06-08 08:50:11,365 Epoch  68 Step:  1364400 Batch Loss:     0.085727 Tokens per Sec:     5461, Lr: 0.000200
2020-06-08 08:50:30,028 Epoch  68: total training loss 5.81
2020-06-08 08:50:30,029 EPOCH 69
2020-06-08 08:50:56,395 Epoch  69 Step:  1364500 Batch Loss:     0.072936 Tokens per Sec:     5453, Lr: 0.000200
2020-06-08 08:50:58,980 Epoch  69: total training loss 5.67
2020-06-08 08:50:58,981 EPOCH 70
2020-06-08 08:51:28,244 Epoch  70: total training loss 5.64
2020-06-08 08:51:28,245 EPOCH 71
2020-06-08 08:51:39,493 Epoch  71 Step:  1364600 Batch Loss:     0.094445 Tokens per Sec:     6023, Lr: 0.000200
2020-06-08 08:51:57,334 Epoch  71: total training loss 5.55
2020-06-08 08:51:57,335 EPOCH 72
2020-06-08 08:52:24,337 Epoch  72 Step:  1364700 Batch Loss:     0.092168 Tokens per Sec:     5755, Lr: 0.000200
2020-06-08 08:52:26,107 Epoch  72: total training loss 5.44
2020-06-08 08:52:26,107 EPOCH 73
2020-06-08 08:52:54,533 Epoch  73: total training loss 5.30
2020-06-08 08:52:54,533 EPOCH 74
2020-06-08 08:53:07,934 Epoch  74 Step:  1364800 Batch Loss:     0.086437 Tokens per Sec:     5767, Lr: 0.000200
2020-06-08 08:53:22,876 Epoch  74: total training loss 5.34
2020-06-08 08:53:22,877 EPOCH 75
2020-06-08 08:53:51,492 Epoch  75 Step:  1364900 Batch Loss:     0.092755 Tokens per Sec:     5695, Lr: 0.000200
2020-06-08 08:53:51,493 Epoch  75: total training loss 5.05
2020-06-08 08:53:51,493 EPOCH 76
2020-06-08 08:54:20,519 Epoch  76: total training loss 4.94
2020-06-08 08:54:20,520 EPOCH 77
2020-06-08 08:54:35,800 Epoch  77 Step:  1365000 Batch Loss:     0.081998 Tokens per Sec:     5822, Lr: 0.000200
2020-06-08 08:55:18,921 Example #0
2020-06-08 08:55:18,922 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-08 08:55:18,922 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-08 08:55:18,922 	Source:     Hello.
2020-06-08 08:55:18,922 	Reference:  Hallo,
2020-06-08 08:55:18,922 	Hypothesis: Hallo.
2020-06-08 08:55:18,922 Example #1
2020-06-08 08:55:18,922 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-08 08:55:18,922 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-08 08:55:18,922 	Source:     Hi, how can I help you?
2020-06-08 08:55:18,922 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:55:18,922 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-08 08:55:18,922 Example #2
2020-06-08 08:55:18,922 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-08 08:55:18,922 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-08 08:55:18,923 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-08 08:55:18,923 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-08 08:55:18,923 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-08 08:55:18,923 Example #3
2020-06-08 08:55:18,923 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-08 08:55:18,923 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-08 08:55:18,923 	Source:     Ok, what type of restaurant are you looking for?
2020-06-08 08:55:18,923 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-08 08:55:18,923 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-08 08:55:18,923 Validation result (greedy) at epoch  77, step  1365000: bleu:  46.00, loss: 23447.6406, ppl:   2.4987, duration: 43.1201s
2020-06-08 08:55:32,336 Epoch  77: total training loss 5.04
2020-06-08 08:55:32,337 EPOCH 78
2020-06-08 08:56:01,185 Epoch  78: total training loss 4.94
2020-06-08 08:56:01,186 EPOCH 79
2020-06-08 08:56:02,667 Epoch  79 Step:  1365100 Batch Loss:     0.085695 Tokens per Sec:     7564, Lr: 0.000200
2020-06-08 08:56:30,051 Epoch  79: total training loss 4.87
2020-06-08 08:56:30,052 EPOCH 80
2020-06-08 08:56:47,048 Epoch  80 Step:  1365200 Batch Loss:     0.080167 Tokens per Sec:     5678, Lr: 0.000200
2020-06-08 08:56:58,890 Epoch  80: total training loss 4.65
2020-06-08 08:56:58,891 EPOCH 81
2020-06-08 08:57:27,730 Epoch  81: total training loss 4.57
2020-06-08 08:57:27,731 EPOCH 82
2020-06-08 08:57:31,869 Epoch  82 Step:  1365300 Batch Loss:     0.078596 Tokens per Sec:     4766, Lr: 0.000200
2020-06-08 08:57:56,461 Epoch  82: total training loss 4.51
2020-06-08 08:57:56,462 EPOCH 83
2020-06-08 08:58:16,623 Epoch  83 Step:  1365400 Batch Loss:     0.075072 Tokens per Sec:     5354, Lr: 0.000200
2020-06-08 08:58:25,545 Epoch  83: total training loss 4.39
2020-06-08 08:58:25,546 EPOCH 84
2020-06-08 08:58:54,134 Epoch  84: total training loss 4.37
2020-06-08 08:58:54,135 EPOCH 85
2020-06-08 08:58:59,568 Epoch  85 Step:  1365500 Batch Loss:     0.077300 Tokens per Sec:     6477, Lr: 0.000200
2020-06-08 08:59:23,057 Epoch  85: total training loss 4.29
2020-06-08 08:59:23,058 EPOCH 86
2020-06-08 08:59:44,663 Epoch  86 Step:  1365600 Batch Loss:     0.057829 Tokens per Sec:     5547, Lr: 0.000200
2020-06-08 08:59:51,874 Epoch  86: total training loss 4.20
2020-06-08 08:59:51,875 EPOCH 87
2020-06-08 09:00:20,556 Epoch  87: total training loss 4.15
2020-06-08 09:00:20,557 EPOCH 88
2020-06-08 09:00:28,950 Epoch  88 Step:  1365700 Batch Loss:     0.054430 Tokens per Sec:     5106, Lr: 0.000200
2020-06-08 09:00:49,557 Epoch  88: total training loss 4.08
2020-06-08 09:00:49,559 EPOCH 89
2020-06-08 09:01:12,425 Epoch  89 Step:  1365800 Batch Loss:     0.065185 Tokens per Sec:     5679, Lr: 0.000200
2020-06-08 09:01:18,724 Epoch  89: total training loss 4.22
2020-06-08 09:01:18,724 EPOCH 90
2020-06-08 09:01:47,370 Epoch  90: total training loss 3.95
2020-06-08 09:01:47,371 EPOCH 91
2020-06-08 09:01:57,924 Epoch  91 Step:  1365900 Batch Loss:     0.063335 Tokens per Sec:     5337, Lr: 0.000200
2020-06-08 09:02:16,532 Epoch  91: total training loss 3.94
2020-06-08 09:02:16,532 EPOCH 92
2020-06-08 09:02:41,770 Epoch  92 Step:  1366000 Batch Loss:     0.069833 Tokens per Sec:     5819, Lr: 0.000200
2020-06-08 09:03:23,367 Example #0
2020-06-08 09:03:23,368 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-08 09:03:23,368 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-08 09:03:23,368 	Source:     Hello.
2020-06-08 09:03:23,368 	Reference:  Hallo,
2020-06-08 09:03:23,368 	Hypothesis: Hallo.
2020-06-08 09:03:23,368 Example #1
2020-06-08 09:03:23,368 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-08 09:03:23,368 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-08 09:03:23,369 	Source:     Hi, how can I help you?
2020-06-08 09:03:23,369 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-08 09:03:23,369 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-08 09:03:23,369 Example #2
2020-06-08 09:03:23,369 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-08 09:03:23,369 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-08 09:03:23,369 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-08 09:03:23,369 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-08 09:03:23,369 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-08 09:03:23,370 Example #3
2020-06-08 09:03:23,370 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-08 09:03:23,370 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-08 09:03:23,370 	Source:     Ok, what type of restaurant are you looking for?
2020-06-08 09:03:23,370 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-08 09:03:23,370 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-08 09:03:23,370 Validation result (greedy) at epoch  92, step  1366000: bleu:  45.36, loss: 24066.4492, ppl:   2.5599, duration: 41.5990s
2020-06-08 09:03:26,505 Epoch  92: total training loss 3.81
2020-06-08 09:03:26,505 EPOCH 93
2020-06-08 09:03:55,582 Epoch  93: total training loss 4.50
2020-06-08 09:03:55,583 EPOCH 94
2020-06-08 09:04:07,232 Epoch  94 Step:  1366100 Batch Loss:     0.060045 Tokens per Sec:     5919, Lr: 0.000200
2020-06-08 09:04:24,375 Epoch  94: total training loss 3.98
2020-06-08 09:04:24,376 EPOCH 95
2020-06-08 09:04:51,444 Epoch  95 Step:  1366200 Batch Loss:     0.039244 Tokens per Sec:     5761, Lr: 0.000200
2020-06-08 09:04:53,192 Epoch  95: total training loss 3.90
2020-06-08 09:04:53,192 EPOCH 96
2020-06-08 09:05:22,018 Epoch  96: total training loss 3.85
2020-06-08 09:05:22,019 EPOCH 97
2020-06-08 09:05:35,182 Epoch  97 Step:  1366300 Batch Loss:     0.063622 Tokens per Sec:     5747, Lr: 0.000200
2020-06-08 09:05:51,133 Epoch  97: total training loss 3.67
2020-06-08 09:05:51,134 EPOCH 98
2020-06-08 09:06:19,990 Epoch  98 Step:  1366400 Batch Loss:     0.061659 Tokens per Sec:     5647, Lr: 0.000200
2020-06-08 09:06:19,992 Epoch  98: total training loss 3.60
2020-06-08 09:06:19,992 EPOCH 99
2020-06-08 09:06:49,285 Epoch  99: total training loss 3.53
2020-06-08 09:06:49,286 EPOCH 100
2020-06-08 09:07:05,821 Epoch 100 Step:  1366500 Batch Loss:     0.058639 Tokens per Sec:     5289, Lr: 0.000200
2020-06-08 09:07:18,891 Epoch 100: total training loss 3.53
2020-06-08 09:07:18,892 Training ended after 100 epochs.
2020-06-08 09:07:18,892 Best validation result (greedy) at step  1361000:   2.23 ppl.
2020-06-08 09:08:35,162  dev bleu:  45.06 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-08 09:08:35,171 Translations saved to: models/transformer_multi_enc_shared_ende-tune/01361000.hyps.dev
2020-06-08 09:09:06,353 test bleu:  41.93 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-08 09:09:06,360 Translations saved to: models/transformer_multi_enc_shared_ende-tune/01361000.hyps.test
