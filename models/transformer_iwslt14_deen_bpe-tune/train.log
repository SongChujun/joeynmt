2020-06-17 17:54:12,423 Hello! This is Joey-NMT.
2020-06-17 17:54:32,508 Total params: 19179520
2020-06-17 17:54:32,510 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-06-17 17:54:39,243 Loading model from models/transformer_iwslt14_deen_bpe/best.ckpt
2020-06-17 17:54:43,290 Reset optimizer.
2020-06-17 17:54:43,290 Reset scheduler.
2020-06-17 17:54:43,290 Reset tracking of the best checkpoint.
2020-06-17 17:54:43,309 cfg.name                           : iwslt14-deen-bpe-transformer-tune
2020-06-17 17:54:43,310 cfg.data.src                       : de
2020-06-17 17:54:43,310 cfg.data.trg                       : en
2020-06-17 17:54:43,310 cfg.data.train                     : chatnmt/prep/train.tags.bpe.iwslt14-deen-bpe
2020-06-17 17:54:43,310 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.iwslt14-deen-bpe
2020-06-17 17:54:43,310 cfg.data.test                      : chatnmt/prep/test.tags.bpe.iwslt14-deen-bpe
2020-06-17 17:54:43,310 cfg.data.level                     : bpe
2020-06-17 17:54:43,310 cfg.data.lowercase                 : True
2020-06-17 17:54:43,310 cfg.data.max_sent_length           : 62
2020-06-17 17:54:43,310 cfg.data.src_vocab                 : models/transformer_iwslt14_deen_bpe/src_vocab.txt
2020-06-17 17:54:43,310 cfg.data.trg_vocab                 : models/transformer_iwslt14_deen_bpe/trg_vocab.txt
2020-06-17 17:54:43,310 cfg.testing.beam_size              : 5
2020-06-17 17:54:43,310 cfg.testing.alpha                  : 1.0
2020-06-17 17:54:43,310 cfg.training.random_seed           : 42
2020-06-17 17:54:43,310 cfg.training.optimizer             : adam
2020-06-17 17:54:43,310 cfg.training.normalization         : tokens
2020-06-17 17:54:43,310 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-17 17:54:43,310 cfg.training.scheduling            : plateau
2020-06-17 17:54:43,310 cfg.training.patience              : 5
2020-06-17 17:54:43,311 cfg.training.decrease_factor       : 0.7
2020-06-17 17:54:43,311 cfg.training.loss                  : crossentropy
2020-06-17 17:54:43,311 cfg.training.learning_rate         : 0.0003
2020-06-17 17:54:43,311 cfg.training.learning_rate_min     : 1e-08
2020-06-17 17:54:43,311 cfg.training.weight_decay          : 0.0
2020-06-17 17:54:43,311 cfg.training.label_smoothing       : 0.1
2020-06-17 17:54:43,311 cfg.training.batch_size            : 4096
2020-06-17 17:54:43,311 cfg.training.batch_type            : token
2020-06-17 17:54:43,311 cfg.training.early_stopping_metric : eval_metric
2020-06-17 17:54:43,311 cfg.training.epochs                : 100
2020-06-17 17:54:43,311 cfg.training.validation_freq       : 1000
2020-06-17 17:54:43,311 cfg.training.logging_freq          : 100
2020-06-17 17:54:43,311 cfg.training.eval_metric           : bleu
2020-06-17 17:54:43,311 cfg.training.model_dir             : models/transformer_iwslt14_deen_bpe-tune
2020-06-17 17:54:43,311 cfg.training.load_model            : models/transformer_iwslt14_deen_bpe/best.ckpt
2020-06-17 17:54:43,311 cfg.training.reset_best_ckpt       : True
2020-06-17 17:54:43,311 cfg.training.reset_scheduler       : True
2020-06-17 17:54:43,311 cfg.training.reset_optimizer       : True
2020-06-17 17:54:43,311 cfg.training.overwrite             : False
2020-06-17 17:54:43,311 cfg.training.shuffle               : True
2020-06-17 17:54:43,311 cfg.training.use_cuda              : True
2020-06-17 17:54:43,312 cfg.training.max_output_length     : 100
2020-06-17 17:54:43,312 cfg.training.print_valid_sents     : [0, 1, 2, 3, 4]
2020-06-17 17:54:43,312 cfg.training.keep_last_ckpts       : 5
2020-06-17 17:54:43,312 cfg.model.initializer              : xavier
2020-06-17 17:54:43,312 cfg.model.embed_initializer        : xavier
2020-06-17 17:54:43,312 cfg.model.embed_init_gain          : 1.0
2020-06-17 17:54:43,312 cfg.model.init_gain                : 1.0
2020-06-17 17:54:43,312 cfg.model.bias_initializer         : zeros
2020-06-17 17:54:43,312 cfg.model.tied_embeddings          : True
2020-06-17 17:54:43,312 cfg.model.tied_softmax             : True
2020-06-17 17:54:43,312 cfg.model.encoder.type             : transformer
2020-06-17 17:54:43,312 cfg.model.encoder.num_layers       : 6
2020-06-17 17:54:43,312 cfg.model.encoder.num_heads        : 4
2020-06-17 17:54:43,312 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-17 17:54:43,312 cfg.model.encoder.embeddings.scale : True
2020-06-17 17:54:43,312 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-17 17:54:43,312 cfg.model.encoder.hidden_size      : 256
2020-06-17 17:54:43,312 cfg.model.encoder.ff_size          : 1024
2020-06-17 17:54:43,312 cfg.model.encoder.dropout          : 0.3
2020-06-17 17:54:43,312 cfg.model.decoder.type             : transformer
2020-06-17 17:54:43,313 cfg.model.decoder.num_layers       : 6
2020-06-17 17:54:43,313 cfg.model.decoder.num_heads        : 4
2020-06-17 17:54:43,313 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-17 17:54:43,313 cfg.model.decoder.embeddings.scale : True
2020-06-17 17:54:43,313 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-17 17:54:43,313 cfg.model.decoder.hidden_size      : 256
2020-06-17 17:54:43,313 cfg.model.decoder.ff_size          : 1024
2020-06-17 17:54:43,313 cfg.model.decoder.dropout          : 0.3
2020-06-17 17:54:43,313 Data set sizes: 
	train 9688,
	valid 1525,
	test 1165
2020-06-17 17:54:43,313 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-17 17:54:43,313 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) in (9) und
2020-06-17 17:54:43,313 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) in (9) und
2020-06-17 17:54:43,313 Number of Src words (types): 31716
2020-06-17 17:54:43,313 Number of Trg words (types): 31716
2020-06-17 17:54:43,313 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=256, vocab_size=31716),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=31716))
2020-06-17 17:54:43,350 EPOCH 1
2020-06-17 17:54:50,788 Epoch   1: total training loss 109.54
2020-06-17 17:54:50,789 EPOCH 2
2020-06-17 17:54:55,979 Epoch   2 Step:    32100 Batch Loss:     1.778443 Tokens per Sec:    23074, Lr: 0.000300
2020-06-17 17:54:56,873 Epoch   2: total training loss 77.67
2020-06-17 17:54:56,873 EPOCH 3
2020-06-17 17:55:03,339 Epoch   3: total training loss 65.94
2020-06-17 17:55:03,340 EPOCH 4
2020-06-17 17:55:08,848 Epoch   4 Step:    32200 Batch Loss:     0.452142 Tokens per Sec:    18590, Lr: 0.000300
2020-06-17 17:55:10,301 Epoch   4: total training loss 58.68
2020-06-17 17:55:10,301 EPOCH 5
2020-06-17 17:55:16,820 Epoch   5: total training loss 53.22
2020-06-17 17:55:16,821 EPOCH 6
2020-06-17 17:55:20,935 Epoch   6 Step:    32300 Batch Loss:     1.121324 Tokens per Sec:    21560, Lr: 0.000300
2020-06-17 17:55:23,335 Epoch   6: total training loss 49.28
2020-06-17 17:55:23,335 EPOCH 7
2020-06-17 17:55:30,419 Epoch   7: total training loss 47.02
2020-06-17 17:55:30,420 EPOCH 8
2020-06-17 17:55:33,543 Epoch   8 Step:    32400 Batch Loss:     0.955948 Tokens per Sec:    22245, Lr: 0.000300
2020-06-17 17:55:36,968 Epoch   8: total training loss 43.12
2020-06-17 17:55:36,969 EPOCH 9
2020-06-17 17:55:43,807 Epoch   9: total training loss 39.39
2020-06-17 17:55:43,808 EPOCH 10
2020-06-17 17:55:46,717 Epoch  10 Step:    32500 Batch Loss:     0.608602 Tokens per Sec:    20210, Lr: 0.000300
2020-06-17 17:55:50,734 Epoch  10: total training loss 37.90
2020-06-17 17:55:50,735 EPOCH 11
2020-06-17 17:55:57,688 Epoch  11: total training loss 36.28
2020-06-17 17:55:57,689 EPOCH 12
2020-06-17 17:55:59,811 Epoch  12 Step:    32600 Batch Loss:     0.552739 Tokens per Sec:    20013, Lr: 0.000300
2020-06-17 17:56:04,712 Epoch  12: total training loss 34.55
2020-06-17 17:56:04,713 EPOCH 13
2020-06-17 17:56:11,857 Epoch  13: total training loss 32.96
2020-06-17 17:56:11,858 EPOCH 14
2020-06-17 17:56:13,232 Epoch  14 Step:    32700 Batch Loss:     0.492669 Tokens per Sec:    18978, Lr: 0.000300
2020-06-17 17:56:18,995 Epoch  14: total training loss 31.55
2020-06-17 17:56:18,996 EPOCH 15
2020-06-17 17:56:26,137 Epoch  15: total training loss 30.56
2020-06-17 17:56:26,140 EPOCH 16
2020-06-17 17:56:26,680 Epoch  16 Step:    32800 Batch Loss:     0.457525 Tokens per Sec:    19430, Lr: 0.000300
2020-06-17 17:56:32,696 Epoch  16: total training loss 29.10
2020-06-17 17:56:32,697 EPOCH 17
2020-06-17 17:56:39,560 Epoch  17 Step:    32900 Batch Loss:     0.397064 Tokens per Sec:    19151, Lr: 0.000300
2020-06-17 17:56:39,853 Epoch  17: total training loss 28.13
2020-06-17 17:56:39,854 EPOCH 18
2020-06-17 17:56:47,093 Epoch  18: total training loss 28.02
2020-06-17 17:56:47,093 EPOCH 19
2020-06-17 17:56:53,107 Epoch  19 Step:    33000 Batch Loss:     0.392690 Tokens per Sec:    19107, Lr: 0.000300
2020-06-17 17:57:12,936 Hooray! New best validation result [eval_metric]!
2020-06-17 17:57:12,937 Saving new checkpoint.
2020-06-17 17:57:15,407 Example #0
2020-06-17 17:57:15,408 	Raw source:     ['hallo', ',']
2020-06-17 17:57:15,408 	Raw hypothesis: ['hi', '.']
2020-06-17 17:57:15,408 	Source:     hallo ,
2020-06-17 17:57:15,408 	Reference:  hello .
2020-06-17 17:57:15,408 	Hypothesis: hi .
2020-06-17 17:57:15,408 Example #1
2020-06-17 17:57:15,408 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 17:57:15,408 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 17:57:15,408 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 17:57:15,408 	Reference:  hi , how can i help you ?
2020-06-17 17:57:15,408 	Hypothesis: hi , how can i help you ?
2020-06-17 17:57:15,408 Example #2
2020-06-17 17:57:15,408 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 17:57:15,408 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 17:57:15,408 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 17:57:15,408 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 17:57:15,408 	Hypothesis: hi , i &apos;m looking for a restaurant at the arden fair mall in san francisco , california .
2020-06-17 17:57:15,408 Example #3
2020-06-17 17:57:15,408 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 17:57:15,408 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 17:57:15,408 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 17:57:15,408 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 17:57:15,408 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-17 17:57:15,408 Example #4
2020-06-17 17:57:15,408 	Raw source:     ['ich', 'suche', 'ein', 'gün@@', 'stiges', 'fa@@', 'st@@', 'food-@@', 'restaurant', '.']
2020-06-17 17:57:15,408 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'a', 'inexpensive', 'food', 'restaurant', '.']
2020-06-17 17:57:15,408 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 17:57:15,408 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 17:57:15,408 	Hypothesis: i &apos;m looking for a inexpensive food restaurant .
2020-06-17 17:57:15,408 Validation result (greedy) at epoch  19, step    33000: bleu:  54.99, loss: 23432.4355, ppl:   2.8468, duration: 22.3003s
2020-06-17 17:57:16,608 Epoch  19: total training loss 26.88
2020-06-17 17:57:16,609 EPOCH 20
2020-06-17 17:57:23,793 Epoch  20: total training loss 25.54
2020-06-17 17:57:23,794 EPOCH 21
2020-06-17 17:57:28,928 Epoch  21 Step:    33100 Batch Loss:     0.438269 Tokens per Sec:    19167, Lr: 0.000300
2020-06-17 17:57:30,961 Epoch  21: total training loss 24.71
2020-06-17 17:57:30,967 EPOCH 22
2020-06-17 17:57:38,183 Epoch  22: total training loss 23.58
2020-06-17 17:57:38,184 EPOCH 23
2020-06-17 17:57:42,482 Epoch  23 Step:    33200 Batch Loss:     0.502613 Tokens per Sec:    19271, Lr: 0.000300
2020-06-17 17:57:45,300 Epoch  23: total training loss 22.86
2020-06-17 17:57:45,301 EPOCH 24
2020-06-17 17:57:52,469 Epoch  24: total training loss 22.18
2020-06-17 17:57:52,469 EPOCH 25
2020-06-17 17:57:55,872 Epoch  25 Step:    33300 Batch Loss:     0.427837 Tokens per Sec:    19923, Lr: 0.000300
2020-06-17 17:57:59,240 Epoch  25: total training loss 21.49
2020-06-17 17:57:59,245 EPOCH 26
2020-06-17 17:58:06,309 Epoch  26: total training loss 20.93
2020-06-17 17:58:06,309 EPOCH 27
2020-06-17 17:58:09,014 Epoch  27 Step:    33400 Batch Loss:     0.418414 Tokens per Sec:    19456, Lr: 0.000300
2020-06-17 17:58:13,445 Epoch  27: total training loss 20.43
2020-06-17 17:58:13,445 EPOCH 28
2020-06-17 17:58:20,566 Epoch  28: total training loss 19.88
2020-06-17 17:58:20,566 EPOCH 29
2020-06-17 17:58:22,544 Epoch  29 Step:    33500 Batch Loss:     0.342114 Tokens per Sec:    18409, Lr: 0.000300
2020-06-17 17:58:27,961 Epoch  29: total training loss 19.22
2020-06-17 17:58:27,961 EPOCH 30
2020-06-17 17:58:35,568 Epoch  30: total training loss 19.09
2020-06-17 17:58:35,568 EPOCH 31
2020-06-17 17:58:36,639 Epoch  31 Step:    33600 Batch Loss:     0.373573 Tokens per Sec:    18037, Lr: 0.000300
2020-06-17 17:58:42,704 Epoch  31: total training loss 18.53
2020-06-17 17:58:42,704 EPOCH 32
2020-06-17 17:58:49,793 Epoch  32: total training loss 18.02
2020-06-17 17:58:49,794 EPOCH 33
2020-06-17 17:58:50,059 Epoch  33 Step:    33700 Batch Loss:     0.219650 Tokens per Sec:    16068, Lr: 0.000300
2020-06-17 17:58:56,809 Epoch  33: total training loss 17.45
2020-06-17 17:58:56,812 EPOCH 34
2020-06-17 17:59:03,067 Epoch  34 Step:    33800 Batch Loss:     0.264987 Tokens per Sec:    20340, Lr: 0.000300
2020-06-17 17:59:03,600 Epoch  34: total training loss 17.02
2020-06-17 17:59:03,600 EPOCH 35
2020-06-17 17:59:10,736 Epoch  35: total training loss 16.30
2020-06-17 17:59:10,736 EPOCH 36
2020-06-17 17:59:16,621 Epoch  36 Step:    33900 Batch Loss:     0.310603 Tokens per Sec:    18879, Lr: 0.000300
2020-06-17 17:59:17,969 Epoch  36: total training loss 16.60
2020-06-17 17:59:17,969 EPOCH 37
2020-06-17 17:59:24,965 Epoch  37: total training loss 15.91
2020-06-17 17:59:24,970 EPOCH 38
2020-06-17 17:59:29,918 Epoch  38 Step:    34000 Batch Loss:     0.319918 Tokens per Sec:    19268, Lr: 0.000300
2020-06-17 17:59:47,039 Hooray! New best validation result [eval_metric]!
2020-06-17 17:59:47,040 Saving new checkpoint.
2020-06-17 17:59:49,577 Example #0
2020-06-17 17:59:49,578 	Raw source:     ['hallo', ',']
2020-06-17 17:59:49,578 	Raw hypothesis: ['hello', '?']
2020-06-17 17:59:49,578 	Source:     hallo ,
2020-06-17 17:59:49,578 	Reference:  hello .
2020-06-17 17:59:49,578 	Hypothesis: hello ?
2020-06-17 17:59:49,578 Example #1
2020-06-17 17:59:49,578 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 17:59:49,578 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 17:59:49,578 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 17:59:49,578 	Reference:  hi , how can i help you ?
2020-06-17 17:59:49,578 	Hypothesis: hi , how can i help you ?
2020-06-17 17:59:49,578 Example #2
2020-06-17 17:59:49,578 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 17:59:49,578 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 17:59:49,578 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 17:59:49,578 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 17:59:49,578 	Hypothesis: hi , i was looking for a restaurant at the arden fair mall in san francisco , california .
2020-06-17 17:59:49,578 Example #3
2020-06-17 17:59:49,578 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 17:59:49,578 	Raw hypothesis: ['ok', '.', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', 'for', '?']
2020-06-17 17:59:49,578 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 17:59:49,578 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 17:59:49,578 	Hypothesis: ok . what type of restaurant are you looking for for ?
2020-06-17 17:59:49,578 Example #4
2020-06-17 17:59:49,578 	Raw source:     ['ich', 'suche', 'ein', 'gün@@', 'stiges', 'fa@@', 'st@@', 'food-@@', 'restaurant', '.']
2020-06-17 17:59:49,578 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'an', 'inexpensive', 'food', 'restaurant', '.']
2020-06-17 17:59:49,578 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 17:59:49,579 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 17:59:49,579 	Hypothesis: i &apos;m looking for an inexpensive food restaurant .
2020-06-17 17:59:49,579 Validation result (greedy) at epoch  38, step    34000: bleu:  56.38, loss: 24990.1875, ppl:   3.0518, duration: 19.6595s
2020-06-17 17:59:51,714 Epoch  38: total training loss 15.59
2020-06-17 17:59:51,715 EPOCH 39
2020-06-17 17:59:58,804 Epoch  39: total training loss 15.24
2020-06-17 17:59:58,805 EPOCH 40
2020-06-17 18:00:02,827 Epoch  40 Step:    34100 Batch Loss:     0.279811 Tokens per Sec:    19854, Lr: 0.000300
2020-06-17 18:00:05,782 Epoch  40: total training loss 14.95
2020-06-17 18:00:05,783 EPOCH 41
2020-06-17 18:00:12,883 Epoch  41: total training loss 14.91
2020-06-17 18:00:12,883 EPOCH 42
2020-06-17 18:00:16,111 Epoch  42 Step:    34200 Batch Loss:     0.270070 Tokens per Sec:    19860, Lr: 0.000300
2020-06-17 18:00:19,925 Epoch  42: total training loss 14.37
2020-06-17 18:00:19,926 EPOCH 43
2020-06-17 18:00:26,996 Epoch  43: total training loss 14.16
2020-06-17 18:00:26,997 EPOCH 44
2020-06-17 18:00:29,450 Epoch  44 Step:    34300 Batch Loss:     0.260009 Tokens per Sec:    19614, Lr: 0.000300
2020-06-17 18:00:34,069 Epoch  44: total training loss 14.04
2020-06-17 18:00:34,073 EPOCH 45
2020-06-17 18:00:41,126 Epoch  45: total training loss 13.72
2020-06-17 18:00:41,126 EPOCH 46
2020-06-17 18:00:42,598 Epoch  46 Step:    34400 Batch Loss:     0.246449 Tokens per Sec:    19363, Lr: 0.000300
2020-06-17 18:00:48,166 Epoch  46: total training loss 13.29
2020-06-17 18:00:48,167 EPOCH 47
2020-06-17 18:00:55,219 Epoch  47: total training loss 13.11
2020-06-17 18:00:55,220 EPOCH 48
2020-06-17 18:00:55,940 Epoch  48 Step:    34500 Batch Loss:     0.237959 Tokens per Sec:    18011, Lr: 0.000300
2020-06-17 18:01:02,468 Epoch  48: total training loss 13.17
2020-06-17 18:01:02,469 EPOCH 49
2020-06-17 18:01:09,336 Epoch  49 Step:    34600 Batch Loss:     0.262897 Tokens per Sec:    19266, Lr: 0.000300
2020-06-17 18:01:09,595 Epoch  49: total training loss 12.76
2020-06-17 18:01:09,595 EPOCH 50
2020-06-17 18:01:16,747 Epoch  50: total training loss 12.66
2020-06-17 18:01:16,747 EPOCH 51
2020-06-17 18:01:22,650 Epoch  51 Step:    34700 Batch Loss:     0.240183 Tokens per Sec:    19541, Lr: 0.000300
2020-06-17 18:01:23,873 Epoch  51: total training loss 12.38
2020-06-17 18:01:23,874 EPOCH 52
2020-06-17 18:01:31,066 Epoch  52: total training loss 12.29
2020-06-17 18:01:31,071 EPOCH 53
2020-06-17 18:01:35,924 Epoch  53 Step:    34800 Batch Loss:     0.200348 Tokens per Sec:    19575, Lr: 0.000300
2020-06-17 18:01:38,155 Epoch  53: total training loss 12.11
2020-06-17 18:01:38,156 EPOCH 54
2020-06-17 18:01:45,270 Epoch  54: total training loss 11.64
2020-06-17 18:01:45,271 EPOCH 55
2020-06-17 18:01:49,313 Epoch  55 Step:    34900 Batch Loss:     0.194671 Tokens per Sec:    19443, Lr: 0.000300
2020-06-17 18:01:52,425 Epoch  55: total training loss 11.58
2020-06-17 18:01:52,426 EPOCH 56
2020-06-17 18:01:59,361 Epoch  56: total training loss 11.33
2020-06-17 18:01:59,362 EPOCH 57
2020-06-17 18:02:02,286 Epoch  57 Step:    35000 Batch Loss:     0.198072 Tokens per Sec:    21465, Lr: 0.000300
2020-06-17 18:02:18,822 Example #0
2020-06-17 18:02:18,822 	Raw source:     ['hallo', ',']
2020-06-17 18:02:18,822 	Raw hypothesis: ['hello', '?']
2020-06-17 18:02:18,822 	Source:     hallo ,
2020-06-17 18:02:18,822 	Reference:  hello .
2020-06-17 18:02:18,822 	Hypothesis: hello ?
2020-06-17 18:02:18,822 Example #1
2020-06-17 18:02:18,822 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 18:02:18,822 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 18:02:18,822 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 18:02:18,822 	Reference:  hi , how can i help you ?
2020-06-17 18:02:18,822 	Hypothesis: hi , how can i help you ?
2020-06-17 18:02:18,822 Example #2
2020-06-17 18:02:18,822 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 18:02:18,822 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 18:02:18,823 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 18:02:18,823 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 18:02:18,823 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 18:02:18,823 Example #3
2020-06-17 18:02:18,823 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 18:02:18,823 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 18:02:18,823 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 18:02:18,823 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 18:02:18,823 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 18:02:18,823 Example #4
2020-06-17 18:02:18,823 	Raw source:     ['ich', 'suche', 'ein', 'gün@@', 'stiges', 'fa@@', 'st@@', 'food-@@', 'restaurant', '.']
2020-06-17 18:02:18,823 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'an', 'inexpensive', 'food', 'restaurant', '.']
2020-06-17 18:02:18,823 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 18:02:18,823 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 18:02:18,823 	Hypothesis: i &apos;m looking for an inexpensive food restaurant .
2020-06-17 18:02:18,823 Validation result (greedy) at epoch  57, step    35000: bleu:  56.27, loss: 26201.0508, ppl:   3.2213, duration: 16.5363s
2020-06-17 18:02:22,736 Epoch  57: total training loss 11.09
2020-06-17 18:02:22,736 EPOCH 58
2020-06-17 18:02:29,849 Epoch  58: total training loss 11.05
2020-06-17 18:02:29,850 EPOCH 59
2020-06-17 18:02:32,105 Epoch  59 Step:    35100 Batch Loss:     0.207801 Tokens per Sec:    20800, Lr: 0.000300
2020-06-17 18:02:36,822 Epoch  59: total training loss 11.14
2020-06-17 18:02:36,823 EPOCH 60
2020-06-17 18:02:43,973 Epoch  60: total training loss 10.95
2020-06-17 18:02:43,973 EPOCH 61
2020-06-17 18:02:45,475 Epoch  61 Step:    35200 Batch Loss:     0.195365 Tokens per Sec:    19479, Lr: 0.000300
2020-06-17 18:02:51,152 Epoch  61: total training loss 10.61
2020-06-17 18:02:51,153 EPOCH 62
2020-06-17 18:02:58,168 Epoch  62: total training loss 10.43
2020-06-17 18:02:58,169 EPOCH 63
2020-06-17 18:02:58,856 Epoch  63 Step:    35300 Batch Loss:     0.194222 Tokens per Sec:    19749, Lr: 0.000300
2020-06-17 18:03:05,246 Epoch  63: total training loss 10.28
2020-06-17 18:03:05,246 EPOCH 64
2020-06-17 18:03:11,563 Epoch  64 Step:    35400 Batch Loss:     0.191126 Tokens per Sec:    21018, Lr: 0.000300
2020-06-17 18:03:11,799 Epoch  64: total training loss 10.18
2020-06-17 18:03:11,799 EPOCH 65
2020-06-17 18:03:18,617 Epoch  65: total training loss 10.04
2020-06-17 18:03:18,618 EPOCH 66
2020-06-17 18:03:24,135 Epoch  66 Step:    35500 Batch Loss:     0.183417 Tokens per Sec:    21292, Lr: 0.000300
2020-06-17 18:03:25,101 Epoch  66: total training loss 9.79
2020-06-17 18:03:25,101 EPOCH 67
2020-06-17 18:03:31,846 Epoch  67: total training loss 9.93
2020-06-17 18:03:31,846 EPOCH 68
2020-06-17 18:03:36,486 Epoch  68 Step:    35600 Batch Loss:     0.192872 Tokens per Sec:    21497, Lr: 0.000300
2020-06-17 18:03:38,334 Epoch  68: total training loss 9.56
2020-06-17 18:03:38,335 EPOCH 69
2020-06-17 18:03:45,288 Epoch  69: total training loss 9.72
2020-06-17 18:03:45,288 EPOCH 70
2020-06-17 18:03:49,312 Epoch  70 Step:    35700 Batch Loss:     0.164625 Tokens per Sec:    20056, Lr: 0.000300
2020-06-17 18:03:52,238 Epoch  70: total training loss 9.38
2020-06-17 18:03:52,242 EPOCH 71
2020-06-17 18:03:59,409 Epoch  71: total training loss 9.24
2020-06-17 18:03:59,410 EPOCH 72
2020-06-17 18:04:02,618 Epoch  72 Step:    35800 Batch Loss:     0.180126 Tokens per Sec:    19741, Lr: 0.000300
2020-06-17 18:04:06,112 Epoch  72: total training loss 9.20
2020-06-17 18:04:06,113 EPOCH 73
2020-06-17 18:04:12,934 Epoch  73: total training loss 9.06
2020-06-17 18:04:12,934 EPOCH 74
2020-06-17 18:04:15,489 Epoch  74 Step:    35900 Batch Loss:     0.156487 Tokens per Sec:    18391, Lr: 0.000300
2020-06-17 18:04:19,978 Epoch  74: total training loss 8.93
2020-06-17 18:04:19,979 EPOCH 75
2020-06-17 18:04:26,920 Epoch  75: total training loss 8.59
2020-06-17 18:04:26,920 EPOCH 76
2020-06-17 18:04:28,763 Epoch  76 Step:    36000 Batch Loss:     0.159631 Tokens per Sec:    19461, Lr: 0.000300
2020-06-17 18:04:45,303 Hooray! New best validation result [eval_metric]!
2020-06-17 18:04:45,304 Saving new checkpoint.
2020-06-17 18:04:47,839 Example #0
2020-06-17 18:04:47,839 	Raw source:     ['hallo', ',']
2020-06-17 18:04:47,839 	Raw hypothesis: ['hello', '?']
2020-06-17 18:04:47,839 	Source:     hallo ,
2020-06-17 18:04:47,840 	Reference:  hello .
2020-06-17 18:04:47,840 	Hypothesis: hello ?
2020-06-17 18:04:47,840 Example #1
2020-06-17 18:04:47,840 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 18:04:47,840 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 18:04:47,840 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 18:04:47,840 	Reference:  hi , how can i help you ?
2020-06-17 18:04:47,840 	Hypothesis: hi , how can i help you ?
2020-06-17 18:04:47,840 Example #2
2020-06-17 18:04:47,840 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 18:04:47,840 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 18:04:47,840 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 18:04:47,840 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 18:04:47,840 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 18:04:47,840 Example #3
2020-06-17 18:04:47,840 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 18:04:47,840 	Raw hypothesis: ['ok', '.', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', 'for', 'that', 'right', '?']
2020-06-17 18:04:47,840 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 18:04:47,840 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 18:04:47,840 	Hypothesis: ok . what type of restaurant are you looking for for that right ?
2020-06-17 18:04:47,840 Example #4
2020-06-17 18:04:47,840 	Raw source:     ['ich', 'suche', 'ein', 'gün@@', 'stiges', 'fa@@', 'st@@', 'food-@@', 'restaurant', '.']
2020-06-17 18:04:47,840 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'an', 'inexpensive', 'food', 'restaurant', '.']
2020-06-17 18:04:47,840 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 18:04:47,840 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 18:04:47,840 	Hypothesis: i &apos;m looking for an inexpensive food restaurant .
2020-06-17 18:04:47,840 Validation result (greedy) at epoch  76, step    36000: bleu:  56.39, loss: 27182.2812, ppl:   3.3656, duration: 19.0766s
2020-06-17 18:04:53,159 Epoch  76: total training loss 8.97
2020-06-17 18:04:53,159 EPOCH 77
2020-06-17 18:05:00,199 Epoch  77: total training loss 8.81
2020-06-17 18:05:00,200 EPOCH 78
2020-06-17 18:05:01,184 Epoch  78 Step:    36100 Batch Loss:     0.166360 Tokens per Sec:    19288, Lr: 0.000300
2020-06-17 18:05:07,255 Epoch  78: total training loss 8.62
2020-06-17 18:05:07,255 EPOCH 79
2020-06-17 18:05:14,308 Epoch  79 Step:    36200 Batch Loss:     0.148761 Tokens per Sec:    19428, Lr: 0.000300
2020-06-17 18:05:14,309 Epoch  79: total training loss 8.58
2020-06-17 18:05:14,309 EPOCH 80
2020-06-17 18:05:21,316 Epoch  80: total training loss 8.62
2020-06-17 18:05:21,317 EPOCH 81
2020-06-17 18:05:27,083 Epoch  81 Step:    36300 Batch Loss:     0.179383 Tokens per Sec:    20993, Lr: 0.000300
2020-06-17 18:05:27,873 Epoch  81: total training loss 8.71
2020-06-17 18:05:27,873 EPOCH 82
2020-06-17 18:05:35,289 Epoch  82: total training loss 8.63
2020-06-17 18:05:35,290 EPOCH 83
2020-06-17 18:05:40,591 Epoch  83 Step:    36400 Batch Loss:     0.173215 Tokens per Sec:    19177, Lr: 0.000300
2020-06-17 18:05:42,414 Epoch  83: total training loss 8.46
2020-06-17 18:05:42,414 EPOCH 84
2020-06-17 18:05:48,952 Epoch  84: total training loss 8.16
2020-06-17 18:05:48,953 EPOCH 85
2020-06-17 18:05:53,054 Epoch  85 Step:    36500 Batch Loss:     0.135270 Tokens per Sec:    21392, Lr: 0.000300
2020-06-17 18:05:55,436 Epoch  85: total training loss 8.15
2020-06-17 18:05:55,436 EPOCH 86
2020-06-17 18:06:01,990 Epoch  86: total training loss 8.25
2020-06-17 18:06:01,991 EPOCH 87
2020-06-17 18:06:05,530 Epoch  87 Step:    36600 Batch Loss:     0.142317 Tokens per Sec:    20394, Lr: 0.000300
2020-06-17 18:06:08,904 Epoch  87: total training loss 8.03
2020-06-17 18:06:08,905 EPOCH 88
2020-06-17 18:06:16,024 Epoch  88: total training loss 8.00
2020-06-17 18:06:16,024 EPOCH 89
2020-06-17 18:06:18,799 Epoch  89 Step:    36700 Batch Loss:     0.192346 Tokens per Sec:    19304, Lr: 0.000300
2020-06-17 18:06:23,190 Epoch  89: total training loss 8.15
2020-06-17 18:06:23,191 EPOCH 90
2020-06-17 18:06:30,200 Epoch  90: total training loss 7.81
2020-06-17 18:06:30,200 EPOCH 91
2020-06-17 18:06:32,075 Epoch  91 Step:    36800 Batch Loss:     0.173327 Tokens per Sec:    19018, Lr: 0.000300
2020-06-17 18:06:36,824 Epoch  91: total training loss 7.69
2020-06-17 18:06:36,825 EPOCH 92
2020-06-17 18:06:43,529 Epoch  92: total training loss 7.53
2020-06-17 18:06:43,530 EPOCH 93
2020-06-17 18:06:44,630 Epoch  93 Step:    36900 Batch Loss:     0.124637 Tokens per Sec:    20087, Lr: 0.000300
2020-06-17 18:06:50,611 Epoch  93: total training loss 7.48
2020-06-17 18:06:50,611 EPOCH 94
2020-06-17 18:06:57,669 Epoch  94: total training loss 7.47
2020-06-17 18:06:57,670 EPOCH 95
2020-06-17 18:06:57,965 Epoch  95 Step:    37000 Batch Loss:     0.122060 Tokens per Sec:    17504, Lr: 0.000300
2020-06-17 18:07:13,890 Example #0
2020-06-17 18:07:13,890 	Raw source:     ['hallo', ',']
2020-06-17 18:07:13,890 	Raw hypothesis: ['hello', '?']
2020-06-17 18:07:13,890 	Source:     hallo ,
2020-06-17 18:07:13,890 	Reference:  hello .
2020-06-17 18:07:13,890 	Hypothesis: hello ?
2020-06-17 18:07:13,890 Example #1
2020-06-17 18:07:13,890 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 18:07:13,890 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 18:07:13,890 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 18:07:13,890 	Reference:  hi , how can i help you ?
2020-06-17 18:07:13,890 	Hypothesis: hi , how can i help you ?
2020-06-17 18:07:13,890 Example #2
2020-06-17 18:07:13,890 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 18:07:13,890 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 18:07:13,890 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 18:07:13,890 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 18:07:13,891 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 18:07:13,891 Example #3
2020-06-17 18:07:13,891 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 18:07:13,891 	Raw hypothesis: ['ok', '.', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', 'for', '?']
2020-06-17 18:07:13,891 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 18:07:13,891 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 18:07:13,891 	Hypothesis: ok . what type of restaurant are you looking for for ?
2020-06-17 18:07:13,891 Example #4
2020-06-17 18:07:13,891 	Raw source:     ['ich', 'suche', 'ein', 'gün@@', 'stiges', 'fa@@', 'st@@', 'food-@@', 'restaurant', '.']
2020-06-17 18:07:13,891 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'an', 'inexpensive', 'food', 'restaurant', '.']
2020-06-17 18:07:13,891 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 18:07:13,891 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 18:07:13,891 	Hypothesis: i &apos;m looking for an inexpensive food restaurant .
2020-06-17 18:07:13,891 Validation result (greedy) at epoch  95, step    37000: bleu:  55.88, loss: 28041.3477, ppl:   3.4972, duration: 15.9249s
2020-06-17 18:07:20,696 Epoch  95: total training loss 7.48
2020-06-17 18:07:20,697 EPOCH 96
2020-06-17 18:07:26,716 Epoch  96 Step:    37100 Batch Loss:     0.127344 Tokens per Sec:    20608, Lr: 0.000300
2020-06-17 18:07:27,389 Epoch  96: total training loss 7.36
2020-06-17 18:07:27,389 EPOCH 97
2020-06-17 18:07:34,352 Epoch  97: total training loss 7.23
2020-06-17 18:07:34,353 EPOCH 98
2020-06-17 18:07:39,950 Epoch  98 Step:    37200 Batch Loss:     0.132062 Tokens per Sec:    19378, Lr: 0.000300
2020-06-17 18:07:41,461 Epoch  98: total training loss 7.23
2020-06-17 18:07:41,464 EPOCH 99
2020-06-17 18:07:48,584 Epoch  99: total training loss 7.03
2020-06-17 18:07:48,584 EPOCH 100
2020-06-17 18:07:53,498 Epoch 100 Step:    37300 Batch Loss:     0.149225 Tokens per Sec:    18942, Lr: 0.000300
2020-06-17 18:07:55,751 Epoch 100: total training loss 7.00
2020-06-17 18:07:55,751 Training ended after 100 epochs.
2020-06-17 18:07:55,751 Best validation result (greedy) at step    36000:  56.39 eval_metric.
2020-06-17 18:08:22,190  dev bleu:  57.20 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 18:08:22,195 Translations saved to: models/transformer_iwslt14_deen_bpe-tune/00036000.hyps.dev
2020-06-17 18:08:34,196 test bleu:  52.58 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 18:08:34,201 Translations saved to: models/transformer_iwslt14_deen_bpe-tune/00036000.hyps.test
