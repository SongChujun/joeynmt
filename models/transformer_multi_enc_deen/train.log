2020-06-17 09:49:49,128 Hello! This is Joey-NMT.
2020-06-17 09:49:53,829 Total params: 50537985
2020-06-17 09:49:53,832 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-17 09:49:55,613 cfg.name                           : transformer_multi_enc_deen
2020-06-17 09:49:55,613 cfg.data.src                       : de
2020-06-17 09:49:55,613 cfg.data.trg                       : en
2020-06-17 09:49:55,613 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-17 09:49:55,613 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-17 09:49:55,613 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-17 09:49:55,613 cfg.data.level                     : bpe
2020-06-17 09:49:55,614 cfg.data.lowercase                 : True
2020-06-17 09:49:55,614 cfg.data.max_sent_length           : 100
2020-06-17 09:49:55,614 cfg.testing.beam_size              : 5
2020-06-17 09:49:55,614 cfg.testing.alpha                  : 1.0
2020-06-17 09:49:55,614 cfg.training.random_seed           : 42
2020-06-17 09:49:55,614 cfg.training.optimizer             : adam
2020-06-17 09:49:55,614 cfg.training.normalization         : tokens
2020-06-17 09:49:55,614 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-17 09:49:55,614 cfg.training.scheduling            : plateau
2020-06-17 09:49:55,614 cfg.training.patience              : 8
2020-06-17 09:49:55,614 cfg.training.decrease_factor       : 0.7
2020-06-17 09:49:55,614 cfg.training.loss                  : crossentropy
2020-06-17 09:49:55,614 cfg.training.learning_rate         : 0.0002
2020-06-17 09:49:55,614 cfg.training.learning_rate_min     : 1e-08
2020-06-17 09:49:55,614 cfg.training.weight_decay          : 0.0
2020-06-17 09:49:55,614 cfg.training.label_smoothing       : 0.1
2020-06-17 09:49:55,614 cfg.training.batch_size            : 4096
2020-06-17 09:49:55,614 cfg.training.batch_type            : token
2020-06-17 09:49:55,614 cfg.training.eval_batch_size       : 3600
2020-06-17 09:49:55,615 cfg.training.eval_batch_type       : token
2020-06-17 09:49:55,615 cfg.training.batch_multiplier      : 1
2020-06-17 09:49:55,615 cfg.training.early_stopping_metric : ppl
2020-06-17 09:49:55,615 cfg.training.epochs                : 100
2020-06-17 09:49:55,615 cfg.training.validation_freq       : 1000
2020-06-17 09:49:55,615 cfg.training.logging_freq          : 100
2020-06-17 09:49:55,615 cfg.training.eval_metric           : bleu
2020-06-17 09:49:55,615 cfg.training.model_dir             : models/transformer_multi_enc_deen
2020-06-17 09:49:55,615 cfg.training.overwrite             : True
2020-06-17 09:49:55,615 cfg.training.shuffle               : True
2020-06-17 09:49:55,615 cfg.training.use_cuda              : True
2020-06-17 09:49:55,615 cfg.training.max_output_length     : 100
2020-06-17 09:49:55,615 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-17 09:49:55,615 cfg.training.keep_last_ckpts       : 3
2020-06-17 09:49:55,615 cfg.model.initializer              : xavier
2020-06-17 09:49:55,615 cfg.model.bias_initializer         : zeros
2020-06-17 09:49:55,615 cfg.model.init_gain                : 1.0
2020-06-17 09:49:55,615 cfg.model.embed_initializer        : xavier
2020-06-17 09:49:55,615 cfg.model.embed_init_gain          : 1.0
2020-06-17 09:49:55,615 cfg.model.tied_embeddings          : False
2020-06-17 09:49:55,615 cfg.model.tied_softmax             : True
2020-06-17 09:49:55,616 cfg.model.encoder.type             : transformer
2020-06-17 09:49:55,616 cfg.model.encoder.num_layers       : 3
2020-06-17 09:49:55,616 cfg.model.encoder.num_heads        : 8
2020-06-17 09:49:55,616 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-17 09:49:55,616 cfg.model.encoder.embeddings.scale : True
2020-06-17 09:49:55,616 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-17 09:49:55,616 cfg.model.encoder.hidden_size      : 512
2020-06-17 09:49:55,616 cfg.model.encoder.ff_size          : 2048
2020-06-17 09:49:55,616 cfg.model.encoder.dropout          : 0.1
2020-06-17 09:49:55,616 cfg.model.encoder.freeze           : False
2020-06-17 09:49:55,616 cfg.model.encoder.multi_encoder    : True
2020-06-17 09:49:55,616 cfg.model.decoder.type             : transformer
2020-06-17 09:49:55,616 cfg.model.decoder.num_layers       : 6
2020-06-17 09:49:55,616 cfg.model.decoder.num_heads        : 8
2020-06-17 09:49:55,616 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-17 09:49:55,616 cfg.model.decoder.embeddings.scale : True
2020-06-17 09:49:55,616 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-17 09:49:55,616 cfg.model.decoder.hidden_size      : 512
2020-06-17 09:49:55,616 cfg.model.decoder.ff_size          : 2048
2020-06-17 09:49:55,616 cfg.model.decoder.dropout          : 0.1
2020-06-17 09:49:55,616 cfg.model.decoder.freeze           : False
2020-06-17 09:49:55,617 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-17 09:49:55,617 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-17 09:49:55,617 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-17 09:49:55,617 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-17 09:49:55,617 Number of Src words (types): 5876
2020-06-17 09:49:55,617 Number of Trg words (types): 4561
2020-06-17 09:49:55,617 Model(
	encoder=TransformerEncoder(num_layers=2, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=5876),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4561))
2020-06-17 09:49:55,626 EPOCH 1
2020-06-17 09:50:24,344 Epoch   1: total training loss 281.21
2020-06-17 09:50:24,345 EPOCH 2
2020-06-17 09:50:49,331 Epoch   2 Step:      100 Batch Loss:     4.630429 Tokens per Sec:     4638, Lr: 0.000200
2020-06-17 09:50:52,429 Epoch   2: total training loss 234.39
2020-06-17 09:50:52,429 EPOCH 3
2020-06-17 09:51:23,187 Epoch   3: total training loss 218.39
2020-06-17 09:51:23,187 EPOCH 4
2020-06-17 09:51:46,207 Epoch   4 Step:      200 Batch Loss:     4.330013 Tokens per Sec:     4176, Lr: 0.000200
2020-06-17 09:51:54,253 Epoch   4: total training loss 196.97
2020-06-17 09:51:54,254 EPOCH 5
2020-06-17 09:52:25,464 Epoch   5: total training loss 175.51
2020-06-17 09:52:25,465 EPOCH 6
2020-06-17 09:52:43,918 Epoch   6 Step:      300 Batch Loss:     3.268533 Tokens per Sec:     4110, Lr: 0.000200
2020-06-17 09:52:57,702 Epoch   6: total training loss 151.26
2020-06-17 09:52:57,702 EPOCH 7
2020-06-17 09:53:29,986 Epoch   7: total training loss 139.49
2020-06-17 09:53:29,987 EPOCH 8
2020-06-17 09:53:43,664 Epoch   8 Step:      400 Batch Loss:     1.429213 Tokens per Sec:     4198, Lr: 0.000200
2020-06-17 09:54:01,732 Epoch   8: total training loss 122.60
2020-06-17 09:54:01,733 EPOCH 9
2020-06-17 09:54:33,591 Epoch   9: total training loss 102.84
2020-06-17 09:54:33,592 EPOCH 10
2020-06-17 09:54:43,633 Epoch  10 Step:      500 Batch Loss:     2.596980 Tokens per Sec:     4190, Lr: 0.000200
2020-06-17 09:55:05,522 Epoch  10: total training loss 101.00
2020-06-17 09:55:05,522 EPOCH 11
2020-06-17 09:55:37,238 Epoch  11: total training loss 92.41
2020-06-17 09:55:37,238 EPOCH 12
2020-06-17 09:55:42,795 Epoch  12 Step:      600 Batch Loss:     1.072733 Tokens per Sec:     4091, Lr: 0.000200
2020-06-17 09:56:08,736 Epoch  12: total training loss 85.92
2020-06-17 09:56:08,736 EPOCH 13
2020-06-17 09:56:40,444 Epoch  13: total training loss 77.48
2020-06-17 09:56:40,444 EPOCH 14
2020-06-17 09:56:41,695 Epoch  14 Step:      700 Batch Loss:     1.488076 Tokens per Sec:     3541, Lr: 0.000200
2020-06-17 09:57:11,923 Epoch  14: total training loss 70.66
2020-06-17 09:57:11,924 EPOCH 15
2020-06-17 09:57:40,804 Epoch  15 Step:      800 Batch Loss:     1.007542 Tokens per Sec:     4181, Lr: 0.000200
2020-06-17 09:57:43,733 Epoch  15: total training loss 62.47
2020-06-17 09:57:43,734 EPOCH 16
2020-06-17 09:58:15,496 Epoch  16: total training loss 58.79
2020-06-17 09:58:15,497 EPOCH 17
2020-06-17 09:58:40,893 Epoch  17 Step:      900 Batch Loss:     1.299090 Tokens per Sec:     4128, Lr: 0.000200
2020-06-17 09:58:46,846 Epoch  17: total training loss 51.77
2020-06-17 09:58:46,847 EPOCH 18
2020-06-17 09:59:18,132 Epoch  18: total training loss 50.68
2020-06-17 09:59:18,133 EPOCH 19
2020-06-17 09:59:37,831 Epoch  19 Step:     1000 Batch Loss:     0.573252 Tokens per Sec:     4303, Lr: 0.000200
2020-06-17 10:01:02,377 Hooray! New best validation result [ppl]!
2020-06-17 10:01:02,378 Saving new checkpoint.
2020-06-17 10:01:09,477 Example #0
2020-06-17 10:01:09,478 	Raw source:     ['hallo', ',']
2020-06-17 10:01:09,478 	Raw hypothesis: ['hello', '.']
2020-06-17 10:01:09,478 	Source:     hallo ,
2020-06-17 10:01:09,478 	Reference:  hello .
2020-06-17 10:01:09,478 	Hypothesis: hello .
2020-06-17 10:01:09,478 Example #1
2020-06-17 10:01:09,478 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:01:09,478 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:01:09,478 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:01:09,478 	Reference:  hi , how can i help you ?
2020-06-17 10:01:09,478 	Hypothesis: hi , how can i help you ?
2020-06-17 10:01:09,478 Example #2
2020-06-17 10:01:09,478 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:01:09,479 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:01:09,479 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:01:09,479 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:01:09,479 	Hypothesis: hi , i &apos;m looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:01:09,479 Example #3
2020-06-17 10:01:09,479 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:01:09,479 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:01:09,479 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:01:09,479 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:01:09,479 	Hypothesis: ok , what kind of restaurant are you looking for ?
2020-06-17 10:01:09,479 Validation result (greedy) at epoch  19, step     1000: bleu:  37.08, loss: 40025.0547, ppl:   6.7270, duration: 91.6475s
2020-06-17 10:01:20,169 Epoch  19: total training loss 46.94
2020-06-17 10:01:20,169 EPOCH 20
2020-06-17 10:01:51,344 Epoch  20: total training loss 43.02
2020-06-17 10:01:51,345 EPOCH 21
2020-06-17 10:02:07,004 Epoch  21 Step:     1100 Batch Loss:     0.435914 Tokens per Sec:     4007, Lr: 0.000200
2020-06-17 10:02:23,259 Epoch  21: total training loss 43.35
2020-06-17 10:02:23,259 EPOCH 22
2020-06-17 10:02:54,518 Epoch  22: total training loss 36.29
2020-06-17 10:02:54,519 EPOCH 23
2020-06-17 10:03:05,150 Epoch  23 Step:     1200 Batch Loss:     1.288029 Tokens per Sec:     4126, Lr: 0.000200
2020-06-17 10:03:26,158 Epoch  23: total training loss 34.53
2020-06-17 10:03:26,159 EPOCH 24
2020-06-17 10:03:57,345 Epoch  24: total training loss 31.34
2020-06-17 10:03:57,345 EPOCH 25
2020-06-17 10:04:03,293 Epoch  25 Step:     1300 Batch Loss:     0.333196 Tokens per Sec:     4164, Lr: 0.000200
2020-06-17 10:04:28,659 Epoch  25: total training loss 32.06
2020-06-17 10:04:28,660 EPOCH 26
2020-06-17 10:05:00,831 Epoch  26: total training loss 28.17
2020-06-17 10:05:00,831 EPOCH 27
2020-06-17 10:05:01,450 Epoch  27 Step:     1400 Batch Loss:     0.313948 Tokens per Sec:     4357, Lr: 0.000200
2020-06-17 10:05:32,491 Epoch  27: total training loss 25.20
2020-06-17 10:05:32,492 EPOCH 28
2020-06-17 10:05:59,462 Epoch  28 Step:     1500 Batch Loss:     0.285615 Tokens per Sec:     4256, Lr: 0.000200
2020-06-17 10:06:03,664 Epoch  28: total training loss 23.28
2020-06-17 10:06:03,664 EPOCH 29
2020-06-17 10:06:35,184 Epoch  29: total training loss 21.24
2020-06-17 10:06:35,185 EPOCH 30
2020-06-17 10:06:58,797 Epoch  30 Step:     1600 Batch Loss:     0.292118 Tokens per Sec:     4170, Lr: 0.000200
2020-06-17 10:07:06,811 Epoch  30: total training loss 18.52
2020-06-17 10:07:06,811 EPOCH 31
2020-06-17 10:07:38,101 Epoch  31: total training loss 16.57
2020-06-17 10:07:38,101 EPOCH 32
2020-06-17 10:07:57,662 Epoch  32 Step:     1700 Batch Loss:     0.212280 Tokens per Sec:     4164, Lr: 0.000200
2020-06-17 10:08:09,589 Epoch  32: total training loss 15.68
2020-06-17 10:08:09,590 EPOCH 33
2020-06-17 10:08:41,381 Epoch  33: total training loss 14.89
2020-06-17 10:08:41,381 EPOCH 34
2020-06-17 10:08:56,118 Epoch  34 Step:     1800 Batch Loss:     0.209418 Tokens per Sec:     4114, Lr: 0.000200
2020-06-17 10:09:12,977 Epoch  34: total training loss 14.32
2020-06-17 10:09:13,054 EPOCH 35
2020-06-17 10:09:44,900 Epoch  35: total training loss 13.45
2020-06-17 10:09:44,900 EPOCH 36
2020-06-17 10:09:53,984 Epoch  36 Step:     1900 Batch Loss:     0.199232 Tokens per Sec:     4410, Lr: 0.000200
2020-06-17 10:10:16,858 Epoch  36: total training loss 13.47
2020-06-17 10:10:16,858 EPOCH 37
2020-06-17 10:10:48,370 Epoch  37: total training loss 11.78
2020-06-17 10:10:48,370 EPOCH 38
2020-06-17 10:10:53,347 Epoch  38 Step:     2000 Batch Loss:     0.241196 Tokens per Sec:     4253, Lr: 0.000200
2020-06-17 10:11:55,115 Hooray! New best validation result [ppl]!
2020-06-17 10:11:55,115 Saving new checkpoint.
2020-06-17 10:12:02,724 Example #0
2020-06-17 10:12:02,725 	Raw source:     ['hallo', ',']
2020-06-17 10:12:02,725 	Raw hypothesis: ['hello', '.']
2020-06-17 10:12:02,725 	Source:     hallo ,
2020-06-17 10:12:02,725 	Reference:  hello .
2020-06-17 10:12:02,725 	Hypothesis: hello .
2020-06-17 10:12:02,725 Example #1
2020-06-17 10:12:02,726 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:12:02,726 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:12:02,726 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:12:02,726 	Reference:  hi , how can i help you ?
2020-06-17 10:12:02,726 	Hypothesis: hi , how can i help you ?
2020-06-17 10:12:02,726 Example #2
2020-06-17 10:12:02,726 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:12:02,727 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:12:02,727 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:12:02,727 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:12:02,727 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:12:02,727 Example #3
2020-06-17 10:12:02,727 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:12:02,727 	Raw hypothesis: ['ok', '.', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:12:02,728 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:12:02,728 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:12:02,728 	Hypothesis: ok . what type of restaurant are you looking for ?
2020-06-17 10:12:02,728 Validation result (greedy) at epoch  38, step     2000: bleu:  42.28, loss: 39746.2617, ppl:   6.6383, duration: 69.3800s
2020-06-17 10:12:27,730 Epoch  38: total training loss 11.44
2020-06-17 10:12:27,730 EPOCH 39
2020-06-17 10:13:00,436 Epoch  39: total training loss 10.86
2020-06-17 10:13:00,437 EPOCH 40
2020-06-17 10:13:00,920 Epoch  40 Step:     2100 Batch Loss:     0.177477 Tokens per Sec:     5312, Lr: 0.000200
2020-06-17 10:13:32,700 Epoch  40: total training loss 10.36
2020-06-17 10:13:32,700 EPOCH 41
2020-06-17 10:14:00,868 Epoch  41 Step:     2200 Batch Loss:     0.171889 Tokens per Sec:     4133, Lr: 0.000200
2020-06-17 10:14:04,582 Epoch  41: total training loss 9.76
2020-06-17 10:14:04,582 EPOCH 42
2020-06-17 10:14:36,597 Epoch  42: total training loss 9.38
2020-06-17 10:14:36,598 EPOCH 43
2020-06-17 10:14:59,069 Epoch  43 Step:     2300 Batch Loss:     0.192723 Tokens per Sec:     4243, Lr: 0.000200
2020-06-17 10:15:08,597 Epoch  43: total training loss 9.00
2020-06-17 10:15:08,597 EPOCH 44
2020-06-17 10:15:40,029 Epoch  44: total training loss 8.65
2020-06-17 10:15:40,030 EPOCH 45
2020-06-17 10:15:59,118 Epoch  45 Step:     2400 Batch Loss:     0.140973 Tokens per Sec:     4184, Lr: 0.000200
2020-06-17 10:16:11,639 Epoch  45: total training loss 8.40
2020-06-17 10:16:11,640 EPOCH 46
2020-06-17 10:16:43,151 Epoch  46: total training loss 8.28
2020-06-17 10:16:43,152 EPOCH 47
2020-06-17 10:16:58,283 Epoch  47 Step:     2500 Batch Loss:     0.159256 Tokens per Sec:     4062, Lr: 0.000200
2020-06-17 10:17:14,362 Epoch  47: total training loss 8.94
2020-06-17 10:17:14,363 EPOCH 48
2020-06-17 10:17:45,746 Epoch  48: total training loss 8.24
2020-06-17 10:17:45,747 EPOCH 49
2020-06-17 10:17:55,821 Epoch  49 Step:     2600 Batch Loss:     0.139081 Tokens per Sec:     4043, Lr: 0.000200
2020-06-17 10:18:17,786 Epoch  49: total training loss 8.24
2020-06-17 10:18:17,787 EPOCH 50
2020-06-17 10:18:49,379 Epoch  50: total training loss 7.90
2020-06-17 10:18:49,380 EPOCH 51
2020-06-17 10:18:54,298 Epoch  51 Step:     2700 Batch Loss:     0.146259 Tokens per Sec:     4177, Lr: 0.000200
2020-06-17 10:19:21,062 Epoch  51: total training loss 7.95
2020-06-17 10:19:21,063 EPOCH 52
2020-06-17 10:19:51,993 Epoch  52 Step:     2800 Batch Loss:     0.125312 Tokens per Sec:     4166, Lr: 0.000200
2020-06-17 10:19:52,631 Epoch  52: total training loss 7.93
2020-06-17 10:19:52,631 EPOCH 53
2020-06-17 10:20:24,323 Epoch  53: total training loss 7.30
2020-06-17 10:20:24,324 EPOCH 54
2020-06-17 10:20:50,821 Epoch  54 Step:     2900 Batch Loss:     0.139160 Tokens per Sec:     4075, Lr: 0.000200
2020-06-17 10:20:55,865 Epoch  54: total training loss 7.16
2020-06-17 10:20:55,865 EPOCH 55
2020-06-17 10:21:27,709 Epoch  55: total training loss 6.98
2020-06-17 10:21:27,710 EPOCH 56
2020-06-17 10:21:49,514 Epoch  56 Step:     3000 Batch Loss:     0.119604 Tokens per Sec:     4181, Lr: 0.000200
2020-06-17 10:22:45,821 Example #0
2020-06-17 10:22:45,822 	Raw source:     ['hallo', ',']
2020-06-17 10:22:45,822 	Raw hypothesis: ['hello', '.']
2020-06-17 10:22:45,822 	Source:     hallo ,
2020-06-17 10:22:45,822 	Reference:  hello .
2020-06-17 10:22:45,823 	Hypothesis: hello .
2020-06-17 10:22:45,823 Example #1
2020-06-17 10:22:45,823 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:22:45,823 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:22:45,823 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:22:45,823 	Reference:  hi , how can i help you ?
2020-06-17 10:22:45,823 	Hypothesis: hi , how can i help you ?
2020-06-17 10:22:45,823 Example #2
2020-06-17 10:22:45,823 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:22:45,823 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:22:45,823 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:22:45,823 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:22:45,823 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:22:45,824 Example #3
2020-06-17 10:22:45,824 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:22:45,824 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:22:45,824 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:22:45,824 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:22:45,824 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 10:22:45,824 Validation result (greedy) at epoch  56, step     3000: bleu:  42.68, loss: 40039.0703, ppl:   6.7315, duration: 56.3086s
2020-06-17 10:22:55,688 Epoch  56: total training loss 6.79
2020-06-17 10:22:55,688 EPOCH 57
2020-06-17 10:23:27,537 Epoch  57: total training loss 6.56
2020-06-17 10:23:27,537 EPOCH 58
2020-06-17 10:23:44,912 Epoch  58 Step:     3100 Batch Loss:     0.115237 Tokens per Sec:     4166, Lr: 0.000200
2020-06-17 10:23:59,292 Epoch  58: total training loss 6.21
2020-06-17 10:23:59,292 EPOCH 59
2020-06-17 10:24:30,792 Epoch  59: total training loss 6.16
2020-06-17 10:24:30,793 EPOCH 60
2020-06-17 10:24:43,771 Epoch  60 Step:     3200 Batch Loss:     0.118144 Tokens per Sec:     4159, Lr: 0.000200
2020-06-17 10:25:02,484 Epoch  60: total training loss 5.90
2020-06-17 10:25:02,485 EPOCH 61
2020-06-17 10:25:34,021 Epoch  61: total training loss 5.99
2020-06-17 10:25:34,022 EPOCH 62
2020-06-17 10:25:42,167 Epoch  62 Step:     3300 Batch Loss:     0.101075 Tokens per Sec:     4416, Lr: 0.000200
2020-06-17 10:26:05,348 Epoch  62: total training loss 5.97
2020-06-17 10:26:05,349 EPOCH 63
2020-06-17 10:26:36,694 Epoch  63: total training loss 5.76
2020-06-17 10:26:36,695 EPOCH 64
2020-06-17 10:26:40,246 Epoch  64 Step:     3400 Batch Loss:     0.099020 Tokens per Sec:     4979, Lr: 0.000200
2020-06-17 10:27:08,076 Epoch  64: total training loss 5.72
2020-06-17 10:27:08,076 EPOCH 65
2020-06-17 10:27:39,836 Epoch  65: total training loss 5.80
2020-06-17 10:27:39,836 EPOCH 66
2020-06-17 10:27:40,255 Epoch  66 Step:     3500 Batch Loss:     0.106077 Tokens per Sec:     6503, Lr: 0.000200
2020-06-17 10:28:11,258 Epoch  66: total training loss 5.70
2020-06-17 10:28:11,259 EPOCH 67
2020-06-17 10:28:39,422 Epoch  67 Step:     3600 Batch Loss:     0.084304 Tokens per Sec:     4149, Lr: 0.000200
2020-06-17 10:28:43,095 Epoch  67: total training loss 5.67
2020-06-17 10:28:43,095 EPOCH 68
2020-06-17 10:29:14,958 Epoch  68: total training loss 5.66
2020-06-17 10:29:14,959 EPOCH 69
2020-06-17 10:29:38,524 Epoch  69 Step:     3700 Batch Loss:     0.089833 Tokens per Sec:     4223, Lr: 0.000200
2020-06-17 10:29:46,154 Epoch  69: total training loss 5.51
2020-06-17 10:29:46,154 EPOCH 70
2020-06-17 10:30:17,636 Epoch  70: total training loss 5.55
2020-06-17 10:30:17,636 EPOCH 71
2020-06-17 10:30:35,655 Epoch  71 Step:     3800 Batch Loss:     0.111888 Tokens per Sec:     4559, Lr: 0.000200
2020-06-17 10:30:49,042 Epoch  71: total training loss 5.41
2020-06-17 10:30:49,042 EPOCH 72
2020-06-17 10:31:20,144 Epoch  72: total training loss 5.36
2020-06-17 10:31:20,145 EPOCH 73
2020-06-17 10:31:34,762 Epoch  73 Step:     3900 Batch Loss:     0.109162 Tokens per Sec:     4178, Lr: 0.000200
2020-06-17 10:31:51,481 Epoch  73: total training loss 5.47
2020-06-17 10:31:51,482 EPOCH 74
2020-06-17 10:32:23,112 Epoch  74: total training loss 5.14
2020-06-17 10:32:23,112 EPOCH 75
2020-06-17 10:32:32,918 Epoch  75 Step:     4000 Batch Loss:     0.098015 Tokens per Sec:     4434, Lr: 0.000200
2020-06-17 10:33:35,919 Example #0
2020-06-17 10:33:35,920 	Raw source:     ['hallo', ',']
2020-06-17 10:33:35,920 	Raw hypothesis: ['hello', '?']
2020-06-17 10:33:35,920 	Source:     hallo ,
2020-06-17 10:33:35,920 	Reference:  hello .
2020-06-17 10:33:35,920 	Hypothesis: hello ?
2020-06-17 10:33:35,920 Example #1
2020-06-17 10:33:35,920 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:33:35,920 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:33:35,921 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:33:35,921 	Reference:  hi , how can i help you ?
2020-06-17 10:33:35,921 	Hypothesis: hi , how can i help you ?
2020-06-17 10:33:35,921 Example #2
2020-06-17 10:33:35,921 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:33:35,921 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:33:35,921 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:33:35,921 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:33:35,921 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:33:35,921 Example #3
2020-06-17 10:33:35,921 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:33:35,921 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:33:35,921 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:33:35,921 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:33:35,921 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 10:33:35,921 Validation result (greedy) at epoch  75, step     4000: bleu:  43.37, loss: 40029.4023, ppl:   6.7284, duration: 63.0025s
2020-06-17 10:33:57,695 Epoch  75: total training loss 5.00
2020-06-17 10:33:57,696 EPOCH 76
2020-06-17 10:34:28,789 Epoch  76: total training loss 5.01
2020-06-17 10:34:28,790 EPOCH 77
2020-06-17 10:34:34,641 Epoch  77 Step:     4100 Batch Loss:     0.073304 Tokens per Sec:     4288, Lr: 0.000200
2020-06-17 10:34:59,862 Epoch  77: total training loss 4.86
2020-06-17 10:34:59,863 EPOCH 78
2020-06-17 10:35:30,742 Epoch  78: total training loss 4.85
2020-06-17 10:35:30,743 EPOCH 79
2020-06-17 10:35:32,376 Epoch  79 Step:     4200 Batch Loss:     0.118925 Tokens per Sec:     4539, Lr: 0.000200
2020-06-17 10:36:02,078 Epoch  79: total training loss 5.40
2020-06-17 10:36:02,078 EPOCH 80
2020-06-17 10:36:31,290 Epoch  80 Step:     4300 Batch Loss:     0.122073 Tokens per Sec:     4102, Lr: 0.000200
2020-06-17 10:36:34,045 Epoch  80: total training loss 5.36
2020-06-17 10:36:34,046 EPOCH 81
2020-06-17 10:37:05,425 Epoch  81: total training loss 5.60
2020-06-17 10:37:05,425 EPOCH 82
2020-06-17 10:37:30,371 Epoch  82 Step:     4400 Batch Loss:     0.089142 Tokens per Sec:     4157, Lr: 0.000200
2020-06-17 10:37:36,736 Epoch  82: total training loss 5.12
2020-06-17 10:37:36,737 EPOCH 83
2020-06-17 10:38:07,742 Epoch  83: total training loss 5.00
2020-06-17 10:38:07,742 EPOCH 84
2020-06-17 10:38:27,203 Epoch  84 Step:     4500 Batch Loss:     0.087342 Tokens per Sec:     4295, Lr: 0.000200
2020-06-17 10:38:38,738 Epoch  84: total training loss 4.85
2020-06-17 10:38:38,739 EPOCH 85
2020-06-17 10:39:10,113 Epoch  85: total training loss 4.60
2020-06-17 10:39:10,114 EPOCH 86
2020-06-17 10:39:26,229 Epoch  86 Step:     4600 Batch Loss:     0.071478 Tokens per Sec:     4195, Lr: 0.000200
2020-06-17 10:39:41,543 Epoch  86: total training loss 4.59
2020-06-17 10:39:41,544 EPOCH 87
2020-06-17 10:40:12,808 Epoch  87: total training loss 4.82
2020-06-17 10:40:12,809 EPOCH 88
2020-06-17 10:40:23,017 Epoch  88 Step:     4700 Batch Loss:     0.129513 Tokens per Sec:     4355, Lr: 0.000200
2020-06-17 10:40:44,029 Epoch  88: total training loss 4.91
2020-06-17 10:40:44,030 EPOCH 89
2020-06-17 10:41:15,143 Epoch  89: total training loss 4.92
2020-06-17 10:41:15,143 EPOCH 90
2020-06-17 10:41:22,654 Epoch  90 Step:     4800 Batch Loss:     0.083043 Tokens per Sec:     3495, Lr: 0.000200
2020-06-17 10:41:46,202 Epoch  90: total training loss 4.47
2020-06-17 10:41:46,202 EPOCH 91
2020-06-17 10:42:17,614 Epoch  91: total training loss 4.42
2020-06-17 10:42:17,615 EPOCH 92
2020-06-17 10:42:20,125 Epoch  92 Step:     4900 Batch Loss:     0.070661 Tokens per Sec:     2550, Lr: 0.000200
2020-06-17 10:42:49,208 Epoch  92: total training loss 4.59
2020-06-17 10:42:49,208 EPOCH 93
2020-06-17 10:43:17,394 Epoch  93 Step:     5000 Batch Loss:     0.104399 Tokens per Sec:     4270, Lr: 0.000200
2020-06-17 10:44:14,995 Example #0
2020-06-17 10:44:14,996 	Raw source:     ['hallo', ',']
2020-06-17 10:44:14,996 	Raw hypothesis: ['hello', '?']
2020-06-17 10:44:14,996 	Source:     hallo ,
2020-06-17 10:44:14,996 	Reference:  hello .
2020-06-17 10:44:14,996 	Hypothesis: hello ?
2020-06-17 10:44:14,996 Example #1
2020-06-17 10:44:14,996 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:44:14,996 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:44:14,996 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:44:14,996 	Reference:  hi , how can i help you ?
2020-06-17 10:44:14,996 	Hypothesis: hi , how can i help you ?
2020-06-17 10:44:14,996 Example #2
2020-06-17 10:44:14,996 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:44:14,997 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:44:14,997 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:44:14,997 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:44:14,997 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:44:14,997 Example #3
2020-06-17 10:44:14,997 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:44:14,997 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:44:14,997 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:44:14,997 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:44:14,997 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 10:44:14,997 Validation result (greedy) at epoch  93, step     5000: bleu:  43.40, loss: 40235.5820, ppl:   6.7948, duration: 57.6018s
2020-06-17 10:44:17,761 Epoch  93: total training loss 4.38
2020-06-17 10:44:17,761 EPOCH 94
2020-06-17 10:44:48,790 Epoch  94: total training loss 4.36
2020-06-17 10:44:48,791 EPOCH 95
2020-06-17 10:45:13,564 Epoch  95 Step:     5100 Batch Loss:     0.067068 Tokens per Sec:     4073, Lr: 0.000200
2020-06-17 10:45:19,928 Epoch  95: total training loss 4.35
2020-06-17 10:45:19,929 EPOCH 96
2020-06-17 10:45:51,276 Epoch  96: total training loss 4.29
2020-06-17 10:45:51,277 EPOCH 97
2020-06-17 10:46:10,191 Epoch  97 Step:     5200 Batch Loss:     0.068211 Tokens per Sec:     4245, Lr: 0.000200
2020-06-17 10:46:22,504 Epoch  97: total training loss 4.35
2020-06-17 10:46:22,504 EPOCH 98
2020-06-17 10:46:53,872 Epoch  98: total training loss 4.32
2020-06-17 10:46:53,873 EPOCH 99
2020-06-17 10:47:09,160 Epoch  99 Step:     5300 Batch Loss:     0.065849 Tokens per Sec:     4273, Lr: 0.000200
2020-06-17 10:47:25,325 Epoch  99: total training loss 4.14
2020-06-17 10:47:25,326 EPOCH 100
2020-06-17 10:47:56,288 Epoch 100: total training loss 4.03
2020-06-17 10:47:56,289 Training ended after 100 epochs.
2020-06-17 10:47:56,289 Best validation result (greedy) at step     2000:   6.64 ppl.
2020-06-17 10:48:38,767  dev bleu:  44.84 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 10:48:38,771 Translations saved to: models/transformer_multi_enc_deen/00002000.hyps.dev
2020-06-17 10:49:09,806 test bleu:  42.16 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 10:49:09,813 Translations saved to: models/transformer_multi_enc_deen/00002000.hyps.test
