2020-06-22 01:35:27,793 Hello! This is Joey-NMT.
2020-06-22 01:35:33,975 Total params: 18735361
2020-06-22 01:35:33,978 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-22 01:35:36,101 cfg.name                           : transformer_multi_enc6x2_hid256_ende
2020-06-22 01:35:36,101 cfg.data.src                       : en
2020-06-22 01:35:36,101 cfg.data.trg                       : de
2020-06-22 01:35:36,101 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-22 01:35:36,101 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-22 01:35:36,101 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-22 01:35:36,102 cfg.data.level                     : bpe
2020-06-22 01:35:36,102 cfg.data.lowercase                 : True
2020-06-22 01:35:36,102 cfg.data.max_sent_length           : 100
2020-06-22 01:35:36,102 cfg.testing.beam_size              : 5
2020-06-22 01:35:36,102 cfg.testing.alpha                  : 1.0
2020-06-22 01:35:36,102 cfg.training.random_seed           : 42
2020-06-22 01:35:36,102 cfg.training.optimizer             : adam
2020-06-22 01:35:36,102 cfg.training.normalization         : tokens
2020-06-22 01:35:36,102 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-22 01:35:36,102 cfg.training.scheduling            : plateau
2020-06-22 01:35:36,102 cfg.training.patience              : 8
2020-06-22 01:35:36,102 cfg.training.decrease_factor       : 0.7
2020-06-22 01:35:36,102 cfg.training.loss                  : crossentropy
2020-06-22 01:35:36,102 cfg.training.learning_rate         : 0.0002
2020-06-22 01:35:36,102 cfg.training.learning_rate_min     : 1e-08
2020-06-22 01:35:36,102 cfg.training.weight_decay          : 0.0
2020-06-22 01:35:36,102 cfg.training.label_smoothing       : 0.1
2020-06-22 01:35:36,102 cfg.training.batch_size            : 4096
2020-06-22 01:35:36,102 cfg.training.batch_type            : token
2020-06-22 01:35:36,102 cfg.training.eval_batch_size       : 3600
2020-06-22 01:35:36,102 cfg.training.eval_batch_type       : token
2020-06-22 01:35:36,102 cfg.training.batch_multiplier      : 1
2020-06-22 01:35:36,102 cfg.training.early_stopping_metric : ppl
2020-06-22 01:35:36,102 cfg.training.epochs                : 100
2020-06-22 01:35:36,102 cfg.training.validation_freq       : 1000
2020-06-22 01:35:36,102 cfg.training.logging_freq          : 100
2020-06-22 01:35:36,102 cfg.training.eval_metric           : bleu
2020-06-22 01:35:36,102 cfg.training.model_dir             : models/transformer_multi_enc6x2_hid256_ende
2020-06-22 01:35:36,102 cfg.training.overwrite             : True
2020-06-22 01:35:36,102 cfg.training.shuffle               : True
2020-06-22 01:35:36,102 cfg.training.use_cuda              : True
2020-06-22 01:35:36,102 cfg.training.max_output_length     : 100
2020-06-22 01:35:36,102 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-22 01:35:36,102 cfg.training.keep_last_ckpts       : 3
2020-06-22 01:35:36,103 cfg.model.initializer              : xavier
2020-06-22 01:35:36,103 cfg.model.bias_initializer         : zeros
2020-06-22 01:35:36,103 cfg.model.init_gain                : 1.0
2020-06-22 01:35:36,103 cfg.model.embed_initializer        : xavier
2020-06-22 01:35:36,103 cfg.model.embed_init_gain          : 1.0
2020-06-22 01:35:36,103 cfg.model.tied_embeddings          : False
2020-06-22 01:35:36,103 cfg.model.tied_softmax             : True
2020-06-22 01:35:36,103 cfg.model.encoder.type             : transformer
2020-06-22 01:35:36,103 cfg.model.encoder.num_layers       : 6
2020-06-22 01:35:36,103 cfg.model.encoder.num_heads        : 8
2020-06-22 01:35:36,103 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-22 01:35:36,103 cfg.model.encoder.embeddings.scale : True
2020-06-22 01:35:36,103 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-22 01:35:36,103 cfg.model.encoder.hidden_size      : 256
2020-06-22 01:35:36,103 cfg.model.encoder.ff_size          : 1024
2020-06-22 01:35:36,103 cfg.model.encoder.dropout          : 0.1
2020-06-22 01:35:36,103 cfg.model.encoder.freeze           : False
2020-06-22 01:35:36,103 cfg.model.encoder.multi_encoder    : True
2020-06-22 01:35:36,103 cfg.model.decoder.type             : transformer
2020-06-22 01:35:36,103 cfg.model.decoder.num_layers       : 6
2020-06-22 01:35:36,103 cfg.model.decoder.num_heads        : 8
2020-06-22 01:35:36,103 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-22 01:35:36,103 cfg.model.decoder.embeddings.scale : True
2020-06-22 01:35:36,103 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-22 01:35:36,103 cfg.model.decoder.hidden_size      : 256
2020-06-22 01:35:36,103 cfg.model.decoder.ff_size          : 1024
2020-06-22 01:35:36,103 cfg.model.decoder.dropout          : 0.1
2020-06-22 01:35:36,103 cfg.model.decoder.freeze           : False
2020-06-22 01:35:36,103 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-22 01:35:36,103 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-22 01:35:36,103 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-22 01:35:36,103 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-22 01:35:36,103 Number of Src words (types): 4561
2020-06-22 01:35:36,103 Number of Trg words (types): 5876
2020-06-22 01:35:36,104 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4561),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=5876))
2020-06-22 01:35:36,113 EPOCH 1
2020-06-22 01:35:46,491 Epoch   1: total training loss 339.88
2020-06-22 01:35:46,491 EPOCH 2
2020-06-22 01:35:54,340 Epoch   2 Step:      100 Batch Loss:     4.709360 Tokens per Sec:    13511, Lr: 0.000200
2020-06-22 01:35:56,072 Epoch   2: total training loss 275.95
2020-06-22 01:35:56,073 EPOCH 3
2020-06-22 01:36:05,722 Epoch   3: total training loss 260.67
2020-06-22 01:36:05,722 EPOCH 4
2020-06-22 01:36:11,968 Epoch   4 Step:      200 Batch Loss:     2.554762 Tokens per Sec:    13006, Lr: 0.000200
2020-06-22 01:36:15,466 Epoch   4: total training loss 249.57
2020-06-22 01:36:15,466 EPOCH 5
2020-06-22 01:36:25,108 Epoch   5: total training loss 233.19
2020-06-22 01:36:25,109 EPOCH 6
2020-06-22 01:36:29,542 Epoch   6 Step:      300 Batch Loss:     5.541299 Tokens per Sec:    13451, Lr: 0.000200
2020-06-22 01:36:34,945 Epoch   6: total training loss 226.52
2020-06-22 01:36:34,946 EPOCH 7
2020-06-22 01:36:44,776 Epoch   7: total training loss 209.83
2020-06-22 01:36:44,777 EPOCH 8
2020-06-22 01:36:47,667 Epoch   8 Step:      400 Batch Loss:     3.851128 Tokens per Sec:    12815, Lr: 0.000200
2020-06-22 01:36:54,622 Epoch   8: total training loss 199.92
2020-06-22 01:36:54,622 EPOCH 9
2020-06-22 01:37:04,531 Epoch   9: total training loss 192.23
2020-06-22 01:37:04,531 EPOCH 10
2020-06-22 01:37:05,599 Epoch  10 Step:      500 Batch Loss:     3.937521 Tokens per Sec:    17074, Lr: 0.000200
2020-06-22 01:37:14,500 Epoch  10: total training loss 185.81
2020-06-22 01:37:14,500 EPOCH 11
2020-06-22 01:37:24,053 Epoch  11 Step:      600 Batch Loss:     2.372451 Tokens per Sec:    12923, Lr: 0.000200
2020-06-22 01:37:24,520 Epoch  11: total training loss 180.10
2020-06-22 01:37:24,520 EPOCH 12
2020-06-22 01:37:34,537 Epoch  12: total training loss 169.41
2020-06-22 01:37:34,537 EPOCH 13
2020-06-22 01:37:42,320 Epoch  13 Step:      700 Batch Loss:     3.052153 Tokens per Sec:    13359, Lr: 0.000200
2020-06-22 01:37:44,671 Epoch  13: total training loss 162.20
2020-06-22 01:37:44,672 EPOCH 14
2020-06-22 01:37:55,599 Epoch  14: total training loss 157.00
2020-06-22 01:37:55,599 EPOCH 15
2020-06-22 01:38:01,901 Epoch  15 Step:      800 Batch Loss:     3.079569 Tokens per Sec:    12305, Lr: 0.000200
2020-06-22 01:38:06,734 Epoch  15: total training loss 147.97
2020-06-22 01:38:06,737 EPOCH 16
2020-06-22 01:38:17,622 Epoch  16: total training loss 142.25
2020-06-22 01:38:17,626 EPOCH 17
2020-06-22 01:38:22,134 Epoch  17 Step:      900 Batch Loss:     2.526027 Tokens per Sec:    11634, Lr: 0.000200
2020-06-22 01:38:28,534 Epoch  17: total training loss 136.59
2020-06-22 01:38:28,534 EPOCH 18
2020-06-22 01:38:39,471 Epoch  18: total training loss 130.59
2020-06-22 01:38:39,473 EPOCH 19
2020-06-22 01:38:42,152 Epoch  19 Step:     1000 Batch Loss:     2.479539 Tokens per Sec:    10748, Lr: 0.000200
2020-06-22 01:39:01,027 Hooray! New best validation result [ppl]!
2020-06-22 01:39:01,028 Saving new checkpoint.
2020-06-22 01:39:03,763 Example #0
2020-06-22 01:39:03,763 	Raw source:     ['hello', '.']
2020-06-22 01:39:03,764 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:39:03,764 	Source:     hello .
2020-06-22 01:39:03,764 	Reference:  hallo ,
2020-06-22 01:39:03,764 	Hypothesis: hallo .
2020-06-22 01:39:03,764 Example #1
2020-06-22 01:39:03,764 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:39:03,764 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:39:03,764 	Source:     hi , how can i help you ?
2020-06-22 01:39:03,764 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:39:03,764 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:39:03,764 Example #2
2020-06-22 01:39:03,764 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:39:03,764 	Raw hypothesis: ['hallo', ',', 'ich', 'bin', 'in', 'san', 'francisco', ',', 'kalifornien', '.', 'ich', 'bin', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:39:03,764 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:39:03,764 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:39:03,764 	Hypothesis: hallo , ich bin in san francisco , kalifornien . ich bin in san francisco , kalifornien .
2020-06-22 01:39:03,764 Example #3
2020-06-22 01:39:03,764 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:39:03,764 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'möchten', 'sie', '?']
2020-06-22 01:39:03,764 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:39:03,764 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:39:03,764 	Hypothesis: ok , welche art von restaurant möchten sie ?
2020-06-22 01:39:03,764 Validation result (greedy) at epoch  19, step     1000: bleu:  15.44, loss: 60255.8359, ppl:  17.4059, duration: 21.6123s
2020-06-22 01:39:11,697 Epoch  19: total training loss 124.94
2020-06-22 01:39:11,698 EPOCH 20
2020-06-22 01:39:22,027 Epoch  20: total training loss 120.54
2020-06-22 01:39:22,027 EPOCH 21
2020-06-22 01:39:22,430 Epoch  21 Step:     1100 Batch Loss:     2.477082 Tokens per Sec:    13829, Lr: 0.000200
2020-06-22 01:39:32,453 Epoch  21: total training loss 115.34
2020-06-22 01:39:32,453 EPOCH 22
2020-06-22 01:39:41,349 Epoch  22 Step:     1200 Batch Loss:     1.331713 Tokens per Sec:    12784, Lr: 0.000200
2020-06-22 01:39:42,732 Epoch  22: total training loss 106.26
2020-06-22 01:39:42,732 EPOCH 23
2020-06-22 01:39:53,005 Epoch  23: total training loss 102.94
2020-06-22 01:39:53,005 EPOCH 24
2020-06-22 01:40:00,387 Epoch  24 Step:     1300 Batch Loss:     1.661828 Tokens per Sec:    12572, Lr: 0.000200
2020-06-22 01:40:03,577 Epoch  24: total training loss 100.98
2020-06-22 01:40:03,577 EPOCH 25
2020-06-22 01:40:14,075 Epoch  25: total training loss 100.40
2020-06-22 01:40:14,075 EPOCH 26
2020-06-22 01:40:19,226 Epoch  26 Step:     1400 Batch Loss:     1.168565 Tokens per Sec:    12933, Lr: 0.000200
2020-06-22 01:40:24,506 Epoch  26: total training loss 94.48
2020-06-22 01:40:24,507 EPOCH 27
2020-06-22 01:40:35,065 Epoch  27: total training loss 90.91
2020-06-22 01:40:35,066 EPOCH 28
2020-06-22 01:40:38,777 Epoch  28 Step:     1500 Batch Loss:     2.362584 Tokens per Sec:    11408, Lr: 0.000200
2020-06-22 01:40:45,669 Epoch  28: total training loss 84.88
2020-06-22 01:40:45,669 EPOCH 29
2020-06-22 01:40:56,490 Epoch  29: total training loss 87.42
2020-06-22 01:40:56,490 EPOCH 30
2020-06-22 01:40:57,896 Epoch  30 Step:     1600 Batch Loss:     1.702443 Tokens per Sec:    14134, Lr: 0.000200
2020-06-22 01:41:07,278 Epoch  30: total training loss 81.53
2020-06-22 01:41:07,278 EPOCH 31
2020-06-22 01:41:17,994 Epoch  31 Step:     1700 Batch Loss:     2.458292 Tokens per Sec:    11525, Lr: 0.000200
2020-06-22 01:41:18,594 Epoch  31: total training loss 80.56
2020-06-22 01:41:18,594 EPOCH 32
2020-06-22 01:41:29,638 Epoch  32: total training loss 78.16
2020-06-22 01:41:29,642 EPOCH 33
2020-06-22 01:41:38,040 Epoch  33 Step:     1800 Batch Loss:     0.693249 Tokens per Sec:    11512, Lr: 0.000200
2020-06-22 01:41:40,716 Epoch  33: total training loss 73.75
2020-06-22 01:41:40,720 EPOCH 34
2020-06-22 01:41:51,379 Epoch  34: total training loss 73.68
2020-06-22 01:41:51,382 EPOCH 35
2020-06-22 01:41:57,448 Epoch  35 Step:     1900 Batch Loss:     1.427823 Tokens per Sec:    12127, Lr: 0.000200
2020-06-22 01:42:02,403 Epoch  35: total training loss 67.04
2020-06-22 01:42:02,404 EPOCH 36
2020-06-22 01:42:13,209 Epoch  36: total training loss 65.89
2020-06-22 01:42:13,210 EPOCH 37
2020-06-22 01:42:16,990 Epoch  37 Step:     2000 Batch Loss:     0.579301 Tokens per Sec:    12564, Lr: 0.000200
2020-06-22 01:42:26,468 Hooray! New best validation result [ppl]!
2020-06-22 01:42:26,469 Saving new checkpoint.
2020-06-22 01:42:29,246 Example #0
2020-06-22 01:42:29,246 	Raw source:     ['hello', '.']
2020-06-22 01:42:29,247 	Raw hypothesis: ['hallo', ',']
2020-06-22 01:42:29,247 	Source:     hello .
2020-06-22 01:42:29,247 	Reference:  hallo ,
2020-06-22 01:42:29,247 	Hypothesis: hallo ,
2020-06-22 01:42:29,247 Example #1
2020-06-22 01:42:29,247 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:42:29,247 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:42:29,247 	Source:     hi , how can i help you ?
2020-06-22 01:42:29,247 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:42:29,247 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:42:29,247 Example #2
2020-06-22 01:42:29,247 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:42:29,247 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'nach', 'einem', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', '.']
2020-06-22 01:42:29,247 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:42:29,247 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:42:29,247 	Hypothesis: hallo , ich suche nach einem restaurant in der arden fair mall .
2020-06-22 01:42:29,248 Example #3
2020-06-22 01:42:29,248 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:42:29,248 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:42:29,248 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:42:29,248 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:42:29,248 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:42:29,248 Validation result (greedy) at epoch  37, step     2000: bleu:  28.58, loss: 52746.5742, ppl:  12.1921, duration: 12.2574s
2020-06-22 01:42:36,295 Epoch  37: total training loss 66.67
2020-06-22 01:42:36,295 EPOCH 38
2020-06-22 01:42:46,825 Epoch  38: total training loss 61.32
2020-06-22 01:42:46,825 EPOCH 39
2020-06-22 01:42:48,579 Epoch  39 Step:     2100 Batch Loss:     0.989920 Tokens per Sec:    13537, Lr: 0.000200
2020-06-22 01:42:57,088 Epoch  39: total training loss 59.77
2020-06-22 01:42:57,088 EPOCH 40
2020-06-22 01:43:07,293 Epoch  40: total training loss 54.28
2020-06-22 01:43:07,293 EPOCH 41
2020-06-22 01:43:07,486 Epoch  41 Step:     2200 Batch Loss:     1.007784 Tokens per Sec:    12151, Lr: 0.000200
2020-06-22 01:43:17,565 Epoch  41: total training loss 53.96
2020-06-22 01:43:17,566 EPOCH 42
2020-06-22 01:43:26,473 Epoch  42 Step:     2300 Batch Loss:     0.768833 Tokens per Sec:    12499, Lr: 0.000200
2020-06-22 01:43:27,932 Epoch  42: total training loss 50.72
2020-06-22 01:43:27,932 EPOCH 43
2020-06-22 01:43:38,296 Epoch  43: total training loss 51.55
2020-06-22 01:43:38,297 EPOCH 44
2020-06-22 01:43:44,745 Epoch  44 Step:     2400 Batch Loss:     1.170509 Tokens per Sec:    13642, Lr: 0.000200
2020-06-22 01:43:48,721 Epoch  44: total training loss 48.16
2020-06-22 01:43:48,721 EPOCH 45
2020-06-22 01:43:59,369 Epoch  45: total training loss 48.36
2020-06-22 01:43:59,370 EPOCH 46
2020-06-22 01:44:04,118 Epoch  46 Step:     2500 Batch Loss:     0.720165 Tokens per Sec:    12637, Lr: 0.000200
2020-06-22 01:44:09,656 Epoch  46: total training loss 44.58
2020-06-22 01:44:09,656 EPOCH 47
2020-06-22 01:44:19,973 Epoch  47: total training loss 44.77
2020-06-22 01:44:19,973 EPOCH 48
2020-06-22 01:44:22,973 Epoch  48 Step:     2600 Batch Loss:     1.027754 Tokens per Sec:    12087, Lr: 0.000200
2020-06-22 01:44:30,278 Epoch  48: total training loss 43.42
2020-06-22 01:44:30,278 EPOCH 49
2020-06-22 01:44:40,536 Epoch  49: total training loss 41.35
2020-06-22 01:44:40,537 EPOCH 50
2020-06-22 01:44:41,670 Epoch  50 Step:     2700 Batch Loss:     0.454848 Tokens per Sec:     9436, Lr: 0.000200
2020-06-22 01:44:50,855 Epoch  50: total training loss 39.60
2020-06-22 01:44:50,855 EPOCH 51
2020-06-22 01:45:00,170 Epoch  51 Step:     2800 Batch Loss:     0.940571 Tokens per Sec:    12502, Lr: 0.000200
2020-06-22 01:45:01,274 Epoch  51: total training loss 39.56
2020-06-22 01:45:01,274 EPOCH 52
2020-06-22 01:45:11,742 Epoch  52: total training loss 37.65
2020-06-22 01:45:11,742 EPOCH 53
2020-06-22 01:45:18,848 Epoch  53 Step:     2900 Batch Loss:     0.806560 Tokens per Sec:    12713, Lr: 0.000200
2020-06-22 01:45:22,378 Epoch  53: total training loss 38.38
2020-06-22 01:45:22,378 EPOCH 54
2020-06-22 01:45:32,919 Epoch  54: total training loss 35.42
2020-06-22 01:45:32,920 EPOCH 55
2020-06-22 01:45:38,537 Epoch  55 Step:     3000 Batch Loss:     1.029779 Tokens per Sec:    11925, Lr: 0.000200
2020-06-22 01:45:50,551 Hooray! New best validation result [ppl]!
2020-06-22 01:45:50,551 Saving new checkpoint.
2020-06-22 01:45:53,316 Example #0
2020-06-22 01:45:53,316 	Raw source:     ['hello', '.']
2020-06-22 01:45:53,316 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:45:53,316 	Source:     hello .
2020-06-22 01:45:53,316 	Reference:  hallo ,
2020-06-22 01:45:53,316 	Hypothesis: hallo .
2020-06-22 01:45:53,316 Example #1
2020-06-22 01:45:53,316 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:45:53,316 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:45:53,316 	Source:     hi , how can i help you ?
2020-06-22 01:45:53,316 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:45:53,316 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:45:53,316 Example #2
2020-06-22 01:45:53,316 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:45:53,317 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:45:53,317 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:45:53,317 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:45:53,317 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:45:53,317 Example #3
2020-06-22 01:45:53,317 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:45:53,317 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:45:53,317 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:45:53,317 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:45:53,317 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:45:53,317 Validation result (greedy) at epoch  55, step     3000: bleu:  32.75, loss: 52481.2188, ppl:  12.0396, duration: 14.7795s
2020-06-22 01:45:58,200 Epoch  55: total training loss 32.95
2020-06-22 01:45:58,201 EPOCH 56
2020-06-22 01:46:08,366 Epoch  56: total training loss 30.99
2020-06-22 01:46:08,367 EPOCH 57
2020-06-22 01:46:12,020 Epoch  57 Step:     3100 Batch Loss:     0.909832 Tokens per Sec:    11987, Lr: 0.000200
2020-06-22 01:46:18,822 Epoch  57: total training loss 30.18
2020-06-22 01:46:18,822 EPOCH 58
2020-06-22 01:46:29,548 Epoch  58: total training loss 30.12
2020-06-22 01:46:29,548 EPOCH 59
2020-06-22 01:46:31,147 Epoch  59 Step:     3200 Batch Loss:     0.434045 Tokens per Sec:    11369, Lr: 0.000200
2020-06-22 01:46:40,250 Epoch  59: total training loss 28.49
2020-06-22 01:46:40,254 EPOCH 60
2020-06-22 01:46:49,991 Epoch  60 Step:     3300 Batch Loss:     0.739273 Tokens per Sec:    12450, Lr: 0.000200
2020-06-22 01:46:50,661 Epoch  60: total training loss 28.63
2020-06-22 01:46:50,662 EPOCH 61
2020-06-22 01:47:00,934 Epoch  61: total training loss 26.79
2020-06-22 01:47:00,934 EPOCH 62
2020-06-22 01:47:08,498 Epoch  62 Step:     3400 Batch Loss:     0.493499 Tokens per Sec:    13189, Lr: 0.000200
2020-06-22 01:47:11,190 Epoch  62: total training loss 25.48
2020-06-22 01:47:11,190 EPOCH 63
2020-06-22 01:47:21,648 Epoch  63: total training loss 27.48
2020-06-22 01:47:21,649 EPOCH 64
2020-06-22 01:47:27,574 Epoch  64 Step:     3500 Batch Loss:     0.376264 Tokens per Sec:    12849, Lr: 0.000200
2020-06-22 01:47:31,952 Epoch  64: total training loss 25.39
2020-06-22 01:47:31,953 EPOCH 65
2020-06-22 01:47:42,370 Epoch  65: total training loss 23.51
2020-06-22 01:47:42,370 EPOCH 66
2020-06-22 01:47:46,620 Epoch  66 Step:     3600 Batch Loss:     0.356311 Tokens per Sec:    12400, Lr: 0.000200
2020-06-22 01:47:52,858 Epoch  66: total training loss 22.11
2020-06-22 01:47:52,859 EPOCH 67
2020-06-22 01:48:03,248 Epoch  67: total training loss 21.35
2020-06-22 01:48:03,248 EPOCH 68
2020-06-22 01:48:05,452 Epoch  68 Step:     3700 Batch Loss:     0.287359 Tokens per Sec:    11829, Lr: 0.000200
2020-06-22 01:48:13,848 Epoch  68: total training loss 20.87
2020-06-22 01:48:13,849 EPOCH 69
2020-06-22 01:48:24,814 Epoch  69: total training loss 20.15
2020-06-22 01:48:24,815 EPOCH 70
2020-06-22 01:48:25,132 Epoch  70 Step:     3800 Batch Loss:     0.406254 Tokens per Sec:    17926, Lr: 0.000200
2020-06-22 01:48:35,778 Epoch  70: total training loss 20.21
2020-06-22 01:48:35,778 EPOCH 71
2020-06-22 01:48:45,067 Epoch  71 Step:     3900 Batch Loss:     0.302440 Tokens per Sec:    12149, Lr: 0.000200
2020-06-22 01:48:46,441 Epoch  71: total training loss 20.05
2020-06-22 01:48:46,441 EPOCH 72
2020-06-22 01:48:56,940 Epoch  72: total training loss 18.57
2020-06-22 01:48:56,941 EPOCH 73
2020-06-22 01:49:04,226 Epoch  73 Step:     4000 Batch Loss:     0.251829 Tokens per Sec:    12323, Lr: 0.000200
2020-06-22 01:49:17,092 Example #0
2020-06-22 01:49:17,092 	Raw source:     ['hello', '.']
2020-06-22 01:49:17,092 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:49:17,092 	Source:     hello .
2020-06-22 01:49:17,092 	Reference:  hallo ,
2020-06-22 01:49:17,092 	Hypothesis: hallo .
2020-06-22 01:49:17,092 Example #1
2020-06-22 01:49:17,092 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:49:17,092 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:49:17,092 	Source:     hi , how can i help you ?
2020-06-22 01:49:17,092 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:49:17,092 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:49:17,092 Example #2
2020-06-22 01:49:17,092 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:49:17,092 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:49:17,092 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:49:17,092 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:49:17,092 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:49:17,092 Example #3
2020-06-22 01:49:17,092 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:49:17,092 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:49:17,093 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:49:17,093 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:49:17,093 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:49:17,093 Validation result (greedy) at epoch  73, step     4000: bleu:  34.21, loss: 53865.4180, ppl:  12.8563, duration: 12.8656s
2020-06-22 01:49:20,459 Epoch  73: total training loss 19.00
2020-06-22 01:49:20,459 EPOCH 74
2020-06-22 01:49:30,854 Epoch  74: total training loss 18.21
2020-06-22 01:49:30,857 EPOCH 75
2020-06-22 01:49:36,069 Epoch  75 Step:     4100 Batch Loss:     0.267855 Tokens per Sec:    12648, Lr: 0.000200
2020-06-22 01:49:41,493 Epoch  75: total training loss 17.71
2020-06-22 01:49:41,494 EPOCH 76
2020-06-22 01:49:52,117 Epoch  76: total training loss 18.65
2020-06-22 01:49:52,118 EPOCH 77
2020-06-22 01:49:55,619 Epoch  77 Step:     4200 Batch Loss:     0.285823 Tokens per Sec:    12155, Lr: 0.000200
2020-06-22 01:50:02,465 Epoch  77: total training loss 17.38
2020-06-22 01:50:02,465 EPOCH 78
2020-06-22 01:50:13,343 Epoch  78: total training loss 15.96
2020-06-22 01:50:13,346 EPOCH 79
2020-06-22 01:50:15,063 Epoch  79 Step:     4300 Batch Loss:     0.273462 Tokens per Sec:    13064, Lr: 0.000200
2020-06-22 01:50:24,303 Epoch  79: total training loss 15.62
2020-06-22 01:50:24,304 EPOCH 80
2020-06-22 01:50:35,071 Epoch  80 Step:     4400 Batch Loss:     0.218071 Tokens per Sec:    11865, Lr: 0.000200
2020-06-22 01:50:35,243 Epoch  80: total training loss 15.16
2020-06-22 01:50:35,244 EPOCH 81
2020-06-22 01:50:46,206 Epoch  81: total training loss 14.92
2020-06-22 01:50:46,207 EPOCH 82
2020-06-22 01:50:54,966 Epoch  82 Step:     4500 Batch Loss:     0.165663 Tokens per Sec:    12028, Lr: 0.000200
2020-06-22 01:50:57,278 Epoch  82: total training loss 15.28
2020-06-22 01:50:57,279 EPOCH 83
2020-06-22 01:51:08,083 Epoch  83: total training loss 14.59
2020-06-22 01:51:08,084 EPOCH 84
2020-06-22 01:51:14,765 Epoch  84 Step:     4600 Batch Loss:     0.274715 Tokens per Sec:    12362, Lr: 0.000200
2020-06-22 01:51:18,814 Epoch  84: total training loss 13.76
2020-06-22 01:51:18,815 EPOCH 85
2020-06-22 01:51:29,440 Epoch  85: total training loss 13.99
2020-06-22 01:51:29,441 EPOCH 86
2020-06-22 01:51:34,215 Epoch  86 Step:     4700 Batch Loss:     0.240300 Tokens per Sec:    11682, Lr: 0.000200
2020-06-22 01:51:40,483 Epoch  86: total training loss 14.12
2020-06-22 01:51:40,483 EPOCH 87
2020-06-22 01:51:51,424 Epoch  87: total training loss 13.68
2020-06-22 01:51:51,424 EPOCH 88
2020-06-22 01:51:53,968 Epoch  88 Step:     4800 Batch Loss:     0.284077 Tokens per Sec:    12061, Lr: 0.000200
2020-06-22 01:52:02,127 Epoch  88: total training loss 13.28
2020-06-22 01:52:02,128 EPOCH 89
2020-06-22 01:52:12,965 Epoch  89: total training loss 13.20
2020-06-22 01:52:12,965 EPOCH 90
2020-06-22 01:52:13,677 Epoch  90 Step:     4900 Batch Loss:     0.212919 Tokens per Sec:    10131, Lr: 0.000200
2020-06-22 01:52:23,508 Epoch  90: total training loss 12.97
2020-06-22 01:52:23,508 EPOCH 91
2020-06-22 01:52:32,278 Epoch  91 Step:     5000 Batch Loss:     0.209580 Tokens per Sec:    12796, Lr: 0.000200
2020-06-22 01:52:42,752 Example #0
2020-06-22 01:52:42,752 	Raw source:     ['hello', '.']
2020-06-22 01:52:42,752 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:52:42,752 	Source:     hello .
2020-06-22 01:52:42,752 	Reference:  hallo ,
2020-06-22 01:52:42,752 	Hypothesis: hallo .
2020-06-22 01:52:42,753 Example #1
2020-06-22 01:52:42,753 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:52:42,753 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:52:42,753 	Source:     hi , how can i help you ?
2020-06-22 01:52:42,753 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:52:42,753 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:52:42,753 Example #2
2020-06-22 01:52:42,753 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:52:42,753 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:52:42,753 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:52:42,753 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:52:42,753 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:52:42,753 Example #3
2020-06-22 01:52:42,753 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:52:42,753 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:52:42,753 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:52:42,753 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:52:42,753 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:52:42,753 Validation result (greedy) at epoch  91, step     5000: bleu:  34.34, loss: 54640.5820, ppl:  13.3375, duration: 10.4743s
2020-06-22 01:52:44,437 Epoch  91: total training loss 12.93
2020-06-22 01:52:44,437 EPOCH 92
2020-06-22 01:52:55,380 Epoch  92: total training loss 13.00
2020-06-22 01:52:55,381 EPOCH 93
2020-06-22 01:53:02,188 Epoch  93 Step:     5100 Batch Loss:     0.237870 Tokens per Sec:    12744, Lr: 0.000200
2020-06-22 01:53:06,301 Epoch  93: total training loss 12.91
2020-06-22 01:53:06,301 EPOCH 94
2020-06-22 01:53:17,199 Epoch  94: total training loss 12.03
2020-06-22 01:53:17,199 EPOCH 95
2020-06-22 01:53:22,592 Epoch  95 Step:     5200 Batch Loss:     0.195699 Tokens per Sec:    11563, Lr: 0.000200
2020-06-22 01:53:27,910 Epoch  95: total training loss 11.88
2020-06-22 01:53:27,910 EPOCH 96
2020-06-22 01:53:38,270 Epoch  96: total training loss 11.70
2020-06-22 01:53:38,271 EPOCH 97
2020-06-22 01:53:41,309 Epoch  97 Step:     5300 Batch Loss:     0.195665 Tokens per Sec:    13233, Lr: 0.000200
2020-06-22 01:53:48,716 Epoch  97: total training loss 11.56
2020-06-22 01:53:48,716 EPOCH 98
2020-06-22 01:53:59,061 Epoch  98: total training loss 11.45
2020-06-22 01:53:59,061 EPOCH 99
2020-06-22 01:54:00,084 Epoch  99 Step:     5400 Batch Loss:     0.231019 Tokens per Sec:    13150, Lr: 0.000200
2020-06-22 01:54:09,412 Epoch  99: total training loss 11.15
2020-06-22 01:54:09,413 EPOCH 100
2020-06-22 01:54:19,617 Epoch 100 Step:     5500 Batch Loss:     0.192339 Tokens per Sec:    12300, Lr: 0.000200
2020-06-22 01:54:20,101 Epoch 100: total training loss 10.80
2020-06-22 01:54:20,101 Training ended after 100 epochs.
2020-06-22 01:54:20,102 Best validation result (greedy) at step     3000:  12.04 ppl.
2020-06-22 01:54:34,696  dev bleu:  34.18 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 01:54:34,700 Translations saved to: models/transformer_multi_enc6x2_hid256_ende/00003000.hyps.dev
2020-06-22 01:54:43,990 test bleu:  31.79 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 01:54:43,995 Translations saved to: models/transformer_multi_enc6x2_hid256_ende/00003000.hyps.test
