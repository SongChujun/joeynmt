2020-06-30 13:26:15,871 Hello! This is Joey-NMT.
2020-06-30 13:26:30,038 Total params: 49501184
2020-06-30 13:26:30,040 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-30 13:26:37,911 cfg.name                           : transformer
2020-06-30 13:26:37,911 cfg.data.src                       : en
2020-06-30 13:26:37,911 cfg.data.trg                       : de
2020-06-30 13:26:37,911 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-06-30 13:26:37,911 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-06-30 13:26:37,911 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-06-30 13:26:37,911 cfg.data.level                     : bpe
2020-06-30 13:26:37,911 cfg.data.lowercase                 : False
2020-06-30 13:26:37,911 cfg.data.max_sent_length           : 100
2020-06-30 13:26:37,911 cfg.testing.beam_size              : 5
2020-06-30 13:26:37,911 cfg.testing.alpha                  : 1.0
2020-06-30 13:26:37,911 cfg.training.random_seed           : 42
2020-06-30 13:26:37,911 cfg.training.optimizer             : adam
2020-06-30 13:26:37,911 cfg.training.normalization         : tokens
2020-06-30 13:26:37,911 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-30 13:26:37,911 cfg.training.scheduling            : plateau
2020-06-30 13:26:37,912 cfg.training.patience              : 8
2020-06-30 13:26:37,912 cfg.training.decrease_factor       : 0.7
2020-06-30 13:26:37,912 cfg.training.loss                  : crossentropy
2020-06-30 13:26:37,912 cfg.training.learning_rate         : 0.0002
2020-06-30 13:26:37,912 cfg.training.learning_rate_min     : 1e-08
2020-06-30 13:26:37,912 cfg.training.weight_decay          : 0.0
2020-06-30 13:26:37,912 cfg.training.label_smoothing       : 0.1
2020-06-30 13:26:37,912 cfg.training.batch_size            : 4096
2020-06-30 13:26:37,912 cfg.training.batch_type            : token
2020-06-30 13:26:37,912 cfg.training.batch_multiplier      : 1
2020-06-30 13:26:37,912 cfg.training.early_stopping_metric : ppl
2020-06-30 13:26:37,912 cfg.training.epochs                : 100
2020-06-30 13:26:37,912 cfg.training.validation_freq       : 1000
2020-06-30 13:26:37,912 cfg.training.logging_freq          : 100
2020-06-30 13:26:37,912 cfg.training.eval_metric           : bleu
2020-06-30 13:26:37,912 cfg.training.model_dir             : models/transformer_wmt17bpe_ende
2020-06-30 13:26:37,912 cfg.training.overwrite             : True
2020-06-30 13:26:37,912 cfg.training.shuffle               : True
2020-06-30 13:26:37,912 cfg.training.use_cuda              : True
2020-06-30 13:26:37,912 cfg.training.max_output_length     : 100
2020-06-30 13:26:37,912 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-30 13:26:37,912 cfg.training.keep_last_ckpts       : 3
2020-06-30 13:26:37,912 cfg.model.initializer              : xavier
2020-06-30 13:26:37,912 cfg.model.bias_initializer         : zeros
2020-06-30 13:26:37,912 cfg.model.init_gain                : 1.0
2020-06-30 13:26:37,912 cfg.model.embed_initializer        : xavier
2020-06-30 13:26:37,912 cfg.model.embed_init_gain          : 1.0
2020-06-30 13:26:37,912 cfg.model.tied_embeddings          : False
2020-06-30 13:26:37,912 cfg.model.tied_softmax             : True
2020-06-30 13:26:37,912 cfg.model.encoder.type             : transformer
2020-06-30 13:26:37,912 cfg.model.encoder.num_layers       : 6
2020-06-30 13:26:37,912 cfg.model.encoder.num_heads        : 8
2020-06-30 13:26:37,912 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-30 13:26:37,912 cfg.model.encoder.embeddings.scale : True
2020-06-30 13:26:37,912 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-30 13:26:37,912 cfg.model.encoder.hidden_size      : 512
2020-06-30 13:26:37,912 cfg.model.encoder.ff_size          : 2048
2020-06-30 13:26:37,912 cfg.model.encoder.freeze           : False
2020-06-30 13:26:37,912 cfg.model.encoder.dropout          : 0.1
2020-06-30 13:26:37,912 cfg.model.decoder.type             : transformer
2020-06-30 13:26:37,913 cfg.model.decoder.num_layers       : 6
2020-06-30 13:26:37,913 cfg.model.decoder.num_heads        : 8
2020-06-30 13:26:37,913 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-30 13:26:37,913 cfg.model.decoder.embeddings.scale : True
2020-06-30 13:26:37,913 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-30 13:26:37,913 cfg.model.decoder.hidden_size      : 512
2020-06-30 13:26:37,913 cfg.model.decoder.ff_size          : 2048
2020-06-30 13:26:37,913 cfg.model.decoder.freeze           : False
2020-06-30 13:26:37,913 cfg.model.decoder.dropout          : 0.1
2020-06-30 13:26:37,913 Data set sizes: 
	train 9747,
	valid 1527,
	test 1192
2020-06-30 13:26:37,913 First training example:
	[SRC] H@@ i there@@ ! How can I hel@@ p@@ ?
	[TRG] Hal@@ lo@@ ! Wie kann ich hel@@ fen@@ ?
2020-06-30 13:26:37,913 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) the (9) to
2020-06-30 13:26:37,913 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ? (7) Sie (8) ich (9) en@@
2020-06-30 13:26:37,913 Number of Src words (types): 4579
2020-06-30 13:26:37,913 Number of Trg words (types): 5891
2020-06-30 13:26:37,913 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4579),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5891))
2020-06-30 13:26:37,921 EPOCH 1
2020-06-30 13:26:49,039 Epoch   1: total training loss 377.12
2020-06-30 13:26:49,040 EPOCH 2
2020-06-30 13:26:54,603 Epoch   2 Step:      100 Batch Loss:     5.692191 Tokens per Sec:    15626, Lr: 0.000200
2020-06-30 13:26:59,475 Epoch   2: total training loss 329.02
2020-06-30 13:26:59,476 EPOCH 3
2020-06-30 13:27:09,944 Epoch   3: total training loss 289.59
2020-06-30 13:27:09,944 EPOCH 4
2020-06-30 13:27:10,691 Epoch   4 Step:      200 Batch Loss:     3.168322 Tokens per Sec:    15994, Lr: 0.000200
2020-06-30 13:27:19,782 Epoch   4: total training loss 249.37
2020-06-30 13:27:19,783 EPOCH 5
2020-06-30 13:27:26,006 Epoch   5 Step:      300 Batch Loss:     3.740517 Tokens per Sec:    16330, Lr: 0.000200
2020-06-30 13:27:29,945 Epoch   5: total training loss 218.38
2020-06-30 13:27:29,946 EPOCH 6
2020-06-30 13:27:40,716 Epoch   6: total training loss 188.19
2020-06-30 13:27:40,717 EPOCH 7
2020-06-30 13:27:42,560 Epoch   7 Step:      400 Batch Loss:     3.168928 Tokens per Sec:    13996, Lr: 0.000200
2020-06-30 13:27:52,380 Epoch   7: total training loss 173.56
2020-06-30 13:27:52,380 EPOCH 8
2020-06-30 13:28:00,239 Epoch   8 Step:      500 Batch Loss:     2.943681 Tokens per Sec:    13974, Lr: 0.000200
2020-06-30 13:28:04,005 Epoch   8: total training loss 152.40
2020-06-30 13:28:04,006 EPOCH 9
2020-06-30 13:28:15,725 Epoch   9: total training loss 135.37
2020-06-30 13:28:15,726 EPOCH 10
2020-06-30 13:28:17,874 Epoch  10 Step:      600 Batch Loss:     1.138426 Tokens per Sec:    14175, Lr: 0.000200
2020-06-30 13:28:27,260 Epoch  10: total training loss 117.65
2020-06-30 13:28:27,261 EPOCH 11
2020-06-30 13:28:35,257 Epoch  11 Step:      700 Batch Loss:     2.283154 Tokens per Sec:    14826, Lr: 0.000200
2020-06-30 13:28:38,396 Epoch  11: total training loss 111.63
2020-06-30 13:28:38,396 EPOCH 12
2020-06-30 13:28:49,585 Epoch  12: total training loss 99.24
2020-06-30 13:28:49,585 EPOCH 13
2020-06-30 13:28:52,586 Epoch  13 Step:      800 Batch Loss:     1.399162 Tokens per Sec:    13261, Lr: 0.000200
2020-06-30 13:29:01,188 Epoch  13: total training loss 92.77
2020-06-30 13:29:01,188 EPOCH 14
2020-06-30 13:29:09,690 Epoch  14 Step:      900 Batch Loss:     1.247634 Tokens per Sec:    14520, Lr: 0.000200
2020-06-30 13:29:12,426 Epoch  14: total training loss 83.52
2020-06-30 13:29:12,426 EPOCH 15
2020-06-30 13:29:23,632 Epoch  15: total training loss 71.57
2020-06-30 13:29:23,633 EPOCH 16
2020-06-30 13:29:26,833 Epoch  16 Step:     1000 Batch Loss:     1.508081 Tokens per Sec:    14692, Lr: 0.000200
2020-06-30 13:30:09,259 Hooray! New best validation result [ppl]!
2020-06-30 13:30:09,259 Saving new checkpoint.
2020-06-30 13:30:15,652 Example #0
2020-06-30 13:30:15,652 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 13:30:15,652 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 13:30:15,652 	Source:     Hello.
2020-06-30 13:30:15,652 	Reference:  Hallo,
2020-06-30 13:30:15,652 	Hypothesis: Hallo.
2020-06-30 13:30:15,652 Example #1
2020-06-30 13:30:15,652 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 13:30:15,652 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 13:30:15,652 	Source:     Hi, how can I help you?
2020-06-30 13:30:15,652 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:30:15,652 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:30:15,652 Example #2
2020-06-30 13:30:15,652 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 13:30:15,652 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 13:30:15,652 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 13:30:15,652 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 13:30:15,652 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall in San Francisco, Kalifornien.
2020-06-30 13:30:15,652 Example #3
2020-06-30 13:30:15,652 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 13:30:15,652 	Raw hypothesis: ['Ok@@', ',', 'welche', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 13:30:15,652 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 13:30:15,652 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 13:30:15,653 	Hypothesis: Ok, welche Art von Restaurant suchen Sie?
2020-06-30 13:30:15,653 Validation result (greedy) at epoch  16, step     1000: bleu:  23.96, loss: 51394.8516, ppl:   7.1538, duration: 48.8188s
2020-06-30 13:30:23,793 Epoch  16: total training loss 67.12
2020-06-30 13:30:23,794 EPOCH 17
2020-06-30 13:30:33,401 Epoch  17 Step:     1100 Batch Loss:     0.872884 Tokens per Sec:    14236, Lr: 0.000200
2020-06-30 13:30:35,365 Epoch  17: total training loss 59.78
2020-06-30 13:30:35,365 EPOCH 18
2020-06-30 13:30:47,146 Epoch  18: total training loss 53.80
2020-06-30 13:30:47,146 EPOCH 19
2020-06-30 13:30:51,652 Epoch  19 Step:     1200 Batch Loss:     1.031874 Tokens per Sec:    14296, Lr: 0.000200
2020-06-30 13:30:58,432 Epoch  19: total training loss 49.96
2020-06-30 13:30:58,433 EPOCH 20
2020-06-30 13:31:08,676 Epoch  20 Step:     1300 Batch Loss:     0.919387 Tokens per Sec:    14626, Lr: 0.000200
2020-06-30 13:31:09,554 Epoch  20: total training loss 45.46
2020-06-30 13:31:09,554 EPOCH 21
2020-06-30 13:31:20,759 Epoch  21: total training loss 43.67
2020-06-30 13:31:20,759 EPOCH 22
2020-06-30 13:31:25,765 Epoch  22 Step:     1400 Batch Loss:     0.574431 Tokens per Sec:    14776, Lr: 0.000200
2020-06-30 13:31:31,913 Epoch  22: total training loss 38.52
2020-06-30 13:31:31,913 EPOCH 23
2020-06-30 13:31:43,283 Epoch  23 Step:     1500 Batch Loss:     0.457514 Tokens per Sec:    13837, Lr: 0.000200
2020-06-30 13:31:43,652 Epoch  23: total training loss 33.83
2020-06-30 13:31:43,652 EPOCH 24
2020-06-30 13:31:55,261 Epoch  24: total training loss 30.69
2020-06-30 13:31:55,262 EPOCH 25
2020-06-30 13:32:00,968 Epoch  25 Step:     1600 Batch Loss:     0.280726 Tokens per Sec:    14012, Lr: 0.000200
2020-06-30 13:32:06,974 Epoch  25: total training loss 27.01
2020-06-30 13:32:06,975 EPOCH 26
2020-06-30 13:32:18,438 Epoch  26: total training loss 24.72
2020-06-30 13:32:18,439 EPOCH 27
2020-06-30 13:32:18,795 Epoch  27 Step:     1700 Batch Loss:     0.222653 Tokens per Sec:    11917, Lr: 0.000200
2020-06-30 13:32:29,672 Epoch  27: total training loss 22.68
2020-06-30 13:32:29,673 EPOCH 28
2020-06-30 13:32:35,754 Epoch  28 Step:     1800 Batch Loss:     0.308166 Tokens per Sec:    14591, Lr: 0.000200
2020-06-30 13:32:40,846 Epoch  28: total training loss 20.84
2020-06-30 13:32:40,846 EPOCH 29
2020-06-30 13:32:51,940 Epoch  29: total training loss 19.23
2020-06-30 13:32:51,940 EPOCH 30
2020-06-30 13:32:52,799 Epoch  30 Step:     1900 Batch Loss:     0.307870 Tokens per Sec:    15058, Lr: 0.000200
2020-06-30 13:33:03,111 Epoch  30: total training loss 18.06
2020-06-30 13:33:03,112 EPOCH 31
2020-06-30 13:33:09,747 Epoch  31 Step:     2000 Batch Loss:     0.280428 Tokens per Sec:    14613, Lr: 0.000200
2020-06-30 13:33:50,685 Hooray! New best validation result [ppl]!
2020-06-30 13:33:50,685 Saving new checkpoint.
2020-06-30 13:33:57,076 Example #0
2020-06-30 13:33:57,077 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 13:33:57,077 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 13:33:57,077 	Source:     Hello.
2020-06-30 13:33:57,077 	Reference:  Hallo,
2020-06-30 13:33:57,077 	Hypothesis: Hallo.
2020-06-30 13:33:57,077 Example #1
2020-06-30 13:33:57,077 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 13:33:57,077 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 13:33:57,077 	Source:     Hi, how can I help you?
2020-06-30 13:33:57,077 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:33:57,077 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:33:57,077 Example #2
2020-06-30 13:33:57,077 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 13:33:57,077 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', '.', 'Ich', 'bin', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 13:33:57,077 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 13:33:57,077 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 13:33:57,077 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall. Ich bin in San Francisco, Kalifornien.
2020-06-30 13:33:57,077 Example #3
2020-06-30 13:33:57,077 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 13:33:57,077 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 13:33:57,077 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 13:33:57,077 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 13:33:57,077 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 13:33:57,078 Validation result (greedy) at epoch  31, step     2000: bleu:  28.84, loss: 49163.7656, ppl:   6.5681, duration: 47.3304s
2020-06-30 13:34:01,401 Epoch  31: total training loss 16.70
2020-06-30 13:34:01,401 EPOCH 32
2020-06-30 13:34:12,335 Epoch  32: total training loss 14.60
2020-06-30 13:34:12,336 EPOCH 33
2020-06-30 13:34:13,975 Epoch  33 Step:     2100 Batch Loss:     0.293953 Tokens per Sec:    13442, Lr: 0.000200
2020-06-30 13:34:23,950 Epoch  33: total training loss 14.36
2020-06-30 13:34:23,950 EPOCH 34
2020-06-30 13:34:31,491 Epoch  34 Step:     2200 Batch Loss:     0.194384 Tokens per Sec:    13961, Lr: 0.000200
2020-06-30 13:34:35,657 Epoch  34: total training loss 14.29
2020-06-30 13:34:35,658 EPOCH 35
2020-06-30 13:34:47,185 Epoch  35: total training loss 12.70
2020-06-30 13:34:47,186 EPOCH 36
2020-06-30 13:34:49,297 Epoch  36 Step:     2300 Batch Loss:     0.173624 Tokens per Sec:    13571, Lr: 0.000200
2020-06-30 13:34:58,712 Epoch  36: total training loss 12.23
2020-06-30 13:34:58,713 EPOCH 37
2020-06-30 13:35:06,880 Epoch  37 Step:     2400 Batch Loss:     0.177431 Tokens per Sec:    13927, Lr: 0.000200
2020-06-30 13:35:10,307 Epoch  37: total training loss 11.97
2020-06-30 13:35:10,307 EPOCH 38
2020-06-30 13:35:21,900 Epoch  38: total training loss 11.36
2020-06-30 13:35:21,901 EPOCH 39
2020-06-30 13:35:24,594 Epoch  39 Step:     2500 Batch Loss:     0.192038 Tokens per Sec:    14363, Lr: 0.000200
2020-06-30 13:35:33,578 Epoch  39: total training loss 11.02
2020-06-30 13:35:33,579 EPOCH 40
2020-06-30 13:35:42,295 Epoch  40 Step:     2600 Batch Loss:     0.178347 Tokens per Sec:    14354, Lr: 0.000200
2020-06-30 13:35:45,115 Epoch  40: total training loss 11.18
2020-06-30 13:35:45,115 EPOCH 41
2020-06-30 13:35:56,777 Epoch  41: total training loss 10.31
2020-06-30 13:35:56,777 EPOCH 42
2020-06-30 13:36:00,260 Epoch  42 Step:     2700 Batch Loss:     0.170589 Tokens per Sec:    13708, Lr: 0.000200
2020-06-30 13:36:08,439 Epoch  42: total training loss 9.85
2020-06-30 13:36:08,440 EPOCH 43
2020-06-30 13:36:18,263 Epoch  43 Step:     2800 Batch Loss:     0.149375 Tokens per Sec:    13753, Lr: 0.000200
2020-06-30 13:36:20,280 Epoch  43: total training loss 9.44
2020-06-30 13:36:20,280 EPOCH 44
2020-06-30 13:36:31,675 Epoch  44: total training loss 9.35
2020-06-30 13:36:31,675 EPOCH 45
2020-06-30 13:36:35,735 Epoch  45 Step:     2900 Batch Loss:     0.134110 Tokens per Sec:    14889, Lr: 0.000200
2020-06-30 13:36:42,846 Epoch  45: total training loss 9.32
2020-06-30 13:36:42,846 EPOCH 46
2020-06-30 13:36:52,696 Epoch  46 Step:     3000 Batch Loss:     0.139628 Tokens per Sec:    14813, Lr: 0.000200
2020-06-30 13:37:37,243 Example #0
2020-06-30 13:37:37,243 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 13:37:37,243 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 13:37:37,243 	Source:     Hello.
2020-06-30 13:37:37,243 	Reference:  Hallo,
2020-06-30 13:37:37,243 	Hypothesis: Hallo.
2020-06-30 13:37:37,243 Example #1
2020-06-30 13:37:37,244 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 13:37:37,244 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 13:37:37,244 	Source:     Hi, how can I help you?
2020-06-30 13:37:37,244 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:37:37,244 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:37:37,244 Example #2
2020-06-30 13:37:37,244 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 13:37:37,244 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 13:37:37,244 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 13:37:37,244 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 13:37:37,244 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-30 13:37:37,244 Example #3
2020-06-30 13:37:37,244 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 13:37:37,244 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 13:37:37,244 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 13:37:37,244 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 13:37:37,244 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 13:37:37,244 Validation result (greedy) at epoch  46, step     3000: bleu:  29.50, loss: 49971.5078, ppl:   6.7744, duration: 44.5474s
2020-06-30 13:37:38,472 Epoch  46: total training loss 9.09
2020-06-30 13:37:38,472 EPOCH 47
2020-06-30 13:37:49,719 Epoch  47: total training loss 8.73
2020-06-30 13:37:49,720 EPOCH 48
2020-06-30 13:37:54,530 Epoch  48 Step:     3100 Batch Loss:     0.110973 Tokens per Sec:    14658, Lr: 0.000200
2020-06-30 13:38:00,896 Epoch  48: total training loss 9.03
2020-06-30 13:38:00,896 EPOCH 49
2020-06-30 13:38:11,544 Epoch  49 Step:     3200 Batch Loss:     0.137330 Tokens per Sec:    14522, Lr: 0.000200
2020-06-30 13:38:12,126 Epoch  49: total training loss 9.89
2020-06-30 13:38:12,126 EPOCH 50
2020-06-30 13:38:23,269 Epoch  50: total training loss 8.58
2020-06-30 13:38:23,269 EPOCH 51
2020-06-30 13:38:28,926 Epoch  51 Step:     3300 Batch Loss:     0.135182 Tokens per Sec:    13758, Lr: 0.000200
2020-06-30 13:38:35,040 Epoch  51: total training loss 8.35
2020-06-30 13:38:35,040 EPOCH 52
2020-06-30 13:38:46,820 Epoch  52: total training loss 8.17
2020-06-30 13:38:46,821 EPOCH 53
2020-06-30 13:38:47,024 Epoch  53 Step:     3400 Batch Loss:     0.116852 Tokens per Sec:    14578, Lr: 0.000200
2020-06-30 13:38:58,335 Epoch  53: total training loss 7.89
2020-06-30 13:38:58,336 EPOCH 54
2020-06-30 13:39:04,229 Epoch  54 Step:     3500 Batch Loss:     0.130622 Tokens per Sec:    14798, Lr: 0.000200
2020-06-30 13:39:09,513 Epoch  54: total training loss 8.14
2020-06-30 13:39:09,513 EPOCH 55
2020-06-30 13:39:21,063 Epoch  55: total training loss 7.33
2020-06-30 13:39:21,064 EPOCH 56
2020-06-30 13:39:21,975 Epoch  56 Step:     3600 Batch Loss:     0.118259 Tokens per Sec:    13973, Lr: 0.000200
2020-06-30 13:39:32,736 Epoch  56: total training loss 7.23
2020-06-30 13:39:32,737 EPOCH 57
2020-06-30 13:39:39,794 Epoch  57 Step:     3700 Batch Loss:     0.097388 Tokens per Sec:    14657, Lr: 0.000200
2020-06-30 13:39:43,880 Epoch  57: total training loss 6.87
2020-06-30 13:39:43,880 EPOCH 58
2020-06-30 13:39:54,995 Epoch  58: total training loss 7.10
2020-06-30 13:39:54,995 EPOCH 59
2020-06-30 13:39:56,585 Epoch  59 Step:     3800 Batch Loss:     0.107987 Tokens per Sec:    15105, Lr: 0.000200
2020-06-30 13:40:06,314 Epoch  59: total training loss 7.15
2020-06-30 13:40:06,315 EPOCH 60
2020-06-30 13:40:14,255 Epoch  60 Step:     3900 Batch Loss:     0.112142 Tokens per Sec:    14005, Lr: 0.000200
2020-06-30 13:40:18,051 Epoch  60: total training loss 6.85
2020-06-30 13:40:18,051 EPOCH 61
2020-06-30 13:40:29,596 Epoch  61: total training loss 6.80
2020-06-30 13:40:29,597 EPOCH 62
2020-06-30 13:40:32,019 Epoch  62 Step:     4000 Batch Loss:     0.089409 Tokens per Sec:    15221, Lr: 0.000200
2020-06-30 13:41:15,561 Example #0
2020-06-30 13:41:15,562 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 13:41:15,562 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 13:41:15,562 	Source:     Hello.
2020-06-30 13:41:15,562 	Reference:  Hallo,
2020-06-30 13:41:15,562 	Hypothesis: Hallo.
2020-06-30 13:41:15,562 Example #1
2020-06-30 13:41:15,562 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 13:41:15,562 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 13:41:15,562 	Source:     Hi, how can I help you?
2020-06-30 13:41:15,562 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:41:15,562 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:41:15,562 Example #2
2020-06-30 13:41:15,562 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 13:41:15,562 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 13:41:15,562 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 13:41:15,562 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 13:41:15,562 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-30 13:41:15,562 Example #3
2020-06-30 13:41:15,562 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 13:41:15,562 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 13:41:15,562 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 13:41:15,562 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 13:41:15,562 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 13:41:15,562 Validation result (greedy) at epoch  62, step     4000: bleu:  29.36, loss: 50378.0195, ppl:   6.8807, duration: 43.5425s
2020-06-30 13:41:24,372 Epoch  62: total training loss 6.75
2020-06-30 13:41:24,372 EPOCH 63
2020-06-30 13:41:32,986 Epoch  63 Step:     4100 Batch Loss:     0.095757 Tokens per Sec:    14416, Lr: 0.000200
2020-06-30 13:41:35,878 Epoch  63: total training loss 6.67
2020-06-30 13:41:35,878 EPOCH 64
2020-06-30 13:41:47,691 Epoch  64: total training loss 6.38
2020-06-30 13:41:47,691 EPOCH 65
2020-06-30 13:41:51,270 Epoch  65 Step:     4200 Batch Loss:     0.097714 Tokens per Sec:    13911, Lr: 0.000200
2020-06-30 13:41:59,674 Epoch  65: total training loss 6.56
2020-06-30 13:41:59,675 EPOCH 66
2020-06-30 13:42:09,442 Epoch  66 Step:     4300 Batch Loss:     0.101521 Tokens per Sec:    14000, Lr: 0.000200
2020-06-30 13:42:11,407 Epoch  66: total training loss 6.38
2020-06-30 13:42:11,408 EPOCH 67
2020-06-30 13:42:23,185 Epoch  67: total training loss 6.19
2020-06-30 13:42:23,186 EPOCH 68
2020-06-30 13:42:27,404 Epoch  68 Step:     4400 Batch Loss:     0.100653 Tokens per Sec:    13601, Lr: 0.000200
2020-06-30 13:42:34,652 Epoch  68: total training loss 6.35
2020-06-30 13:42:34,652 EPOCH 69
2020-06-30 13:42:44,552 Epoch  69 Step:     4500 Batch Loss:     0.094842 Tokens per Sec:    14523, Lr: 0.000200
2020-06-30 13:42:45,747 Epoch  69: total training loss 6.45
2020-06-30 13:42:45,748 EPOCH 70
2020-06-30 13:42:56,975 Epoch  70: total training loss 6.73
2020-06-30 13:42:56,975 EPOCH 71
2020-06-30 13:43:01,602 Epoch  71 Step:     4600 Batch Loss:     0.092443 Tokens per Sec:    14645, Lr: 0.000200
2020-06-30 13:43:08,152 Epoch  71: total training loss 6.47
2020-06-30 13:43:08,153 EPOCH 72
2020-06-30 13:43:18,827 Epoch  72 Step:     4700 Batch Loss:     0.104221 Tokens per Sec:    14558, Lr: 0.000200
2020-06-30 13:43:19,460 Epoch  72: total training loss 6.17
2020-06-30 13:43:19,460 EPOCH 73
2020-06-30 13:43:31,245 Epoch  73: total training loss 6.05
2020-06-30 13:43:31,245 EPOCH 74
2020-06-30 13:43:36,757 Epoch  74 Step:     4800 Batch Loss:     0.123127 Tokens per Sec:    14022, Lr: 0.000200
2020-06-30 13:43:42,921 Epoch  74: total training loss 6.15
2020-06-30 13:43:42,922 EPOCH 75
2020-06-30 13:43:54,535 Epoch  75 Step:     4900 Batch Loss:     0.098298 Tokens per Sec:    14033, Lr: 0.000200
2020-06-30 13:43:54,536 Epoch  75: total training loss 5.92
2020-06-30 13:43:54,536 EPOCH 76
2020-06-30 13:44:06,076 Epoch  76: total training loss 5.68
2020-06-30 13:44:06,076 EPOCH 77
2020-06-30 13:44:12,451 Epoch  77 Step:     5000 Batch Loss:     0.083346 Tokens per Sec:    13955, Lr: 0.000200
2020-06-30 13:44:53,593 Example #0
2020-06-30 13:44:53,594 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 13:44:53,594 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 13:44:53,594 	Source:     Hello.
2020-06-30 13:44:53,594 	Reference:  Hallo,
2020-06-30 13:44:53,594 	Hypothesis: Hallo.
2020-06-30 13:44:53,594 Example #1
2020-06-30 13:44:53,594 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 13:44:53,594 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 13:44:53,594 	Source:     Hi, how can I help you?
2020-06-30 13:44:53,594 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:44:53,594 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:44:53,594 Example #2
2020-06-30 13:44:53,594 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 13:44:53,594 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', '.']
2020-06-30 13:44:53,594 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 13:44:53,594 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 13:44:53,594 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall.
2020-06-30 13:44:53,594 Example #3
2020-06-30 13:44:53,594 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 13:44:53,594 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 13:44:53,594 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 13:44:53,594 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 13:44:53,594 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 13:44:53,594 Validation result (greedy) at epoch  77, step     5000: bleu:  29.58, loss: 50428.3594, ppl:   6.8939, duration: 41.1427s
2020-06-30 13:44:59,023 Epoch  77: total training loss 5.74
2020-06-30 13:44:59,023 EPOCH 78
2020-06-30 13:45:10,986 Epoch  78: total training loss 5.70
2020-06-30 13:45:10,987 EPOCH 79
2020-06-30 13:45:11,715 Epoch  79 Step:     5100 Batch Loss:     0.101023 Tokens per Sec:    15391, Lr: 0.000200
2020-06-30 13:45:22,847 Epoch  79: total training loss 5.92
2020-06-30 13:45:22,847 EPOCH 80
2020-06-30 13:45:29,827 Epoch  80 Step:     5200 Batch Loss:     0.094387 Tokens per Sec:    13827, Lr: 0.000200
2020-06-30 13:45:34,689 Epoch  80: total training loss 6.10
2020-06-30 13:45:34,690 EPOCH 81
2020-06-30 13:45:46,499 Epoch  81: total training loss 7.78
2020-06-30 13:45:46,501 EPOCH 82
2020-06-30 13:45:47,964 Epoch  82 Step:     5300 Batch Loss:     0.094702 Tokens per Sec:    13478, Lr: 0.000200
2020-06-30 13:45:58,400 Epoch  82: total training loss 6.27
2020-06-30 13:45:58,401 EPOCH 83
2020-06-30 13:46:06,219 Epoch  83 Step:     5400 Batch Loss:     0.085621 Tokens per Sec:    13808, Lr: 0.000200
2020-06-30 13:46:09,996 Epoch  83: total training loss 5.79
2020-06-30 13:46:09,996 EPOCH 84
2020-06-30 13:46:21,191 Epoch  84: total training loss 5.89
2020-06-30 13:46:21,191 EPOCH 85
2020-06-30 13:46:23,500 Epoch  85 Step:     5500 Batch Loss:     0.089882 Tokens per Sec:    15242, Lr: 0.000200
2020-06-30 13:46:32,442 Epoch  85: total training loss 5.55
2020-06-30 13:46:32,442 EPOCH 86
2020-06-30 13:46:40,747 Epoch  86 Step:     5600 Batch Loss:     0.071181 Tokens per Sec:    14430, Lr: 0.000200
2020-06-30 13:46:43,680 Epoch  86: total training loss 5.23
2020-06-30 13:46:43,680 EPOCH 87
2020-06-30 13:46:55,172 Epoch  87: total training loss 5.07
2020-06-30 13:46:55,172 EPOCH 88
2020-06-30 13:46:58,265 Epoch  88 Step:     5700 Batch Loss:     0.075109 Tokens per Sec:    13857, Lr: 0.000200
2020-06-30 13:47:06,719 Epoch  88: total training loss 4.95
2020-06-30 13:47:06,719 EPOCH 89
2020-06-30 13:47:15,792 Epoch  89 Step:     5800 Batch Loss:     0.072679 Tokens per Sec:    14312, Lr: 0.000200
2020-06-30 13:47:18,155 Epoch  89: total training loss 5.15
2020-06-30 13:47:18,155 EPOCH 90
2020-06-30 13:47:29,681 Epoch  90: total training loss 4.94
2020-06-30 13:47:29,681 EPOCH 91
2020-06-30 13:47:33,691 Epoch  91 Step:     5900 Batch Loss:     0.065486 Tokens per Sec:    14049, Lr: 0.000200
2020-06-30 13:47:41,391 Epoch  91: total training loss 5.09
2020-06-30 13:47:41,392 EPOCH 92
2020-06-30 13:47:51,646 Epoch  92 Step:     6000 Batch Loss:     0.074048 Tokens per Sec:    14321, Lr: 0.000200
2020-06-30 13:48:28,641 Example #0
2020-06-30 13:48:28,641 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 13:48:28,641 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 13:48:28,641 	Source:     Hello.
2020-06-30 13:48:28,641 	Reference:  Hallo,
2020-06-30 13:48:28,641 	Hypothesis: Hallo.
2020-06-30 13:48:28,641 Example #1
2020-06-30 13:48:28,641 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 13:48:28,641 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 13:48:28,641 	Source:     Hi, how can I help you?
2020-06-30 13:48:28,641 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:48:28,641 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 13:48:28,641 Example #2
2020-06-30 13:48:28,641 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 13:48:28,641 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', '.']
2020-06-30 13:48:28,642 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 13:48:28,642 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 13:48:28,642 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall.
2020-06-30 13:48:28,642 Example #3
2020-06-30 13:48:28,642 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 13:48:28,642 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 13:48:28,642 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 13:48:28,642 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 13:48:28,642 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 13:48:28,642 Validation result (greedy) at epoch  92, step     6000: bleu:  30.18, loss: 50766.2539, ppl:   6.9837, duration: 36.9948s
2020-06-30 13:48:29,887 Epoch  92: total training loss 4.84
2020-06-30 13:48:29,888 EPOCH 93
2020-06-30 13:48:41,904 Epoch  93: total training loss 5.01
2020-06-30 13:48:41,905 EPOCH 94
2020-06-30 13:48:46,901 Epoch  94 Step:     6100 Batch Loss:     0.066803 Tokens per Sec:    13801, Lr: 0.000200
2020-06-30 13:48:53,852 Epoch  94: total training loss 5.09
2020-06-30 13:48:53,852 EPOCH 95
2020-06-30 13:49:05,125 Epoch  95 Step:     6200 Batch Loss:     0.097595 Tokens per Sec:    13832, Lr: 0.000200
2020-06-30 13:49:05,667 Epoch  95: total training loss 5.23
2020-06-30 13:49:05,667 EPOCH 96
2020-06-30 13:49:17,312 Epoch  96: total training loss 5.24
2020-06-30 13:49:17,313 EPOCH 97
2020-06-30 13:49:22,902 Epoch  97 Step:     6300 Batch Loss:     0.070596 Tokens per Sec:    13532, Lr: 0.000200
2020-06-30 13:49:29,324 Epoch  97: total training loss 4.90
2020-06-30 13:49:29,325 EPOCH 98
2020-06-30 13:49:41,227 Epoch  98 Step:     6400 Batch Loss:     0.071651 Tokens per Sec:    13692, Lr: 0.000200
2020-06-30 13:49:41,228 Epoch  98: total training loss 4.75
2020-06-30 13:49:41,229 EPOCH 99
2020-06-30 13:49:52,710 Epoch  99: total training loss 4.68
2020-06-30 13:49:52,711 EPOCH 100
2020-06-30 13:49:58,739 Epoch 100 Step:     6500 Batch Loss:     0.064175 Tokens per Sec:    14510, Lr: 0.000200
2020-06-30 13:50:03,961 Epoch 100: total training loss 4.68
2020-06-30 13:50:03,961 Training ended after 100 epochs.
2020-06-30 13:50:03,961 Best validation result (greedy) at step     2000:   6.57 ppl.
2020-06-30 13:50:34,731  dev bleu:  29.78 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-30 13:50:34,735 Translations saved to: models/transformer_wmt17bpe_ende/00002000.hyps.dev
2020-06-30 13:50:57,559 test bleu:  27.50 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-30 13:50:57,563 Translations saved to: models/transformer_wmt17bpe_ende/00002000.hyps.test
