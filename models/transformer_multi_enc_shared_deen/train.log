2020-06-17 10:48:25,701 Hello! This is Joey-NMT.
2020-06-17 10:48:32,834 Total params: 53690369
2020-06-17 10:48:32,836 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-17 10:48:35,173 cfg.name                           : transformer_multi_enc_deen
2020-06-17 10:48:35,174 cfg.data.src                       : de
2020-06-17 10:48:35,174 cfg.data.trg                       : en
2020-06-17 10:48:35,174 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-17 10:48:35,174 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-17 10:48:35,174 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-17 10:48:35,174 cfg.data.level                     : bpe
2020-06-17 10:48:35,174 cfg.data.lowercase                 : True
2020-06-17 10:48:35,174 cfg.data.max_sent_length           : 100
2020-06-17 10:48:35,174 cfg.testing.beam_size              : 5
2020-06-17 10:48:35,174 cfg.testing.alpha                  : 1.0
2020-06-17 10:48:35,174 cfg.training.random_seed           : 42
2020-06-17 10:48:35,174 cfg.training.optimizer             : adam
2020-06-17 10:48:35,174 cfg.training.normalization         : tokens
2020-06-17 10:48:35,174 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-17 10:48:35,174 cfg.training.scheduling            : plateau
2020-06-17 10:48:35,174 cfg.training.patience              : 8
2020-06-17 10:48:35,174 cfg.training.decrease_factor       : 0.7
2020-06-17 10:48:35,174 cfg.training.loss                  : crossentropy
2020-06-17 10:48:35,174 cfg.training.learning_rate         : 0.0002
2020-06-17 10:48:35,174 cfg.training.learning_rate_min     : 1e-08
2020-06-17 10:48:35,174 cfg.training.weight_decay          : 0.0
2020-06-17 10:48:35,174 cfg.training.label_smoothing       : 0.1
2020-06-17 10:48:35,174 cfg.training.batch_size            : 4096
2020-06-17 10:48:35,174 cfg.training.batch_type            : token
2020-06-17 10:48:35,174 cfg.training.eval_batch_size       : 3600
2020-06-17 10:48:35,174 cfg.training.eval_batch_type       : token
2020-06-17 10:48:35,175 cfg.training.batch_multiplier      : 1
2020-06-17 10:48:35,175 cfg.training.early_stopping_metric : ppl
2020-06-17 10:48:35,175 cfg.training.epochs                : 100
2020-06-17 10:48:35,175 cfg.training.validation_freq       : 1000
2020-06-17 10:48:35,175 cfg.training.logging_freq          : 100
2020-06-17 10:48:35,175 cfg.training.eval_metric           : bleu
2020-06-17 10:48:35,175 cfg.training.model_dir             : models/transformer_multi_enc_shared_deen
2020-06-17 10:48:35,175 cfg.training.overwrite             : True
2020-06-17 10:48:35,175 cfg.training.shuffle               : True
2020-06-17 10:48:35,175 cfg.training.use_cuda              : True
2020-06-17 10:48:35,175 cfg.training.max_output_length     : 100
2020-06-17 10:48:35,175 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-17 10:48:35,175 cfg.training.keep_last_ckpts       : 3
2020-06-17 10:48:35,175 cfg.model.initializer              : xavier
2020-06-17 10:48:35,175 cfg.model.bias_initializer         : zeros
2020-06-17 10:48:35,175 cfg.model.init_gain                : 1.0
2020-06-17 10:48:35,175 cfg.model.embed_initializer        : xavier
2020-06-17 10:48:35,175 cfg.model.embed_init_gain          : 1.0
2020-06-17 10:48:35,175 cfg.model.tied_embeddings          : False
2020-06-17 10:48:35,175 cfg.model.tied_softmax             : True
2020-06-17 10:48:35,175 cfg.model.encoder.type             : transformer
2020-06-17 10:48:35,175 cfg.model.encoder.num_layers       : 6
2020-06-17 10:48:35,175 cfg.model.encoder.num_heads        : 8
2020-06-17 10:48:35,175 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-17 10:48:35,175 cfg.model.encoder.embeddings.scale : True
2020-06-17 10:48:35,175 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-17 10:48:35,175 cfg.model.encoder.hidden_size      : 512
2020-06-17 10:48:35,175 cfg.model.encoder.ff_size          : 2048
2020-06-17 10:48:35,175 cfg.model.encoder.dropout          : 0.1
2020-06-17 10:48:35,175 cfg.model.encoder.freeze           : False
2020-06-17 10:48:35,175 cfg.model.encoder.multi_encoder    : True
2020-06-17 10:48:35,175 cfg.model.encoder.share_encoder    : True
2020-06-17 10:48:35,175 cfg.model.decoder.type             : transformer
2020-06-17 10:48:35,175 cfg.model.decoder.num_layers       : 6
2020-06-17 10:48:35,175 cfg.model.decoder.num_heads        : 8
2020-06-17 10:48:35,175 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-17 10:48:35,175 cfg.model.decoder.embeddings.scale : True
2020-06-17 10:48:35,175 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-17 10:48:35,175 cfg.model.decoder.hidden_size      : 512
2020-06-17 10:48:35,175 cfg.model.decoder.ff_size          : 2048
2020-06-17 10:48:35,175 cfg.model.decoder.dropout          : 0.1
2020-06-17 10:48:35,176 cfg.model.decoder.freeze           : False
2020-06-17 10:48:35,176 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-17 10:48:35,176 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-17 10:48:35,176 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-17 10:48:35,176 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-17 10:48:35,176 Number of Src words (types): 5876
2020-06-17 10:48:35,176 Number of Trg words (types): 4561
2020-06-17 10:48:35,176 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=5876),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4561))
2020-06-17 10:48:35,186 EPOCH 1
2020-06-17 10:48:55,795 Epoch   1: total training loss 282.77
2020-06-17 10:48:55,796 EPOCH 2
2020-06-17 10:49:13,986 Epoch   2 Step:      100 Batch Loss:     4.695806 Tokens per Sec:     6370, Lr: 0.000200
2020-06-17 10:49:16,250 Epoch   2: total training loss 238.59
2020-06-17 10:49:16,251 EPOCH 3
2020-06-17 10:49:38,618 Epoch   3: total training loss 226.55
2020-06-17 10:49:38,618 EPOCH 4
2020-06-17 10:49:55,674 Epoch   4 Step:      200 Batch Loss:     4.487894 Tokens per Sec:     5636, Lr: 0.000200
2020-06-17 10:50:01,529 Epoch   4: total training loss 207.96
2020-06-17 10:50:01,529 EPOCH 5
2020-06-17 10:50:24,357 Epoch   5: total training loss 190.16
2020-06-17 10:50:24,358 EPOCH 6
2020-06-17 10:50:37,629 Epoch   6 Step:      300 Batch Loss:     3.571187 Tokens per Sec:     5716, Lr: 0.000200
2020-06-17 10:50:47,339 Epoch   6: total training loss 171.65
2020-06-17 10:50:47,340 EPOCH 7
2020-06-17 10:51:10,745 Epoch   7: total training loss 160.38
2020-06-17 10:51:10,746 EPOCH 8
2020-06-17 10:51:20,610 Epoch   8 Step:      400 Batch Loss:     2.038409 Tokens per Sec:     5821, Lr: 0.000200
2020-06-17 10:51:33,876 Epoch   8: total training loss 150.61
2020-06-17 10:51:33,876 EPOCH 9
2020-06-17 10:51:57,013 Epoch   9: total training loss 135.22
2020-06-17 10:51:57,014 EPOCH 10
2020-06-17 10:52:04,231 Epoch  10 Step:      500 Batch Loss:     3.005708 Tokens per Sec:     5830, Lr: 0.000200
2020-06-17 10:52:20,158 Epoch  10: total training loss 136.94
2020-06-17 10:52:20,159 EPOCH 11
2020-06-17 10:52:43,093 Epoch  11: total training loss 128.35
2020-06-17 10:52:43,094 EPOCH 12
2020-06-17 10:52:47,177 Epoch  12 Step:      600 Batch Loss:     1.767917 Tokens per Sec:     5568, Lr: 0.000200
2020-06-17 10:53:05,910 Epoch  12: total training loss 118.42
2020-06-17 10:53:05,910 EPOCH 13
2020-06-17 10:53:28,878 Epoch  13: total training loss 114.40
2020-06-17 10:53:28,878 EPOCH 14
2020-06-17 10:53:29,796 Epoch  14 Step:      700 Batch Loss:     2.258226 Tokens per Sec:     4828, Lr: 0.000200
2020-06-17 10:53:51,497 Epoch  14: total training loss 106.06
2020-06-17 10:53:51,498 EPOCH 15
2020-06-17 10:54:12,201 Epoch  15 Step:      800 Batch Loss:     1.771706 Tokens per Sec:     5832, Lr: 0.000200
2020-06-17 10:54:14,361 Epoch  15: total training loss 95.11
2020-06-17 10:54:14,361 EPOCH 16
2020-06-17 10:54:37,392 Epoch  16: total training loss 91.58
2020-06-17 10:54:37,392 EPOCH 17
2020-06-17 10:54:55,764 Epoch  17 Step:      900 Batch Loss:     1.998274 Tokens per Sec:     5706, Lr: 0.000200
2020-06-17 10:54:59,901 Epoch  17: total training loss 82.92
2020-06-17 10:54:59,901 EPOCH 18
2020-06-17 10:55:22,353 Epoch  18: total training loss 78.35
2020-06-17 10:55:22,356 EPOCH 19
2020-06-17 10:55:36,325 Epoch  19 Step:     1000 Batch Loss:     1.022329 Tokens per Sec:     6068, Lr: 0.000200
2020-06-17 10:55:58,749 Hooray! New best validation result [ppl]!
2020-06-17 10:55:58,750 Saving new checkpoint.
2020-06-17 10:56:05,860 Example #0
2020-06-17 10:56:05,861 	Raw source:     ['hallo', ',']
2020-06-17 10:56:05,861 	Raw hypothesis: ['hi', ',', 'how']
2020-06-17 10:56:05,861 	Source:     hallo ,
2020-06-17 10:56:05,861 	Reference:  hello .
2020-06-17 10:56:05,861 	Hypothesis: hi , how
2020-06-17 10:56:05,861 Example #1
2020-06-17 10:56:05,861 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:56:05,861 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:56:05,861 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:56:05,861 	Reference:  hi , how can i help you ?
2020-06-17 10:56:05,861 	Hypothesis: hi , how can i help you ?
2020-06-17 10:56:05,861 Example #2
2020-06-17 10:56:05,861 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:56:05,861 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'san', 'francisco', ',', 'california', '.', 'i', '&apos;m', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:56:05,861 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:56:05,861 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:56:05,861 	Hypothesis: hi , i &apos;m looking for a restaurant in san francisco , california . i &apos;m in san francisco , california .
2020-06-17 10:56:05,861 Example #3
2020-06-17 10:56:05,861 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:56:05,861 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:56:05,861 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:56:05,861 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:56:05,861 	Hypothesis: ok , what kind of restaurant are you looking for ?
2020-06-17 10:56:05,861 Validation result (greedy) at epoch  19, step     1000: bleu:  22.84, loss: 47014.2852, ppl:   9.3838, duration: 29.5358s
2020-06-17 10:56:14,009 Epoch  19: total training loss 72.01
2020-06-17 10:56:14,009 EPOCH 20
2020-06-17 10:56:36,398 Epoch  20: total training loss 68.52
2020-06-17 10:56:36,398 EPOCH 21
2020-06-17 10:56:47,821 Epoch  21 Step:     1100 Batch Loss:     0.747630 Tokens per Sec:     5493, Lr: 0.000200
2020-06-17 10:56:59,541 Epoch  21: total training loss 64.37
2020-06-17 10:56:59,541 EPOCH 22
2020-06-17 10:57:22,289 Epoch  22: total training loss 55.59
2020-06-17 10:57:22,290 EPOCH 23
2020-06-17 10:57:29,938 Epoch  23 Step:     1200 Batch Loss:     1.597655 Tokens per Sec:     5736, Lr: 0.000200
2020-06-17 10:57:45,216 Epoch  23: total training loss 54.93
2020-06-17 10:57:45,217 EPOCH 24
2020-06-17 10:58:07,742 Epoch  24: total training loss 50.90
2020-06-17 10:58:07,743 EPOCH 25
2020-06-17 10:58:12,055 Epoch  25 Step:     1300 Batch Loss:     0.627010 Tokens per Sec:     5745, Lr: 0.000200
2020-06-17 10:58:30,465 Epoch  25: total training loss 48.12
2020-06-17 10:58:30,466 EPOCH 26
2020-06-17 10:58:53,514 Epoch  26: total training loss 42.46
2020-06-17 10:58:53,515 EPOCH 27
2020-06-17 10:58:53,943 Epoch  27 Step:     1400 Batch Loss:     0.516973 Tokens per Sec:     6304, Lr: 0.000200
2020-06-17 10:59:16,301 Epoch  27: total training loss 38.13
2020-06-17 10:59:16,303 EPOCH 28
2020-06-17 10:59:35,559 Epoch  28 Step:     1500 Batch Loss:     0.447649 Tokens per Sec:     5961, Lr: 0.000200
2020-06-17 10:59:38,582 Epoch  28: total training loss 35.40
2020-06-17 10:59:38,582 EPOCH 29
2020-06-17 11:00:01,164 Epoch  29: total training loss 31.95
2020-06-17 11:00:01,165 EPOCH 30
2020-06-17 11:00:18,145 Epoch  30 Step:     1600 Batch Loss:     0.529157 Tokens per Sec:     5799, Lr: 0.000200
2020-06-17 11:00:23,985 Epoch  30: total training loss 28.78
2020-06-17 11:00:23,985 EPOCH 31
2020-06-17 11:00:46,548 Epoch  31: total training loss 25.85
2020-06-17 11:00:46,549 EPOCH 32
2020-06-17 11:01:00,671 Epoch  32 Step:     1700 Batch Loss:     0.319377 Tokens per Sec:     5767, Lr: 0.000200
2020-06-17 11:01:09,180 Epoch  32: total training loss 24.16
2020-06-17 11:01:09,181 EPOCH 33
2020-06-17 11:01:31,437 Epoch  33: total training loss 22.58
2020-06-17 11:01:31,438 EPOCH 34
2020-06-17 11:01:41,765 Epoch  34 Step:     1800 Batch Loss:     0.306282 Tokens per Sec:     5871, Lr: 0.000200
2020-06-17 11:01:53,696 Epoch  34: total training loss 21.47
2020-06-17 11:01:53,697 EPOCH 35
2020-06-17 11:02:16,452 Epoch  35: total training loss 20.11
2020-06-17 11:02:16,452 EPOCH 36
2020-06-17 11:02:22,865 Epoch  36 Step:     1900 Batch Loss:     0.292467 Tokens per Sec:     6248, Lr: 0.000200
2020-06-17 11:02:39,445 Epoch  36: total training loss 19.98
2020-06-17 11:02:39,446 EPOCH 37
2020-06-17 11:03:02,184 Epoch  37: total training loss 17.31
2020-06-17 11:03:02,185 EPOCH 38
2020-06-17 11:03:05,717 Epoch  38 Step:     2000 Batch Loss:     0.356945 Tokens per Sec:     5993, Lr: 0.000200
2020-06-17 11:03:35,592 Hooray! New best validation result [ppl]!
2020-06-17 11:03:35,593 Saving new checkpoint.
2020-06-17 11:03:42,815 Example #0
2020-06-17 11:03:42,816 	Raw source:     ['hallo', ',']
2020-06-17 11:03:42,816 	Raw hypothesis: ['hi', 'there', '?']
2020-06-17 11:03:42,816 	Source:     hallo ,
2020-06-17 11:03:42,816 	Reference:  hello .
2020-06-17 11:03:42,816 	Hypothesis: hi there ?
2020-06-17 11:03:42,816 Example #1
2020-06-17 11:03:42,816 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 11:03:42,816 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 11:03:42,816 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 11:03:42,816 	Reference:  hi , how can i help you ?
2020-06-17 11:03:42,816 	Hypothesis: hi , how can i help you ?
2020-06-17 11:03:42,816 Example #2
2020-06-17 11:03:42,816 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 11:03:42,816 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'sacramento', ',', 'california', '.']
2020-06-17 11:03:42,816 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 11:03:42,816 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 11:03:42,816 	Hypothesis: hi , i &apos;m looking for a restaurant in the arden fair mall in sacramento , california .
2020-06-17 11:03:42,817 Example #3
2020-06-17 11:03:42,817 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 11:03:42,817 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 11:03:42,817 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 11:03:42,817 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 11:03:42,817 	Hypothesis: ok , what kind of restaurant are you looking for ?
2020-06-17 11:03:42,817 Validation result (greedy) at epoch  38, step     2000: bleu:  34.58, loss: 44927.0859, ppl:   8.4959, duration: 37.0985s
2020-06-17 11:04:01,082 Epoch  38: total training loss 15.89
2020-06-17 11:04:01,083 EPOCH 39
2020-06-17 11:04:23,980 Epoch  39: total training loss 14.45
2020-06-17 11:04:23,981 EPOCH 40
2020-06-17 11:04:24,322 Epoch  40 Step:     2100 Batch Loss:     0.257745 Tokens per Sec:     7542, Lr: 0.000200
2020-06-17 11:04:46,659 Epoch  40: total training loss 13.63
2020-06-17 11:04:46,660 EPOCH 41
2020-06-17 11:05:06,766 Epoch  41 Step:     2200 Batch Loss:     0.178550 Tokens per Sec:     5790, Lr: 0.000200
2020-06-17 11:05:09,499 Epoch  41: total training loss 12.64
2020-06-17 11:05:09,500 EPOCH 42
2020-06-17 11:05:32,493 Epoch  42: total training loss 12.20
2020-06-17 11:05:32,494 EPOCH 43
2020-06-17 11:05:48,360 Epoch  43 Step:     2300 Batch Loss:     0.267502 Tokens per Sec:     6009, Lr: 0.000200
2020-06-17 11:05:55,258 Epoch  43: total training loss 11.55
2020-06-17 11:05:55,258 EPOCH 44
2020-06-17 11:06:17,546 Epoch  44: total training loss 11.17
2020-06-17 11:06:17,547 EPOCH 45
2020-06-17 11:06:31,012 Epoch  45 Step:     2400 Batch Loss:     0.180721 Tokens per Sec:     5932, Lr: 0.000200
2020-06-17 11:06:39,814 Epoch  45: total training loss 10.65
2020-06-17 11:06:39,815 EPOCH 46
2020-06-17 11:07:02,014 Epoch  46: total training loss 10.20
2020-06-17 11:07:02,014 EPOCH 47
2020-06-17 11:07:12,866 Epoch  47 Step:     2500 Batch Loss:     0.165745 Tokens per Sec:     5665, Lr: 0.000200
2020-06-17 11:07:24,071 Epoch  47: total training loss 10.08
2020-06-17 11:07:24,072 EPOCH 48
2020-06-17 11:07:46,206 Epoch  48: total training loss 9.71
2020-06-17 11:07:46,207 EPOCH 49
2020-06-17 11:07:53,275 Epoch  49 Step:     2600 Batch Loss:     0.155550 Tokens per Sec:     5762, Lr: 0.000200
2020-06-17 11:08:08,526 Epoch  49: total training loss 9.84
2020-06-17 11:08:08,527 EPOCH 50
2020-06-17 11:08:31,182 Epoch  50: total training loss 11.07
2020-06-17 11:08:31,182 EPOCH 51
2020-06-17 11:08:34,745 Epoch  51 Step:     2700 Batch Loss:     0.178821 Tokens per Sec:     5767, Lr: 0.000200
2020-06-17 11:08:54,053 Epoch  51: total training loss 10.30
2020-06-17 11:08:54,054 EPOCH 52
2020-06-17 11:09:16,374 Epoch  52 Step:     2800 Batch Loss:     0.161357 Tokens per Sec:     5773, Lr: 0.000200
2020-06-17 11:09:16,827 Epoch  52: total training loss 9.23
2020-06-17 11:09:16,828 EPOCH 53
2020-06-17 11:09:39,654 Epoch  53: total training loss 8.49
2020-06-17 11:09:39,654 EPOCH 54
2020-06-17 11:09:58,798 Epoch  54 Step:     2900 Batch Loss:     0.126712 Tokens per Sec:     5640, Lr: 0.000200
2020-06-17 11:10:02,344 Epoch  54: total training loss 8.13
2020-06-17 11:10:02,344 EPOCH 55
2020-06-17 11:10:25,170 Epoch  55: total training loss 7.71
2020-06-17 11:10:25,171 EPOCH 56
2020-06-17 11:10:40,850 Epoch  56 Step:     3000 Batch Loss:     0.133824 Tokens per Sec:     5815, Lr: 0.000200
2020-06-17 11:11:07,456 Example #0
2020-06-17 11:11:07,456 	Raw source:     ['hallo', ',']
2020-06-17 11:11:07,456 	Raw hypothesis: ['hello', '.']
2020-06-17 11:11:07,456 	Source:     hallo ,
2020-06-17 11:11:07,456 	Reference:  hello .
2020-06-17 11:11:07,456 	Hypothesis: hello .
2020-06-17 11:11:07,456 Example #1
2020-06-17 11:11:07,456 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 11:11:07,456 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 11:11:07,456 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 11:11:07,456 	Reference:  hi , how can i help you ?
2020-06-17 11:11:07,456 	Hypothesis: hi , how can i help you ?
2020-06-17 11:11:07,456 Example #2
2020-06-17 11:11:07,456 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 11:11:07,456 	Raw hypothesis: ['hey', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', '.']
2020-06-17 11:11:07,457 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 11:11:07,457 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 11:11:07,457 	Hypothesis: hey , i &apos;m looking for a restaurant in the arden fair mall .
2020-06-17 11:11:07,457 Example #3
2020-06-17 11:11:07,457 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 11:11:07,457 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 11:11:07,457 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 11:11:07,457 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 11:11:07,457 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 11:11:07,457 Validation result (greedy) at epoch  56, step     3000: bleu:  36.29, loss: 45255.2070, ppl:   8.6297, duration: 26.6061s
2020-06-17 11:11:14,538 Epoch  56: total training loss 7.44
2020-06-17 11:11:14,538 EPOCH 57
2020-06-17 11:11:37,161 Epoch  57: total training loss 7.32
2020-06-17 11:11:37,161 EPOCH 58
2020-06-17 11:11:49,419 Epoch  58 Step:     3100 Batch Loss:     0.122062 Tokens per Sec:     5905, Lr: 0.000200
2020-06-17 11:11:59,682 Epoch  58: total training loss 7.10
2020-06-17 11:11:59,682 EPOCH 59
2020-06-17 11:12:21,843 Epoch  59: total training loss 7.21
2020-06-17 11:12:21,844 EPOCH 60
2020-06-17 11:12:31,109 Epoch  60 Step:     3200 Batch Loss:     0.148344 Tokens per Sec:     5826, Lr: 0.000200
2020-06-17 11:12:44,308 Epoch  60: total training loss 6.99
2020-06-17 11:12:44,308 EPOCH 61
2020-06-17 11:13:06,892 Epoch  61: total training loss 6.98
2020-06-17 11:13:06,893 EPOCH 62
2020-06-17 11:13:12,714 Epoch  62 Step:     3300 Batch Loss:     0.124229 Tokens per Sec:     6179, Lr: 0.000200
2020-06-17 11:13:29,396 Epoch  62: total training loss 7.07
2020-06-17 11:13:29,397 EPOCH 63
2020-06-17 11:13:51,973 Epoch  63: total training loss 6.59
2020-06-17 11:13:51,974 EPOCH 64
2020-06-17 11:13:54,415 Epoch  64 Step:     3400 Batch Loss:     0.106560 Tokens per Sec:     7244, Lr: 0.000200
2020-06-17 11:14:14,472 Epoch  64: total training loss 6.52
2020-06-17 11:14:14,473 EPOCH 65
2020-06-17 11:14:37,440 Epoch  65: total training loss 6.42
2020-06-17 11:14:37,441 EPOCH 66
2020-06-17 11:14:37,716 Epoch  66 Step:     3500 Batch Loss:     0.120889 Tokens per Sec:     9912, Lr: 0.000200
2020-06-17 11:15:00,120 Epoch  66: total training loss 6.55
2020-06-17 11:15:00,121 EPOCH 67
2020-06-17 11:15:20,274 Epoch  67 Step:     3600 Batch Loss:     0.096744 Tokens per Sec:     5799, Lr: 0.000200
2020-06-17 11:15:22,928 Epoch  67: total training loss 6.37
2020-06-17 11:15:22,928 EPOCH 68
2020-06-17 11:15:45,998 Epoch  68: total training loss 5.97
2020-06-17 11:15:45,999 EPOCH 69
2020-06-17 11:16:02,978 Epoch  69 Step:     3700 Batch Loss:     0.104058 Tokens per Sec:     5862, Lr: 0.000200
2020-06-17 11:16:08,445 Epoch  69: total training loss 6.11
2020-06-17 11:16:08,446 EPOCH 70
2020-06-17 11:16:31,132 Epoch  70: total training loss 6.46
2020-06-17 11:16:31,132 EPOCH 71
2020-06-17 11:16:44,015 Epoch  71 Step:     3800 Batch Loss:     0.123099 Tokens per Sec:     6377, Lr: 0.000200
2020-06-17 11:16:53,857 Epoch  71: total training loss 6.06
2020-06-17 11:16:53,857 EPOCH 72
2020-06-17 11:17:16,088 Epoch  72: total training loss 5.99
2020-06-17 11:17:16,089 EPOCH 73
2020-06-17 11:17:26,496 Epoch  73 Step:     3900 Batch Loss:     0.141818 Tokens per Sec:     5868, Lr: 0.000200
2020-06-17 11:17:38,668 Epoch  73: total training loss 7.02
2020-06-17 11:17:38,668 EPOCH 74
2020-06-17 11:18:01,392 Epoch  74: total training loss 6.32
2020-06-17 11:18:01,392 EPOCH 75
2020-06-17 11:18:08,225 Epoch  75 Step:     4000 Batch Loss:     0.112926 Tokens per Sec:     6364, Lr: 0.000200
2020-06-17 11:18:35,441 Example #0
2020-06-17 11:18:35,442 	Raw source:     ['hallo', ',']
2020-06-17 11:18:35,442 	Raw hypothesis: ['hello', '?']
2020-06-17 11:18:35,442 	Source:     hallo ,
2020-06-17 11:18:35,442 	Reference:  hello .
2020-06-17 11:18:35,442 	Hypothesis: hello ?
2020-06-17 11:18:35,442 Example #1
2020-06-17 11:18:35,442 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 11:18:35,442 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 11:18:35,442 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 11:18:35,442 	Reference:  hi , how can i help you ?
2020-06-17 11:18:35,442 	Hypothesis: hi , how can i help you ?
2020-06-17 11:18:35,442 Example #2
2020-06-17 11:18:35,442 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 11:18:35,442 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'sacramento', ',', 'california', '.']
2020-06-17 11:18:35,442 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 11:18:35,442 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 11:18:35,442 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in sacramento , california .
2020-06-17 11:18:35,442 Example #3
2020-06-17 11:18:35,442 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 11:18:35,442 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 11:18:35,442 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 11:18:35,442 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 11:18:35,442 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 11:18:35,442 Validation result (greedy) at epoch  75, step     4000: bleu:  36.67, loss: 45306.7461, ppl:   8.6510, duration: 27.2161s
2020-06-17 11:18:51,006 Epoch  75: total training loss 6.13
2020-06-17 11:18:51,006 EPOCH 76
2020-06-17 11:19:13,847 Epoch  76: total training loss 5.83
2020-06-17 11:19:13,848 EPOCH 77
2020-06-17 11:19:18,076 Epoch  77 Step:     4100 Batch Loss:     0.091283 Tokens per Sec:     5934, Lr: 0.000200
2020-06-17 11:19:36,560 Epoch  77: total training loss 5.76
2020-06-17 11:19:36,561 EPOCH 78
2020-06-17 11:19:59,173 Epoch  78: total training loss 5.67
2020-06-17 11:19:59,174 EPOCH 79
2020-06-17 11:20:00,299 Epoch  79 Step:     4200 Batch Loss:     0.118992 Tokens per Sec:     6589, Lr: 0.000200
2020-06-17 11:20:22,057 Epoch  79: total training loss 6.05
2020-06-17 11:20:22,058 EPOCH 80
2020-06-17 11:20:43,507 Epoch  80 Step:     4300 Batch Loss:     0.107087 Tokens per Sec:     5587, Lr: 0.000200
2020-06-17 11:20:45,529 Epoch  80: total training loss 5.86
2020-06-17 11:20:45,530 EPOCH 81
2020-06-17 11:21:08,401 Epoch  81: total training loss 5.87
2020-06-17 11:21:08,402 EPOCH 82
2020-06-17 11:21:26,669 Epoch  82 Step:     4400 Batch Loss:     0.100450 Tokens per Sec:     5677, Lr: 0.000200
2020-06-17 11:21:31,316 Epoch  82: total training loss 5.57
2020-06-17 11:21:31,316 EPOCH 83
2020-06-17 11:21:53,824 Epoch  83: total training loss 5.57
2020-06-17 11:21:53,824 EPOCH 84
2020-06-17 11:22:08,031 Epoch  84 Step:     4500 Batch Loss:     0.089675 Tokens per Sec:     5884, Lr: 0.000200
2020-06-17 11:22:16,490 Epoch  84: total training loss 5.21
2020-06-17 11:22:16,490 EPOCH 85
2020-06-17 11:22:39,516 Epoch  85: total training loss 5.91
2020-06-17 11:22:39,517 EPOCH 86
2020-06-17 11:22:51,276 Epoch  86 Step:     4600 Batch Loss:     0.138018 Tokens per Sec:     5749, Lr: 0.000200
2020-06-17 11:23:02,367 Epoch  86: total training loss 8.47
2020-06-17 11:23:02,367 EPOCH 87
2020-06-17 11:23:25,133 Epoch  87: total training loss 9.36
2020-06-17 11:23:25,134 EPOCH 88
2020-06-17 11:23:32,557 Epoch  88 Step:     4700 Batch Loss:     0.160175 Tokens per Sec:     5989, Lr: 0.000200
2020-06-17 11:23:47,900 Epoch  88: total training loss 6.63
2020-06-17 11:23:47,901 EPOCH 89
2020-06-17 11:24:10,461 Epoch  89: total training loss 6.03
2020-06-17 11:24:10,462 EPOCH 90
2020-06-17 11:24:16,070 Epoch  90 Step:     4800 Batch Loss:     0.101806 Tokens per Sec:     4682, Lr: 0.000200
2020-06-17 11:24:32,974 Epoch  90: total training loss 5.22
2020-06-17 11:24:32,974 EPOCH 91
2020-06-17 11:24:55,983 Epoch  91: total training loss 4.96
2020-06-17 11:24:55,984 EPOCH 92
2020-06-17 11:24:57,994 Epoch  92 Step:     4900 Batch Loss:     0.081622 Tokens per Sec:     3185, Lr: 0.000200
2020-06-17 11:25:18,892 Epoch  92: total training loss 5.00
2020-06-17 11:25:18,892 EPOCH 93
2020-06-17 11:25:39,391 Epoch  93 Step:     5000 Batch Loss:     0.107690 Tokens per Sec:     5871, Lr: 0.000200
2020-06-17 11:26:03,500 Example #0
2020-06-17 11:26:03,501 	Raw source:     ['hallo', ',']
2020-06-17 11:26:03,501 	Raw hypothesis: ['hello', '.']
2020-06-17 11:26:03,501 	Source:     hallo ,
2020-06-17 11:26:03,501 	Reference:  hello .
2020-06-17 11:26:03,501 	Hypothesis: hello .
2020-06-17 11:26:03,501 Example #1
2020-06-17 11:26:03,501 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 11:26:03,501 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 11:26:03,501 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 11:26:03,501 	Reference:  hi , how can i help you ?
2020-06-17 11:26:03,501 	Hypothesis: hi , how can i help you ?
2020-06-17 11:26:03,501 Example #2
2020-06-17 11:26:03,501 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 11:26:03,501 	Raw hypothesis: ['hey', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'sacramento', ',', 'california', '.']
2020-06-17 11:26:03,501 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 11:26:03,501 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 11:26:03,501 	Hypothesis: hey , i was looking for a restaurant in the arden fair mall in sacramento , california .
2020-06-17 11:26:03,501 Example #3
2020-06-17 11:26:03,501 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 11:26:03,501 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 11:26:03,501 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 11:26:03,501 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 11:26:03,501 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 11:26:03,502 Validation result (greedy) at epoch  93, step     5000: bleu:  37.65, loss: 45581.1953, ppl:   8.7648, duration: 24.1090s
2020-06-17 11:26:05,336 Epoch  93: total training loss 4.70
2020-06-17 11:26:05,337 EPOCH 94
2020-06-17 11:26:27,849 Epoch  94: total training loss 4.61
2020-06-17 11:26:27,850 EPOCH 95
2020-06-17 11:26:46,154 Epoch  95 Step:     5100 Batch Loss:     0.072054 Tokens per Sec:     5513, Lr: 0.000200
2020-06-17 11:26:50,678 Epoch  95: total training loss 4.56
2020-06-17 11:26:50,678 EPOCH 96
2020-06-17 11:27:13,280 Epoch  96: total training loss 4.41
2020-06-17 11:27:13,281 EPOCH 97
2020-06-17 11:27:27,039 Epoch  97 Step:     5200 Batch Loss:     0.066231 Tokens per Sec:     5836, Lr: 0.000200
2020-06-17 11:27:36,158 Epoch  97: total training loss 4.49
2020-06-17 11:27:36,159 EPOCH 98
2020-06-17 11:27:58,917 Epoch  98: total training loss 5.05
2020-06-17 11:27:58,918 EPOCH 99
2020-06-17 11:28:10,050 Epoch  99 Step:     5300 Batch Loss:     0.077319 Tokens per Sec:     5869, Lr: 0.000200
2020-06-17 11:28:21,612 Epoch  99: total training loss 4.71
2020-06-17 11:28:21,612 EPOCH 100
2020-06-17 11:28:43,497 Epoch 100: total training loss 4.40
2020-06-17 11:28:43,497 Training ended after 100 epochs.
2020-06-17 11:28:43,498 Best validation result (greedy) at step     2000:   8.50 ppl.
2020-06-17 11:29:06,311  dev bleu:  36.74 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 11:29:06,315 Translations saved to: models/transformer_multi_enc_shared_deen/00002000.hyps.dev
2020-06-17 11:29:23,718 test bleu:  34.85 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 11:29:23,722 Translations saved to: models/transformer_multi_enc_shared_deen/00002000.hyps.test
