2020-06-17 09:48:57,187 Hello! This is Joey-NMT.
2020-06-17 09:49:04,591 Total params: 13996801
2020-06-17 09:49:04,594 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-17 09:49:06,557 cfg.name                           : transformer_arch-iwslt14_multi_enc_deen
2020-06-17 09:49:06,557 cfg.data.src                       : de
2020-06-17 09:49:06,557 cfg.data.trg                       : en
2020-06-17 09:49:06,558 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-17 09:49:06,558 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-17 09:49:06,558 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-17 09:49:06,558 cfg.data.level                     : bpe
2020-06-17 09:49:06,558 cfg.data.lowercase                 : True
2020-06-17 09:49:06,558 cfg.data.max_sent_length           : 100
2020-06-17 09:49:06,558 cfg.testing.beam_size              : 5
2020-06-17 09:49:06,558 cfg.testing.alpha                  : 1.0
2020-06-17 09:49:06,558 cfg.training.random_seed           : 42
2020-06-17 09:49:06,558 cfg.training.optimizer             : adam
2020-06-17 09:49:06,558 cfg.training.normalization         : tokens
2020-06-17 09:49:06,558 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-17 09:49:06,558 cfg.training.scheduling            : plateau
2020-06-17 09:49:06,558 cfg.training.patience              : 5
2020-06-17 09:49:06,558 cfg.training.decrease_factor       : 0.7
2020-06-17 09:49:06,558 cfg.training.loss                  : crossentropy
2020-06-17 09:49:06,558 cfg.training.learning_rate         : 0.0003
2020-06-17 09:49:06,558 cfg.training.learning_rate_min     : 1e-08
2020-06-17 09:49:06,558 cfg.training.weight_decay          : 0.0
2020-06-17 09:49:06,558 cfg.training.label_smoothing       : 0.1
2020-06-17 09:49:06,558 cfg.training.batch_size            : 4096
2020-06-17 09:49:06,558 cfg.training.batch_type            : token
2020-06-17 09:49:06,558 cfg.training.eval_batch_size       : 3600
2020-06-17 09:49:06,558 cfg.training.eval_batch_type       : token
2020-06-17 09:49:06,559 cfg.training.batch_multiplier      : 1
2020-06-17 09:49:06,559 cfg.training.early_stopping_metric : eval_metric
2020-06-17 09:49:06,559 cfg.training.epochs                : 100
2020-06-17 09:49:06,559 cfg.training.validation_freq       : 1000
2020-06-17 09:49:06,559 cfg.training.logging_freq          : 100
2020-06-17 09:49:06,559 cfg.training.eval_metric           : bleu
2020-06-17 09:49:06,559 cfg.training.model_dir             : models/transformer_arch-iwslt14_multi_enc_deen
2020-06-17 09:49:06,559 cfg.training.overwrite             : True
2020-06-17 09:49:06,559 cfg.training.shuffle               : True
2020-06-17 09:49:06,559 cfg.training.use_cuda              : True
2020-06-17 09:49:06,559 cfg.training.max_output_length     : 100
2020-06-17 09:49:06,559 cfg.training.print_valid_sents     : [0, 1, 2, 3, 4]
2020-06-17 09:49:06,559 cfg.training.keep_last_ckpts       : 5
2020-06-17 09:49:06,559 cfg.model.initializer              : xavier
2020-06-17 09:49:06,559 cfg.model.bias_initializer         : zeros
2020-06-17 09:49:06,559 cfg.model.init_gain                : 1.0
2020-06-17 09:49:06,559 cfg.model.embed_initializer        : xavier
2020-06-17 09:49:06,559 cfg.model.embed_init_gain          : 1.0
2020-06-17 09:49:06,559 cfg.model.tied_embeddings          : False
2020-06-17 09:49:06,559 cfg.model.tied_softmax             : True
2020-06-17 09:49:06,559 cfg.model.encoder.type             : transformer
2020-06-17 09:49:06,559 cfg.model.encoder.num_layers       : 3
2020-06-17 09:49:06,559 cfg.model.encoder.num_heads        : 4
2020-06-17 09:49:06,559 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-17 09:49:06,559 cfg.model.encoder.embeddings.scale : True
2020-06-17 09:49:06,559 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-17 09:49:06,559 cfg.model.encoder.hidden_size      : 256
2020-06-17 09:49:06,559 cfg.model.encoder.ff_size          : 1024
2020-06-17 09:49:06,560 cfg.model.encoder.freeze           : False
2020-06-17 09:49:06,560 cfg.model.encoder.multi_encoder    : True
2020-06-17 09:49:06,560 cfg.model.encoder.dropout          : 0.3
2020-06-17 09:49:06,560 cfg.model.decoder.type             : transformer
2020-06-17 09:49:06,560 cfg.model.decoder.num_layers       : 6
2020-06-17 09:49:06,560 cfg.model.decoder.num_heads        : 4
2020-06-17 09:49:06,560 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-17 09:49:06,560 cfg.model.decoder.embeddings.scale : True
2020-06-17 09:49:06,560 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-17 09:49:06,560 cfg.model.decoder.hidden_size      : 256
2020-06-17 09:49:06,560 cfg.model.decoder.ff_size          : 1024
2020-06-17 09:49:06,560 cfg.model.decoder.freeze           : False
2020-06-17 09:49:06,560 cfg.model.decoder.dropout          : 0.3
2020-06-17 09:49:06,560 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-17 09:49:06,560 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-17 09:49:06,560 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-17 09:49:06,560 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-17 09:49:06,560 Number of Src words (types): 5876
2020-06-17 09:49:06,560 Number of Trg words (types): 4561
2020-06-17 09:49:06,561 Model(
	encoder=TransformerEncoder(num_layers=2, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=256, vocab_size=5876),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4561))
2020-06-17 09:49:06,569 EPOCH 1
2020-06-17 09:49:15,058 Epoch   1: total training loss 299.34
2020-06-17 09:49:15,059 EPOCH 2
2020-06-17 09:49:22,200 Epoch   2 Step:      100 Batch Loss:     4.909566 Tokens per Sec:    16226, Lr: 0.000300
2020-06-17 09:49:23,111 Epoch   2: total training loss 251.97
2020-06-17 09:49:23,111 EPOCH 3
2020-06-17 09:49:31,257 Epoch   3: total training loss 244.17
2020-06-17 09:49:31,257 EPOCH 4
2020-06-17 09:49:37,250 Epoch   4 Step:      200 Batch Loss:     4.955188 Tokens per Sec:    16043, Lr: 0.000300
2020-06-17 09:49:39,381 Epoch   4: total training loss 235.23
2020-06-17 09:49:39,381 EPOCH 5
2020-06-17 09:49:47,504 Epoch   5: total training loss 221.70
2020-06-17 09:49:47,504 EPOCH 6
2020-06-17 09:49:52,264 Epoch   6 Step:      300 Batch Loss:     4.226367 Tokens per Sec:    15936, Lr: 0.000300
2020-06-17 09:49:55,660 Epoch   6: total training loss 202.93
2020-06-17 09:49:55,660 EPOCH 7
2020-06-17 09:50:03,875 Epoch   7: total training loss 194.05
2020-06-17 09:50:03,876 EPOCH 8
2020-06-17 09:50:07,411 Epoch   8 Step:      400 Batch Loss:     2.546139 Tokens per Sec:    16242, Lr: 0.000300
2020-06-17 09:50:12,037 Epoch   8: total training loss 180.69
2020-06-17 09:50:12,037 EPOCH 9
2020-06-17 09:50:20,174 Epoch   9: total training loss 161.96
2020-06-17 09:50:20,175 EPOCH 10
2020-06-17 09:50:22,766 Epoch  10 Step:      500 Batch Loss:     3.746360 Tokens per Sec:    16242, Lr: 0.000300
2020-06-17 09:50:28,345 Epoch  10: total training loss 158.52
2020-06-17 09:50:28,346 EPOCH 11
2020-06-17 09:50:36,501 Epoch  11: total training loss 146.50
2020-06-17 09:50:36,502 EPOCH 12
2020-06-17 09:50:37,921 Epoch  12 Step:      600 Batch Loss:     1.878961 Tokens per Sec:    16026, Lr: 0.000300
2020-06-17 09:50:44,598 Epoch  12: total training loss 136.65
2020-06-17 09:50:44,598 EPOCH 13
2020-06-17 09:50:52,774 Epoch  13: total training loss 130.07
2020-06-17 09:50:52,774 EPOCH 14
2020-06-17 09:50:53,107 Epoch  14 Step:      700 Batch Loss:     2.646981 Tokens per Sec:    13323, Lr: 0.000300
2020-06-17 09:51:00,926 Epoch  14: total training loss 123.38
2020-06-17 09:51:00,926 EPOCH 15
2020-06-17 09:51:08,339 Epoch  15 Step:      800 Batch Loss:     1.967481 Tokens per Sec:    16292, Lr: 0.000300
2020-06-17 09:51:09,090 Epoch  15: total training loss 113.10
2020-06-17 09:51:09,091 EPOCH 16
2020-06-17 09:51:17,305 Epoch  16: total training loss 112.74
2020-06-17 09:51:17,305 EPOCH 17
2020-06-17 09:51:23,801 Epoch  17 Step:      900 Batch Loss:     2.586197 Tokens per Sec:    16140, Lr: 0.000300
2020-06-17 09:51:25,389 Epoch  17: total training loss 103.26
2020-06-17 09:51:25,389 EPOCH 18
2020-06-17 09:51:33,481 Epoch  18: total training loss 100.78
2020-06-17 09:51:33,481 EPOCH 19
2020-06-17 09:51:38,575 Epoch  19 Step:     1000 Batch Loss:     1.303050 Tokens per Sec:    16644, Lr: 0.000300
2020-06-17 09:51:52,318 Hooray! New best validation result [eval_metric]!
2020-06-17 09:51:52,319 Saving new checkpoint.
2020-06-17 09:51:54,281 Example #0
2020-06-17 09:51:54,282 	Raw source:     ['hallo', ',']
2020-06-17 09:51:54,282 	Raw hypothesis: ['hi', ',']
2020-06-17 09:51:54,282 	Source:     hallo ,
2020-06-17 09:51:54,282 	Reference:  hello .
2020-06-17 09:51:54,282 	Hypothesis: hi ,
2020-06-17 09:51:54,282 Example #1
2020-06-17 09:51:54,282 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 09:51:54,282 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 09:51:54,282 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 09:51:54,282 	Reference:  hi , how can i help you ?
2020-06-17 09:51:54,282 	Hypothesis: hi , how can i help you ?
2020-06-17 09:51:54,282 Example #2
2020-06-17 09:51:54,282 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 09:51:54,282 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'the', 'arden', 'mall', 'in', 'san', 'francisco', '.']
2020-06-17 09:51:54,282 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 09:51:54,282 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 09:51:54,282 	Hypothesis: hi , i &apos;m looking for a restaurant at the arden mall in san francisco .
2020-06-17 09:51:54,282 Example #3
2020-06-17 09:51:54,282 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 09:51:54,282 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 09:51:54,282 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 09:51:54,282 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 09:51:54,282 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-17 09:51:54,282 Example #4
2020-06-17 09:51:54,282 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 09:51:54,282 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'a', 'restaurant', '.']
2020-06-17 09:51:54,282 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 09:51:54,282 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 09:51:54,282 	Hypothesis: i &apos;m looking for a restaurant .
2020-06-17 09:51:54,283 Validation result (greedy) at epoch  19, step     1000: bleu:  28.40, loss: 48302.1758, ppl:   9.9774, duration: 15.7072s
2020-06-17 09:51:57,363 Epoch  19: total training loss 95.38
2020-06-17 09:51:57,363 EPOCH 20
2020-06-17 09:52:05,509 Epoch  20: total training loss 93.83
2020-06-17 09:52:05,510 EPOCH 21
2020-06-17 09:52:09,584 Epoch  21 Step:     1100 Batch Loss:     1.052940 Tokens per Sec:    15401, Lr: 0.000300
2020-06-17 09:52:13,838 Epoch  21: total training loss 93.33
2020-06-17 09:52:13,839 EPOCH 22
2020-06-17 09:52:21,964 Epoch  22: total training loss 86.02
2020-06-17 09:52:21,964 EPOCH 23
2020-06-17 09:52:24,717 Epoch  23 Step:     1200 Batch Loss:     2.680467 Tokens per Sec:    15940, Lr: 0.000300
2020-06-17 09:52:30,182 Epoch  23: total training loss 84.50
2020-06-17 09:52:30,182 EPOCH 24
2020-06-17 09:52:38,304 Epoch  24: total training loss 81.96
2020-06-17 09:52:38,305 EPOCH 25
2020-06-17 09:52:39,855 Epoch  25 Step:     1300 Batch Loss:     0.977255 Tokens per Sec:    15991, Lr: 0.000300
2020-06-17 09:52:46,492 Epoch  25: total training loss 80.52
2020-06-17 09:52:46,493 EPOCH 26
2020-06-17 09:52:54,937 Epoch  26: total training loss 74.38
2020-06-17 09:52:54,938 EPOCH 27
2020-06-17 09:52:55,111 Epoch  27 Step:     1400 Batch Loss:     0.898922 Tokens per Sec:    15648, Lr: 0.000300
2020-06-17 09:53:03,231 Epoch  27: total training loss 73.91
2020-06-17 09:53:03,231 EPOCH 28
2020-06-17 09:53:10,263 Epoch  28 Step:     1500 Batch Loss:     0.790446 Tokens per Sec:    16328, Lr: 0.000300
2020-06-17 09:53:11,392 Epoch  28: total training loss 71.69
2020-06-17 09:53:11,392 EPOCH 29
2020-06-17 09:53:19,532 Epoch  29: total training loss 66.49
2020-06-17 09:53:19,532 EPOCH 30
2020-06-17 09:53:25,624 Epoch  30 Step:     1600 Batch Loss:     1.069293 Tokens per Sec:    16163, Lr: 0.000300
2020-06-17 09:53:27,712 Epoch  30: total training loss 64.04
2020-06-17 09:53:27,712 EPOCH 31
2020-06-17 09:53:35,819 Epoch  31: total training loss 61.37
2020-06-17 09:53:35,820 EPOCH 32
2020-06-17 09:53:40,901 Epoch  32 Step:     1700 Batch Loss:     0.757989 Tokens per Sec:    16029, Lr: 0.000300
2020-06-17 09:53:43,992 Epoch  32: total training loss 61.24
2020-06-17 09:53:43,993 EPOCH 33
2020-06-17 09:53:52,227 Epoch  33: total training loss 60.32
2020-06-17 09:53:52,227 EPOCH 34
2020-06-17 09:53:56,044 Epoch  34 Step:     1800 Batch Loss:     0.754521 Tokens per Sec:    15886, Lr: 0.000300
2020-06-17 09:54:00,481 Epoch  34: total training loss 61.07
2020-06-17 09:54:00,481 EPOCH 35
2020-06-17 09:54:08,750 Epoch  35: total training loss 57.48
2020-06-17 09:54:08,751 EPOCH 36
2020-06-17 09:54:11,106 Epoch  36 Step:     1900 Batch Loss:     0.783705 Tokens per Sec:    17013, Lr: 0.000300
2020-06-17 09:54:17,000 Epoch  36: total training loss 57.38
2020-06-17 09:54:17,001 EPOCH 37
2020-06-17 09:54:25,142 Epoch  37: total training loss 52.92
2020-06-17 09:54:25,142 EPOCH 38
2020-06-17 09:54:26,469 Epoch  38 Step:     2000 Batch Loss:     1.381737 Tokens per Sec:    15963, Lr: 0.000300
2020-06-17 09:54:41,608 Hooray! New best validation result [eval_metric]!
2020-06-17 09:54:41,609 Saving new checkpoint.
2020-06-17 09:54:43,605 Example #0
2020-06-17 09:54:43,606 	Raw source:     ['hallo', ',']
2020-06-17 09:54:43,606 	Raw hypothesis: ['hi', 'there', '.']
2020-06-17 09:54:43,606 	Source:     hallo ,
2020-06-17 09:54:43,606 	Reference:  hello .
2020-06-17 09:54:43,606 	Hypothesis: hi there .
2020-06-17 09:54:43,606 Example #1
2020-06-17 09:54:43,606 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 09:54:43,606 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 09:54:43,606 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 09:54:43,606 	Reference:  hi , how can i help you ?
2020-06-17 09:54:43,606 	Hypothesis: hi , how can i help you ?
2020-06-17 09:54:43,606 Example #2
2020-06-17 09:54:43,606 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 09:54:43,606 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'lunch', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 09:54:43,606 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 09:54:43,606 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 09:54:43,606 	Hypothesis: hi , i &apos;m looking for a restaurant at lunch at the arden fair mall in san francisco , california .
2020-06-17 09:54:43,606 Example #3
2020-06-17 09:54:43,606 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 09:54:43,606 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 09:54:43,606 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 09:54:43,606 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 09:54:43,606 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-17 09:54:43,606 Example #4
2020-06-17 09:54:43,606 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 09:54:43,606 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'some', 'food', '.']
2020-06-17 09:54:43,606 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 09:54:43,606 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 09:54:43,606 	Hypothesis: i &apos;m looking for some food .
2020-06-17 09:54:43,606 Validation result (greedy) at epoch  38, step     2000: bleu:  38.08, loss: 43380.6406, ppl:   7.8927, duration: 17.1375s
2020-06-17 09:54:50,441 Epoch  38: total training loss 53.63
2020-06-17 09:54:50,442 EPOCH 39
2020-06-17 09:54:58,713 Epoch  39: total training loss 50.48
2020-06-17 09:54:58,713 EPOCH 40
2020-06-17 09:54:58,861 Epoch  40 Step:     2100 Batch Loss:     0.992177 Tokens per Sec:    17517, Lr: 0.000300
2020-06-17 09:55:06,889 Epoch  40: total training loss 50.31
2020-06-17 09:55:06,890 EPOCH 41
2020-06-17 09:55:14,096 Epoch  41 Step:     2200 Batch Loss:     0.397396 Tokens per Sec:    16155, Lr: 0.000300
2020-06-17 09:55:15,084 Epoch  41: total training loss 48.97
2020-06-17 09:55:15,084 EPOCH 42
2020-06-17 09:55:23,364 Epoch  42: total training loss 48.07
2020-06-17 09:55:23,364 EPOCH 43
2020-06-17 09:55:29,136 Epoch  43 Step:     2300 Batch Loss:     1.175216 Tokens per Sec:    16522, Lr: 0.000300
2020-06-17 09:55:31,552 Epoch  43: total training loss 46.25
2020-06-17 09:55:31,553 EPOCH 44
2020-06-17 09:55:39,689 Epoch  44: total training loss 44.14
2020-06-17 09:55:39,690 EPOCH 45
2020-06-17 09:55:44,614 Epoch  45 Step:     2400 Batch Loss:     0.792671 Tokens per Sec:    16224, Lr: 0.000300
2020-06-17 09:55:47,837 Epoch  45: total training loss 43.39
2020-06-17 09:55:47,837 EPOCH 46
2020-06-17 09:55:56,014 Epoch  46: total training loss 42.35
2020-06-17 09:55:56,015 EPOCH 47
2020-06-17 09:55:59,935 Epoch  47 Step:     2500 Batch Loss:     0.492285 Tokens per Sec:    15682, Lr: 0.000300
2020-06-17 09:56:04,128 Epoch  47: total training loss 43.44
2020-06-17 09:56:04,128 EPOCH 48
2020-06-17 09:56:12,288 Epoch  48: total training loss 40.93
2020-06-17 09:56:12,288 EPOCH 49
2020-06-17 09:56:14,972 Epoch  49 Step:     2600 Batch Loss:     0.517443 Tokens per Sec:    15182, Lr: 0.000300
2020-06-17 09:56:20,542 Epoch  49: total training loss 40.39
2020-06-17 09:56:20,542 EPOCH 50
2020-06-17 09:56:28,796 Epoch  50: total training loss 40.20
2020-06-17 09:56:28,796 EPOCH 51
2020-06-17 09:56:30,074 Epoch  51 Step:     2700 Batch Loss:     0.522638 Tokens per Sec:    16081, Lr: 0.000300
2020-06-17 09:56:37,075 Epoch  51: total training loss 41.46
2020-06-17 09:56:37,075 EPOCH 52
2020-06-17 09:56:45,105 Epoch  52 Step:     2800 Batch Loss:     0.693037 Tokens per Sec:    16047, Lr: 0.000300
2020-06-17 09:56:45,267 Epoch  52: total training loss 39.46
2020-06-17 09:56:45,267 EPOCH 53
2020-06-17 09:56:53,481 Epoch  53: total training loss 36.90
2020-06-17 09:56:53,481 EPOCH 54
2020-06-17 09:57:00,365 Epoch  54 Step:     2900 Batch Loss:     0.822083 Tokens per Sec:    15687, Lr: 0.000300
2020-06-17 09:57:01,704 Epoch  54: total training loss 35.79
2020-06-17 09:57:01,704 EPOCH 55
2020-06-17 09:57:09,962 Epoch  55: total training loss 34.60
2020-06-17 09:57:09,962 EPOCH 56
2020-06-17 09:57:15,585 Epoch  56 Step:     3000 Batch Loss:     0.464613 Tokens per Sec:    16216, Lr: 0.000300
2020-06-17 09:57:30,562 Hooray! New best validation result [eval_metric]!
2020-06-17 09:57:30,563 Saving new checkpoint.
2020-06-17 09:57:32,656 Example #0
2020-06-17 09:57:32,656 	Raw source:     ['hallo', ',']
2020-06-17 09:57:32,656 	Raw hypothesis: ['hi', 'there', '.']
2020-06-17 09:57:32,656 	Source:     hallo ,
2020-06-17 09:57:32,656 	Reference:  hello .
2020-06-17 09:57:32,656 	Hypothesis: hi there .
2020-06-17 09:57:32,656 Example #1
2020-06-17 09:57:32,656 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 09:57:32,656 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 09:57:32,657 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 09:57:32,657 	Reference:  hi , how can i help you ?
2020-06-17 09:57:32,657 	Hypothesis: hi , how can i help you ?
2020-06-17 09:57:32,657 Example #2
2020-06-17 09:57:32,657 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 09:57:32,657 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'for', 'a', 'place', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 09:57:32,657 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 09:57:32,657 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 09:57:32,657 	Hypothesis: hi , i &apos;m looking for a restaurant for a place at the arden fair mall in san francisco , california .
2020-06-17 09:57:32,657 Example #3
2020-06-17 09:57:32,657 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 09:57:32,657 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 09:57:32,657 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 09:57:32,657 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 09:57:32,657 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-17 09:57:32,657 Example #4
2020-06-17 09:57:32,657 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 09:57:32,657 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'some', 'fast', 'food', '.']
2020-06-17 09:57:32,657 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 09:57:32,657 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 09:57:32,657 	Hypothesis: i &apos;m looking for some fast food .
2020-06-17 09:57:32,657 Validation result (greedy) at epoch  56, step     3000: bleu:  41.65, loss: 42438.5195, ppl:   7.5464, duration: 17.0714s
2020-06-17 09:57:35,209 Epoch  56: total training loss 33.37
2020-06-17 09:57:35,209 EPOCH 57
2020-06-17 09:57:43,443 Epoch  57: total training loss 33.58
2020-06-17 09:57:43,443 EPOCH 58
2020-06-17 09:57:47,950 Epoch  58 Step:     3100 Batch Loss:     0.433817 Tokens per Sec:    16063, Lr: 0.000300
2020-06-17 09:57:51,670 Epoch  58: total training loss 31.89
2020-06-17 09:57:51,671 EPOCH 59
2020-06-17 09:57:59,920 Epoch  59: total training loss 32.55
2020-06-17 09:57:59,920 EPOCH 60
2020-06-17 09:58:03,269 Epoch  60 Step:     3200 Batch Loss:     0.766612 Tokens per Sec:    16120, Lr: 0.000300
2020-06-17 09:58:08,141 Epoch  60: total training loss 30.92
2020-06-17 09:58:08,142 EPOCH 61
2020-06-17 09:58:16,368 Epoch  61: total training loss 31.06
2020-06-17 09:58:16,369 EPOCH 62
2020-06-17 09:58:18,504 Epoch  62 Step:     3300 Batch Loss:     0.606416 Tokens per Sec:    16852, Lr: 0.000300
2020-06-17 09:58:24,531 Epoch  62: total training loss 31.80
2020-06-17 09:58:24,532 EPOCH 63
2020-06-17 09:58:32,670 Epoch  63: total training loss 29.96
2020-06-17 09:58:32,671 EPOCH 64
2020-06-17 09:58:33,623 Epoch  64 Step:     3400 Batch Loss:     0.438788 Tokens per Sec:    18574, Lr: 0.000300
2020-06-17 09:58:40,786 Epoch  64: total training loss 28.52
2020-06-17 09:58:40,787 EPOCH 65
2020-06-17 09:58:49,001 Epoch  65: total training loss 27.74
2020-06-17 09:58:49,002 EPOCH 66
2020-06-17 09:58:49,132 Epoch  66 Step:     3500 Batch Loss:     0.597924 Tokens per Sec:    21029, Lr: 0.000300
2020-06-17 09:58:57,210 Epoch  66: total training loss 28.07
2020-06-17 09:58:57,211 EPOCH 67
2020-06-17 09:59:04,489 Epoch  67 Step:     3600 Batch Loss:     0.462145 Tokens per Sec:    16059, Lr: 0.000300
2020-06-17 09:59:05,463 Epoch  67: total training loss 27.59
2020-06-17 09:59:05,464 EPOCH 68
2020-06-17 09:59:13,704 Epoch  68: total training loss 26.50
2020-06-17 09:59:13,704 EPOCH 69
2020-06-17 09:59:19,825 Epoch  69 Step:     3700 Batch Loss:     0.437613 Tokens per Sec:    16261, Lr: 0.000300
2020-06-17 09:59:21,853 Epoch  69: total training loss 26.33
2020-06-17 09:59:21,854 EPOCH 70
2020-06-17 09:59:30,089 Epoch  70: total training loss 26.02
2020-06-17 09:59:30,090 EPOCH 71
2020-06-17 09:59:34,849 Epoch  71 Step:     3800 Batch Loss:     0.384431 Tokens per Sec:    17265, Lr: 0.000300
2020-06-17 09:59:38,286 Epoch  71: total training loss 25.74
2020-06-17 09:59:38,286 EPOCH 72
2020-06-17 09:59:46,496 Epoch  72: total training loss 25.30
2020-06-17 09:59:46,496 EPOCH 73
2020-06-17 09:59:50,322 Epoch  73 Step:     3900 Batch Loss:     0.563326 Tokens per Sec:    15963, Lr: 0.000300
2020-06-17 09:59:54,724 Epoch  73: total training loss 25.66
2020-06-17 09:59:54,724 EPOCH 74
2020-06-17 10:00:03,014 Epoch  74: total training loss 24.97
2020-06-17 10:00:03,015 EPOCH 75
2020-06-17 10:00:05,582 Epoch  75 Step:     4000 Batch Loss:     0.367779 Tokens per Sec:    16937, Lr: 0.000300
2020-06-17 10:00:20,130 Hooray! New best validation result [eval_metric]!
2020-06-17 10:00:20,130 Saving new checkpoint.
2020-06-17 10:00:22,175 Example #0
2020-06-17 10:00:22,175 	Raw source:     ['hallo', ',']
2020-06-17 10:00:22,175 	Raw hypothesis: ['hello', '.']
2020-06-17 10:00:22,175 	Source:     hallo ,
2020-06-17 10:00:22,175 	Reference:  hello .
2020-06-17 10:00:22,175 	Hypothesis: hello .
2020-06-17 10:00:22,175 Example #1
2020-06-17 10:00:22,175 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:00:22,175 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:00:22,175 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:00:22,176 	Reference:  hi , how can i help you ?
2020-06-17 10:00:22,176 	Hypothesis: hi , how can i help you ?
2020-06-17 10:00:22,176 Example #2
2020-06-17 10:00:22,176 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:00:22,176 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'for', 'a', 'place', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:00:22,176 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:00:22,176 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:00:22,176 	Hypothesis: hi , i &apos;m looking for a restaurant for a place in the arden fair mall in san francisco , california .
2020-06-17 10:00:22,176 Example #3
2020-06-17 10:00:22,176 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:00:22,176 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:00:22,176 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:00:22,176 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:00:22,176 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 10:00:22,176 Example #4
2020-06-17 10:00:22,176 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 10:00:22,176 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'some', 'fast', 'food', 'restaurant', '.']
2020-06-17 10:00:22,177 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 10:00:22,177 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 10:00:22,177 	Hypothesis: i &apos;m looking for some fast food restaurant .
2020-06-17 10:00:22,177 Validation result (greedy) at epoch  75, step     4000: bleu:  42.73, loss: 43394.3555, ppl:   7.8979, duration: 16.5937s
2020-06-17 10:00:27,829 Epoch  75: total training loss 24.67
2020-06-17 10:00:27,829 EPOCH 76
2020-06-17 10:00:35,970 Epoch  76: total training loss 23.72
2020-06-17 10:00:35,970 EPOCH 77
2020-06-17 10:00:37,521 Epoch  77 Step:     4100 Batch Loss:     0.377789 Tokens per Sec:    16189, Lr: 0.000300
2020-06-17 10:00:44,144 Epoch  77: total training loss 23.23
2020-06-17 10:00:44,145 EPOCH 78
2020-06-17 10:00:52,323 Epoch  78: total training loss 23.11
2020-06-17 10:00:52,323 EPOCH 79
2020-06-17 10:00:52,770 Epoch  79 Step:     4200 Batch Loss:     0.349211 Tokens per Sec:    16599, Lr: 0.000300
2020-06-17 10:01:00,580 Epoch  79: total training loss 23.28
2020-06-17 10:01:00,581 EPOCH 80
2020-06-17 10:01:08,247 Epoch  80 Step:     4300 Batch Loss:     0.690087 Tokens per Sec:    15631, Lr: 0.000300
2020-06-17 10:01:08,981 Epoch  80: total training loss 23.00
2020-06-17 10:01:08,981 EPOCH 81
2020-06-17 10:01:17,178 Epoch  81: total training loss 22.61
2020-06-17 10:01:17,178 EPOCH 82
2020-06-17 10:01:23,706 Epoch  82 Step:     4400 Batch Loss:     0.422612 Tokens per Sec:    15890, Lr: 0.000300
2020-06-17 10:01:25,419 Epoch  82: total training loss 22.02
2020-06-17 10:01:25,420 EPOCH 83
2020-06-17 10:01:33,564 Epoch  83: total training loss 21.89
2020-06-17 10:01:33,565 EPOCH 84
2020-06-17 10:01:38,707 Epoch  84 Step:     4500 Batch Loss:     0.454948 Tokens per Sec:    16256, Lr: 0.000300
2020-06-17 10:01:41,742 Epoch  84: total training loss 21.29
2020-06-17 10:01:41,742 EPOCH 85
2020-06-17 10:01:49,944 Epoch  85: total training loss 20.49
2020-06-17 10:01:49,945 EPOCH 86
2020-06-17 10:01:54,173 Epoch  86 Step:     4600 Batch Loss:     0.398678 Tokens per Sec:    15992, Lr: 0.000300
2020-06-17 10:01:58,213 Epoch  86: total training loss 20.50
2020-06-17 10:01:58,214 EPOCH 87
2020-06-17 10:02:06,483 Epoch  87: total training loss 21.27
2020-06-17 10:02:06,484 EPOCH 88
2020-06-17 10:02:09,180 Epoch  88 Step:     4700 Batch Loss:     0.527179 Tokens per Sec:    16490, Lr: 0.000300
2020-06-17 10:02:14,709 Epoch  88: total training loss 20.75
2020-06-17 10:02:14,710 EPOCH 89
2020-06-17 10:02:22,858 Epoch  89: total training loss 20.31
2020-06-17 10:02:22,859 EPOCH 90
2020-06-17 10:02:24,785 Epoch  90 Step:     4800 Batch Loss:     0.427080 Tokens per Sec:    13636, Lr: 0.000300
2020-06-17 10:02:31,020 Epoch  90: total training loss 19.96
2020-06-17 10:02:31,020 EPOCH 91
2020-06-17 10:02:39,257 Epoch  91: total training loss 19.68
2020-06-17 10:02:39,258 EPOCH 92
2020-06-17 10:02:39,898 Epoch  92 Step:     4900 Batch Loss:     0.306623 Tokens per Sec:    10010, Lr: 0.000300
2020-06-17 10:02:47,566 Epoch  92: total training loss 19.74
2020-06-17 10:02:47,566 EPOCH 93
2020-06-17 10:02:54,950 Epoch  93 Step:     5000 Batch Loss:     0.320491 Tokens per Sec:    16302, Lr: 0.000300
2020-06-17 10:03:08,375 Hooray! New best validation result [eval_metric]!
2020-06-17 10:03:08,375 Saving new checkpoint.
2020-06-17 10:03:10,377 Example #0
2020-06-17 10:03:10,377 	Raw source:     ['hallo', ',']
2020-06-17 10:03:10,377 	Raw hypothesis: ['hello', '.']
2020-06-17 10:03:10,377 	Source:     hallo ,
2020-06-17 10:03:10,377 	Reference:  hello .
2020-06-17 10:03:10,377 	Hypothesis: hello .
2020-06-17 10:03:10,377 Example #1
2020-06-17 10:03:10,378 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:03:10,378 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:03:10,378 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:03:10,378 	Reference:  hi , how can i help you ?
2020-06-17 10:03:10,378 	Hypothesis: hi , how can i help you ?
2020-06-17 10:03:10,378 Example #2
2020-06-17 10:03:10,378 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:03:10,378 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:03:10,378 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:03:10,378 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:03:10,378 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:03:10,378 Example #3
2020-06-17 10:03:10,378 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:03:10,378 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:03:10,378 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:03:10,378 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:03:10,378 	Hypothesis: ok , what kind of restaurant are you looking for ?
2020-06-17 10:03:10,378 Example #4
2020-06-17 10:03:10,378 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 10:03:10,378 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'some', 'inexpensive', 'food', '.']
2020-06-17 10:03:10,378 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 10:03:10,378 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 10:03:10,378 	Hypothesis: i &apos;m looking for some inexpensive food .
2020-06-17 10:03:10,378 Validation result (greedy) at epoch  93, step     5000: bleu:  44.17, loss: 43372.9258, ppl:   7.8898, duration: 15.4281s
2020-06-17 10:03:11,093 Epoch  93: total training loss 18.76
2020-06-17 10:03:11,093 EPOCH 94
2020-06-17 10:03:19,259 Epoch  94: total training loss 18.67
2020-06-17 10:03:19,260 EPOCH 95
2020-06-17 10:03:25,771 Epoch  95 Step:     5100 Batch Loss:     0.307527 Tokens per Sec:    15500, Lr: 0.000300
2020-06-17 10:03:27,488 Epoch  95: total training loss 18.65
2020-06-17 10:03:27,488 EPOCH 96
2020-06-17 10:03:35,753 Epoch  96: total training loss 18.20
2020-06-17 10:03:35,753 EPOCH 97
2020-06-17 10:03:40,772 Epoch  97 Step:     5200 Batch Loss:     0.343396 Tokens per Sec:    15999, Lr: 0.000300
2020-06-17 10:03:44,018 Epoch  97: total training loss 18.63
2020-06-17 10:03:44,018 EPOCH 98
2020-06-17 10:03:52,227 Epoch  98: total training loss 17.73
2020-06-17 10:03:52,228 EPOCH 99
2020-06-17 10:03:56,243 Epoch  99 Step:     5300 Batch Loss:     0.320423 Tokens per Sec:    16274, Lr: 0.000300
2020-06-17 10:04:00,503 Epoch  99: total training loss 17.69
2020-06-17 10:04:00,503 EPOCH 100
2020-06-17 10:04:08,636 Epoch 100: total training loss 17.01
2020-06-17 10:04:08,637 Training ended after 100 epochs.
2020-06-17 10:04:08,637 Best validation result (greedy) at step     5000:  44.17 eval_metric.
2020-06-17 10:04:24,346  dev bleu:  45.15 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 10:04:24,351 Translations saved to: models/transformer_arch-iwslt14_multi_enc_deen/00005000.hyps.dev
2020-06-17 10:04:34,903 test bleu:  42.75 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 10:04:34,908 Translations saved to: models/transformer_arch-iwslt14_multi_enc_deen/00005000.hyps.test
