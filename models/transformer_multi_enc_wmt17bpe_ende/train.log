2020-07-01 10:46:39,080 Hello! This is Joey-NMT.
2020-07-01 10:46:44,796 Total params: 50436097
2020-07-01 10:46:44,798 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-07-01 10:46:47,047 cfg.name                           : transformer_multi_enc_ende
2020-07-01 10:46:47,048 cfg.data.src                       : en
2020-07-01 10:46:47,048 cfg.data.trg                       : de
2020-07-01 10:46:47,048 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-01 10:46:47,048 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-01 10:46:47,048 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-01 10:46:47,048 cfg.data.level                     : bpe
2020-07-01 10:46:47,048 cfg.data.lowercase                 : False
2020-07-01 10:46:47,048 cfg.data.max_sent_length           : 100
2020-07-01 10:46:47,048 cfg.testing.beam_size              : 5
2020-07-01 10:46:47,048 cfg.testing.alpha                  : 1.0
2020-07-01 10:46:47,048 cfg.training.random_seed           : 42
2020-07-01 10:46:47,048 cfg.training.optimizer             : adam
2020-07-01 10:46:47,048 cfg.training.normalization         : tokens
2020-07-01 10:46:47,048 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-01 10:46:47,049 cfg.training.scheduling            : plateau
2020-07-01 10:46:47,049 cfg.training.patience              : 8
2020-07-01 10:46:47,049 cfg.training.decrease_factor       : 0.7
2020-07-01 10:46:47,049 cfg.training.loss                  : crossentropy
2020-07-01 10:46:47,049 cfg.training.learning_rate         : 0.0002
2020-07-01 10:46:47,049 cfg.training.learning_rate_min     : 1e-08
2020-07-01 10:46:47,049 cfg.training.weight_decay          : 0.0
2020-07-01 10:46:47,049 cfg.training.label_smoothing       : 0.1
2020-07-01 10:46:47,049 cfg.training.batch_size            : 4096
2020-07-01 10:46:47,049 cfg.training.batch_type            : token
2020-07-01 10:46:47,049 cfg.training.batch_multiplier      : 1
2020-07-01 10:46:47,049 cfg.training.early_stopping_metric : ppl
2020-07-01 10:46:47,049 cfg.training.epochs                : 100
2020-07-01 10:46:47,049 cfg.training.validation_freq       : 1000
2020-07-01 10:46:47,049 cfg.training.logging_freq          : 100
2020-07-01 10:46:47,049 cfg.training.eval_metric           : bleu
2020-07-01 10:46:47,049 cfg.training.model_dir             : models/transformer_multi_enc_wmt17bpe_ende
2020-07-01 10:46:47,050 cfg.training.overwrite             : True
2020-07-01 10:46:47,050 cfg.training.shuffle               : True
2020-07-01 10:46:47,050 cfg.training.use_cuda              : True
2020-07-01 10:46:47,050 cfg.training.max_output_length     : 100
2020-07-01 10:46:47,050 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-01 10:46:47,050 cfg.training.keep_last_ckpts       : 3
2020-07-01 10:46:47,050 cfg.model.initializer              : xavier
2020-07-01 10:46:47,050 cfg.model.bias_initializer         : zeros
2020-07-01 10:46:47,050 cfg.model.init_gain                : 1.0
2020-07-01 10:46:47,050 cfg.model.embed_initializer        : xavier
2020-07-01 10:46:47,050 cfg.model.embed_init_gain          : 1.0
2020-07-01 10:46:47,050 cfg.model.tied_embeddings          : False
2020-07-01 10:46:47,050 cfg.model.tied_softmax             : True
2020-07-01 10:46:47,050 cfg.model.encoder.type             : transformer
2020-07-01 10:46:47,050 cfg.model.encoder.num_layers       : 3
2020-07-01 10:46:47,050 cfg.model.encoder.num_heads        : 8
2020-07-01 10:46:47,050 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-01 10:46:47,050 cfg.model.encoder.embeddings.scale : True
2020-07-01 10:46:47,051 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-01 10:46:47,051 cfg.model.encoder.hidden_size      : 512
2020-07-01 10:46:47,051 cfg.model.encoder.ff_size          : 2048
2020-07-01 10:46:47,051 cfg.model.encoder.dropout          : 0.1
2020-07-01 10:46:47,051 cfg.model.encoder.freeze           : False
2020-07-01 10:46:47,051 cfg.model.encoder.multi_encoder    : True
2020-07-01 10:46:47,051 cfg.model.decoder.type             : transformer
2020-07-01 10:46:47,051 cfg.model.decoder.num_layers       : 6
2020-07-01 10:46:47,051 cfg.model.decoder.num_heads        : 8
2020-07-01 10:46:47,051 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-01 10:46:47,051 cfg.model.decoder.embeddings.scale : True
2020-07-01 10:46:47,051 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-01 10:46:47,051 cfg.model.decoder.hidden_size      : 512
2020-07-01 10:46:47,051 cfg.model.decoder.ff_size          : 2048
2020-07-01 10:46:47,051 cfg.model.decoder.dropout          : 0.1
2020-07-01 10:46:47,051 cfg.model.decoder.freeze           : False
2020-07-01 10:46:47,051 Data set sizes: 
	train 9771,
	valid 1522,
	test 1160
2020-07-01 10:46:47,052 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-01 10:46:47,052 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) &@@ (9) a@@
2020-07-01 10:46:47,052 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) Sie (8) ich (9) das
2020-07-01 10:46:47,052 Number of Src words (types): 4442
2020-07-01 10:46:47,052 Number of Trg words (types): 5796
2020-07-01 10:46:47,052 Model(
	encoder=TransformerEncoder(num_layers=2, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4442),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5796))
2020-07-01 10:46:47,065 EPOCH 1
2020-07-01 10:47:04,061 Epoch   1: total training loss 373.23
2020-07-01 10:47:04,062 EPOCH 2
2020-07-01 10:47:12,598 Epoch   2 Step:      100 Batch Loss:     4.201469 Tokens per Sec:     9359, Lr: 0.000200
2020-07-01 10:47:20,794 Epoch   2: total training loss 330.65
2020-07-01 10:47:20,794 EPOCH 3
2020-07-01 10:47:37,235 Epoch   3 Step:      200 Batch Loss:     3.993238 Tokens per Sec:     9055, Lr: 0.000200
2020-07-01 10:47:37,660 Epoch   3: total training loss 309.67
2020-07-01 10:47:37,661 EPOCH 4
2020-07-01 10:47:54,592 Epoch   4: total training loss 270.25
2020-07-01 10:47:54,592 EPOCH 5
2020-07-01 10:48:02,010 Epoch   5 Step:      300 Batch Loss:     2.206034 Tokens per Sec:     8972, Lr: 0.000200
2020-07-01 10:48:11,535 Epoch   5: total training loss 239.46
2020-07-01 10:48:11,536 EPOCH 6
2020-07-01 10:48:26,862 Epoch   6 Step:      400 Batch Loss:     2.582681 Tokens per Sec:     9048, Lr: 0.000200
2020-07-01 10:48:28,277 Epoch   6: total training loss 204.78
2020-07-01 10:48:28,277 EPOCH 7
2020-07-01 10:48:44,532 Epoch   7: total training loss 180.80
2020-07-01 10:48:44,532 EPOCH 8
2020-07-01 10:48:50,799 Epoch   8 Step:      500 Batch Loss:     2.145879 Tokens per Sec:     9473, Lr: 0.000200
2020-07-01 10:49:00,933 Epoch   8: total training loss 155.30
2020-07-01 10:49:00,934 EPOCH 9
2020-07-01 10:49:15,358 Epoch   9 Step:      600 Batch Loss:     2.315721 Tokens per Sec:     9223, Lr: 0.000200
2020-07-01 10:49:17,284 Epoch   9: total training loss 138.71
2020-07-01 10:49:17,284 EPOCH 10
2020-07-01 10:49:33,820 Epoch  10: total training loss 121.51
2020-07-01 10:49:33,820 EPOCH 11
2020-07-01 10:49:39,840 Epoch  11 Step:      700 Batch Loss:     1.010329 Tokens per Sec:     9263, Lr: 0.000200
2020-07-01 10:49:50,414 Epoch  11: total training loss 109.33
2020-07-01 10:49:50,414 EPOCH 12
2020-07-01 10:50:04,181 Epoch  12 Step:      800 Batch Loss:     1.634174 Tokens per Sec:     9434, Lr: 0.000200
2020-07-01 10:50:06,950 Epoch  12: total training loss 100.97
2020-07-01 10:50:06,950 EPOCH 13
2020-07-01 10:50:23,867 Epoch  13: total training loss 87.99
2020-07-01 10:50:23,867 EPOCH 14
2020-07-01 10:50:29,369 Epoch  14 Step:      900 Batch Loss:     0.929548 Tokens per Sec:     8887, Lr: 0.000200
2020-07-01 10:50:40,686 Epoch  14: total training loss 84.13
2020-07-01 10:50:40,686 EPOCH 15
2020-07-01 10:50:54,086 Epoch  15 Step:     1000 Batch Loss:     0.934566 Tokens per Sec:     9210, Lr: 0.000200
2020-07-01 10:51:34,025 Hooray! New best validation result [ppl]!
2020-07-01 10:51:34,025 Saving new checkpoint.
2020-07-01 10:51:41,003 Example #0
2020-07-01 10:51:41,004 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 10:51:41,004 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 10:51:41,004 	Source:     Hello .
2020-07-01 10:51:41,004 	Reference:  Hallo ,
2020-07-01 10:51:41,004 	Hypothesis: Hallo .
2020-07-01 10:51:41,004 Example #1
2020-07-01 10:51:41,004 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 10:51:41,004 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 10:51:41,004 	Source:     Hi , how can I help you ?
2020-07-01 10:51:41,004 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:51:41,004 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:51:41,005 Example #2
2020-07-01 10:51:41,005 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 10:51:41,005 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 10:51:41,005 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 10:51:41,005 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 10:51:41,005 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 10:51:41,005 Example #3
2020-07-01 10:51:41,005 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 10:51:41,005 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 10:51:41,005 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 10:51:41,005 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 10:51:41,005 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 10:51:41,005 Validation result (greedy) at epoch  15, step     1000: bleu:  31.45, loss: 46088.6797, ppl:   6.7881, duration: 46.9183s
2020-07-01 10:51:44,883 Epoch  15: total training loss 80.39
2020-07-01 10:51:44,883 EPOCH 16
2020-07-01 10:52:02,230 Epoch  16: total training loss 66.63
2020-07-01 10:52:02,231 EPOCH 17
2020-07-01 10:52:07,292 Epoch  17 Step:     1100 Batch Loss:     0.529741 Tokens per Sec:     7765, Lr: 0.000200
2020-07-01 10:52:19,938 Epoch  17: total training loss 61.17
2020-07-01 10:52:19,939 EPOCH 18
2020-07-01 10:52:33,640 Epoch  18 Step:     1200 Batch Loss:     0.634836 Tokens per Sec:     8627, Lr: 0.000200
2020-07-01 10:52:37,899 Epoch  18: total training loss 54.45
2020-07-01 10:52:37,900 EPOCH 19
2020-07-01 10:52:55,562 Epoch  19: total training loss 49.35
2020-07-01 10:52:55,562 EPOCH 20
2020-07-01 10:52:59,064 Epoch  20 Step:     1300 Batch Loss:     0.505933 Tokens per Sec:     9571, Lr: 0.000200
2020-07-01 10:53:12,687 Epoch  20: total training loss 44.74
2020-07-01 10:53:12,687 EPOCH 21
2020-07-01 10:53:24,664 Epoch  21 Step:     1400 Batch Loss:     0.521710 Tokens per Sec:     9124, Lr: 0.000200
2020-07-01 10:53:29,762 Epoch  21: total training loss 39.25
2020-07-01 10:53:29,762 EPOCH 22
2020-07-01 10:53:46,884 Epoch  22: total training loss 34.70
2020-07-01 10:53:46,885 EPOCH 23
2020-07-01 10:53:49,925 Epoch  23 Step:     1500 Batch Loss:     0.397959 Tokens per Sec:     9648, Lr: 0.000200
2020-07-01 10:54:04,045 Epoch  23: total training loss 32.43
2020-07-01 10:54:04,045 EPOCH 24
2020-07-01 10:54:15,558 Epoch  24 Step:     1600 Batch Loss:     0.706725 Tokens per Sec:     9089, Lr: 0.000200
2020-07-01 10:54:21,289 Epoch  24: total training loss 30.49
2020-07-01 10:54:21,290 EPOCH 25
2020-07-01 10:54:38,753 Epoch  25: total training loss 26.82
2020-07-01 10:54:38,754 EPOCH 26
2020-07-01 10:54:41,490 Epoch  26 Step:     1700 Batch Loss:     0.357915 Tokens per Sec:     9079, Lr: 0.000200
2020-07-01 10:54:56,555 Epoch  26: total training loss 25.01
2020-07-01 10:54:56,556 EPOCH 27
2020-07-01 10:55:07,825 Epoch  27 Step:     1800 Batch Loss:     0.347799 Tokens per Sec:     8708, Lr: 0.000200
2020-07-01 10:55:13,987 Epoch  27: total training loss 23.17
2020-07-01 10:55:13,988 EPOCH 28
2020-07-01 10:55:30,795 Epoch  28: total training loss 21.83
2020-07-01 10:55:30,795 EPOCH 29
2020-07-01 10:55:32,518 Epoch  29 Step:     1900 Batch Loss:     0.281084 Tokens per Sec:     7014, Lr: 0.000200
2020-07-01 10:55:47,741 Epoch  29: total training loss 19.85
2020-07-01 10:55:47,741 EPOCH 30
2020-07-01 10:55:57,613 Epoch  30 Step:     2000 Batch Loss:     0.202100 Tokens per Sec:     8891, Lr: 0.000200
2020-07-01 10:56:27,338 Hooray! New best validation result [ppl]!
2020-07-01 10:56:27,338 Saving new checkpoint.
2020-07-01 10:56:33,828 Example #0
2020-07-01 10:56:33,829 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 10:56:33,829 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 10:56:33,829 	Source:     Hello .
2020-07-01 10:56:33,829 	Reference:  Hallo ,
2020-07-01 10:56:33,829 	Hypothesis: Hallo .
2020-07-01 10:56:33,829 Example #1
2020-07-01 10:56:33,829 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 10:56:33,830 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 10:56:33,830 	Source:     Hi , how can I help you ?
2020-07-01 10:56:33,830 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:56:33,830 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:56:33,830 Example #2
2020-07-01 10:56:33,830 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 10:56:33,830 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', '.']
2020-07-01 10:56:33,830 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 10:56:33,830 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 10:56:33,830 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall .
2020-07-01 10:56:33,830 Example #3
2020-07-01 10:56:33,830 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 10:56:33,830 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 10:56:33,830 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 10:56:33,830 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 10:56:33,830 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 10:56:33,830 Validation result (greedy) at epoch  30, step     2000: bleu:  36.61, loss: 44178.8203, ppl:   6.2702, duration: 36.2168s
2020-07-01 10:56:41,100 Epoch  30: total training loss 17.98
2020-07-01 10:56:41,100 EPOCH 31
2020-07-01 10:56:58,436 Epoch  31: total training loss 16.39
2020-07-01 10:56:58,436 EPOCH 32
2020-07-01 10:56:59,018 Epoch  32 Step:     2100 Batch Loss:     0.317208 Tokens per Sec:    11236, Lr: 0.000200
2020-07-01 10:57:15,834 Epoch  32: total training loss 15.87
2020-07-01 10:57:15,835 EPOCH 33
2020-07-01 10:57:24,943 Epoch  33 Step:     2200 Batch Loss:     0.241861 Tokens per Sec:     8996, Lr: 0.000200
2020-07-01 10:57:33,601 Epoch  33: total training loss 15.11
2020-07-01 10:57:33,601 EPOCH 34
2020-07-01 10:57:51,320 Epoch  34: total training loss 13.90
2020-07-01 10:57:51,320 EPOCH 35
2020-07-01 10:57:51,698 Epoch  35 Step:     2300 Batch Loss:     0.157428 Tokens per Sec:     5484, Lr: 0.000200
2020-07-01 10:58:08,257 Epoch  35: total training loss 12.84
2020-07-01 10:58:08,257 EPOCH 36
2020-07-01 10:58:16,992 Epoch  36 Step:     2400 Batch Loss:     0.190647 Tokens per Sec:     9256, Lr: 0.000200
2020-07-01 10:58:25,210 Epoch  36: total training loss 12.82
2020-07-01 10:58:25,210 EPOCH 37
2020-07-01 10:58:41,945 Epoch  37 Step:     2500 Batch Loss:     0.181680 Tokens per Sec:     9012, Lr: 0.000200
2020-07-01 10:58:42,197 Epoch  37: total training loss 12.25
2020-07-01 10:58:42,197 EPOCH 38
2020-07-01 10:58:59,210 Epoch  38: total training loss 11.98
2020-07-01 10:58:59,211 EPOCH 39
2020-07-01 10:59:07,377 Epoch  39 Step:     2600 Batch Loss:     0.170224 Tokens per Sec:     8983, Lr: 0.000200
2020-07-01 10:59:16,347 Epoch  39: total training loss 11.57
2020-07-01 10:59:16,347 EPOCH 40
2020-07-01 10:59:32,215 Epoch  40 Step:     2700 Batch Loss:     0.161394 Tokens per Sec:     9117, Lr: 0.000200
2020-07-01 10:59:33,454 Epoch  40: total training loss 11.15
2020-07-01 10:59:33,454 EPOCH 41
2020-07-01 10:59:50,309 Epoch  41: total training loss 10.54
2020-07-01 10:59:50,310 EPOCH 42
2020-07-01 10:59:57,236 Epoch  42 Step:     2800 Batch Loss:     0.146026 Tokens per Sec:     9219, Lr: 0.000200
2020-07-01 11:00:07,294 Epoch  42: total training loss 10.63
2020-07-01 11:00:07,294 EPOCH 43
2020-07-01 11:00:22,703 Epoch  43 Step:     2900 Batch Loss:     0.153936 Tokens per Sec:     8851, Lr: 0.000200
2020-07-01 11:00:24,371 Epoch  43: total training loss 10.23
2020-07-01 11:00:24,371 EPOCH 44
2020-07-01 11:00:41,468 Epoch  44: total training loss 9.76
2020-07-01 11:00:41,468 EPOCH 45
2020-07-01 11:00:47,700 Epoch  45 Step:     3000 Batch Loss:     0.132113 Tokens per Sec:     9062, Lr: 0.000200
2020-07-01 11:01:21,055 Example #0
2020-07-01 11:01:21,056 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:01:21,056 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:01:21,056 	Source:     Hello .
2020-07-01 11:01:21,056 	Reference:  Hallo ,
2020-07-01 11:01:21,056 	Hypothesis: Hallo .
2020-07-01 11:01:21,056 Example #1
2020-07-01 11:01:21,056 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:01:21,056 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:01:21,056 	Source:     Hi , how can I help you ?
2020-07-01 11:01:21,056 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:01:21,056 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:01:21,056 Example #2
2020-07-01 11:01:21,056 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:01:21,056 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:01:21,056 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:01:21,056 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:01:21,056 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:01:21,056 Example #3
2020-07-01 11:01:21,056 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:01:21,056 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:01:21,056 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:01:21,056 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:01:21,056 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:01:21,056 Validation result (greedy) at epoch  45, step     3000: bleu:  37.34, loss: 45069.4961, ppl:   6.5066, duration: 33.3559s
2020-07-01 11:01:31,685 Epoch  45: total training loss 9.41
2020-07-01 11:01:31,686 EPOCH 46
2020-07-01 11:01:46,316 Epoch  46 Step:     3100 Batch Loss:     0.147401 Tokens per Sec:     8993, Lr: 0.000200
2020-07-01 11:01:48,693 Epoch  46: total training loss 9.17
2020-07-01 11:01:48,694 EPOCH 47
2020-07-01 11:02:05,690 Epoch  47: total training loss 9.07
2020-07-01 11:02:05,691 EPOCH 48
2020-07-01 11:02:11,693 Epoch  48 Step:     3200 Batch Loss:     0.136001 Tokens per Sec:     8644, Lr: 0.000200
2020-07-01 11:02:22,599 Epoch  48: total training loss 8.89
2020-07-01 11:02:22,599 EPOCH 49
2020-07-01 11:02:37,178 Epoch  49 Step:     3300 Batch Loss:     0.127051 Tokens per Sec:     8790, Lr: 0.000200
2020-07-01 11:02:40,195 Epoch  49: total training loss 8.97
2020-07-01 11:02:40,195 EPOCH 50
2020-07-01 11:02:57,807 Epoch  50: total training loss 8.92
2020-07-01 11:02:57,808 EPOCH 51
2020-07-01 11:03:03,441 Epoch  51 Step:     3400 Batch Loss:     0.127175 Tokens per Sec:     8109, Lr: 0.000200
2020-07-01 11:03:15,443 Epoch  51: total training loss 8.90
2020-07-01 11:03:15,444 EPOCH 52
2020-07-01 11:03:28,687 Epoch  52 Step:     3500 Batch Loss:     0.121136 Tokens per Sec:     8816, Lr: 0.000200
2020-07-01 11:03:33,087 Epoch  52: total training loss 9.24
2020-07-01 11:03:33,088 EPOCH 53
2020-07-01 11:03:50,718 Epoch  53: total training loss 9.81
2020-07-01 11:03:50,718 EPOCH 54
2020-07-01 11:03:54,418 Epoch  54 Step:     3600 Batch Loss:     0.147251 Tokens per Sec:     8395, Lr: 0.000200
2020-07-01 11:04:08,510 Epoch  54: total training loss 10.08
2020-07-01 11:04:08,510 EPOCH 55
2020-07-01 11:04:20,156 Epoch  55 Step:     3700 Batch Loss:     0.120902 Tokens per Sec:     8963, Lr: 0.000200
2020-07-01 11:04:25,947 Epoch  55: total training loss 8.76
2020-07-01 11:04:25,947 EPOCH 56
2020-07-01 11:04:43,717 Epoch  56: total training loss 8.75
2020-07-01 11:04:43,717 EPOCH 57
2020-07-01 11:04:46,213 Epoch  57 Step:     3800 Batch Loss:     0.129727 Tokens per Sec:     8215, Lr: 0.000200
2020-07-01 11:05:01,482 Epoch  57: total training loss 10.19
2020-07-01 11:05:01,483 EPOCH 58
2020-07-01 11:05:12,089 Epoch  58 Step:     3900 Batch Loss:     0.398906 Tokens per Sec:     8732, Lr: 0.000200
2020-07-01 11:05:19,332 Epoch  58: total training loss 10.26
2020-07-01 11:05:19,333 EPOCH 59
2020-07-01 11:05:36,987 Epoch  59: total training loss 8.55
2020-07-01 11:05:36,987 EPOCH 60
2020-07-01 11:05:38,543 Epoch  60 Step:     4000 Batch Loss:     0.104494 Tokens per Sec:     8405, Lr: 0.000200
2020-07-01 11:06:01,510 Example #0
2020-07-01 11:06:01,511 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:06:01,511 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:06:01,511 	Source:     Hello .
2020-07-01 11:06:01,511 	Reference:  Hallo ,
2020-07-01 11:06:01,511 	Hypothesis: Hallo .
2020-07-01 11:06:01,511 Example #1
2020-07-01 11:06:01,511 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:06:01,511 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:06:01,511 	Source:     Hi , how can I help you ?
2020-07-01 11:06:01,511 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:06:01,511 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:06:01,511 Example #2
2020-07-01 11:06:01,512 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:06:01,512 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:06:01,512 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:06:01,512 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:06:01,512 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:06:01,512 Example #3
2020-07-01 11:06:01,512 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:06:01,512 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:06:01,512 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:06:01,512 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:06:01,512 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:06:01,512 Validation result (greedy) at epoch  60, step     4000: bleu:  37.41, loss: 45229.2734, ppl:   6.5500, duration: 22.9688s
2020-07-01 11:06:17,161 Epoch  60: total training loss 7.78
2020-07-01 11:06:17,162 EPOCH 61
2020-07-01 11:06:26,673 Epoch  61 Step:     4100 Batch Loss:     0.097408 Tokens per Sec:     9086, Lr: 0.000200
2020-07-01 11:06:34,349 Epoch  61: total training loss 7.37
2020-07-01 11:06:34,349 EPOCH 62
2020-07-01 11:06:51,467 Epoch  62: total training loss 7.26
2020-07-01 11:06:51,468 EPOCH 63
2020-07-01 11:06:52,234 Epoch  63 Step:     4200 Batch Loss:     0.096314 Tokens per Sec:     4095, Lr: 0.000200
2020-07-01 11:07:08,415 Epoch  63: total training loss 6.99
2020-07-01 11:07:08,416 EPOCH 64
2020-07-01 11:07:17,426 Epoch  64 Step:     4300 Batch Loss:     0.106611 Tokens per Sec:     8473, Lr: 0.000200
2020-07-01 11:07:25,357 Epoch  64: total training loss 6.89
2020-07-01 11:07:25,358 EPOCH 65
2020-07-01 11:07:42,212 Epoch  65 Step:     4400 Batch Loss:     0.095679 Tokens per Sec:     8980, Lr: 0.000200
2020-07-01 11:07:42,510 Epoch  65: total training loss 6.72
2020-07-01 11:07:42,510 EPOCH 66
2020-07-01 11:07:59,513 Epoch  66: total training loss 6.65
2020-07-01 11:07:59,514 EPOCH 67
2020-07-01 11:08:07,302 Epoch  67 Step:     4500 Batch Loss:     0.091990 Tokens per Sec:     9210, Lr: 0.000200
2020-07-01 11:08:16,291 Epoch  67: total training loss 6.50
2020-07-01 11:08:16,291 EPOCH 68
2020-07-01 11:08:32,563 Epoch  68 Step:     4600 Batch Loss:     0.099217 Tokens per Sec:     9059, Lr: 0.000200
2020-07-01 11:08:33,348 Epoch  68: total training loss 6.36
2020-07-01 11:08:33,348 EPOCH 69
2020-07-01 11:08:50,397 Epoch  69: total training loss 6.28
2020-07-01 11:08:50,397 EPOCH 70
2020-07-01 11:08:57,831 Epoch  70 Step:     4700 Batch Loss:     0.083362 Tokens per Sec:     9201, Lr: 0.000200
2020-07-01 11:09:07,508 Epoch  70: total training loss 6.37
2020-07-01 11:09:07,509 EPOCH 71
2020-07-01 11:09:23,088 Epoch  71 Step:     4800 Batch Loss:     0.102805 Tokens per Sec:     9088, Lr: 0.000200
2020-07-01 11:09:24,423 Epoch  71: total training loss 6.26
2020-07-01 11:09:24,423 EPOCH 72
2020-07-01 11:09:41,447 Epoch  72: total training loss 6.53
2020-07-01 11:09:41,447 EPOCH 73
2020-07-01 11:09:48,112 Epoch  73 Step:     4900 Batch Loss:     0.093167 Tokens per Sec:     9249, Lr: 0.000200
2020-07-01 11:09:58,423 Epoch  73: total training loss 6.66
2020-07-01 11:09:58,424 EPOCH 74
2020-07-01 11:10:13,358 Epoch  74 Step:     5000 Batch Loss:     0.105126 Tokens per Sec:     9072, Lr: 0.000200
2020-07-01 11:10:42,972 Example #0
2020-07-01 11:10:42,972 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:10:42,972 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:10:42,973 	Source:     Hello .
2020-07-01 11:10:42,973 	Reference:  Hallo ,
2020-07-01 11:10:42,973 	Hypothesis: Hallo .
2020-07-01 11:10:42,973 Example #1
2020-07-01 11:10:42,973 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:10:42,973 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:10:42,973 	Source:     Hi , how can I help you ?
2020-07-01 11:10:42,973 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:10:42,973 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:10:42,973 Example #2
2020-07-01 11:10:42,973 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:10:42,973 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:10:42,973 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:10:42,974 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:10:42,974 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:10:42,974 Example #3
2020-07-01 11:10:42,974 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:10:42,974 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:10:42,974 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:10:42,974 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:10:42,974 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:10:42,974 Validation result (greedy) at epoch  74, step     5000: bleu:  37.67, loss: 45700.7344, ppl:   6.6796, duration: 29.6150s
2020-07-01 11:10:45,167 Epoch  74: total training loss 6.34
2020-07-01 11:10:45,168 EPOCH 75
2020-07-01 11:11:02,092 Epoch  75: total training loss 6.20
2020-07-01 11:11:02,093 EPOCH 76
2020-07-01 11:11:08,416 Epoch  76 Step:     5100 Batch Loss:     0.097426 Tokens per Sec:     8811, Lr: 0.000200
2020-07-01 11:11:19,028 Epoch  76: total training loss 6.12
2020-07-01 11:11:19,029 EPOCH 77
2020-07-01 11:11:33,221 Epoch  77 Step:     5200 Batch Loss:     0.089080 Tokens per Sec:     9171, Lr: 0.000200
2020-07-01 11:11:36,121 Epoch  77: total training loss 6.06
2020-07-01 11:11:36,121 EPOCH 78
2020-07-01 11:11:53,026 Epoch  78: total training loss 6.18
2020-07-01 11:11:53,027 EPOCH 79
2020-07-01 11:11:58,568 Epoch  79 Step:     5300 Batch Loss:     0.085393 Tokens per Sec:     9032, Lr: 0.000200
2020-07-01 11:12:10,063 Epoch  79: total training loss 6.15
2020-07-01 11:12:10,063 EPOCH 80
2020-07-01 11:12:23,760 Epoch  80 Step:     5400 Batch Loss:     0.086141 Tokens per Sec:     9074, Lr: 0.000200
2020-07-01 11:12:27,117 Epoch  80: total training loss 6.01
2020-07-01 11:12:27,117 EPOCH 81
2020-07-01 11:12:44,225 Epoch  81: total training loss 6.06
2020-07-01 11:12:44,225 EPOCH 82
2020-07-01 11:12:49,089 Epoch  82 Step:     5500 Batch Loss:     0.107313 Tokens per Sec:     8815, Lr: 0.000200
2020-07-01 11:13:01,130 Epoch  82: total training loss 6.37
2020-07-01 11:13:01,131 EPOCH 83
2020-07-01 11:13:14,422 Epoch  83 Step:     5600 Batch Loss:     0.086252 Tokens per Sec:     9054, Lr: 0.000200
2020-07-01 11:13:18,169 Epoch  83: total training loss 6.17
2020-07-01 11:13:18,170 EPOCH 84
2020-07-01 11:13:35,062 Epoch  84: total training loss 6.02
2020-07-01 11:13:35,062 EPOCH 85
2020-07-01 11:13:39,448 Epoch  85 Step:     5700 Batch Loss:     0.086183 Tokens per Sec:     9169, Lr: 0.000200
2020-07-01 11:13:51,921 Epoch  85: total training loss 5.94
2020-07-01 11:13:51,922 EPOCH 86
2020-07-01 11:14:04,567 Epoch  86 Step:     5800 Batch Loss:     0.087398 Tokens per Sec:     9163, Lr: 0.000200
2020-07-01 11:14:08,766 Epoch  86: total training loss 6.08
2020-07-01 11:14:08,767 EPOCH 87
2020-07-01 11:14:25,530 Epoch  87: total training loss 5.83
2020-07-01 11:14:25,530 EPOCH 88
2020-07-01 11:14:29,118 Epoch  88 Step:     5900 Batch Loss:     0.082018 Tokens per Sec:     9880, Lr: 0.000200
2020-07-01 11:14:42,443 Epoch  88: total training loss 6.15
2020-07-01 11:14:42,443 EPOCH 89
2020-07-01 11:14:54,480 Epoch  89 Step:     6000 Batch Loss:     0.090012 Tokens per Sec:     9136, Lr: 0.000200
2020-07-01 11:15:21,506 Example #0
2020-07-01 11:15:21,506 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:15:21,506 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:15:21,506 	Source:     Hello .
2020-07-01 11:15:21,507 	Reference:  Hallo ,
2020-07-01 11:15:21,507 	Hypothesis: Hallo .
2020-07-01 11:15:21,507 Example #1
2020-07-01 11:15:21,507 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:15:21,507 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:15:21,507 	Source:     Hi , how can I help you ?
2020-07-01 11:15:21,507 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:15:21,507 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:15:21,507 Example #2
2020-07-01 11:15:21,507 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:15:21,507 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:15:21,507 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:15:21,507 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:15:21,507 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:15:21,507 Example #3
2020-07-01 11:15:21,507 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:15:21,507 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:15:21,507 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:15:21,507 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:15:21,507 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:15:21,507 Validation result (greedy) at epoch  89, step     6000: bleu:  37.79, loss: 45756.7070, ppl:   6.6951, duration: 27.0259s
2020-07-01 11:15:26,769 Epoch  89: total training loss 6.14
2020-07-01 11:15:26,769 EPOCH 90
2020-07-01 11:15:43,979 Epoch  90: total training loss 5.76
2020-07-01 11:15:43,980 EPOCH 91
2020-07-01 11:15:47,166 Epoch  91 Step:     6100 Batch Loss:     0.077081 Tokens per Sec:     9493, Lr: 0.000200
2020-07-01 11:16:01,190 Epoch  91: total training loss 5.45
2020-07-01 11:16:01,190 EPOCH 92
2020-07-01 11:16:13,920 Epoch  92 Step:     6200 Batch Loss:     0.089828 Tokens per Sec:     8222, Lr: 0.000200
2020-07-01 11:16:18,952 Epoch  92: total training loss 5.56
2020-07-01 11:16:18,952 EPOCH 93
2020-07-01 11:16:36,548 Epoch  93: total training loss 5.52
2020-07-01 11:16:36,548 EPOCH 94
2020-07-01 11:16:39,341 Epoch  94 Step:     6300 Batch Loss:     0.069675 Tokens per Sec:     8811, Lr: 0.000200
2020-07-01 11:16:54,089 Epoch  94: total training loss 5.47
2020-07-01 11:16:54,089 EPOCH 95
2020-07-01 11:17:05,118 Epoch  95 Step:     6400 Batch Loss:     0.086162 Tokens per Sec:     8740, Lr: 0.000200
2020-07-01 11:17:11,713 Epoch  95: total training loss 5.31
2020-07-01 11:17:11,713 EPOCH 96
2020-07-01 11:17:29,098 Epoch  96: total training loss 5.27
2020-07-01 11:17:29,099 EPOCH 97
2020-07-01 11:17:31,054 Epoch  97 Step:     6500 Batch Loss:     0.070419 Tokens per Sec:     8245, Lr: 0.000200
2020-07-01 11:17:46,638 Epoch  97: total training loss 5.25
2020-07-01 11:17:46,639 EPOCH 98
2020-07-01 11:17:56,591 Epoch  98 Step:     6600 Batch Loss:     0.077972 Tokens per Sec:     8694, Lr: 0.000200
2020-07-01 11:18:04,201 Epoch  98: total training loss 5.24
2020-07-01 11:18:04,202 EPOCH 99
2020-07-01 11:18:21,159 Epoch  99: total training loss 5.08
2020-07-01 11:18:21,160 EPOCH 100
2020-07-01 11:18:22,326 Epoch 100 Step:     6700 Batch Loss:     0.095467 Tokens per Sec:     6693, Lr: 0.000200
2020-07-01 11:18:38,212 Epoch 100: total training loss 5.10
2020-07-01 11:18:38,212 Training ended after 100 epochs.
2020-07-01 11:18:38,212 Best validation result (greedy) at step     2000:   6.27 ppl.
2020-07-01 11:19:03,621  dev bleu:  37.53 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 11:19:03,626 Translations saved to: models/transformer_multi_enc_wmt17bpe_ende/00002000.hyps.dev
2020-07-01 11:19:19,836 test bleu:  35.48 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 11:19:19,841 Translations saved to: models/transformer_multi_enc_wmt17bpe_ende/00002000.hyps.test
