2020-06-30 14:03:24,905 Hello! This is Joey-NMT.
2020-06-30 14:03:30,351 Total params: 50554881
2020-06-30 14:03:30,353 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-30 14:03:32,568 cfg.name                           : transformer_multi_enc_ende
2020-06-30 14:03:32,569 cfg.data.src                       : en
2020-06-30 14:03:32,569 cfg.data.trg                       : de
2020-06-30 14:03:32,569 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-06-30 14:03:32,569 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-06-30 14:03:32,569 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-06-30 14:03:32,569 cfg.data.level                     : bpe
2020-06-30 14:03:32,569 cfg.data.lowercase                 : False
2020-06-30 14:03:32,569 cfg.data.max_sent_length           : 100
2020-06-30 14:03:32,569 cfg.testing.beam_size              : 5
2020-06-30 14:03:32,569 cfg.testing.alpha                  : 1.0
2020-06-30 14:03:32,569 cfg.training.random_seed           : 42
2020-06-30 14:03:32,569 cfg.training.optimizer             : adam
2020-06-30 14:03:32,569 cfg.training.normalization         : tokens
2020-06-30 14:03:32,569 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-30 14:03:32,569 cfg.training.scheduling            : plateau
2020-06-30 14:03:32,569 cfg.training.patience              : 8
2020-06-30 14:03:32,569 cfg.training.decrease_factor       : 0.7
2020-06-30 14:03:32,569 cfg.training.loss                  : crossentropy
2020-06-30 14:03:32,569 cfg.training.learning_rate         : 0.0002
2020-06-30 14:03:32,569 cfg.training.learning_rate_min     : 1e-08
2020-06-30 14:03:32,569 cfg.training.weight_decay          : 0.0
2020-06-30 14:03:32,569 cfg.training.label_smoothing       : 0.1
2020-06-30 14:03:32,569 cfg.training.batch_size            : 4096
2020-06-30 14:03:32,569 cfg.training.batch_type            : token
2020-06-30 14:03:32,569 cfg.training.batch_multiplier      : 1
2020-06-30 14:03:32,569 cfg.training.early_stopping_metric : ppl
2020-06-30 14:03:32,569 cfg.training.epochs                : 100
2020-06-30 14:03:32,569 cfg.training.validation_freq       : 1000
2020-06-30 14:03:32,569 cfg.training.logging_freq          : 100
2020-06-30 14:03:32,569 cfg.training.eval_metric           : bleu
2020-06-30 14:03:32,569 cfg.training.model_dir             : models/transformer_multi_enc_wmt17bpe_ende
2020-06-30 14:03:32,569 cfg.training.overwrite             : True
2020-06-30 14:03:32,569 cfg.training.shuffle               : True
2020-06-30 14:03:32,569 cfg.training.use_cuda              : True
2020-06-30 14:03:32,569 cfg.training.max_output_length     : 100
2020-06-30 14:03:32,569 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-30 14:03:32,569 cfg.training.keep_last_ckpts       : 3
2020-06-30 14:03:32,570 cfg.model.initializer              : xavier
2020-06-30 14:03:32,570 cfg.model.bias_initializer         : zeros
2020-06-30 14:03:32,570 cfg.model.init_gain                : 1.0
2020-06-30 14:03:32,570 cfg.model.embed_initializer        : xavier
2020-06-30 14:03:32,570 cfg.model.embed_init_gain          : 1.0
2020-06-30 14:03:32,570 cfg.model.tied_embeddings          : False
2020-06-30 14:03:32,570 cfg.model.tied_softmax             : True
2020-06-30 14:03:32,570 cfg.model.encoder.type             : transformer
2020-06-30 14:03:32,570 cfg.model.encoder.num_layers       : 3
2020-06-30 14:03:32,570 cfg.model.encoder.num_heads        : 8
2020-06-30 14:03:32,570 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-30 14:03:32,570 cfg.model.encoder.embeddings.scale : True
2020-06-30 14:03:32,570 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-30 14:03:32,570 cfg.model.encoder.hidden_size      : 512
2020-06-30 14:03:32,570 cfg.model.encoder.ff_size          : 2048
2020-06-30 14:03:32,570 cfg.model.encoder.dropout          : 0.1
2020-06-30 14:03:32,570 cfg.model.encoder.freeze           : False
2020-06-30 14:03:32,570 cfg.model.encoder.multi_encoder    : True
2020-06-30 14:03:32,570 cfg.model.decoder.type             : transformer
2020-06-30 14:03:32,570 cfg.model.decoder.num_layers       : 6
2020-06-30 14:03:32,570 cfg.model.decoder.num_heads        : 8
2020-06-30 14:03:32,570 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-30 14:03:32,570 cfg.model.decoder.embeddings.scale : True
2020-06-30 14:03:32,570 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-30 14:03:32,570 cfg.model.decoder.hidden_size      : 512
2020-06-30 14:03:32,570 cfg.model.decoder.ff_size          : 2048
2020-06-30 14:03:32,570 cfg.model.decoder.dropout          : 0.1
2020-06-30 14:03:32,570 cfg.model.decoder.freeze           : False
2020-06-30 14:03:32,570 Data set sizes: 
	train 9747,
	valid 1523,
	test 1186
2020-06-30 14:03:32,570 First training example:
	[SRC] H@@ i there@@ ! How can I hel@@ p@@ ?
	[TRG] Hal@@ lo@@ ! Wie kann ich hel@@ fen@@ ?
2020-06-30 14:03:32,570 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) the (9) to
2020-06-30 14:03:32,570 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ? (7) Sie (8) ich (9) en@@
2020-06-30 14:03:32,570 Number of Src words (types): 4579
2020-06-30 14:03:32,571 Number of Trg words (types): 5891
2020-06-30 14:03:32,571 Model(
	encoder=TransformerEncoder(num_layers=2, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4579),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5891))
2020-06-30 14:03:32,580 EPOCH 1
2020-06-30 14:03:50,291 Epoch   1: total training loss 377.35
2020-06-30 14:03:50,295 EPOCH 2
2020-06-30 14:03:59,401 Epoch   2 Step:      100 Batch Loss:     5.793183 Tokens per Sec:     9547, Lr: 0.000200
2020-06-30 14:04:08,024 Epoch   2: total training loss 330.87
2020-06-30 14:04:08,025 EPOCH 3
2020-06-30 14:04:26,668 Epoch   3: total training loss 290.26
2020-06-30 14:04:26,669 EPOCH 4
2020-06-30 14:04:28,086 Epoch   4 Step:      200 Batch Loss:     3.285614 Tokens per Sec:     8422, Lr: 0.000200
2020-06-30 14:04:46,197 Epoch   4: total training loss 255.67
2020-06-30 14:04:46,198 EPOCH 5
2020-06-30 14:04:58,248 Epoch   5 Step:      300 Batch Loss:     3.857683 Tokens per Sec:     8433, Lr: 0.000200
2020-06-30 14:05:06,550 Epoch   5: total training loss 225.07
2020-06-30 14:05:06,551 EPOCH 6
2020-06-30 14:05:27,021 Epoch   6: total training loss 194.45
2020-06-30 14:05:27,021 EPOCH 7
2020-06-30 14:05:29,956 Epoch   7 Step:      400 Batch Loss:     3.244866 Tokens per Sec:     8789, Lr: 0.000200
2020-06-30 14:05:47,688 Epoch   7: total training loss 178.74
2020-06-30 14:05:47,688 EPOCH 8
2020-06-30 14:06:01,098 Epoch   8 Step:      500 Batch Loss:     2.995601 Tokens per Sec:     8189, Lr: 0.000200
2020-06-30 14:06:08,648 Epoch   8: total training loss 156.97
2020-06-30 14:06:08,648 EPOCH 9
2020-06-30 14:06:28,839 Epoch   9: total training loss 140.27
2020-06-30 14:06:28,839 EPOCH 10
2020-06-30 14:06:32,794 Epoch  10 Step:      600 Batch Loss:     1.193197 Tokens per Sec:     7701, Lr: 0.000200
2020-06-30 14:06:49,204 Epoch  10: total training loss 123.19
2020-06-30 14:06:49,205 EPOCH 11
2020-06-30 14:07:03,976 Epoch  11 Step:      700 Batch Loss:     2.292894 Tokens per Sec:     8026, Lr: 0.000200
2020-06-30 14:07:09,407 Epoch  11: total training loss 111.43
2020-06-30 14:07:09,407 EPOCH 12
2020-06-30 14:07:29,218 Epoch  12: total training loss 102.12
2020-06-30 14:07:29,218 EPOCH 13
2020-06-30 14:07:34,227 Epoch  13 Step:      800 Batch Loss:     1.457100 Tokens per Sec:     7944, Lr: 0.000200
2020-06-30 14:07:49,301 Epoch  13: total training loss 95.66
2020-06-30 14:07:49,301 EPOCH 14
2020-06-30 14:08:05,084 Epoch  14 Step:      900 Batch Loss:     1.265662 Tokens per Sec:     7822, Lr: 0.000200
2020-06-30 14:08:09,732 Epoch  14: total training loss 86.92
2020-06-30 14:08:09,733 EPOCH 15
2020-06-30 14:08:29,654 Epoch  15: total training loss 74.85
2020-06-30 14:08:29,655 EPOCH 16
2020-06-30 14:08:35,369 Epoch  16 Step:     1000 Batch Loss:     1.567097 Tokens per Sec:     8227, Lr: 0.000200
2020-06-30 14:09:25,605 Hooray! New best validation result [ppl]!
2020-06-30 14:09:25,605 Saving new checkpoint.
2020-06-30 14:09:32,245 Example #0
2020-06-30 14:09:32,246 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 14:09:32,246 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 14:09:32,246 	Source:     Hello.
2020-06-30 14:09:32,246 	Reference:  Hallo,
2020-06-30 14:09:32,246 	Hypothesis: Hallo.
2020-06-30 14:09:32,246 Example #1
2020-06-30 14:09:32,246 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 14:09:32,246 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 14:09:32,246 	Source:     Hi, how can I help you?
2020-06-30 14:09:32,246 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:09:32,247 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:09:32,247 Example #2
2020-06-30 14:09:32,247 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 14:09:32,247 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 14:09:32,247 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 14:09:32,247 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 14:09:32,247 	Hypothesis: Hallo, ich suche ein Restaurant in San Francisco, Kalifornien, in San Francisco, Kalifornien.
2020-06-30 14:09:32,247 Example #3
2020-06-30 14:09:32,247 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 14:09:32,247 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 14:09:32,247 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 14:09:32,247 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 14:09:32,247 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 14:09:32,247 Validation result (greedy) at epoch  16, step     1000: bleu:  22.87, loss: 49546.5234, ppl:   6.9248, duration: 56.8770s
2020-06-30 14:09:46,031 Epoch  16: total training loss 68.63
2020-06-30 14:09:46,032 EPOCH 17
2020-06-30 14:10:02,486 Epoch  17 Step:     1100 Batch Loss:     0.921003 Tokens per Sec:     8312, Lr: 0.000200
2020-06-30 14:10:05,660 Epoch  17: total training loss 61.28
2020-06-30 14:10:05,661 EPOCH 18
2020-06-30 14:10:25,586 Epoch  18: total training loss 55.32
2020-06-30 14:10:25,587 EPOCH 19
2020-06-30 14:10:33,783 Epoch  19 Step:     1200 Batch Loss:     1.076740 Tokens per Sec:     7858, Lr: 0.000200
2020-06-30 14:10:45,700 Epoch  19: total training loss 51.18
2020-06-30 14:10:45,700 EPOCH 20
2020-06-30 14:11:04,963 Epoch  20 Step:     1300 Batch Loss:     0.889586 Tokens per Sec:     7777, Lr: 0.000200
2020-06-30 14:11:06,421 Epoch  20: total training loss 46.48
2020-06-30 14:11:06,421 EPOCH 21
2020-06-30 14:11:26,757 Epoch  21: total training loss 44.58
2020-06-30 14:11:26,757 EPOCH 22
2020-06-30 14:11:35,833 Epoch  22 Step:     1400 Batch Loss:     0.590342 Tokens per Sec:     8149, Lr: 0.000200
2020-06-30 14:11:46,594 Epoch  22: total training loss 39.45
2020-06-30 14:11:46,595 EPOCH 23
2020-06-30 14:12:06,180 Epoch  23 Step:     1500 Batch Loss:     0.462256 Tokens per Sec:     8033, Lr: 0.000200
2020-06-30 14:12:06,750 Epoch  23: total training loss 35.06
2020-06-30 14:12:06,751 EPOCH 24
2020-06-30 14:12:26,782 Epoch  24: total training loss 31.86
2020-06-30 14:12:26,783 EPOCH 25
2020-06-30 14:12:37,130 Epoch  25 Step:     1600 Batch Loss:     0.288746 Tokens per Sec:     7727, Lr: 0.000200
2020-06-30 14:12:47,786 Epoch  25: total training loss 28.21
2020-06-30 14:12:47,787 EPOCH 26
2020-06-30 14:13:07,577 Epoch  26: total training loss 26.36
2020-06-30 14:13:07,578 EPOCH 27
2020-06-30 14:13:08,535 Epoch  27 Step:     1700 Batch Loss:     0.233695 Tokens per Sec:     4430, Lr: 0.000200
2020-06-30 14:13:27,608 Epoch  27: total training loss 23.82
2020-06-30 14:13:27,609 EPOCH 28
2020-06-30 14:13:38,494 Epoch  28 Step:     1800 Batch Loss:     0.342780 Tokens per Sec:     8151, Lr: 0.000200
2020-06-30 14:13:47,453 Epoch  28: total training loss 22.16
2020-06-30 14:13:47,454 EPOCH 29
2020-06-30 14:14:07,432 Epoch  29: total training loss 20.31
2020-06-30 14:14:07,432 EPOCH 30
2020-06-30 14:14:08,896 Epoch  30 Step:     1900 Batch Loss:     0.326948 Tokens per Sec:     8824, Lr: 0.000200
2020-06-30 14:14:27,681 Epoch  30: total training loss 18.64
2020-06-30 14:14:27,682 EPOCH 31
2020-06-30 14:14:39,797 Epoch  31 Step:     2000 Batch Loss:     0.291878 Tokens per Sec:     8002, Lr: 0.000200
2020-06-30 14:15:21,151 Hooray! New best validation result [ppl]!
2020-06-30 14:15:21,152 Saving new checkpoint.
2020-06-30 14:15:27,740 Example #0
2020-06-30 14:15:27,740 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 14:15:27,740 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 14:15:27,740 	Source:     Hello.
2020-06-30 14:15:27,740 	Reference:  Hallo,
2020-06-30 14:15:27,740 	Hypothesis: Hallo.
2020-06-30 14:15:27,740 Example #1
2020-06-30 14:15:27,740 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 14:15:27,740 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 14:15:27,741 	Source:     Hi, how can I help you?
2020-06-30 14:15:27,741 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:15:27,741 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:15:27,741 Example #2
2020-06-30 14:15:27,741 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 14:15:27,741 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', '.']
2020-06-30 14:15:27,741 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 14:15:27,741 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 14:15:27,741 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall.
2020-06-30 14:15:27,741 Example #3
2020-06-30 14:15:27,741 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 14:15:27,741 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 14:15:27,741 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 14:15:27,741 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 14:15:27,741 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 14:15:27,741 Validation result (greedy) at epoch  31, step     2000: bleu:  28.45, loss: 46910.8398, ppl:   6.2474, duration: 47.9436s
2020-06-30 14:15:35,500 Epoch  31: total training loss 16.90
2020-06-30 14:15:35,500 EPOCH 32
2020-06-30 14:15:55,062 Epoch  32: total training loss 14.99
2020-06-30 14:15:55,063 EPOCH 33
2020-06-30 14:15:57,935 Epoch  33 Step:     2100 Batch Loss:     0.314030 Tokens per Sec:     7669, Lr: 0.000200
2020-06-30 14:16:14,571 Epoch  33: total training loss 14.78
2020-06-30 14:16:14,572 EPOCH 34
2020-06-30 14:16:27,290 Epoch  34 Step:     2200 Batch Loss:     0.200986 Tokens per Sec:     8277, Lr: 0.000200
2020-06-30 14:16:35,411 Epoch  34: total training loss 14.13
2020-06-30 14:16:35,411 EPOCH 35
2020-06-30 14:16:55,565 Epoch  35: total training loss 13.08
2020-06-30 14:16:55,566 EPOCH 36
2020-06-30 14:16:59,453 Epoch  36 Step:     2300 Batch Loss:     0.184181 Tokens per Sec:     7370, Lr: 0.000200
2020-06-30 14:17:16,106 Epoch  36: total training loss 12.78
2020-06-30 14:17:16,107 EPOCH 37
2020-06-30 14:17:30,262 Epoch  37 Step:     2400 Batch Loss:     0.183031 Tokens per Sec:     8035, Lr: 0.000200
2020-06-30 14:17:36,898 Epoch  37: total training loss 12.29
2020-06-30 14:17:36,898 EPOCH 38
2020-06-30 14:17:56,341 Epoch  38: total training loss 11.62
2020-06-30 14:17:56,342 EPOCH 39
2020-06-30 14:18:01,329 Epoch  39 Step:     2500 Batch Loss:     0.204346 Tokens per Sec:     7757, Lr: 0.000200
2020-06-30 14:18:16,367 Epoch  39: total training loss 11.42
2020-06-30 14:18:16,367 EPOCH 40
2020-06-30 14:18:31,388 Epoch  40 Step:     2600 Batch Loss:     0.173456 Tokens per Sec:     8329, Lr: 0.000200
2020-06-30 14:18:36,393 Epoch  40: total training loss 11.09
2020-06-30 14:18:36,394 EPOCH 41
2020-06-30 14:18:56,350 Epoch  41: total training loss 10.92
2020-06-30 14:18:56,350 EPOCH 42
2020-06-30 14:19:02,340 Epoch  42 Step:     2700 Batch Loss:     0.179101 Tokens per Sec:     7969, Lr: 0.000200
2020-06-30 14:19:16,513 Epoch  42: total training loss 10.33
2020-06-30 14:19:16,513 EPOCH 43
2020-06-30 14:19:32,873 Epoch  43 Step:     2800 Batch Loss:     0.186433 Tokens per Sec:     8258, Lr: 0.000200
2020-06-30 14:19:36,613 Epoch  43: total training loss 10.47
2020-06-30 14:19:36,613 EPOCH 44
2020-06-30 14:19:56,741 Epoch  44: total training loss 10.35
2020-06-30 14:19:56,741 EPOCH 45
2020-06-30 14:20:04,783 Epoch  45 Step:     2900 Batch Loss:     0.137890 Tokens per Sec:     7516, Lr: 0.000200
2020-06-30 14:20:17,105 Epoch  45: total training loss 9.80
2020-06-30 14:20:17,106 EPOCH 46
2020-06-30 14:20:35,129 Epoch  46 Step:     3000 Batch Loss:     0.148223 Tokens per Sec:     8095, Lr: 0.000200
2020-06-30 14:21:08,543 Example #0
2020-06-30 14:21:08,543 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 14:21:08,543 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 14:21:08,543 	Source:     Hello.
2020-06-30 14:21:08,543 	Reference:  Hallo,
2020-06-30 14:21:08,543 	Hypothesis: Hallo.
2020-06-30 14:21:08,543 Example #1
2020-06-30 14:21:08,544 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 14:21:08,544 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 14:21:08,544 	Source:     Hi, how can I help you?
2020-06-30 14:21:08,544 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:21:08,544 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:21:08,544 Example #2
2020-06-30 14:21:08,544 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 14:21:08,544 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 14:21:08,544 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 14:21:08,544 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 14:21:08,544 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall in San Francisco, Kalifornien.
2020-06-30 14:21:08,544 Example #3
2020-06-30 14:21:08,544 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 14:21:08,544 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 14:21:08,544 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 14:21:08,544 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 14:21:08,544 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 14:21:08,544 Validation result (greedy) at epoch  46, step     3000: bleu:  28.74, loss: 48362.4336, ppl:   6.6118, duration: 33.4138s
2020-06-30 14:21:10,866 Epoch  46: total training loss 9.79
2020-06-30 14:21:10,866 EPOCH 47
2020-06-30 14:21:31,464 Epoch  47: total training loss 9.38
2020-06-30 14:21:31,465 EPOCH 48
2020-06-30 14:21:40,837 Epoch  48 Step:     3100 Batch Loss:     0.110297 Tokens per Sec:     7522, Lr: 0.000200
2020-06-30 14:21:51,954 Epoch  48: total training loss 9.24
2020-06-30 14:21:51,955 EPOCH 49
2020-06-30 14:22:11,393 Epoch  49 Step:     3200 Batch Loss:     0.130312 Tokens per Sec:     7955, Lr: 0.000200
2020-06-30 14:22:12,198 Epoch  49: total training loss 9.13
2020-06-30 14:22:12,198 EPOCH 50
2020-06-30 14:22:32,031 Epoch  50: total training loss 8.77
2020-06-30 14:22:32,032 EPOCH 51
2020-06-30 14:22:42,121 Epoch  51 Step:     3300 Batch Loss:     0.129250 Tokens per Sec:     7714, Lr: 0.000200
2020-06-30 14:22:51,911 Epoch  51: total training loss 8.39
2020-06-30 14:22:51,912 EPOCH 52
2020-06-30 14:23:11,328 Epoch  52: total training loss 8.18
2020-06-30 14:23:11,329 EPOCH 53
2020-06-30 14:23:11,596 Epoch  53 Step:     3400 Batch Loss:     0.118315 Tokens per Sec:    11094, Lr: 0.000200
2020-06-30 14:23:31,194 Epoch  53: total training loss 8.07
2020-06-30 14:23:31,194 EPOCH 54
2020-06-30 14:23:42,071 Epoch  54 Step:     3500 Batch Loss:     0.138625 Tokens per Sec:     8017, Lr: 0.000200
2020-06-30 14:23:51,762 Epoch  54: total training loss 8.25
2020-06-30 14:23:51,763 EPOCH 55
2020-06-30 14:24:11,841 Epoch  55: total training loss 7.59
2020-06-30 14:24:11,842 EPOCH 56
2020-06-30 14:24:13,402 Epoch  56 Step:     3600 Batch Loss:     0.120630 Tokens per Sec:     8157, Lr: 0.000200
2020-06-30 14:24:31,393 Epoch  56: total training loss 7.51
2020-06-30 14:24:31,394 EPOCH 57
2020-06-30 14:24:44,238 Epoch  57 Step:     3700 Batch Loss:     0.104122 Tokens per Sec:     8053, Lr: 0.000200
2020-06-30 14:24:51,292 Epoch  57: total training loss 7.34
2020-06-30 14:24:51,293 EPOCH 58
2020-06-30 14:25:11,740 Epoch  58: total training loss 7.38
2020-06-30 14:25:11,740 EPOCH 59
2020-06-30 14:25:14,723 Epoch  59 Step:     3800 Batch Loss:     0.113842 Tokens per Sec:     8050, Lr: 0.000200
2020-06-30 14:25:31,872 Epoch  59: total training loss 7.42
2020-06-30 14:25:31,872 EPOCH 60
2020-06-30 14:25:45,603 Epoch  60 Step:     3900 Batch Loss:     0.121914 Tokens per Sec:     8098, Lr: 0.000200
2020-06-30 14:25:51,721 Epoch  60: total training loss 7.35
2020-06-30 14:25:51,722 EPOCH 61
2020-06-30 14:26:11,905 Epoch  61: total training loss 7.17
2020-06-30 14:26:11,905 EPOCH 62
2020-06-30 14:26:16,265 Epoch  62 Step:     4000 Batch Loss:     0.094636 Tokens per Sec:     8456, Lr: 0.000200
2020-06-30 14:26:51,570 Example #0
2020-06-30 14:26:51,570 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 14:26:51,570 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 14:26:51,570 	Source:     Hello.
2020-06-30 14:26:51,570 	Reference:  Hallo,
2020-06-30 14:26:51,570 	Hypothesis: Hallo.
2020-06-30 14:26:51,570 Example #1
2020-06-30 14:26:51,570 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 14:26:51,570 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 14:26:51,570 	Source:     Hi, how can I help you?
2020-06-30 14:26:51,570 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:26:51,570 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:26:51,570 Example #2
2020-06-30 14:26:51,570 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 14:26:51,570 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', '.', 'Das', 'Restaurant', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 14:26:51,570 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 14:26:51,570 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 14:26:51,570 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall. Das Restaurant in San Francisco, Kalifornien.
2020-06-30 14:26:51,570 Example #3
2020-06-30 14:26:51,570 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 14:26:51,570 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 14:26:51,570 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 14:26:51,570 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 14:26:51,570 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 14:26:51,571 Validation result (greedy) at epoch  62, step     4000: bleu:  30.11, loss: 47987.5898, ppl:   6.5158, duration: 35.3041s
2020-06-30 14:27:07,636 Epoch  62: total training loss 7.00
2020-06-30 14:27:07,637 EPOCH 63
2020-06-30 14:27:22,883 Epoch  63 Step:     4100 Batch Loss:     0.103289 Tokens per Sec:     8144, Lr: 0.000200
2020-06-30 14:27:27,769 Epoch  63: total training loss 6.79
2020-06-30 14:27:27,769 EPOCH 64
2020-06-30 14:27:48,005 Epoch  64: total training loss 6.53
2020-06-30 14:27:48,006 EPOCH 65
2020-06-30 14:27:53,733 Epoch  65 Step:     4200 Batch Loss:     0.102662 Tokens per Sec:     8691, Lr: 0.000200
2020-06-30 14:28:08,525 Epoch  65: total training loss 6.82
2020-06-30 14:28:08,525 EPOCH 66
2020-06-30 14:28:25,081 Epoch  66 Step:     4300 Batch Loss:     0.116976 Tokens per Sec:     8260, Lr: 0.000200
2020-06-30 14:28:28,696 Epoch  66: total training loss 6.85
2020-06-30 14:28:28,697 EPOCH 67
2020-06-30 14:28:48,745 Epoch  67: total training loss 6.66
2020-06-30 14:28:48,746 EPOCH 68
2020-06-30 14:28:56,429 Epoch  68 Step:     4400 Batch Loss:     0.110622 Tokens per Sec:     7466, Lr: 0.000200
2020-06-30 14:29:09,361 Epoch  68: total training loss 6.70
2020-06-30 14:29:09,361 EPOCH 69
2020-06-30 14:29:27,570 Epoch  69 Step:     4500 Batch Loss:     0.096216 Tokens per Sec:     7896, Lr: 0.000200
2020-06-30 14:29:29,339 Epoch  69: total training loss 6.70
2020-06-30 14:29:29,339 EPOCH 70
2020-06-30 14:29:50,134 Epoch  70: total training loss 7.36
2020-06-30 14:29:50,135 EPOCH 71
2020-06-30 14:29:58,043 Epoch  71 Step:     4600 Batch Loss:     0.102728 Tokens per Sec:     8567, Lr: 0.000200
2020-06-30 14:30:10,430 Epoch  71: total training loss 7.10
2020-06-30 14:30:10,431 EPOCH 72
2020-06-30 14:30:29,691 Epoch  72 Step:     4700 Batch Loss:     0.116414 Tokens per Sec:     8067, Lr: 0.000200
2020-06-30 14:30:30,939 Epoch  72: total training loss 6.95
2020-06-30 14:30:30,939 EPOCH 73
2020-06-30 14:30:50,490 Epoch  73: total training loss 6.48
2020-06-30 14:30:50,490 EPOCH 74
2020-06-30 14:30:59,970 Epoch  74 Step:     4800 Batch Loss:     0.104808 Tokens per Sec:     8152, Lr: 0.000200
2020-06-30 14:31:10,331 Epoch  74: total training loss 6.26
2020-06-30 14:31:10,332 EPOCH 75
2020-06-30 14:31:30,402 Epoch  75 Step:     4900 Batch Loss:     0.096079 Tokens per Sec:     8119, Lr: 0.000200
2020-06-30 14:31:30,404 Epoch  75: total training loss 6.05
2020-06-30 14:31:30,404 EPOCH 76
2020-06-30 14:31:50,416 Epoch  76: total training loss 5.78
2020-06-30 14:31:50,417 EPOCH 77
2020-06-30 14:32:01,362 Epoch  77 Step:     5000 Batch Loss:     0.083385 Tokens per Sec:     8128, Lr: 0.000200
2020-06-30 14:32:41,060 Example #0
2020-06-30 14:32:41,061 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 14:32:41,061 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 14:32:41,061 	Source:     Hello.
2020-06-30 14:32:41,061 	Reference:  Hallo,
2020-06-30 14:32:41,061 	Hypothesis: Hallo.
2020-06-30 14:32:41,061 Example #1
2020-06-30 14:32:41,061 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 14:32:41,061 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 14:32:41,061 	Source:     Hi, how can I help you?
2020-06-30 14:32:41,061 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:32:41,061 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:32:41,061 Example #2
2020-06-30 14:32:41,061 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 14:32:41,061 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 14:32:41,061 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 14:32:41,061 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 14:32:41,061 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall in San Francisco, Kalifornien.
2020-06-30 14:32:41,061 Example #3
2020-06-30 14:32:41,061 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 14:32:41,061 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 14:32:41,061 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 14:32:41,061 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 14:32:41,061 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 14:32:41,061 Validation result (greedy) at epoch  77, step     5000: bleu:  30.35, loss: 48160.3203, ppl:   6.5599, duration: 39.6987s
2020-06-30 14:32:50,512 Epoch  77: total training loss 5.85
2020-06-30 14:32:50,513 EPOCH 78
2020-06-30 14:33:10,431 Epoch  78: total training loss 5.86
2020-06-30 14:33:10,432 EPOCH 79
2020-06-30 14:33:11,463 Epoch  79 Step:     5100 Batch Loss:     0.094193 Tokens per Sec:    10873, Lr: 0.000200
2020-06-30 14:33:31,158 Epoch  79: total training loss 6.05
2020-06-30 14:33:31,158 EPOCH 80
2020-06-30 14:33:42,953 Epoch  80 Step:     5200 Batch Loss:     0.091553 Tokens per Sec:     8183, Lr: 0.000200
2020-06-30 14:33:51,093 Epoch  80: total training loss 5.73
2020-06-30 14:33:51,094 EPOCH 81
2020-06-30 14:34:11,140 Epoch  81: total training loss 5.65
2020-06-30 14:34:11,141 EPOCH 82
2020-06-30 14:34:14,065 Epoch  82 Step:     5300 Batch Loss:     0.076123 Tokens per Sec:     6744, Lr: 0.000200
2020-06-30 14:34:30,976 Epoch  82: total training loss 5.50
2020-06-30 14:34:30,977 EPOCH 83
2020-06-30 14:34:44,729 Epoch  83 Step:     5400 Batch Loss:     0.076516 Tokens per Sec:     7850, Lr: 0.000200
2020-06-30 14:34:50,824 Epoch  83: total training loss 5.49
2020-06-30 14:34:50,824 EPOCH 84
2020-06-30 14:35:11,626 Epoch  84: total training loss 5.46
2020-06-30 14:35:11,626 EPOCH 85
2020-06-30 14:35:15,463 Epoch  85 Step:     5500 Batch Loss:     0.084228 Tokens per Sec:     9171, Lr: 0.000200
2020-06-30 14:35:31,807 Epoch  85: total training loss 5.52
2020-06-30 14:35:31,808 EPOCH 86
2020-06-30 14:35:47,021 Epoch  86 Step:     5600 Batch Loss:     0.074655 Tokens per Sec:     7877, Lr: 0.000200
2020-06-30 14:35:51,959 Epoch  86: total training loss 5.46
2020-06-30 14:35:51,959 EPOCH 87
2020-06-30 14:36:12,400 Epoch  87: total training loss 5.30
2020-06-30 14:36:12,401 EPOCH 88
2020-06-30 14:36:18,318 Epoch  88 Step:     5700 Batch Loss:     0.074127 Tokens per Sec:     7242, Lr: 0.000200
2020-06-30 14:36:32,832 Epoch  88: total training loss 5.21
2020-06-30 14:36:32,833 EPOCH 89
2020-06-30 14:36:48,961 Epoch  89 Step:     5800 Batch Loss:     0.087344 Tokens per Sec:     8051, Lr: 0.000200
2020-06-30 14:36:53,388 Epoch  89: total training loss 5.93
2020-06-30 14:36:53,389 EPOCH 90
2020-06-30 14:37:13,550 Epoch  90: total training loss 6.50
2020-06-30 14:37:13,550 EPOCH 91
2020-06-30 14:37:20,831 Epoch  91 Step:     5900 Batch Loss:     0.087109 Tokens per Sec:     7736, Lr: 0.000200
2020-06-30 14:37:34,118 Epoch  91: total training loss 6.55
2020-06-30 14:37:34,119 EPOCH 92
2020-06-30 14:37:51,524 Epoch  92 Step:     6000 Batch Loss:     0.093182 Tokens per Sec:     8437, Lr: 0.000200
2020-06-30 14:38:28,449 Example #0
2020-06-30 14:38:28,449 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-30 14:38:28,449 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-30 14:38:28,449 	Source:     Hello.
2020-06-30 14:38:28,449 	Reference:  Hallo,
2020-06-30 14:38:28,449 	Hypothesis: Hallo.
2020-06-30 14:38:28,449 Example #1
2020-06-30 14:38:28,449 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-30 14:38:28,449 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-30 14:38:28,449 	Source:     Hi, how can I help you?
2020-06-30 14:38:28,449 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:38:28,449 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-30 14:38:28,449 Example #2
2020-06-30 14:38:28,449 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-30 14:38:28,449 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-30 14:38:28,449 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-30 14:38:28,450 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-30 14:38:28,450 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, Kalifornien.
2020-06-30 14:38:28,450 Example #3
2020-06-30 14:38:28,450 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-30 14:38:28,450 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-30 14:38:28,450 	Source:     Ok, what type of restaurant are you looking for?
2020-06-30 14:38:28,450 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-30 14:38:28,450 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-30 14:38:28,450 Validation result (greedy) at epoch  92, step     6000: bleu:  29.89, loss: 48428.9102, ppl:   6.6290, duration: 36.9249s
2020-06-30 14:38:30,713 Epoch  92: total training loss 6.01
2020-06-30 14:38:30,713 EPOCH 93
2020-06-30 14:38:51,716 Epoch  93: total training loss 5.57
2020-06-30 14:38:51,716 EPOCH 94
2020-06-30 14:38:59,696 Epoch  94 Step:     6100 Batch Loss:     0.071393 Tokens per Sec:     8641, Lr: 0.000200
2020-06-30 14:39:12,003 Epoch  94: total training loss 5.39
2020-06-30 14:39:12,004 EPOCH 95
2020-06-30 14:39:31,192 Epoch  95 Step:     6200 Batch Loss:     0.104240 Tokens per Sec:     8126, Lr: 0.000200
2020-06-30 14:39:32,440 Epoch  95: total training loss 5.20
2020-06-30 14:39:32,440 EPOCH 96
2020-06-30 14:39:52,715 Epoch  96: total training loss 5.32
2020-06-30 14:39:52,715 EPOCH 97
2020-06-30 14:40:01,735 Epoch  97 Step:     6300 Batch Loss:     0.072252 Tokens per Sec:     8385, Lr: 0.000200
2020-06-30 14:40:13,363 Epoch  97: total training loss 5.20
2020-06-30 14:40:13,364 EPOCH 98
2020-06-30 14:40:33,664 Epoch  98 Step:     6400 Batch Loss:     0.072517 Tokens per Sec:     8028, Lr: 0.000200
2020-06-30 14:40:33,665 Epoch  98: total training loss 4.91
2020-06-30 14:40:33,665 EPOCH 99
2020-06-30 14:40:54,399 Epoch  99: total training loss 4.85
2020-06-30 14:40:54,400 EPOCH 100
2020-06-30 14:41:05,512 Epoch 100 Step:     6500 Batch Loss:     0.074523 Tokens per Sec:     7870, Lr: 0.000200
2020-06-30 14:41:14,328 Epoch 100: total training loss 5.17
2020-06-30 14:41:14,329 Training ended after 100 epochs.
2020-06-30 14:41:14,329 Best validation result (greedy) at step     2000:   6.25 ppl.
2020-06-30 14:41:43,721  dev bleu:  30.40 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-30 14:41:43,725 Translations saved to: models/transformer_multi_enc_wmt17bpe_ende/00002000.hyps.dev
2020-06-30 14:42:04,579 test bleu:  27.75 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-30 14:42:04,584 Translations saved to: models/transformer_multi_enc_wmt17bpe_ende/00002000.hyps.test
