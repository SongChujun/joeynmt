name: "transformer"

data:
    src: "en"
    trg: "de"
    # train: "chatnmt/prep/train.tags.bpe.wmt_ende_best"    # training data
    train: "chatnmt/prep/train.tags.bpe.wmt_ende_best_opensubs"
    dev: "chatnmt/prep/dev.tags.bpe.wmt_ende_best"        # development data for validation
    test:  "chatnmt/prep/test.tags.bpe.wmt_ende_best"
    level: "bpe"
    lowercase: False
    max_sent_length: 100
    src_vocab: "models/wmt_ende_transformer/src_vocab.txt"
    trg_vocab: "models/wmt_ende_transformer/trg_vocab.txt"

testing:
    beam_size: 5
    alpha: 1.0

training:
    random_seed: 42
    optimizer: "adam"
    normalization: "tokens"
    adam_betas: [0.9, 0.999]
    scheduling: "plateau"
    patience: 8
    decrease_factor: 0.7
    loss: "crossentropy"
    learning_rate: 0.0002
    learning_rate_min: 0.00000001
    weight_decay: 0.0
    label_smoothing: 0.1
    batch_size: 250                 # mini-batch size as number of sentences (when batch_type is "sentence"; default) or total number of tokens (when batch_type is "token")
    batch_type: "token"
    eval_batch_size: 3600
    eval_batch_type: "token"
    batch_multiplier: 1
    early_stopping_metric: "ppl"
    epochs: 100
    validation_freq: 1000
    logging_freq: 100
    eval_metric: "bleu"
    model_dir: "models/transformer_multi_enc_ende-tune"
    load_model: "models/wmt_ende_transformer/best.ckpt"
    reset_best_ckpt: True
    reset_scheduler: True
    reset_optimizer: True
    overwrite: False
    shuffle: True
    use_cuda: True
    max_output_length: 100
    print_valid_sents: [0, 1, 2, 3]
    keep_last_ckpts: 3
model:                              # specify your model architecture here
    initializer: "xavier"           # initializer for all trainable weights (xavier, zeros, normal, uniform)
    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)
    bias_initializer: "zeros"       # initializer for bias terms (xavier, zeros, normal, uniform)
    embed_initializer: "xavier"     # initializer for embeddings (xavier, zeros, normal, uniform)
    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)
    tied_embeddings: False          # tie src and trg embeddings, only applicable if vocabularies are the same, default: False
    tied_softmax: True
    encoder:
        type: "transformer"          # encoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
        num_layers: 3               # number of layers
        num_heads: 4                # number of transformer heads
        embeddings:
            embedding_dim: 64       # size of embeddings (for Transformer set equal to hidden_size)
            scale: True             # scale the embeddings by sqrt of their size, default: False
            freeze: False           # if True, embeddings are not updated during training
        hidden_size: 64             # size of hidden layer; must be divisible by number of heads
        ff_size: 128                # size of position-wise feed-forward layer
        dropout: 0.1                # apply dropout to the inputs to the RNN, default: 0.0
        freeze: True               # if True, encoder parameters are not updated during training (does not include embedding parameters)
        multi_encoder: True         # if True, add an encoder to parametrize context sentences
    decoder:
        type: "transformer"         # decoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
        num_layers: 3               # number of layers
        num_heads: 4                # number of transformer heads
        embeddings:
            embedding_dim: 64
            scale: True
            freeze: False           # if True, embeddings are not updated during training
        hidden_size: 64             # size of hidden layer; must be divisible by number of heads
        ff_size: 128                # size of position-wise feed-forward layer
        dropout: 0.1
        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)
