2020-06-22 09:27:42,401 Hello! This is Joey-NMT.
2020-06-22 09:27:50,195 Total params: 82862081
2020-06-22 09:27:50,197 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-06-22 09:27:50,200 Loading model from models/wmt_ende_transformer/best.ckpt
2020-06-22 09:27:50,990 Reset optimizer.
2020-06-22 09:27:50,991 Reset scheduler.
2020-06-22 09:27:50,991 Reset tracking of the best checkpoint.
2020-06-22 09:27:51,009 cfg.name                           : transformer
2020-06-22 09:27:51,009 cfg.data.src                       : en
2020-06-22 09:27:51,009 cfg.data.trg                       : de
2020-06-22 09:27:51,009 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-06-22 09:27:51,010 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-06-22 09:27:51,010 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-06-22 09:27:51,010 cfg.data.level                     : bpe
2020-06-22 09:27:51,010 cfg.data.lowercase                 : False
2020-06-22 09:27:51,010 cfg.data.max_sent_length           : 100
2020-06-22 09:27:51,010 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-06-22 09:27:51,010 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-06-22 09:27:51,010 cfg.testing.beam_size              : 5
2020-06-22 09:27:51,010 cfg.testing.alpha                  : 1.0
2020-06-22 09:27:51,010 cfg.training.random_seed           : 42
2020-06-22 09:27:51,010 cfg.training.optimizer             : adam
2020-06-22 09:27:51,010 cfg.training.normalization         : tokens
2020-06-22 09:27:51,010 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-22 09:27:51,010 cfg.training.scheduling            : noam
2020-06-22 09:27:51,010 cfg.training.learning_rate_warmup  : 2000
2020-06-22 09:27:51,010 cfg.training.patience              : 8
2020-06-22 09:27:51,010 cfg.training.decrease_factor       : 0.7
2020-06-22 09:27:51,010 cfg.training.loss                  : crossentropy
2020-06-22 09:27:51,011 cfg.training.learning_rate         : 0.0002
2020-06-22 09:27:51,011 cfg.training.learning_rate_min     : 1e-08
2020-06-22 09:27:51,011 cfg.training.weight_decay          : 0.0
2020-06-22 09:27:51,011 cfg.training.label_smoothing       : 0.1
2020-06-22 09:27:51,011 cfg.training.batch_size            : 4096
2020-06-22 09:27:51,011 cfg.training.batch_type            : token
2020-06-22 09:27:51,011 cfg.training.eval_batch_size       : 3600
2020-06-22 09:27:51,011 cfg.training.eval_batch_type       : token
2020-06-22 09:27:51,011 cfg.training.batch_multiplier      : 1
2020-06-22 09:27:51,011 cfg.training.early_stopping_metric : ppl
2020-06-22 09:27:51,011 cfg.training.epochs                : 100
2020-06-22 09:27:51,011 cfg.training.validation_freq       : 1000
2020-06-22 09:27:51,011 cfg.training.logging_freq          : 100
2020-06-22 09:27:51,011 cfg.training.eval_metric           : bleu
2020-06-22 09:27:51,011 cfg.training.model_dir             : models/transformer_multi_enc_ende-tune-noam
2020-06-22 09:27:51,011 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-06-22 09:27:51,011 cfg.training.reset_best_ckpt       : True
2020-06-22 09:27:51,011 cfg.training.reset_scheduler       : True
2020-06-22 09:27:51,011 cfg.training.reset_optimizer       : True
2020-06-22 09:27:51,012 cfg.training.overwrite             : False
2020-06-22 09:27:51,012 cfg.training.shuffle               : True
2020-06-22 09:27:51,012 cfg.training.use_cuda              : False
2020-06-22 09:27:51,012 cfg.training.max_output_length     : 100
2020-06-22 09:27:51,012 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-22 09:27:51,012 cfg.training.keep_last_ckpts       : 3
2020-06-22 09:27:51,012 cfg.model.initializer              : xavier
2020-06-22 09:27:51,012 cfg.model.bias_initializer         : zeros
2020-06-22 09:27:51,012 cfg.model.init_gain                : 1.0
2020-06-22 09:27:51,012 cfg.model.embed_initializer        : xavier
2020-06-22 09:27:51,012 cfg.model.embed_init_gain          : 1.0
2020-06-22 09:27:51,012 cfg.model.tied_embeddings          : True
2020-06-22 09:27:51,012 cfg.model.tied_softmax             : True
2020-06-22 09:27:51,012 cfg.model.encoder.type             : transformer
2020-06-22 09:27:51,012 cfg.model.encoder.num_layers       : 6
2020-06-22 09:27:51,012 cfg.model.encoder.num_heads        : 8
2020-06-22 09:27:51,012 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-22 09:27:51,012 cfg.model.encoder.embeddings.scale : True
2020-06-22 09:27:51,012 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-22 09:27:51,012 cfg.model.encoder.hidden_size      : 512
2020-06-22 09:27:51,013 cfg.model.encoder.ff_size          : 2048
2020-06-22 09:27:51,013 cfg.model.encoder.dropout          : 0.1
2020-06-22 09:27:51,013 cfg.model.encoder.multi_encoder    : True
2020-06-22 09:27:51,013 cfg.model.decoder.type             : transformer
2020-06-22 09:27:51,013 cfg.model.decoder.num_layers       : 6
2020-06-22 09:27:51,013 cfg.model.decoder.num_heads        : 8
2020-06-22 09:27:51,013 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-22 09:27:51,013 cfg.model.decoder.embeddings.scale : True
2020-06-22 09:27:51,013 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-22 09:27:51,013 cfg.model.decoder.hidden_size      : 512
2020-06-22 09:27:51,013 cfg.model.decoder.ff_size          : 2048
2020-06-22 09:27:51,013 cfg.model.decoder.dropout          : 0.1
2020-06-22 09:27:51,013 Data set sizes: 
	train 9747,
	valid 1523,
	test 1186
2020-06-22 09:27:51,013 First training example:
	[SRC] H@@ i there@@ ! How can I hel@@ p@@ ?
	[TRG] Hal@@ lo@@ ! Wie kann ich hel@@ fen@@ ?
2020-06-22 09:27:51,013 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-22 09:27:51,013 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-22 09:27:51,013 Number of Src words (types): 36628
2020-06-22 09:27:51,014 Number of Trg words (types): 36628
2020-06-22 09:27:51,014 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-06-22 09:27:51,070 EPOCH 1
2020-06-22 09:43:10,171 Epoch   1: total training loss 433.56
2020-06-22 09:43:10,172 EPOCH 2
2020-06-22 09:50:56,306 Epoch   2 Step:  1360100 Batch Loss:     5.477756 Tokens per Sec:      186, Lr: 0.000049
2020-06-22 09:58:18,353 Epoch   2: total training loss 302.71
2020-06-22 09:58:18,354 EPOCH 3
2020-06-22 10:13:35,199 Epoch   3: total training loss 173.19
2020-06-22 10:13:35,201 EPOCH 4
2020-06-22 10:14:45,274 Epoch   4 Step:  1360200 Batch Loss:     1.766544 Tokens per Sec:      170, Lr: 0.000099
2020-06-22 10:28:53,718 Epoch   4: total training loss 124.14
2020-06-22 10:28:53,719 EPOCH 5
2020-06-22 10:38:08,317 Epoch   5 Step:  1360300 Batch Loss:     1.771569 Tokens per Sec:      183, Lr: 0.000148
2020-06-22 10:44:16,421 Epoch   5: total training loss 103.45
2020-06-22 10:44:16,422 EPOCH 6
2020-06-22 10:59:27,885 Epoch   6: total training loss 85.22
2020-06-22 10:59:27,886 EPOCH 7
2020-06-22 11:01:36,943 Epoch   7 Step:  1360400 Batch Loss:     1.389904 Tokens per Sec:      200, Lr: 0.000198
2020-06-22 11:14:42,863 Epoch   7: total training loss 77.95
2020-06-22 11:14:42,864 EPOCH 8
2020-06-22 11:24:51,895 Epoch   8 Step:  1360500 Batch Loss:     1.183648 Tokens per Sec:      180, Lr: 0.000247
2020-06-22 11:29:52,921 Epoch   8: total training loss 66.05
2020-06-22 11:29:52,922 EPOCH 9
2020-06-22 11:45:19,207 Epoch   9: total training loss 59.70
2020-06-22 11:45:19,209 EPOCH 10
2020-06-22 11:48:18,924 Epoch  10 Step:  1360600 Batch Loss:     0.651498 Tokens per Sec:      169, Lr: 0.000296
2020-06-22 12:00:35,379 Epoch  10: total training loss 53.09
2020-06-22 12:00:35,380 EPOCH 11
2020-06-22 12:11:42,360 Epoch  11 Step:  1360700 Batch Loss:     0.848398 Tokens per Sec:      178, Lr: 0.000346
2020-06-22 12:15:49,953 Epoch  11: total training loss 47.81
2020-06-22 12:15:49,955 EPOCH 12
2020-06-22 12:30:50,408 Epoch  12: total training loss 43.71
2020-06-22 12:30:50,409 EPOCH 13
2020-06-22 12:34:36,459 Epoch  13 Step:  1360800 Batch Loss:     0.630014 Tokens per Sec:      176, Lr: 0.000395
2020-06-22 12:46:05,688 Epoch  13: total training loss 41.75
2020-06-22 12:46:05,690 EPOCH 14
2020-06-22 12:58:04,448 Epoch  14 Step:  1360900 Batch Loss:     0.610124 Tokens per Sec:      172, Lr: 0.000445
2020-06-22 13:01:33,222 Epoch  14: total training loss 37.92
2020-06-22 13:01:33,223 EPOCH 15
2020-06-22 13:16:33,357 Epoch  15: total training loss 34.07
2020-06-22 13:16:33,358 EPOCH 16
2020-06-22 13:20:47,352 Epoch  16 Step:  1361000 Batch Loss:     0.592224 Tokens per Sec:      185, Lr: 0.000494
2020-06-22 13:37:38,611 Hooray! New best validation result [ppl]!
2020-06-22 13:37:38,612 Saving new checkpoint.
2020-06-22 13:37:52,448 Example #0
2020-06-22 13:37:52,449 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-22 13:37:52,449 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-22 13:37:52,449 	Source:     Hello.
2020-06-22 13:37:52,449 	Reference:  Hallo,
2020-06-22 13:37:52,449 	Hypothesis: Hallo.
2020-06-22 13:37:52,449 Example #1
2020-06-22 13:37:52,449 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-22 13:37:52,449 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-22 13:37:52,449 	Source:     Hi, how can I help you?
2020-06-22 13:37:52,449 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-22 13:37:52,449 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-22 13:37:52,450 Example #2
2020-06-22 13:37:52,450 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-22 13:37:52,450 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-22 13:37:52,450 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-22 13:37:52,450 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-22 13:37:52,450 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-22 13:37:52,450 Example #3
2020-06-22 13:37:52,450 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-22 13:37:52,450 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-22 13:37:52,450 	Source:     Ok, what type of restaurant are you looking for?
2020-06-22 13:37:52,450 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-22 13:37:52,450 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-22 13:37:52,450 Validation result (greedy) at epoch  16, step  1361000: bleu:  43.85, loss: 20743.6699, ppl:   2.2483, duration: 1025.0965s
2020-06-22 13:48:40,503 Epoch  16: total training loss 31.59
2020-06-22 13:48:40,504 EPOCH 17
2020-06-22 14:01:33,088 Epoch  17 Step:  1361100 Batch Loss:     0.467196 Tokens per Sec:      177, Lr: 0.000544
2020-06-22 14:03:55,412 Epoch  17: total training loss 29.15
2020-06-22 14:03:55,414 EPOCH 18
2020-06-22 14:19:11,378 Epoch  18: total training loss 26.32
2020-06-22 14:19:11,380 EPOCH 19
2020-06-22 14:25:21,202 Epoch  19 Step:  1361200 Batch Loss:     0.444592 Tokens per Sec:      174, Lr: 0.000593
2020-06-22 14:34:27,739 Epoch  19: total training loss 24.90
2020-06-22 14:34:27,740 EPOCH 20
2020-06-22 14:48:49,606 Epoch  20 Step:  1361300 Batch Loss:     0.437365 Tokens per Sec:      174, Lr: 0.000642
2020-06-22 14:49:55,113 Epoch  20: total training loss 23.03
2020-06-22 14:49:55,115 EPOCH 21
2020-06-22 15:05:27,964 Epoch  21: total training loss 24.15
2020-06-22 15:05:27,965 EPOCH 22
2020-06-22 15:12:20,651 Epoch  22 Step:  1361400 Batch Loss:     0.348678 Tokens per Sec:      179, Lr: 0.000692
2020-06-22 15:20:36,139 Epoch  22: total training loss 23.20
2020-06-22 15:20:36,140 EPOCH 23
2020-06-22 15:35:41,553 Epoch  23 Step:  1361500 Batch Loss:     0.326521 Tokens per Sec:      174, Lr: 0.000741
2020-06-22 15:36:05,556 Epoch  23: total training loss 20.25
2020-06-22 15:36:05,558 EPOCH 24
2020-06-22 15:51:40,479 Epoch  24: total training loss 19.02
2020-06-22 15:51:40,480 EPOCH 25
2020-06-22 15:59:43,103 Epoch  25 Step:  1361600 Batch Loss:     0.229509 Tokens per Sec:      166, Lr: 0.000791
2020-06-22 16:07:22,922 Epoch  25: total training loss 17.27
2020-06-22 16:07:22,924 EPOCH 26
2020-06-22 16:22:36,669 Epoch  26: total training loss 16.41
2020-06-22 16:22:36,670 EPOCH 27
2020-06-22 16:23:21,664 Epoch  27 Step:  1361700 Batch Loss:     0.178756 Tokens per Sec:       94, Lr: 0.000840
2020-06-22 16:38:09,040 Epoch  27: total training loss 15.09
2020-06-22 16:38:09,041 EPOCH 28
2020-06-22 16:46:21,984 Epoch  28 Step:  1361800 Batch Loss:     0.220015 Tokens per Sec:      180, Lr: 0.000889
2020-06-22 16:53:26,418 Epoch  28: total training loss 14.93
2020-06-22 16:53:26,419 EPOCH 29
2020-06-22 17:09:00,229 Epoch  29: total training loss 13.96
2020-06-22 17:09:00,230 EPOCH 30
2020-06-22 17:10:07,902 Epoch  30 Step:  1361900 Batch Loss:     0.195190 Tokens per Sec:      191, Lr: 0.000939
2020-06-22 17:24:30,309 Epoch  30: total training loss 13.11
2020-06-22 17:24:30,310 EPOCH 31
2020-06-22 17:33:28,009 Epoch  31 Step:  1362000 Batch Loss:     0.217526 Tokens per Sec:      180, Lr: 0.000988
2020-06-22 17:52:11,222 Example #0
2020-06-22 17:52:11,223 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-22 17:52:11,223 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-22 17:52:11,223 	Source:     Hello.
2020-06-22 17:52:11,223 	Reference:  Hallo,
2020-06-22 17:52:11,224 	Hypothesis: Hallo.
2020-06-22 17:52:11,224 Example #1
2020-06-22 17:52:11,224 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-22 17:52:11,224 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-22 17:52:11,224 	Source:     Hi, how can I help you?
2020-06-22 17:52:11,224 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-22 17:52:11,224 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-22 17:52:11,224 Example #2
2020-06-22 17:52:11,225 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-22 17:52:11,225 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-22 17:52:11,225 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-22 17:52:11,225 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-22 17:52:11,225 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-22 17:52:11,225 Example #3
2020-06-22 17:52:11,225 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-22 17:52:11,225 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-22 17:52:11,225 	Source:     Ok, what type of restaurant are you looking for?
2020-06-22 17:52:11,226 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-22 17:52:11,226 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-22 17:52:11,226 Validation result (greedy) at epoch  31, step  1362000: bleu:  43.97, loss: 22093.4961, ppl:   2.3700, duration: 1123.2146s
2020-06-22 17:58:25,017 Epoch  31: total training loss 13.07
2020-06-22 17:58:25,019 EPOCH 32
2020-06-22 18:13:41,319 Epoch  32: total training loss 11.66
2020-06-22 18:13:41,321 EPOCH 33
2020-06-22 18:16:07,391 Epoch  33 Step:  1362100 Batch Loss:     0.197071 Tokens per Sec:      151, Lr: 0.000964
2020-06-22 18:29:01,250 Epoch  33: total training loss 10.93
2020-06-22 18:29:01,252 EPOCH 34
2020-06-22 18:38:32,632 Epoch  34 Step:  1362200 Batch Loss:     0.161038 Tokens per Sec:      184, Lr: 0.000942
2020-06-22 18:44:23,229 Epoch  34: total training loss 10.62
2020-06-22 18:44:23,231 EPOCH 35
2020-06-22 18:59:43,536 Epoch  35: total training loss 9.71
2020-06-22 18:59:43,538 EPOCH 36
2020-06-22 19:02:40,536 Epoch  36 Step:  1362300 Batch Loss:     0.122820 Tokens per Sec:      162, Lr: 0.000922
2020-06-22 19:15:11,264 Epoch  36: total training loss 8.92
2020-06-22 19:15:11,265 EPOCH 37
2020-06-22 19:25:52,876 Epoch  37 Step:  1362400 Batch Loss:     0.138114 Tokens per Sec:      177, Lr: 0.000902
2020-06-22 19:30:40,471 Epoch  37: total training loss 8.87
2020-06-22 19:30:40,472 EPOCH 38
2020-06-22 19:45:43,113 Epoch  38: total training loss 7.95
2020-06-22 19:45:43,114 EPOCH 39
2020-06-22 19:49:30,145 Epoch  39 Step:  1362500 Batch Loss:     0.133493 Tokens per Sec:      170, Lr: 0.000884
2020-06-22 20:00:54,600 Epoch  39: total training loss 7.73
2020-06-22 20:00:54,601 EPOCH 40
2020-06-22 20:12:30,409 Epoch  40 Step:  1362600 Batch Loss:     0.137072 Tokens per Sec:      180, Lr: 0.000867
2020-06-22 20:16:16,701 Epoch  40: total training loss 7.78
2020-06-22 20:16:16,702 EPOCH 41
2020-06-22 20:31:30,105 Epoch  41: total training loss 6.92
2020-06-22 20:31:30,106 EPOCH 42
2020-06-22 20:36:02,778 Epoch  42 Step:  1362700 Batch Loss:     0.119607 Tokens per Sec:      175, Lr: 0.000851
2020-06-22 20:46:48,964 Epoch  42: total training loss 6.13
2020-06-22 20:46:48,965 EPOCH 43
2020-06-22 20:59:13,181 Epoch  43 Step:  1362800 Batch Loss:     0.095967 Tokens per Sec:      182, Lr: 0.000835
2020-06-22 21:02:05,889 Epoch  43: total training loss 5.84
2020-06-22 21:02:05,891 EPOCH 44
2020-06-22 21:17:40,662 Epoch  44: total training loss 5.66
2020-06-22 21:17:40,663 EPOCH 45
2020-06-22 21:23:53,835 Epoch  45 Step:  1362900 Batch Loss:     0.076621 Tokens per Sec:      162, Lr: 0.000821
2020-06-22 21:33:19,962 Epoch  45: total training loss 5.79
2020-06-22 21:33:19,964 EPOCH 46
2020-06-22 21:47:04,872 Epoch  46 Step:  1363000 Batch Loss:     0.082347 Tokens per Sec:      177, Lr: 0.000807
2020-06-22 22:03:41,272 Example #0
2020-06-22 22:03:41,273 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-22 22:03:41,273 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-22 22:03:41,273 	Source:     Hello.
2020-06-22 22:03:41,273 	Reference:  Hallo,
2020-06-22 22:03:41,273 	Hypothesis: Hallo.
2020-06-22 22:03:41,274 Example #1
2020-06-22 22:03:41,274 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-22 22:03:41,274 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-22 22:03:41,274 	Source:     Hi, how can I help you?
2020-06-22 22:03:41,274 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-22 22:03:41,274 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-22 22:03:41,274 Example #2
2020-06-22 22:03:41,274 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-22 22:03:41,275 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-22 22:03:41,275 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-22 22:03:41,275 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-22 22:03:41,275 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-22 22:03:41,275 Example #3
2020-06-22 22:03:41,275 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-22 22:03:41,275 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-22 22:03:41,275 	Source:     Ok, what type of restaurant are you looking for?
2020-06-22 22:03:41,275 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-22 22:03:41,276 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-22 22:03:41,276 Validation result (greedy) at epoch  46, step  1363000: bleu:  44.26, loss: 23297.0859, ppl:   2.4841, duration: 996.4024s
2020-06-22 22:05:26,391 Epoch  46: total training loss 5.59
2020-06-22 22:05:26,392 EPOCH 47
2020-06-22 22:21:01,085 Epoch  47: total training loss 5.12
2020-06-22 22:21:01,087 EPOCH 48
2020-06-22 22:28:20,891 Epoch  48 Step:  1363100 Batch Loss:     0.043726 Tokens per Sec:      160, Lr: 0.000794
2020-06-22 22:36:45,773 Epoch  48: total training loss 5.02
2020-06-22 22:36:45,775 EPOCH 49
2020-06-22 22:51:29,939 Epoch  49 Step:  1363200 Batch Loss:     0.073068 Tokens per Sec:      175, Lr: 0.000781
2020-06-22 22:52:05,788 Epoch  49: total training loss 4.88
2020-06-22 22:52:05,789 EPOCH 50
2020-06-22 23:07:29,255 Epoch  50: total training loss 4.53
2020-06-22 23:07:29,257 EPOCH 51
2020-06-22 23:15:15,063 Epoch  51 Step:  1363300 Batch Loss:     0.055466 Tokens per Sec:      167, Lr: 0.000769
2020-06-22 23:22:50,110 Epoch  51: total training loss 4.39
2020-06-22 23:22:50,111 EPOCH 52
2020-06-22 23:38:05,017 Epoch  52: total training loss 4.32
2020-06-22 23:38:05,018 EPOCH 53
2020-06-22 23:38:15,852 Epoch  53 Step:  1363400 Batch Loss:     0.068128 Tokens per Sec:      273, Lr: 0.000758
2020-06-22 23:53:24,084 Epoch  53: total training loss 4.37
2020-06-22 23:53:24,085 EPOCH 54
2020-06-23 00:01:27,786 Epoch  54 Step:  1363500 Batch Loss:     0.131022 Tokens per Sec:      180, Lr: 0.000747
2020-06-23 00:08:53,206 Epoch  54: total training loss 7.72
2020-06-23 00:08:53,208 EPOCH 55
2020-06-23 00:24:25,383 Epoch  55: total training loss 5.04
2020-06-23 00:24:25,384 EPOCH 56
2020-06-23 00:25:38,412 Epoch  56 Step:  1363600 Batch Loss:     0.088043 Tokens per Sec:      174, Lr: 0.000737
2020-06-23 00:39:36,215 Epoch  56: total training loss 4.49
2020-06-23 00:39:36,217 EPOCH 57
2020-06-23 00:49:35,705 Epoch  57 Step:  1363700 Batch Loss:     0.058961 Tokens per Sec:      173, Lr: 0.000727
2020-06-23 00:54:58,311 Epoch  57: total training loss 4.04
2020-06-23 00:54:58,313 EPOCH 58
2020-06-23 01:10:20,421 Epoch  58: total training loss 3.97
2020-06-23 01:10:20,423 EPOCH 59
2020-06-23 01:12:36,382 Epoch  59 Step:  1363800 Batch Loss:     0.052369 Tokens per Sec:      177, Lr: 0.000717
2020-06-23 01:25:41,556 Epoch  59: total training loss 3.92
2020-06-23 01:25:41,557 EPOCH 60
2020-06-23 01:36:25,334 Epoch  60 Step:  1363900 Batch Loss:     0.064333 Tokens per Sec:      173, Lr: 0.000708
2020-06-23 01:41:01,274 Epoch  60: total training loss 3.79
2020-06-23 01:41:01,276 EPOCH 61
2020-06-23 01:56:37,742 Epoch  61: total training loss 3.53
2020-06-23 01:56:37,743 EPOCH 62
2020-06-23 02:00:05,279 Epoch  62 Step:  1364000 Batch Loss:     0.053819 Tokens per Sec:      178, Lr: 0.000699
2020-06-23 02:17:25,602 Example #0
2020-06-23 02:17:25,602 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-23 02:17:25,603 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-23 02:17:25,603 	Source:     Hello.
2020-06-23 02:17:25,603 	Reference:  Hallo,
2020-06-23 02:17:25,603 	Hypothesis: Hallo.
2020-06-23 02:17:25,603 Example #1
2020-06-23 02:17:25,603 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-23 02:17:25,603 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-23 02:17:25,603 	Source:     Hi, how can I help you?
2020-06-23 02:17:25,603 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-23 02:17:25,603 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-23 02:17:25,603 Example #2
2020-06-23 02:17:25,603 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-23 02:17:25,603 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-23 02:17:25,603 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-23 02:17:25,603 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-23 02:17:25,603 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-23 02:17:25,604 Example #3
2020-06-23 02:17:25,604 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-23 02:17:25,604 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-23 02:17:25,604 	Source:     Ok, what type of restaurant are you looking for?
2020-06-23 02:17:25,604 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-23 02:17:25,604 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-23 02:17:25,604 Validation result (greedy) at epoch  62, step  1364000: bleu:  45.30, loss: 24155.6426, ppl:   2.5688, duration: 1040.3239s
2020-06-23 02:29:36,617 Epoch  62: total training loss 3.40
2020-06-23 02:29:36,619 EPOCH 63
2020-06-23 02:41:19,531 Epoch  63 Step:  1364100 Batch Loss:     0.049251 Tokens per Sec:      177, Lr: 0.000690
2020-06-23 02:45:08,454 Epoch  63: total training loss 3.31
2020-06-23 02:45:08,455 EPOCH 64
2020-06-23 03:00:38,622 Epoch  64: total training loss 3.18
2020-06-23 03:00:38,624 EPOCH 65
2020-06-23 03:05:07,646 Epoch  65 Step:  1364200 Batch Loss:     0.041925 Tokens per Sec:      185, Lr: 0.000682
2020-06-23 03:16:07,300 Epoch  65: total training loss 3.22
2020-06-23 03:16:07,302 EPOCH 66
2020-06-23 03:28:46,709 Epoch  66 Step:  1364300 Batch Loss:     0.059070 Tokens per Sec:      180, Lr: 0.000674
2020-06-23 03:31:35,070 Epoch  66: total training loss 3.13
2020-06-23 03:31:35,072 EPOCH 67
2020-06-23 03:47:09,706 Epoch  67: total training loss 3.13
2020-06-23 03:47:09,707 EPOCH 68
2020-06-23 03:52:45,240 Epoch  68 Step:  1364400 Batch Loss:     0.049209 Tokens per Sec:      171, Lr: 0.000666
2020-06-23 04:02:37,391 Epoch  68: total training loss 3.08
2020-06-23 04:02:37,392 EPOCH 69
2020-06-23 04:16:39,935 Epoch  69 Step:  1364500 Batch Loss:     0.035344 Tokens per Sec:      171, Lr: 0.000659
2020-06-23 04:18:00,002 Epoch  69: total training loss 3.14
2020-06-23 04:18:00,003 EPOCH 70
2020-06-23 04:33:33,025 Epoch  70: total training loss 3.13
2020-06-23 04:33:33,026 EPOCH 71
2020-06-23 04:39:31,601 Epoch  71 Step:  1364600 Batch Loss:     0.049280 Tokens per Sec:      189, Lr: 0.000652
2020-06-23 04:49:03,029 Epoch  71: total training loss 3.10
2020-06-23 04:49:03,031 EPOCH 72
2020-06-23 05:03:24,064 Epoch  72 Step:  1364700 Batch Loss:     0.048812 Tokens per Sec:      180, Lr: 0.000645
2020-06-23 05:04:22,646 Epoch  72: total training loss 2.94
2020-06-23 05:04:22,647 EPOCH 73
2020-06-23 05:19:47,735 Epoch  73: total training loss 2.90
2020-06-23 05:19:47,736 EPOCH 74
2020-06-23 05:26:50,877 Epoch  74 Step:  1364800 Batch Loss:     0.052119 Tokens per Sec:      183, Lr: 0.000638
2020-06-23 05:34:43,446 Epoch  74: total training loss 2.92
2020-06-23 05:34:43,447 EPOCH 75
2020-06-23 05:49:53,220 Epoch  75 Step:  1364900 Batch Loss:     0.052481 Tokens per Sec:      179, Lr: 0.000631
2020-06-23 05:49:53,223 Epoch  75: total training loss 2.83
2020-06-23 05:49:53,223 EPOCH 76
2020-06-23 06:05:21,799 Epoch  76: total training loss 2.78
2020-06-23 06:05:21,800 EPOCH 77
2020-06-23 06:13:29,661 Epoch  77 Step:  1365000 Batch Loss:     0.044184 Tokens per Sec:      182, Lr: 0.000625
2020-06-23 06:30:28,714 Example #0
2020-06-23 06:30:28,714 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-23 06:30:28,715 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-23 06:30:28,715 	Source:     Hello.
2020-06-23 06:30:28,715 	Reference:  Hallo,
2020-06-23 06:30:28,715 	Hypothesis: Hallo.
2020-06-23 06:30:28,715 Example #1
2020-06-23 06:30:28,715 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-23 06:30:28,715 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-23 06:30:28,715 	Source:     Hi, how can I help you?
2020-06-23 06:30:28,715 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-23 06:30:28,715 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-23 06:30:28,715 Example #2
2020-06-23 06:30:28,715 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-23 06:30:28,715 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-23 06:30:28,715 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-23 06:30:28,715 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-23 06:30:28,716 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-23 06:30:28,716 Example #3
2020-06-23 06:30:28,716 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-23 06:30:28,716 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-23 06:30:28,716 	Source:     Ok, what type of restaurant are you looking for?
2020-06-23 06:30:28,716 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-23 06:30:28,716 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-23 06:30:28,716 Validation result (greedy) at epoch  77, step  1365000: bleu:  44.37, loss: 25260.7031, ppl:   2.6821, duration: 1019.0533s
2020-06-23 06:37:35,813 Epoch  77: total training loss 2.85
2020-06-23 06:37:35,814 EPOCH 78
2020-06-23 06:52:55,313 Epoch  78: total training loss 2.83
2020-06-23 06:52:55,314 EPOCH 79
2020-06-23 06:53:42,750 Epoch  79 Step:  1365100 Batch Loss:     0.052981 Tokens per Sec:      236, Lr: 0.000619
2020-06-23 07:08:21,682 Epoch  79: total training loss 2.84
2020-06-23 07:08:21,683 EPOCH 80
2020-06-23 07:17:26,546 Epoch  80 Step:  1365200 Batch Loss:     0.044376 Tokens per Sec:      177, Lr: 0.000613
2020-06-23 07:23:38,789 Epoch  80: total training loss 2.63
2020-06-23 07:23:38,790 EPOCH 81
2020-06-23 07:38:54,363 Epoch  81: total training loss 2.63
2020-06-23 07:38:54,364 EPOCH 82
2020-06-23 07:41:12,581 Epoch  82 Step:  1365300 Batch Loss:     0.039611 Tokens per Sec:      143, Lr: 0.000607
2020-06-23 07:53:56,924 Epoch  82: total training loss 2.53
2020-06-23 07:53:56,925 EPOCH 83
2020-06-23 08:04:43,443 Epoch  83 Step:  1365400 Batch Loss:     0.044245 Tokens per Sec:      167, Lr: 0.000601
2020-06-23 08:09:14,296 Epoch  83: total training loss 2.59
2020-06-23 08:09:14,298 EPOCH 84
2020-06-23 08:24:20,703 Epoch  84: total training loss 2.57
2020-06-23 08:24:20,704 EPOCH 85
2020-06-23 08:27:10,870 Epoch  85 Step:  1365500 Batch Loss:     0.049865 Tokens per Sec:      207, Lr: 0.000596
2020-06-23 08:39:51,012 Epoch  85: total training loss 2.75
2020-06-23 08:39:51,013 EPOCH 86
2020-06-23 08:51:21,847 Epoch  86 Step:  1365600 Batch Loss:     0.037605 Tokens per Sec:      173, Lr: 0.000591
2020-06-23 08:55:06,885 Epoch  86: total training loss 2.76
2020-06-23 08:55:06,886 EPOCH 87
2020-06-23 09:10:32,308 Epoch  87: total training loss 2.68
2020-06-23 09:10:32,311 EPOCH 88
2020-06-23 09:14:59,384 Epoch  88 Step:  1365700 Batch Loss:     0.032614 Tokens per Sec:      160, Lr: 0.000585
2020-06-23 09:25:59,984 Epoch  88: total training loss 2.62
2020-06-23 09:25:59,985 EPOCH 89
2020-06-23 09:38:10,904 Epoch  89 Step:  1365800 Batch Loss:     0.040599 Tokens per Sec:      178, Lr: 0.000580
2020-06-23 09:41:31,545 Epoch  89: total training loss 2.64
2020-06-23 09:41:31,546 EPOCH 90
2020-06-23 09:56:50,187 Epoch  90: total training loss 2.50
2020-06-23 09:56:50,189 EPOCH 91
2020-06-23 10:02:29,684 Epoch  91 Step:  1365900 Batch Loss:     0.037178 Tokens per Sec:      166, Lr: 0.000575
2020-06-23 10:12:20,999 Epoch  91: total training loss 2.58
2020-06-23 10:12:21,000 EPOCH 92
2020-06-23 10:25:47,539 Epoch  92 Step:  1366000 Batch Loss:     0.043091 Tokens per Sec:      182, Lr: 0.000571
2020-06-23 10:42:56,902 Example #0
2020-06-23 10:42:56,903 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-23 10:42:56,903 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-23 10:42:56,903 	Source:     Hello.
2020-06-23 10:42:56,903 	Reference:  Hallo,
2020-06-23 10:42:56,903 	Hypothesis: Hallo.
2020-06-23 10:42:56,903 Example #1
2020-06-23 10:42:56,903 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-23 10:42:56,903 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-23 10:42:56,903 	Source:     Hi, how can I help you?
2020-06-23 10:42:56,903 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-23 10:42:56,904 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-23 10:42:56,904 Example #2
2020-06-23 10:42:56,904 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-23 10:42:56,904 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-23 10:42:56,904 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-23 10:42:56,904 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-23 10:42:56,904 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall in San Francisco, Kalifornien.
2020-06-23 10:42:56,904 Example #3
2020-06-23 10:42:56,904 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-23 10:42:56,904 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-23 10:42:56,904 	Source:     Ok, what type of restaurant are you looking for?
2020-06-23 10:42:56,904 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-23 10:42:56,904 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-23 10:42:56,904 Validation result (greedy) at epoch  92, step  1366000: bleu:  44.85, loss: 25896.5996, ppl:   2.7495, duration: 1029.3643s
2020-06-23 10:44:37,933 Epoch  92: total training loss 2.42
2020-06-23 10:44:37,935 EPOCH 93
2020-06-23 11:00:12,567 Epoch  93: total training loss 2.53
2020-06-23 11:00:12,568 EPOCH 94
2020-06-23 11:06:21,049 Epoch  94 Step:  1366100 Batch Loss:     0.038569 Tokens per Sec:      187, Lr: 0.000566
2020-06-23 11:15:26,394 Epoch  94: total training loss 2.50
2020-06-23 11:15:26,395 EPOCH 95
2020-06-23 11:29:46,899 Epoch  95 Step:  1366200 Batch Loss:     0.026444 Tokens per Sec:      181, Lr: 0.000561
2020-06-23 11:30:44,707 Epoch  95: total training loss 2.44
2020-06-23 11:30:44,708 EPOCH 96
2020-06-23 11:46:09,690 Epoch  96: total training loss 2.53
2020-06-23 11:46:09,692 EPOCH 97
2020-06-23 11:52:59,052 Epoch  97 Step:  1366300 Batch Loss:     0.037402 Tokens per Sec:      185, Lr: 0.000557
2020-06-23 12:01:24,548 Epoch  97: total training loss 2.43
2020-06-23 12:01:24,549 EPOCH 98
2020-06-23 12:16:29,679 Epoch  98 Step:  1366400 Batch Loss:     0.042945 Tokens per Sec:      180, Lr: 0.000552
2020-06-23 12:16:29,682 Epoch  98: total training loss 2.38
2020-06-23 12:16:29,682 EPOCH 99
2020-06-23 12:32:02,200 Epoch  99: total training loss 2.38
2020-06-23 12:32:02,201 EPOCH 100
2020-06-23 12:40:33,630 Epoch 100 Step:  1366500 Batch Loss:     0.033830 Tokens per Sec:      171, Lr: 0.000548
2020-06-23 12:47:09,293 Epoch 100: total training loss 2.28
2020-06-23 12:47:09,294 Training ended after 100 epochs.
2020-06-23 12:47:09,294 Best validation result (greedy) at step  1361000:   2.25 ppl.
2020-06-23 12:59:18,340  dev bleu:  45.08 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-23 12:59:18,347 Translations saved to: models/transformer_multi_enc_ende-tune-noam/01361000.hyps.dev
2020-06-23 13:07:34,010 test bleu:  42.42 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-23 13:07:34,018 Translations saved to: models/transformer_multi_enc_ende-tune-noam/01361000.hyps.test
