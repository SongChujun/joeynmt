2020-07-02 00:34:35,599 Hello! This is Joey-NMT.
2020-07-02 00:34:41,435 Total params: 82862081
2020-07-02 00:34:41,438 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-07-02 00:34:43,728 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-02 00:34:44,025 Reset optimizer.
2020-07-02 00:34:44,025 Reset scheduler.
2020-07-02 00:34:44,025 Reset tracking of the best checkpoint.
2020-07-02 00:34:44,031 cfg.name                           : transformer
2020-07-02 00:34:44,031 cfg.data.src                       : en
2020-07-02 00:34:44,031 cfg.data.trg                       : de
2020-07-02 00:34:44,031 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-02 00:34:44,031 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-02 00:34:44,032 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-02 00:34:44,032 cfg.data.level                     : bpe
2020-07-02 00:34:44,032 cfg.data.lowercase                 : False
2020-07-02 00:34:44,032 cfg.data.max_sent_length           : 100
2020-07-02 00:34:44,032 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-02 00:34:44,032 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-02 00:34:44,032 cfg.testing.beam_size              : 5
2020-07-02 00:34:44,032 cfg.testing.alpha                  : 1.0
2020-07-02 00:34:44,032 cfg.training.random_seed           : 42
2020-07-02 00:34:44,032 cfg.training.optimizer             : adam
2020-07-02 00:34:44,032 cfg.training.normalization         : tokens
2020-07-02 00:34:44,032 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-02 00:34:44,032 cfg.training.scheduling            : plateau
2020-07-02 00:34:44,032 cfg.training.patience              : 3
2020-07-02 00:34:44,032 cfg.training.decrease_factor       : 0.5
2020-07-02 00:34:44,032 cfg.training.loss                  : crossentropy
2020-07-02 00:34:44,032 cfg.training.learning_rate         : 2e-05
2020-07-02 00:34:44,032 cfg.training.learning_rate_min     : 1e-08
2020-07-02 00:34:44,032 cfg.training.weight_decay          : 0.0
2020-07-02 00:34:44,032 cfg.training.label_smoothing       : 0.1
2020-07-02 00:34:44,032 cfg.training.batch_size            : 2048
2020-07-02 00:34:44,032 cfg.training.batch_type            : token
2020-07-02 00:34:44,032 cfg.training.batch_multiplier      : 1
2020-07-02 00:34:44,032 cfg.training.early_stopping_metric : ppl
2020-07-02 00:34:44,032 cfg.training.epochs                : 100
2020-07-02 00:34:44,032 cfg.training.validation_freq       : 1000
2020-07-02 00:34:44,032 cfg.training.logging_freq          : 100
2020-07-02 00:34:44,032 cfg.training.eval_metric           : bleu
2020-07-02 00:34:44,032 cfg.training.model_dir             : models/transformer_multi_enc_lr0.00002p3d0.5_ende-tune
2020-07-02 00:34:44,032 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-02 00:34:44,032 cfg.training.reset_best_ckpt       : True
2020-07-02 00:34:44,032 cfg.training.reset_scheduler       : True
2020-07-02 00:34:44,032 cfg.training.reset_optimizer       : True
2020-07-02 00:34:44,032 cfg.training.overwrite             : False
2020-07-02 00:34:44,032 cfg.training.shuffle               : True
2020-07-02 00:34:44,032 cfg.training.use_cuda              : True
2020-07-02 00:34:44,032 cfg.training.max_output_length     : 100
2020-07-02 00:34:44,032 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-02 00:34:44,032 cfg.training.keep_last_ckpts       : 3
2020-07-02 00:34:44,032 cfg.model.initializer              : xavier
2020-07-02 00:34:44,033 cfg.model.bias_initializer         : zeros
2020-07-02 00:34:44,033 cfg.model.init_gain                : 1.0
2020-07-02 00:34:44,033 cfg.model.embed_initializer        : xavier
2020-07-02 00:34:44,033 cfg.model.embed_init_gain          : 1.0
2020-07-02 00:34:44,033 cfg.model.tied_embeddings          : True
2020-07-02 00:34:44,033 cfg.model.tied_softmax             : True
2020-07-02 00:34:44,033 cfg.model.encoder.type             : transformer
2020-07-02 00:34:44,033 cfg.model.encoder.num_layers       : 6
2020-07-02 00:34:44,033 cfg.model.encoder.num_heads        : 8
2020-07-02 00:34:44,033 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-02 00:34:44,033 cfg.model.encoder.embeddings.scale : True
2020-07-02 00:34:44,033 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-02 00:34:44,033 cfg.model.encoder.hidden_size      : 512
2020-07-02 00:34:44,033 cfg.model.encoder.ff_size          : 2048
2020-07-02 00:34:44,033 cfg.model.encoder.dropout          : 0.1
2020-07-02 00:34:44,033 cfg.model.encoder.multi_encoder    : True
2020-07-02 00:34:44,033 cfg.model.decoder.type             : transformer
2020-07-02 00:34:44,033 cfg.model.decoder.num_layers       : 6
2020-07-02 00:34:44,033 cfg.model.decoder.num_heads        : 8
2020-07-02 00:34:44,033 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-02 00:34:44,033 cfg.model.decoder.embeddings.scale : True
2020-07-02 00:34:44,033 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-02 00:34:44,033 cfg.model.decoder.hidden_size      : 512
2020-07-02 00:34:44,033 cfg.model.decoder.ff_size          : 2048
2020-07-02 00:34:44,033 cfg.model.decoder.dropout          : 0.1
2020-07-02 00:34:44,033 Data set sizes: 
	train 9771,
	valid 1522,
	test 1160
2020-07-02 00:34:44,033 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-02 00:34:44,033 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-02 00:34:44,033 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-02 00:34:44,033 Number of Src words (types): 36628
2020-07-02 00:34:44,033 Number of Trg words (types): 36628
2020-07-02 00:34:44,033 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-02 00:34:44,062 EPOCH 1
2020-07-02 00:35:07,469 Epoch   1 Step:  1360100 Batch Loss:     3.603189 Tokens per Sec:     5249, Lr: 0.000020
2020-07-02 00:35:13,424 Epoch   1: total training loss 659.98
2020-07-02 00:35:13,424 EPOCH 2
2020-07-02 00:35:31,170 Epoch   2 Step:  1360200 Batch Loss:     4.299155 Tokens per Sec:     5046, Lr: 0.000020
2020-07-02 00:35:43,777 Epoch   2: total training loss 358.05
2020-07-02 00:35:43,777 EPOCH 3
2020-07-02 00:35:55,430 Epoch   3 Step:  1360300 Batch Loss:     1.522855 Tokens per Sec:     4976, Lr: 0.000020
2020-07-02 00:36:15,469 Epoch   3: total training loss 262.07
2020-07-02 00:36:15,469 EPOCH 4
2020-07-02 00:36:20,598 Epoch   4 Step:  1360400 Batch Loss:     2.205644 Tokens per Sec:     4949, Lr: 0.000020
2020-07-02 00:36:45,913 Epoch   4 Step:  1360500 Batch Loss:     1.320548 Tokens per Sec:     4714, Lr: 0.000020
2020-07-02 00:36:47,592 Epoch   4: total training loss 224.50
2020-07-02 00:36:47,592 EPOCH 5
2020-07-02 00:37:10,618 Epoch   5 Step:  1360600 Batch Loss:     1.194119 Tokens per Sec:     4846, Lr: 0.000020
2020-07-02 00:37:19,852 Epoch   5: total training loss 203.59
2020-07-02 00:37:19,852 EPOCH 6
2020-07-02 00:37:35,446 Epoch   6 Step:  1360700 Batch Loss:     1.310272 Tokens per Sec:     4888, Lr: 0.000020
2020-07-02 00:37:51,316 Epoch   6: total training loss 182.50
2020-07-02 00:37:51,317 EPOCH 7
2020-07-02 00:38:00,695 Epoch   7 Step:  1360800 Batch Loss:     1.201134 Tokens per Sec:     4935, Lr: 0.000020
2020-07-02 00:38:23,746 Epoch   7: total training loss 175.27
2020-07-02 00:38:23,747 EPOCH 8
2020-07-02 00:38:26,153 Epoch   8 Step:  1360900 Batch Loss:     1.360323 Tokens per Sec:     5061, Lr: 0.000020
2020-07-02 00:38:51,664 Epoch   8 Step:  1361000 Batch Loss:     1.105509 Tokens per Sec:     4706, Lr: 0.000020
2020-07-02 00:39:27,622 Hooray! New best validation result [ppl]!
2020-07-02 00:39:27,623 Saving new checkpoint.
2020-07-02 00:39:39,012 Example #0
2020-07-02 00:39:39,013 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 00:39:39,013 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 00:39:39,013 	Source:     Hello .
2020-07-02 00:39:39,013 	Reference:  Hallo ,
2020-07-02 00:39:39,013 	Hypothesis: Hallo .
2020-07-02 00:39:39,013 Example #1
2020-07-02 00:39:39,013 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 00:39:39,013 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 00:39:39,013 	Source:     Hi , how can I help you ?
2020-07-02 00:39:39,013 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:39:39,013 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:39:39,013 Example #2
2020-07-02 00:39:39,013 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 00:39:39,013 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'im', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 00:39:39,013 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 00:39:39,013 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 00:39:39,013 	Hypothesis: Hallo , ich suche ein Restaurant im Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 00:39:39,013 Example #3
2020-07-02 00:39:39,013 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 00:39:39,013 	Raw hypothesis: ['O@@', 'k', ',', 'welche', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 00:39:39,013 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 00:39:39,014 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 00:39:39,014 	Hypothesis: Ok , welche Art von Restaurant suchen Sie ?
2020-07-02 00:39:39,014 Validation result (greedy) at epoch   8, step  1361000: bleu:  46.48, loss: 26184.3398, ppl:   2.9685, duration: 47.3489s
2020-07-02 00:39:43,461 Epoch   8: total training loss 167.63
2020-07-02 00:39:43,461 EPOCH 9
2020-07-02 00:40:04,482 Epoch   9 Step:  1361100 Batch Loss:     1.885231 Tokens per Sec:     4697, Lr: 0.000020
2020-07-02 00:40:16,164 Epoch   9: total training loss 161.74
2020-07-02 00:40:16,164 EPOCH 10
2020-07-02 00:40:29,526 Epoch  10 Step:  1361200 Batch Loss:     1.759410 Tokens per Sec:     4764, Lr: 0.000020
2020-07-02 00:40:47,771 Epoch  10: total training loss 152.61
2020-07-02 00:40:47,771 EPOCH 11
2020-07-02 00:40:53,885 Epoch  11 Step:  1361300 Batch Loss:     0.438889 Tokens per Sec:     4738, Lr: 0.000020
2020-07-02 00:41:19,437 Epoch  11 Step:  1361400 Batch Loss:     0.928442 Tokens per Sec:     4773, Lr: 0.000020
2020-07-02 00:41:20,028 Epoch  11: total training loss 145.64
2020-07-02 00:41:20,028 EPOCH 12
2020-07-02 00:41:45,494 Epoch  12 Step:  1361500 Batch Loss:     1.432509 Tokens per Sec:     4628, Lr: 0.000020
2020-07-02 00:41:52,840 Epoch  12: total training loss 141.06
2020-07-02 00:41:52,840 EPOCH 13
2020-07-02 00:42:10,732 Epoch  13 Step:  1361600 Batch Loss:     0.920273 Tokens per Sec:     4791, Lr: 0.000020
2020-07-02 00:42:25,176 Epoch  13: total training loss 136.42
2020-07-02 00:42:25,176 EPOCH 14
2020-07-02 00:42:36,699 Epoch  14 Step:  1361700 Batch Loss:     0.866335 Tokens per Sec:     4747, Lr: 0.000020
2020-07-02 00:42:57,782 Epoch  14: total training loss 130.82
2020-07-02 00:42:57,783 EPOCH 15
2020-07-02 00:43:02,360 Epoch  15 Step:  1361800 Batch Loss:     1.248011 Tokens per Sec:     4608, Lr: 0.000020
2020-07-02 00:43:27,823 Epoch  15 Step:  1361900 Batch Loss:     0.670577 Tokens per Sec:     4721, Lr: 0.000020
2020-07-02 00:43:30,124 Epoch  15: total training loss 129.89
2020-07-02 00:43:30,124 EPOCH 16
2020-07-02 00:43:53,358 Epoch  16 Step:  1362000 Batch Loss:     0.747545 Tokens per Sec:     4695, Lr: 0.000020
2020-07-02 00:44:30,938 Hooray! New best validation result [ppl]!
2020-07-02 00:44:30,939 Saving new checkpoint.
2020-07-02 00:44:41,575 Example #0
2020-07-02 00:44:41,576 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 00:44:41,576 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 00:44:41,576 	Source:     Hello .
2020-07-02 00:44:41,576 	Reference:  Hallo ,
2020-07-02 00:44:41,576 	Hypothesis: Hallo .
2020-07-02 00:44:41,576 Example #1
2020-07-02 00:44:41,576 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 00:44:41,576 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 00:44:41,576 	Source:     Hi , how can I help you ?
2020-07-02 00:44:41,576 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:44:41,576 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:44:41,576 Example #2
2020-07-02 00:44:41,576 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 00:44:41,576 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'im', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 00:44:41,576 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 00:44:41,577 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 00:44:41,577 	Hypothesis: Hallo , ich suche ein Restaurant im Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 00:44:41,577 Example #3
2020-07-02 00:44:41,577 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 00:44:41,577 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 00:44:41,577 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 00:44:41,577 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 00:44:41,577 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 00:44:41,577 Validation result (greedy) at epoch  16, step  1362000: bleu:  50.15, loss: 22058.6602, ppl:   2.5008, duration: 48.2179s
2020-07-02 00:44:51,002 Epoch  16: total training loss 124.30
2020-07-02 00:44:51,002 EPOCH 17
2020-07-02 00:45:07,087 Epoch  17 Step:  1362100 Batch Loss:     0.722836 Tokens per Sec:     4755, Lr: 0.000020
2020-07-02 00:45:24,039 Epoch  17: total training loss 124.43
2020-07-02 00:45:24,040 EPOCH 18
2020-07-02 00:45:32,943 Epoch  18 Step:  1362200 Batch Loss:     0.801367 Tokens per Sec:     4764, Lr: 0.000020
2020-07-02 00:45:56,831 Epoch  18: total training loss 119.25
2020-07-02 00:45:56,832 EPOCH 19
2020-07-02 00:45:58,622 Epoch  19 Step:  1362300 Batch Loss:     1.048400 Tokens per Sec:     4789, Lr: 0.000020
2020-07-02 00:46:24,203 Epoch  19 Step:  1362400 Batch Loss:     1.168480 Tokens per Sec:     4651, Lr: 0.000020
2020-07-02 00:46:29,805 Epoch  19: total training loss 119.00
2020-07-02 00:46:29,805 EPOCH 20
2020-07-02 00:46:49,539 Epoch  20 Step:  1362500 Batch Loss:     0.898118 Tokens per Sec:     4738, Lr: 0.000020
2020-07-02 00:47:02,254 Epoch  20: total training loss 113.31
2020-07-02 00:47:02,255 EPOCH 21
2020-07-02 00:47:15,765 Epoch  21 Step:  1362600 Batch Loss:     0.844316 Tokens per Sec:     4612, Lr: 0.000020
2020-07-02 00:47:35,143 Epoch  21: total training loss 110.42
2020-07-02 00:47:35,143 EPOCH 22
2020-07-02 00:47:41,479 Epoch  22 Step:  1362700 Batch Loss:     0.382557 Tokens per Sec:     4844, Lr: 0.000020
2020-07-02 00:48:06,930 Epoch  22 Step:  1362800 Batch Loss:     0.662502 Tokens per Sec:     4730, Lr: 0.000020
2020-07-02 00:48:07,442 Epoch  22: total training loss 108.47
2020-07-02 00:48:07,442 EPOCH 23
2020-07-02 00:48:32,119 Epoch  23 Step:  1362900 Batch Loss:     1.062483 Tokens per Sec:     4823, Lr: 0.000020
2020-07-02 00:48:39,522 Epoch  23: total training loss 107.73
2020-07-02 00:48:39,522 EPOCH 24
2020-07-02 00:48:57,106 Epoch  24 Step:  1363000 Batch Loss:     0.952081 Tokens per Sec:     4730, Lr: 0.000020
2020-07-02 00:49:34,739 Hooray! New best validation result [ppl]!
2020-07-02 00:49:34,739 Saving new checkpoint.
2020-07-02 00:49:46,705 Example #0
2020-07-02 00:49:46,706 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 00:49:46,706 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 00:49:46,706 	Source:     Hello .
2020-07-02 00:49:46,706 	Reference:  Hallo ,
2020-07-02 00:49:46,706 	Hypothesis: Hallo .
2020-07-02 00:49:46,706 Example #1
2020-07-02 00:49:46,707 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 00:49:46,707 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 00:49:46,707 	Source:     Hi , how can I help you ?
2020-07-02 00:49:46,707 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:49:46,707 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:49:46,707 Example #2
2020-07-02 00:49:46,707 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 00:49:46,707 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'im', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 00:49:46,707 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 00:49:46,707 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 00:49:46,707 	Hypothesis: Hallo , ich suche ein Restaurant im Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 00:49:46,707 Example #3
2020-07-02 00:49:46,707 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 00:49:46,707 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 00:49:46,707 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 00:49:46,707 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 00:49:46,707 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 00:49:46,708 Validation result (greedy) at epoch  24, step  1363000: bleu:  51.63, loss: 20228.0059, ppl:   2.3177, duration: 49.6013s
2020-07-02 00:50:00,704 Epoch  24: total training loss 106.19
2020-07-02 00:50:00,704 EPOCH 25
2020-07-02 00:50:11,742 Epoch  25 Step:  1363100 Batch Loss:     0.844468 Tokens per Sec:     4687, Lr: 0.000020
2020-07-02 00:50:33,758 Epoch  25: total training loss 103.54
2020-07-02 00:50:33,759 EPOCH 26
2020-07-02 00:50:37,736 Epoch  26 Step:  1363200 Batch Loss:     0.908111 Tokens per Sec:     4611, Lr: 0.000020
2020-07-02 00:51:03,493 Epoch  26 Step:  1363300 Batch Loss:     1.100016 Tokens per Sec:     4725, Lr: 0.000020
2020-07-02 00:51:06,598 Epoch  26: total training loss 101.09
2020-07-02 00:51:06,599 EPOCH 27
2020-07-02 00:51:28,209 Epoch  27 Step:  1363400 Batch Loss:     1.005898 Tokens per Sec:     4947, Lr: 0.000020
2020-07-02 00:51:38,526 Epoch  27: total training loss 97.83
2020-07-02 00:51:38,527 EPOCH 28
2020-07-02 00:51:54,528 Epoch  28 Step:  1363500 Batch Loss:     0.882010 Tokens per Sec:     4669, Lr: 0.000020
2020-07-02 00:52:10,860 Epoch  28: total training loss 97.19
2020-07-02 00:52:10,861 EPOCH 29
2020-07-02 00:52:19,793 Epoch  29 Step:  1363600 Batch Loss:     0.783485 Tokens per Sec:     4713, Lr: 0.000020
2020-07-02 00:52:42,270 Epoch  29: total training loss 95.65
2020-07-02 00:52:42,271 EPOCH 30
2020-07-02 00:52:44,207 Epoch  30 Step:  1363700 Batch Loss:     0.811353 Tokens per Sec:     4164, Lr: 0.000020
2020-07-02 00:53:09,940 Epoch  30 Step:  1363800 Batch Loss:     0.686298 Tokens per Sec:     4780, Lr: 0.000020
2020-07-02 00:53:14,723 Epoch  30: total training loss 94.63
2020-07-02 00:53:14,724 EPOCH 31
2020-07-02 00:53:35,226 Epoch  31 Step:  1363900 Batch Loss:     0.818661 Tokens per Sec:     4852, Lr: 0.000020
2020-07-02 00:53:47,483 Epoch  31: total training loss 91.90
2020-07-02 00:53:47,484 EPOCH 32
2020-07-02 00:54:01,442 Epoch  32 Step:  1364000 Batch Loss:     0.831066 Tokens per Sec:     4684, Lr: 0.000020
2020-07-02 00:54:39,526 Hooray! New best validation result [ppl]!
2020-07-02 00:54:39,527 Saving new checkpoint.
2020-07-02 00:54:50,253 Example #0
2020-07-02 00:54:50,253 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 00:54:50,253 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 00:54:50,253 	Source:     Hello .
2020-07-02 00:54:50,253 	Reference:  Hallo ,
2020-07-02 00:54:50,253 	Hypothesis: Hallo .
2020-07-02 00:54:50,254 Example #1
2020-07-02 00:54:50,254 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 00:54:50,254 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 00:54:50,254 	Source:     Hi , how can I help you ?
2020-07-02 00:54:50,254 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:54:50,254 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:54:50,254 Example #2
2020-07-02 00:54:50,254 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 00:54:50,254 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'im', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 00:54:50,254 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 00:54:50,254 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 00:54:50,254 	Hypothesis: Hallo , ich suche ein Restaurant im Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 00:54:50,254 Example #3
2020-07-02 00:54:50,254 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 00:54:50,254 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 00:54:50,254 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 00:54:50,254 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 00:54:50,254 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 00:54:50,254 Validation result (greedy) at epoch  32, step  1364000: bleu:  53.01, loss: 18973.6094, ppl:   2.1999, duration: 48.8113s
2020-07-02 00:55:08,752 Epoch  32: total training loss 91.36
2020-07-02 00:55:08,752 EPOCH 33
2020-07-02 00:55:15,472 Epoch  33 Step:  1364100 Batch Loss:     0.613153 Tokens per Sec:     4966, Lr: 0.000020
2020-07-02 00:55:40,876 Epoch  33 Step:  1364200 Batch Loss:     0.457898 Tokens per Sec:     4677, Lr: 0.000020
2020-07-02 00:55:41,151 Epoch  33: total training loss 90.47
2020-07-02 00:55:41,151 EPOCH 34
2020-07-02 00:56:07,204 Epoch  34 Step:  1364300 Batch Loss:     0.523770 Tokens per Sec:     4606, Lr: 0.000020
2020-07-02 00:56:14,037 Epoch  34: total training loss 87.39
2020-07-02 00:56:14,037 EPOCH 35
2020-07-02 00:56:32,673 Epoch  35 Step:  1364400 Batch Loss:     0.508551 Tokens per Sec:     4757, Lr: 0.000020
2020-07-02 00:56:46,493 Epoch  35: total training loss 86.19
2020-07-02 00:56:46,493 EPOCH 36
2020-07-02 00:56:58,449 Epoch  36 Step:  1364500 Batch Loss:     0.592298 Tokens per Sec:     4819, Lr: 0.000020
2020-07-02 00:57:19,153 Epoch  36: total training loss 86.37
2020-07-02 00:57:19,154 EPOCH 37
2020-07-02 00:57:23,747 Epoch  37 Step:  1364600 Batch Loss:     0.571012 Tokens per Sec:     5059, Lr: 0.000020
2020-07-02 00:57:49,667 Epoch  37 Step:  1364700 Batch Loss:     0.724393 Tokens per Sec:     4657, Lr: 0.000020
2020-07-02 00:57:52,230 Epoch  37: total training loss 84.74
2020-07-02 00:57:52,231 EPOCH 38
2020-07-02 00:58:14,691 Epoch  38 Step:  1364800 Batch Loss:     0.637216 Tokens per Sec:     4913, Lr: 0.000020
2020-07-02 00:58:23,883 Epoch  38: total training loss 82.76
2020-07-02 00:58:23,884 EPOCH 39
2020-07-02 00:58:40,387 Epoch  39 Step:  1364900 Batch Loss:     0.818053 Tokens per Sec:     4695, Lr: 0.000020
2020-07-02 00:58:56,771 Epoch  39: total training loss 82.71
2020-07-02 00:58:56,772 EPOCH 40
2020-07-02 00:59:05,750 Epoch  40 Step:  1365000 Batch Loss:     0.445001 Tokens per Sec:     4806, Lr: 0.000020
2020-07-02 00:59:44,361 Hooray! New best validation result [ppl]!
2020-07-02 00:59:44,361 Saving new checkpoint.
2020-07-02 00:59:55,026 Example #0
2020-07-02 00:59:55,027 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 00:59:55,027 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 00:59:55,027 	Source:     Hello .
2020-07-02 00:59:55,027 	Reference:  Hallo ,
2020-07-02 00:59:55,027 	Hypothesis: Hallo .
2020-07-02 00:59:55,027 Example #1
2020-07-02 00:59:55,027 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 00:59:55,027 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 00:59:55,027 	Source:     Hi , how can I help you ?
2020-07-02 00:59:55,027 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:59:55,027 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 00:59:55,027 Example #2
2020-07-02 00:59:55,027 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 00:59:55,027 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 00:59:55,027 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 00:59:55,027 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 00:59:55,027 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 00:59:55,028 Example #3
2020-07-02 00:59:55,028 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 00:59:55,028 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 00:59:55,028 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 00:59:55,028 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 00:59:55,028 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 00:59:55,028 Validation result (greedy) at epoch  40, step  1365000: bleu:  53.56, loss: 18273.3965, ppl:   2.1369, duration: 49.2775s
2020-07-02 01:00:17,944 Epoch  40: total training loss 81.53
2020-07-02 01:00:17,945 EPOCH 41
2020-07-02 01:00:20,137 Epoch  41 Step:  1365100 Batch Loss:     0.418207 Tokens per Sec:     4865, Lr: 0.000020
2020-07-02 01:00:45,821 Epoch  41 Step:  1365200 Batch Loss:     0.712143 Tokens per Sec:     4697, Lr: 0.000020
2020-07-02 01:00:50,594 Epoch  41: total training loss 80.28
2020-07-02 01:00:50,594 EPOCH 42
2020-07-02 01:01:11,245 Epoch  42 Step:  1365300 Batch Loss:     0.463582 Tokens per Sec:     4634, Lr: 0.000020
2020-07-02 01:01:23,063 Epoch  42: total training loss 79.33
2020-07-02 01:01:23,064 EPOCH 43
2020-07-02 01:01:35,743 Epoch  43 Step:  1365400 Batch Loss:     0.832326 Tokens per Sec:     5028, Lr: 0.000020
2020-07-02 01:01:54,494 Epoch  43: total training loss 76.26
2020-07-02 01:01:54,494 EPOCH 44
2020-07-02 01:02:00,759 Epoch  44 Step:  1365500 Batch Loss:     0.686401 Tokens per Sec:     4741, Lr: 0.000020
2020-07-02 01:02:26,072 Epoch  44 Step:  1365600 Batch Loss:     0.620341 Tokens per Sec:     4752, Lr: 0.000020
2020-07-02 01:02:26,858 Epoch  44: total training loss 77.06
2020-07-02 01:02:26,858 EPOCH 45
2020-07-02 01:02:51,809 Epoch  45 Step:  1365700 Batch Loss:     0.647120 Tokens per Sec:     4673, Lr: 0.000020
2020-07-02 01:02:59,591 Epoch  45: total training loss 75.05
2020-07-02 01:02:59,591 EPOCH 46
2020-07-02 01:03:18,069 Epoch  46 Step:  1365800 Batch Loss:     0.593916 Tokens per Sec:     4673, Lr: 0.000020
2020-07-02 01:03:32,014 Epoch  46: total training loss 73.59
2020-07-02 01:03:32,015 EPOCH 47
2020-07-02 01:03:43,272 Epoch  47 Step:  1365900 Batch Loss:     0.468400 Tokens per Sec:     4623, Lr: 0.000020
2020-07-02 01:04:04,297 Epoch  47: total training loss 73.27
2020-07-02 01:04:04,298 EPOCH 48
2020-07-02 01:04:08,386 Epoch  48 Step:  1366000 Batch Loss:     0.523743 Tokens per Sec:     4410, Lr: 0.000020
2020-07-02 01:04:46,950 Hooray! New best validation result [ppl]!
2020-07-02 01:04:46,951 Saving new checkpoint.
2020-07-02 01:04:57,577 Example #0
2020-07-02 01:04:57,577 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 01:04:57,577 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 01:04:57,577 	Source:     Hello .
2020-07-02 01:04:57,577 	Reference:  Hallo ,
2020-07-02 01:04:57,577 	Hypothesis: Hallo .
2020-07-02 01:04:57,577 Example #1
2020-07-02 01:04:57,577 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 01:04:57,578 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 01:04:57,578 	Source:     Hi , how can I help you ?
2020-07-02 01:04:57,578 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:04:57,578 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:04:57,578 Example #2
2020-07-02 01:04:57,578 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 01:04:57,578 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 01:04:57,578 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 01:04:57,578 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 01:04:57,578 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 01:04:57,578 Example #3
2020-07-02 01:04:57,578 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 01:04:57,578 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 01:04:57,578 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 01:04:57,578 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 01:04:57,578 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 01:04:57,579 Validation result (greedy) at epoch  48, step  1366000: bleu:  53.98, loss: 17758.1953, ppl:   2.0916, duration: 49.1918s
2020-07-02 01:05:22,935 Epoch  48 Step:  1366100 Batch Loss:     0.549494 Tokens per Sec:     4808, Lr: 0.000020
2020-07-02 01:05:25,610 Epoch  48: total training loss 72.32
2020-07-02 01:05:25,610 EPOCH 49
2020-07-02 01:05:49,115 Epoch  49 Step:  1366200 Batch Loss:     0.499474 Tokens per Sec:     4495, Lr: 0.000020
2020-07-02 01:05:58,559 Epoch  49: total training loss 70.60
2020-07-02 01:05:58,560 EPOCH 50
2020-07-02 01:06:14,889 Epoch  50 Step:  1366300 Batch Loss:     0.619833 Tokens per Sec:     4507, Lr: 0.000020
2020-07-02 01:06:31,823 Epoch  50: total training loss 69.51
2020-07-02 01:06:31,824 EPOCH 51
2020-07-02 01:06:40,592 Epoch  51 Step:  1366400 Batch Loss:     0.589706 Tokens per Sec:     4459, Lr: 0.000020
2020-07-02 01:07:04,457 Epoch  51: total training loss 69.39
2020-07-02 01:07:04,457 EPOCH 52
2020-07-02 01:07:05,576 Epoch  52 Step:  1366500 Batch Loss:     0.583703 Tokens per Sec:     5397, Lr: 0.000020
2020-07-02 01:07:31,534 Epoch  52 Step:  1366600 Batch Loss:     0.659537 Tokens per Sec:     4665, Lr: 0.000020
2020-07-02 01:07:37,254 Epoch  52: total training loss 67.77
2020-07-02 01:07:37,254 EPOCH 53
2020-07-02 01:07:56,490 Epoch  53 Step:  1366700 Batch Loss:     0.403083 Tokens per Sec:     4968, Lr: 0.000020
2020-07-02 01:08:08,695 Epoch  53: total training loss 66.68
2020-07-02 01:08:08,696 EPOCH 54
2020-07-02 01:08:21,857 Epoch  54 Step:  1366800 Batch Loss:     0.639031 Tokens per Sec:     4656, Lr: 0.000020
2020-07-02 01:08:41,253 Epoch  54: total training loss 65.09
2020-07-02 01:08:41,254 EPOCH 55
2020-07-02 01:08:47,374 Epoch  55 Step:  1366900 Batch Loss:     0.712345 Tokens per Sec:     5009, Lr: 0.000020
2020-07-02 01:09:12,680 Epoch  55 Step:  1367000 Batch Loss:     0.574726 Tokens per Sec:     4755, Lr: 0.000020
2020-07-02 01:09:52,430 Hooray! New best validation result [ppl]!
2020-07-02 01:09:52,431 Saving new checkpoint.
2020-07-02 01:10:03,020 Example #0
2020-07-02 01:10:03,020 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 01:10:03,020 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 01:10:03,020 	Source:     Hello .
2020-07-02 01:10:03,021 	Reference:  Hallo ,
2020-07-02 01:10:03,021 	Hypothesis: Hallo .
2020-07-02 01:10:03,021 Example #1
2020-07-02 01:10:03,021 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 01:10:03,021 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 01:10:03,021 	Source:     Hi , how can I help you ?
2020-07-02 01:10:03,021 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:10:03,021 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:10:03,021 Example #2
2020-07-02 01:10:03,021 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 01:10:03,021 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 01:10:03,021 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 01:10:03,021 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 01:10:03,021 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 01:10:03,021 Example #3
2020-07-02 01:10:03,021 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 01:10:03,021 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 01:10:03,022 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 01:10:03,022 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 01:10:03,022 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 01:10:03,022 Validation result (greedy) at epoch  55, step  1367000: bleu:  54.43, loss: 17542.5820, ppl:   2.0729, duration: 50.3411s
2020-07-02 01:10:03,438 Epoch  55: total training loss 65.26
2020-07-02 01:10:03,438 EPOCH 56
2020-07-02 01:10:28,732 Epoch  56 Step:  1367100 Batch Loss:     0.548735 Tokens per Sec:     4771, Lr: 0.000020
2020-07-02 01:10:36,041 Epoch  56: total training loss 63.66
2020-07-02 01:10:36,041 EPOCH 57
2020-07-02 01:10:54,224 Epoch  57 Step:  1367200 Batch Loss:     0.238675 Tokens per Sec:     4830, Lr: 0.000020
2020-07-02 01:11:08,600 Epoch  57: total training loss 63.01
2020-07-02 01:11:08,600 EPOCH 58
2020-07-02 01:11:20,050 Epoch  58 Step:  1367300 Batch Loss:     0.543529 Tokens per Sec:     4712, Lr: 0.000020
2020-07-02 01:11:41,537 Epoch  58: total training loss 62.37
2020-07-02 01:11:41,537 EPOCH 59
2020-07-02 01:11:45,949 Epoch  59 Step:  1367400 Batch Loss:     0.600973 Tokens per Sec:     5235, Lr: 0.000020
2020-07-02 01:12:11,828 Epoch  59 Step:  1367500 Batch Loss:     0.477147 Tokens per Sec:     4603, Lr: 0.000020
2020-07-02 01:12:13,783 Epoch  59: total training loss 62.14
2020-07-02 01:12:13,784 EPOCH 60
2020-07-02 01:12:37,154 Epoch  60 Step:  1367600 Batch Loss:     0.473250 Tokens per Sec:     4719, Lr: 0.000020
2020-07-02 01:12:45,975 Epoch  60: total training loss 60.72
2020-07-02 01:12:45,976 EPOCH 61
2020-07-02 01:13:03,124 Epoch  61 Step:  1367700 Batch Loss:     0.497795 Tokens per Sec:     4619, Lr: 0.000020
2020-07-02 01:13:19,312 Epoch  61: total training loss 59.43
2020-07-02 01:13:19,312 EPOCH 62
2020-07-02 01:13:29,116 Epoch  62 Step:  1367800 Batch Loss:     0.489307 Tokens per Sec:     4666, Lr: 0.000020
2020-07-02 01:13:51,364 Epoch  62: total training loss 59.48
2020-07-02 01:13:51,365 EPOCH 63
2020-07-02 01:13:53,776 Epoch  63 Step:  1367900 Batch Loss:     0.368872 Tokens per Sec:     5481, Lr: 0.000020
2020-07-02 01:14:20,120 Epoch  63 Step:  1368000 Batch Loss:     0.549638 Tokens per Sec:     4586, Lr: 0.000020
2020-07-02 01:15:01,654 Example #0
2020-07-02 01:15:01,654 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 01:15:01,654 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 01:15:01,654 	Source:     Hello .
2020-07-02 01:15:01,654 	Reference:  Hallo ,
2020-07-02 01:15:01,654 	Hypothesis: Hallo .
2020-07-02 01:15:01,654 Example #1
2020-07-02 01:15:01,654 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 01:15:01,654 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 01:15:01,654 	Source:     Hi , how can I help you ?
2020-07-02 01:15:01,654 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:15:01,654 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:15:01,654 Example #2
2020-07-02 01:15:01,654 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 01:15:01,654 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 01:15:01,654 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 01:15:01,654 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 01:15:01,654 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 01:15:01,654 Example #3
2020-07-02 01:15:01,654 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 01:15:01,654 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 01:15:01,654 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 01:15:01,654 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 01:15:01,654 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 01:15:01,654 Validation result (greedy) at epoch  63, step  1368000: bleu:  54.64, loss: 17574.6172, ppl:   2.0757, duration: 41.5334s
2020-07-02 01:15:06,190 Epoch  63: total training loss 58.21
2020-07-02 01:15:06,190 EPOCH 64
2020-07-02 01:15:27,955 Epoch  64 Step:  1368100 Batch Loss:     0.392886 Tokens per Sec:     4600, Lr: 0.000020
2020-07-02 01:15:39,545 Epoch  64: total training loss 57.58
2020-07-02 01:15:39,546 EPOCH 65
2020-07-02 01:15:54,073 Epoch  65 Step:  1368200 Batch Loss:     0.482152 Tokens per Sec:     4620, Lr: 0.000020
2020-07-02 01:16:11,227 Epoch  65: total training loss 56.85
2020-07-02 01:16:11,227 EPOCH 66
2020-07-02 01:16:18,281 Epoch  66 Step:  1368300 Batch Loss:     0.491002 Tokens per Sec:     4983, Lr: 0.000020
2020-07-02 01:16:42,888 Epoch  66: total training loss 56.30
2020-07-02 01:16:42,889 EPOCH 67
2020-07-02 01:16:43,132 Epoch  67 Step:  1368400 Batch Loss:     0.422284 Tokens per Sec:     5781, Lr: 0.000020
2020-07-02 01:17:08,637 Epoch  67 Step:  1368500 Batch Loss:     0.449545 Tokens per Sec:     4695, Lr: 0.000020
2020-07-02 01:17:15,191 Epoch  67: total training loss 55.39
2020-07-02 01:17:15,191 EPOCH 68
2020-07-02 01:17:34,103 Epoch  68 Step:  1368600 Batch Loss:     0.467758 Tokens per Sec:     4749, Lr: 0.000020
2020-07-02 01:17:47,786 Epoch  68: total training loss 54.41
2020-07-02 01:17:47,787 EPOCH 69
2020-07-02 01:18:00,042 Epoch  69 Step:  1368700 Batch Loss:     0.279273 Tokens per Sec:     4533, Lr: 0.000020
2020-07-02 01:18:20,823 Epoch  69: total training loss 53.97
2020-07-02 01:18:20,824 EPOCH 70
2020-07-02 01:18:26,018 Epoch  70 Step:  1368800 Batch Loss:     0.398776 Tokens per Sec:     4302, Lr: 0.000020
2020-07-02 01:18:52,075 Epoch  70 Step:  1368900 Batch Loss:     0.329462 Tokens per Sec:     4693, Lr: 0.000020
2020-07-02 01:18:53,871 Epoch  70: total training loss 52.97
2020-07-02 01:18:53,871 EPOCH 71
2020-07-02 01:19:17,783 Epoch  71 Step:  1369000 Batch Loss:     0.373656 Tokens per Sec:     4631, Lr: 0.000020
2020-07-02 01:20:04,339 Example #0
2020-07-02 01:20:04,339 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 01:20:04,339 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 01:20:04,339 	Source:     Hello .
2020-07-02 01:20:04,339 	Reference:  Hallo ,
2020-07-02 01:20:04,339 	Hypothesis: Hallo .
2020-07-02 01:20:04,339 Example #1
2020-07-02 01:20:04,339 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 01:20:04,339 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 01:20:04,339 	Source:     Hi , how can I help you ?
2020-07-02 01:20:04,339 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:20:04,339 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:20:04,339 Example #2
2020-07-02 01:20:04,339 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 01:20:04,339 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 01:20:04,339 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 01:20:04,339 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 01:20:04,339 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 01:20:04,339 Example #3
2020-07-02 01:20:04,340 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 01:20:04,340 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 01:20:04,340 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 01:20:04,340 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 01:20:04,340 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 01:20:04,340 Validation result (greedy) at epoch  71, step  1369000: bleu:  54.35, loss: 17702.5840, ppl:   2.0868, duration: 46.5560s
2020-07-02 01:20:13,441 Epoch  71: total training loss 51.96
2020-07-02 01:20:13,442 EPOCH 72
2020-07-02 01:20:30,097 Epoch  72 Step:  1369100 Batch Loss:     0.468793 Tokens per Sec:     4690, Lr: 0.000020
2020-07-02 01:20:46,457 Epoch  72: total training loss 52.07
2020-07-02 01:20:46,457 EPOCH 73
2020-07-02 01:20:55,849 Epoch  73 Step:  1369200 Batch Loss:     0.223925 Tokens per Sec:     4590, Lr: 0.000020
2020-07-02 01:21:19,610 Epoch  73: total training loss 51.48
2020-07-02 01:21:19,611 EPOCH 74
2020-07-02 01:21:21,775 Epoch  74 Step:  1369300 Batch Loss:     0.372025 Tokens per Sec:     4673, Lr: 0.000020
2020-07-02 01:21:47,236 Epoch  74 Step:  1369400 Batch Loss:     0.320358 Tokens per Sec:     4833, Lr: 0.000020
2020-07-02 01:21:51,339 Epoch  74: total training loss 49.23
2020-07-02 01:21:51,339 EPOCH 75
2020-07-02 01:22:13,533 Epoch  75 Step:  1369500 Batch Loss:     0.398127 Tokens per Sec:     4543, Lr: 0.000020
2020-07-02 01:22:24,411 Epoch  75: total training loss 48.74
2020-07-02 01:22:24,412 EPOCH 76
2020-07-02 01:22:39,234 Epoch  76 Step:  1369600 Batch Loss:     0.330853 Tokens per Sec:     4621, Lr: 0.000020
2020-07-02 01:22:57,562 Epoch  76: total training loss 49.24
2020-07-02 01:22:57,562 EPOCH 77
2020-07-02 01:23:05,698 Epoch  77 Step:  1369700 Batch Loss:     0.444333 Tokens per Sec:     4446, Lr: 0.000020
2020-07-02 01:23:30,620 Epoch  77: total training loss 47.91
2020-07-02 01:23:30,620 EPOCH 78
2020-07-02 01:23:31,130 Epoch  78 Step:  1369800 Batch Loss:     0.329652 Tokens per Sec:     4985, Lr: 0.000020
2020-07-02 01:23:56,780 Epoch  78 Step:  1369900 Batch Loss:     0.384155 Tokens per Sec:     4642, Lr: 0.000020
2020-07-02 01:24:03,570 Epoch  78: total training loss 47.56
2020-07-02 01:24:03,571 EPOCH 79
2020-07-02 01:24:23,227 Epoch  79 Step:  1370000 Batch Loss:     0.409692 Tokens per Sec:     4536, Lr: 0.000020
2020-07-02 01:25:14,993 Example #0
2020-07-02 01:25:14,994 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 01:25:14,994 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 01:25:14,994 	Source:     Hello .
2020-07-02 01:25:14,994 	Reference:  Hallo ,
2020-07-02 01:25:14,994 	Hypothesis: Hallo .
2020-07-02 01:25:14,994 Example #1
2020-07-02 01:25:14,994 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 01:25:14,994 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 01:25:14,994 	Source:     Hi , how can I help you ?
2020-07-02 01:25:14,994 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:25:14,994 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:25:14,994 Example #2
2020-07-02 01:25:14,994 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 01:25:14,994 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 01:25:14,994 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 01:25:14,994 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 01:25:14,994 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 01:25:14,994 Example #3
2020-07-02 01:25:14,994 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 01:25:14,994 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 01:25:14,994 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 01:25:14,994 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 01:25:14,994 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 01:25:14,994 Validation result (greedy) at epoch  79, step  1370000: bleu:  53.43, loss: 17969.0820, ppl:   2.1100, duration: 51.7669s
2020-07-02 01:25:29,050 Epoch  79: total training loss 46.70
2020-07-02 01:25:29,051 EPOCH 80
2020-07-02 01:25:41,547 Epoch  80 Step:  1370100 Batch Loss:     0.342832 Tokens per Sec:     4618, Lr: 0.000020
2020-07-02 01:26:02,281 Epoch  80: total training loss 46.17
2020-07-02 01:26:02,282 EPOCH 81
2020-07-02 01:26:07,207 Epoch  81 Step:  1370200 Batch Loss:     0.402893 Tokens per Sec:     4999, Lr: 0.000020
2020-07-02 01:26:32,361 Epoch  81 Step:  1370300 Batch Loss:     0.279317 Tokens per Sec:     4706, Lr: 0.000020
2020-07-02 01:26:34,186 Epoch  81: total training loss 45.51
2020-07-02 01:26:34,187 EPOCH 82
2020-07-02 01:26:57,856 Epoch  82 Step:  1370400 Batch Loss:     0.272938 Tokens per Sec:     4643, Lr: 0.000020
2020-07-02 01:27:06,312 Epoch  82: total training loss 44.77
2020-07-02 01:27:06,312 EPOCH 83
2020-07-02 01:27:22,623 Epoch  83 Step:  1370500 Batch Loss:     0.256273 Tokens per Sec:     4825, Lr: 0.000020
2020-07-02 01:27:38,472 Epoch  83: total training loss 44.17
2020-07-02 01:27:38,472 EPOCH 84
2020-07-02 01:27:48,038 Epoch  84 Step:  1370600 Batch Loss:     0.296820 Tokens per Sec:     4681, Lr: 0.000020
2020-07-02 01:28:11,556 Epoch  84: total training loss 43.14
2020-07-02 01:28:11,556 EPOCH 85
2020-07-02 01:28:14,486 Epoch  85 Step:  1370700 Batch Loss:     0.360361 Tokens per Sec:     4591, Lr: 0.000020
2020-07-02 01:28:40,665 Epoch  85 Step:  1370800 Batch Loss:     0.356497 Tokens per Sec:     4692, Lr: 0.000020
2020-07-02 01:28:44,792 Epoch  85: total training loss 42.60
2020-07-02 01:28:44,793 EPOCH 86
2020-07-02 01:29:06,770 Epoch  86 Step:  1370900 Batch Loss:     0.387959 Tokens per Sec:     4620, Lr: 0.000020
2020-07-02 01:29:17,757 Epoch  86: total training loss 42.37
2020-07-02 01:29:17,758 EPOCH 87
2020-07-02 01:29:32,215 Epoch  87 Step:  1371000 Batch Loss:     0.352802 Tokens per Sec:     4641, Lr: 0.000020
2020-07-02 01:30:26,071 Example #0
2020-07-02 01:30:26,071 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 01:30:26,071 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 01:30:26,071 	Source:     Hello .
2020-07-02 01:30:26,071 	Reference:  Hallo ,
2020-07-02 01:30:26,071 	Hypothesis: Hallo .
2020-07-02 01:30:26,071 Example #1
2020-07-02 01:30:26,071 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 01:30:26,071 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 01:30:26,071 	Source:     Hi , how can I help you ?
2020-07-02 01:30:26,071 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:30:26,071 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:30:26,071 Example #2
2020-07-02 01:30:26,071 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 01:30:26,071 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 01:30:26,071 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 01:30:26,071 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 01:30:26,071 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 01:30:26,071 Example #3
2020-07-02 01:30:26,071 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 01:30:26,072 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 01:30:26,072 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 01:30:26,072 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 01:30:26,072 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 01:30:26,072 Validation result (greedy) at epoch  87, step  1371000: bleu:  53.51, loss: 18132.9375, ppl:   2.1244, duration: 53.8560s
2020-07-02 01:30:45,177 Epoch  87: total training loss 41.61
2020-07-02 01:30:45,178 EPOCH 88
2020-07-02 01:30:52,991 Epoch  88 Step:  1371100 Batch Loss:     0.295836 Tokens per Sec:     4535, Lr: 0.000010
2020-07-02 01:31:18,648 Epoch  88: total training loss 40.61
2020-07-02 01:31:18,649 EPOCH 89
2020-07-02 01:31:19,073 Epoch  89 Step:  1371200 Batch Loss:     0.311320 Tokens per Sec:     5665, Lr: 0.000010
2020-07-02 01:31:44,894 Epoch  89 Step:  1371300 Batch Loss:     0.394623 Tokens per Sec:     4686, Lr: 0.000010
2020-07-02 01:31:51,359 Epoch  89: total training loss 40.28
2020-07-02 01:31:51,359 EPOCH 90
2020-07-02 01:32:10,882 Epoch  90 Step:  1371400 Batch Loss:     0.264546 Tokens per Sec:     4659, Lr: 0.000010
2020-07-02 01:32:24,645 Epoch  90: total training loss 39.98
2020-07-02 01:32:24,646 EPOCH 91
2020-07-02 01:32:36,870 Epoch  91 Step:  1371500 Batch Loss:     0.329665 Tokens per Sec:     4592, Lr: 0.000010
2020-07-02 01:32:57,660 Epoch  91: total training loss 40.71
2020-07-02 01:32:57,661 EPOCH 92
2020-07-02 01:33:02,482 Epoch  92 Step:  1371600 Batch Loss:     0.225319 Tokens per Sec:     4343, Lr: 0.000010
2020-07-02 01:33:27,666 Epoch  92 Step:  1371700 Batch Loss:     0.355536 Tokens per Sec:     4828, Lr: 0.000010
2020-07-02 01:33:29,889 Epoch  92: total training loss 39.16
2020-07-02 01:33:29,889 EPOCH 93
2020-07-02 01:33:51,956 Epoch  93 Step:  1371800 Batch Loss:     0.335397 Tokens per Sec:     4925, Lr: 0.000010
2020-07-02 01:34:02,012 Epoch  93: total training loss 39.27
2020-07-02 01:34:02,013 EPOCH 94
2020-07-02 01:34:18,235 Epoch  94 Step:  1371900 Batch Loss:     0.292829 Tokens per Sec:     4494, Lr: 0.000010
2020-07-02 01:34:35,011 Epoch  94: total training loss 38.76
2020-07-02 01:34:35,011 EPOCH 95
2020-07-02 01:34:43,746 Epoch  95 Step:  1372000 Batch Loss:     0.302137 Tokens per Sec:     4653, Lr: 0.000010
2020-07-02 01:35:38,040 Example #0
2020-07-02 01:35:38,041 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 01:35:38,041 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 01:35:38,041 	Source:     Hello .
2020-07-02 01:35:38,041 	Reference:  Hallo ,
2020-07-02 01:35:38,041 	Hypothesis: Hallo .
2020-07-02 01:35:38,041 Example #1
2020-07-02 01:35:38,041 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 01:35:38,041 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 01:35:38,041 	Source:     Hi , how can I help you ?
2020-07-02 01:35:38,041 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:35:38,041 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 01:35:38,041 Example #2
2020-07-02 01:35:38,041 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 01:35:38,041 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 01:35:38,041 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 01:35:38,041 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 01:35:38,041 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 01:35:38,041 Example #3
2020-07-02 01:35:38,041 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 01:35:38,041 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 01:35:38,041 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 01:35:38,041 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 01:35:38,041 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 01:35:38,041 Validation result (greedy) at epoch  95, step  1372000: bleu:  53.36, loss: 18302.0703, ppl:   2.1394, duration: 54.2943s
2020-07-02 01:36:01,883 Epoch  95: total training loss 38.04
2020-07-02 01:36:01,884 EPOCH 96
2020-07-02 01:36:03,646 Epoch  96 Step:  1372100 Batch Loss:     0.317465 Tokens per Sec:     5549, Lr: 0.000010
2020-07-02 01:36:29,819 Epoch  96 Step:  1372200 Batch Loss:     0.273220 Tokens per Sec:     4601, Lr: 0.000010
2020-07-02 01:36:35,016 Epoch  96: total training loss 37.95
2020-07-02 01:36:35,017 EPOCH 97
2020-07-02 01:36:55,844 Epoch  97 Step:  1372300 Batch Loss:     0.341759 Tokens per Sec:     4676, Lr: 0.000010
2020-07-02 01:37:07,625 Epoch  97: total training loss 37.83
2020-07-02 01:37:07,626 EPOCH 98
2020-07-02 01:37:21,212 Epoch  98 Step:  1372400 Batch Loss:     0.384216 Tokens per Sec:     4996, Lr: 0.000010
2020-07-02 01:37:40,274 Epoch  98: total training loss 37.61
2020-07-02 01:37:40,275 EPOCH 99
2020-07-02 01:37:46,465 Epoch  99 Step:  1372500 Batch Loss:     0.222722 Tokens per Sec:     4964, Lr: 0.000010
2020-07-02 01:38:11,645 Epoch  99 Step:  1372600 Batch Loss:     0.325678 Tokens per Sec:     4829, Lr: 0.000010
2020-07-02 01:38:11,963 Epoch  99: total training loss 37.00
2020-07-02 01:38:11,963 EPOCH 100
2020-07-02 01:38:37,842 Epoch 100 Step:  1372700 Batch Loss:     0.277203 Tokens per Sec:     4653, Lr: 0.000010
2020-07-02 01:38:44,830 Epoch 100: total training loss 36.83
2020-07-02 01:38:44,830 Training ended after 100 epochs.
2020-07-02 01:38:44,830 Best validation result (greedy) at step  1367000:   2.07 ppl.
2020-07-02 01:39:45,972  dev bleu:  55.75 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-02 01:39:45,976 Translations saved to: models/transformer_multi_enc_lr0.00002p3d0.5_ende-tune/01367000.hyps.dev
2020-07-02 01:40:11,885 test bleu:  51.62 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-02 01:40:11,889 Translations saved to: models/transformer_multi_enc_lr0.00002p3d0.5_ende-tune/01367000.hyps.test
