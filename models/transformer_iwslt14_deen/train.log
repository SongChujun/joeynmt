2020-06-16 14:50:02,590 Hello! This is Joey-NMT.
2020-06-16 14:50:23,244 Total params: 13714688
2020-06-16 14:50:23,247 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-16 14:50:31,419 cfg.name                           : transformer_iwslt14_deen
2020-06-16 14:50:31,420 cfg.data.src                       : de
2020-06-16 14:50:31,420 cfg.data.trg                       : en
2020-06-16 14:50:31,420 cfg.data.train                     : chatnmt/prep/train.tags.bpe.10000
2020-06-16 14:50:31,420 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.10000
2020-06-16 14:50:31,420 cfg.data.test                      : chatnmt/prep/test.tags.bpe.10000
2020-06-16 14:50:31,420 cfg.data.level                     : bpe
2020-06-16 14:50:31,420 cfg.data.lowercase                 : True
2020-06-16 14:50:31,420 cfg.data.max_sent_length           : 100
2020-06-16 14:50:31,420 cfg.testing.beam_size              : 5
2020-06-16 14:50:31,420 cfg.testing.alpha                  : 1.0
2020-06-16 14:50:31,420 cfg.training.random_seed           : 42
2020-06-16 14:50:31,420 cfg.training.optimizer             : adam
2020-06-16 14:50:31,420 cfg.training.normalization         : tokens
2020-06-16 14:50:31,421 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-16 14:50:31,421 cfg.training.scheduling            : plateau
2020-06-16 14:50:31,421 cfg.training.patience              : 5
2020-06-16 14:50:31,421 cfg.training.decrease_factor       : 0.7
2020-06-16 14:50:31,421 cfg.training.loss                  : crossentropy
2020-06-16 14:50:31,421 cfg.training.learning_rate         : 0.0003
2020-06-16 14:50:31,421 cfg.training.learning_rate_min     : 1e-08
2020-06-16 14:50:31,421 cfg.training.weight_decay          : 0.0
2020-06-16 14:50:31,421 cfg.training.label_smoothing       : 0.1
2020-06-16 14:50:31,421 cfg.training.batch_size            : 4096
2020-06-16 14:50:31,421 cfg.training.batch_type            : token
2020-06-16 14:50:31,421 cfg.training.eval_batch_size       : 3600
2020-06-16 14:50:31,421 cfg.training.eval_batch_type       : token
2020-06-16 14:50:31,421 cfg.training.batch_multiplier      : 1
2020-06-16 14:50:31,421 cfg.training.early_stopping_metric : eval_metric
2020-06-16 14:50:31,421 cfg.training.epochs                : 100
2020-06-16 14:50:31,421 cfg.training.validation_freq       : 1000
2020-06-16 14:50:31,421 cfg.training.logging_freq          : 100
2020-06-16 14:50:31,422 cfg.training.eval_metric           : bleu
2020-06-16 14:50:31,422 cfg.training.model_dir             : models/transformer_iwslt14_deen
2020-06-16 14:50:31,422 cfg.training.overwrite             : True
2020-06-16 14:50:31,422 cfg.training.shuffle               : True
2020-06-16 14:50:31,422 cfg.training.use_cuda              : True
2020-06-16 14:50:31,422 cfg.training.max_output_length     : 100
2020-06-16 14:50:31,422 cfg.training.print_valid_sents     : [0, 1, 2, 3, 4]
2020-06-16 14:50:31,422 cfg.training.keep_last_ckpts       : 5
2020-06-16 14:50:31,422 cfg.model.initializer              : xavier
2020-06-16 14:50:31,422 cfg.model.bias_initializer         : zeros
2020-06-16 14:50:31,422 cfg.model.init_gain                : 1.0
2020-06-16 14:50:31,422 cfg.model.embed_initializer        : xavier
2020-06-16 14:50:31,422 cfg.model.embed_init_gain          : 1.0
2020-06-16 14:50:31,422 cfg.model.tied_embeddings          : False
2020-06-16 14:50:31,422 cfg.model.tied_softmax             : True
2020-06-16 14:50:31,422 cfg.model.encoder.type             : transformer
2020-06-16 14:50:31,422 cfg.model.encoder.num_layers       : 6
2020-06-16 14:50:31,422 cfg.model.encoder.num_heads        : 4
2020-06-16 14:50:31,423 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-16 14:50:31,423 cfg.model.encoder.embeddings.scale : True
2020-06-16 14:50:31,423 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-16 14:50:31,423 cfg.model.encoder.hidden_size      : 256
2020-06-16 14:50:31,423 cfg.model.encoder.ff_size          : 1024
2020-06-16 14:50:31,423 cfg.model.encoder.freeze           : False
2020-06-16 14:50:31,423 cfg.model.encoder.dropout          : 0.3
2020-06-16 14:50:31,423 cfg.model.decoder.type             : transformer
2020-06-16 14:50:31,423 cfg.model.decoder.num_layers       : 6
2020-06-16 14:50:31,423 cfg.model.decoder.num_heads        : 4
2020-06-16 14:50:31,423 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-16 14:50:31,423 cfg.model.decoder.embeddings.scale : True
2020-06-16 14:50:31,423 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-16 14:50:31,423 cfg.model.decoder.hidden_size      : 256
2020-06-16 14:50:31,423 cfg.model.decoder.ff_size          : 1024
2020-06-16 14:50:31,423 cfg.model.decoder.freeze           : False
2020-06-16 14:50:31,423 cfg.model.decoder.dropout          : 0.3
2020-06-16 14:50:31,423 Data set sizes: 
	train 9467,
	valid 1476,
	test 1163
2020-06-16 14:50:31,424 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-16 14:50:31,424 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-16 14:50:31,424 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-16 14:50:31,424 Number of Src words (types): 5833
2020-06-16 14:50:31,424 Number of Trg words (types): 4536
2020-06-16 14:50:31,424 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=256, vocab_size=5833),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4536))
2020-06-16 14:50:31,434 EPOCH 1
2020-06-16 14:50:40,756 Epoch   1: total training loss 280.52
2020-06-16 14:50:40,757 EPOCH 2
2020-06-16 14:50:48,359 Epoch   2 Step:      100 Batch Loss:     4.819670 Tokens per Sec:    16824, Lr: 0.000300
2020-06-16 14:50:48,360 Epoch   2: total training loss 238.33
2020-06-16 14:50:48,360 EPOCH 3
2020-06-16 14:50:55,963 Epoch   3: total training loss 227.79
2020-06-16 14:50:55,963 EPOCH 4
2020-06-16 14:51:03,588 Epoch   4 Step:      200 Batch Loss:     5.637401 Tokens per Sec:    16776, Lr: 0.000300
2020-06-16 14:51:03,589 Epoch   4: total training loss 221.08
2020-06-16 14:51:03,589 EPOCH 5
2020-06-16 14:51:11,169 Epoch   5: total training loss 213.49
2020-06-16 14:51:11,170 EPOCH 6
2020-06-16 14:51:18,770 Epoch   6: total training loss 197.19
2020-06-16 14:51:18,771 EPOCH 7
2020-06-16 14:51:18,942 Epoch   7 Step:      300 Batch Loss:     4.396238 Tokens per Sec:    16106, Lr: 0.000300
2020-06-16 14:51:26,860 Epoch   7: total training loss 186.02
2020-06-16 14:51:26,860 EPOCH 8
2020-06-16 14:51:35,249 Epoch   8: total training loss 174.99
2020-06-16 14:51:35,249 EPOCH 9
2020-06-16 14:51:35,721 Epoch   9 Step:      400 Batch Loss:     5.065940 Tokens per Sec:    14234, Lr: 0.000300
2020-06-16 14:51:43,596 Epoch   9: total training loss 169.31
2020-06-16 14:51:43,597 EPOCH 10
2020-06-16 14:51:51,794 Epoch  10: total training loss 155.20
2020-06-16 14:51:51,795 EPOCH 11
2020-06-16 14:51:52,491 Epoch  11 Step:      500 Batch Loss:     3.743431 Tokens per Sec:    16396, Lr: 0.000300
2020-06-16 14:52:00,077 Epoch  11: total training loss 146.04
2020-06-16 14:52:00,077 EPOCH 12
2020-06-16 14:52:08,519 Epoch  12: total training loss 141.14
2020-06-16 14:52:08,520 EPOCH 13
2020-06-16 14:52:09,392 Epoch  13 Step:      600 Batch Loss:     1.125487 Tokens per Sec:    13882, Lr: 0.000300
2020-06-16 14:52:16,912 Epoch  13: total training loss 134.60
2020-06-16 14:52:16,913 EPOCH 14
2020-06-16 14:52:25,281 Epoch  14: total training loss 126.64
2020-06-16 14:52:25,281 EPOCH 15
2020-06-16 14:52:26,146 Epoch  15 Step:      700 Batch Loss:     2.180089 Tokens per Sec:    15014, Lr: 0.000300
2020-06-16 14:52:33,727 Epoch  15: total training loss 118.29
2020-06-16 14:52:33,727 EPOCH 16
2020-06-16 14:52:42,109 Epoch  16: total training loss 110.50
2020-06-16 14:52:42,110 EPOCH 17
2020-06-16 14:52:43,331 Epoch  17 Step:      800 Batch Loss:     2.482125 Tokens per Sec:    15852, Lr: 0.000300
2020-06-16 14:52:50,526 Epoch  17: total training loss 109.11
2020-06-16 14:52:50,526 EPOCH 18
2020-06-16 14:52:58,924 Epoch  18: total training loss 105.47
2020-06-16 14:52:58,925 EPOCH 19
2020-06-16 14:53:00,161 Epoch  19 Step:      900 Batch Loss:     2.406505 Tokens per Sec:    16079, Lr: 0.000300
2020-06-16 14:53:07,352 Epoch  19: total training loss 96.80
2020-06-16 14:53:07,352 EPOCH 20
2020-06-16 14:53:15,846 Epoch  20: total training loss 96.41
2020-06-16 14:53:15,847 EPOCH 21
2020-06-16 14:53:17,249 Epoch  21 Step:     1000 Batch Loss:     3.307898 Tokens per Sec:    15032, Lr: 0.000300
2020-06-16 14:54:17,538 Hooray! New best validation result [eval_metric]!
2020-06-16 14:54:17,539 Saving new checkpoint.
2020-06-16 14:54:19,437 Example #0
2020-06-16 14:54:19,437 	Raw source:     ['hallo', ',']
2020-06-16 14:54:19,437 	Raw hypothesis: ['hi', 'there', '.']
2020-06-16 14:54:19,437 	Source:     hallo ,
2020-06-16 14:54:19,437 	Reference:  hello .
2020-06-16 14:54:19,437 	Hypothesis: hi there .
2020-06-16 14:54:19,437 Example #1
2020-06-16 14:54:19,437 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-16 14:54:19,437 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-16 14:54:19,437 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-16 14:54:19,437 	Reference:  hi , how can i help you ?
2020-06-16 14:54:19,437 	Hypothesis: hi , how can i help you ?
2020-06-16 14:54:19,437 Example #2
2020-06-16 14:54:19,437 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-16 14:54:19,437 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-16 14:54:19,438 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-16 14:54:19,438 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-16 14:54:19,438 	Hypothesis: hi , i &apos;m looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-16 14:54:19,438 Example #3
2020-06-16 14:54:19,438 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-16 14:54:19,438 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-16 14:54:19,438 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-16 14:54:19,438 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-16 14:54:19,438 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-16 14:54:19,438 Example #4
2020-06-16 14:54:19,438 	Raw source:     ['klar', ',', 'geben', 'sie', 'mir', 'eine', 'sekunde', ',', 'ich', 'bin', 'gleich', 'zur@@', 'ü@@', 'ck', 'mit', 'ein', 'paar', 'optionen', '.']
2020-06-16 14:54:19,438 	Raw hypothesis: ['sure', ',', 'give', 'me', 'a', 'second', 'i', 'can', 'get', 'a', 'cheese', ',', 'i', 'get', 'a', 'little', 'options', '.']
2020-06-16 14:54:19,438 	Source:     klar , geben sie mir eine sekunde , ich bin gleich zurück mit ein paar optionen .
2020-06-16 14:54:19,438 	Reference:  sure , give me a second , i will be right back with a couple of options .
2020-06-16 14:54:19,438 	Hypothesis: sure , give me a second i can get a cheese , i get a little options .
2020-06-16 14:54:19,438 Validation result (greedy) at epoch  21, step     1000: bleu:  21.01, loss: 49656.0703, ppl:  11.0940, duration: 62.1888s
2020-06-16 14:54:26,236 Epoch  21: total training loss 89.30
2020-06-16 14:54:26,237 EPOCH 22
2020-06-16 14:54:34,739 Epoch  22: total training loss 87.94
2020-06-16 14:54:34,739 EPOCH 23
2020-06-16 14:54:36,332 Epoch  23 Step:     1100 Batch Loss:     1.541561 Tokens per Sec:    15692, Lr: 0.000300
2020-06-16 14:54:43,095 Epoch  23: total training loss 82.65
2020-06-16 14:54:43,095 EPOCH 24
2020-06-16 14:54:51,403 Epoch  24: total training loss 79.32
2020-06-16 14:54:51,404 EPOCH 25
2020-06-16 14:54:53,300 Epoch  25 Step:     1200 Batch Loss:     1.272116 Tokens per Sec:    15551, Lr: 0.000300
2020-06-16 14:54:59,778 Epoch  25: total training loss 80.62
2020-06-16 14:54:59,779 EPOCH 26
2020-06-16 14:55:08,136 Epoch  26: total training loss 79.17
2020-06-16 14:55:08,137 EPOCH 27
2020-06-16 14:55:10,047 Epoch  27 Step:     1300 Batch Loss:     0.914615 Tokens per Sec:    15326, Lr: 0.000300
2020-06-16 14:55:16,448 Epoch  27: total training loss 75.95
2020-06-16 14:55:16,448 EPOCH 28
2020-06-16 14:55:24,855 Epoch  28: total training loss 71.40
2020-06-16 14:55:24,856 EPOCH 29
2020-06-16 14:55:26,669 Epoch  29 Step:     1400 Batch Loss:     1.498893 Tokens per Sec:    15435, Lr: 0.000300
2020-06-16 14:55:33,267 Epoch  29: total training loss 71.55
2020-06-16 14:55:33,267 EPOCH 30
2020-06-16 14:55:41,725 Epoch  30: total training loss 68.41
2020-06-16 14:55:41,726 EPOCH 31
2020-06-16 14:55:43,584 Epoch  31 Step:     1500 Batch Loss:     2.438150 Tokens per Sec:    15961, Lr: 0.000300
2020-06-16 14:55:50,082 Epoch  31: total training loss 63.56
2020-06-16 14:55:50,083 EPOCH 32
2020-06-16 14:55:58,428 Epoch  32: total training loss 63.95
2020-06-16 14:55:58,428 EPOCH 33
2020-06-16 14:56:00,398 Epoch  33 Step:     1600 Batch Loss:     0.785270 Tokens per Sec:    15683, Lr: 0.000300
2020-06-16 14:56:06,826 Epoch  33: total training loss 64.99
2020-06-16 14:56:06,827 EPOCH 34
2020-06-16 14:56:15,254 Epoch  34: total training loss 60.68
2020-06-16 14:56:15,254 EPOCH 35
2020-06-16 14:56:17,218 Epoch  35 Step:     1700 Batch Loss:     2.001106 Tokens per Sec:    15589, Lr: 0.000300
2020-06-16 14:56:23,588 Epoch  35: total training loss 60.20
2020-06-16 14:56:23,588 EPOCH 36
2020-06-16 14:56:31,967 Epoch  36: total training loss 57.82
2020-06-16 14:56:31,968 EPOCH 37
2020-06-16 14:56:34,067 Epoch  37 Step:     1800 Batch Loss:     2.039209 Tokens per Sec:    15835, Lr: 0.000300
2020-06-16 14:56:40,411 Epoch  37: total training loss 56.67
2020-06-16 14:56:40,412 EPOCH 38
2020-06-16 14:56:48,774 Epoch  38: total training loss 53.47
2020-06-16 14:56:48,774 EPOCH 39
2020-06-16 14:56:51,045 Epoch  39 Step:     1900 Batch Loss:     1.340640 Tokens per Sec:    15329, Lr: 0.000300
2020-06-16 14:56:57,161 Epoch  39: total training loss 53.45
2020-06-16 14:56:57,161 EPOCH 40
2020-06-16 14:57:05,516 Epoch  40: total training loss 50.43
2020-06-16 14:57:05,517 EPOCH 41
2020-06-16 14:57:08,069 Epoch  41 Step:     2000 Batch Loss:     0.979465 Tokens per Sec:    16073, Lr: 0.000300
2020-06-16 14:57:36,815 Hooray! New best validation result [eval_metric]!
2020-06-16 14:57:36,816 Saving new checkpoint.
2020-06-16 14:57:38,756 Example #0
2020-06-16 14:57:38,756 	Raw source:     ['hallo', ',']
2020-06-16 14:57:38,756 	Raw hypothesis: ['hi', 'there', '.']
2020-06-16 14:57:38,756 	Source:     hallo ,
2020-06-16 14:57:38,756 	Reference:  hello .
2020-06-16 14:57:38,756 	Hypothesis: hi there .
2020-06-16 14:57:38,757 Example #1
2020-06-16 14:57:38,757 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-16 14:57:38,757 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-16 14:57:38,757 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-16 14:57:38,757 	Reference:  hi , how can i help you ?
2020-06-16 14:57:38,757 	Hypothesis: hi , how can i help you ?
2020-06-16 14:57:38,757 Example #2
2020-06-16 14:57:38,757 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-16 14:57:38,757 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'a', 'restaurant', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-16 14:57:38,757 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-16 14:57:38,757 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-16 14:57:38,757 	Hypothesis: hi , i &apos;m looking for a restaurant at a restaurant in san francisco , california .
2020-06-16 14:57:38,757 Example #3
2020-06-16 14:57:38,757 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-16 14:57:38,757 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-16 14:57:38,757 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-16 14:57:38,757 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-16 14:57:38,757 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-16 14:57:38,757 Example #4
2020-06-16 14:57:38,757 	Raw source:     ['klar', ',', 'geben', 'sie', 'mir', 'eine', 'sekunde', ',', 'ich', 'bin', 'gleich', 'zur@@', 'ü@@', 'ck', 'mit', 'ein', 'paar', 'optionen', '.']
2020-06-16 14:57:38,757 	Raw hypothesis: ['sure', ',', 'give', 'me', 'a', 'second', 'i', '&apos;m', 'in', 'the', 'total', '.']
2020-06-16 14:57:38,757 	Source:     klar , geben sie mir eine sekunde , ich bin gleich zurück mit ein paar optionen .
2020-06-16 14:57:38,757 	Reference:  sure , give me a second , i will be right back with a couple of options .
2020-06-16 14:57:38,757 	Hypothesis: sure , give me a second i &apos;m in the total .
2020-06-16 14:57:38,757 Validation result (greedy) at epoch  41, step     2000: bleu:  38.52, loss: 42434.7188, ppl:   7.8181, duration: 30.6884s
2020-06-16 14:57:44,273 Epoch  41: total training loss 50.65
2020-06-16 14:57:44,274 EPOCH 42
2020-06-16 14:57:52,474 Epoch  42: total training loss 52.01
2020-06-16 14:57:52,475 EPOCH 43
2020-06-16 14:57:54,930 Epoch  43 Step:     2100 Batch Loss:     1.601149 Tokens per Sec:    15923, Lr: 0.000300
2020-06-16 14:58:00,779 Epoch  43: total training loss 50.75
2020-06-16 14:58:00,779 EPOCH 44
2020-06-16 14:58:09,139 Epoch  44: total training loss 47.53
2020-06-16 14:58:09,140 EPOCH 45
2020-06-16 14:58:11,654 Epoch  45 Step:     2200 Batch Loss:     1.195956 Tokens per Sec:    15682, Lr: 0.000300
2020-06-16 14:58:17,468 Epoch  45: total training loss 46.71
2020-06-16 14:58:17,468 EPOCH 46
2020-06-16 14:58:25,847 Epoch  46: total training loss 45.84
2020-06-16 14:58:25,847 EPOCH 47
2020-06-16 14:58:28,357 Epoch  47 Step:     2300 Batch Loss:     1.114162 Tokens per Sec:    15284, Lr: 0.000300
2020-06-16 14:58:34,211 Epoch  47: total training loss 43.51
2020-06-16 14:58:34,211 EPOCH 48
2020-06-16 14:58:42,583 Epoch  48: total training loss 42.58
2020-06-16 14:58:42,584 EPOCH 49
2020-06-16 14:58:45,062 Epoch  49 Step:     2400 Batch Loss:     1.575391 Tokens per Sec:    15090, Lr: 0.000300
2020-06-16 14:58:50,988 Epoch  49: total training loss 43.22
2020-06-16 14:58:50,988 EPOCH 50
2020-06-16 14:58:59,315 Epoch  50: total training loss 40.85
2020-06-16 14:58:59,316 EPOCH 51
2020-06-16 14:59:01,839 Epoch  51 Step:     2500 Batch Loss:     0.968107 Tokens per Sec:    16069, Lr: 0.000300
2020-06-16 14:59:07,502 Epoch  51: total training loss 38.72
2020-06-16 14:59:07,503 EPOCH 52
2020-06-16 14:59:15,865 Epoch  52: total training loss 37.69
2020-06-16 14:59:15,866 EPOCH 53
2020-06-16 14:59:18,794 Epoch  53 Step:     2600 Batch Loss:     0.544201 Tokens per Sec:    15203, Lr: 0.000300
2020-06-16 14:59:24,288 Epoch  53: total training loss 38.13
2020-06-16 14:59:24,288 EPOCH 54
2020-06-16 14:59:32,505 Epoch  54: total training loss 36.56
2020-06-16 14:59:32,505 EPOCH 55
2020-06-16 14:59:35,502 Epoch  55 Step:     2700 Batch Loss:     0.866682 Tokens per Sec:    15936, Lr: 0.000300
2020-06-16 14:59:40,790 Epoch  55: total training loss 36.39
2020-06-16 14:59:40,791 EPOCH 56
2020-06-16 14:59:49,105 Epoch  56: total training loss 34.84
2020-06-16 14:59:49,105 EPOCH 57
2020-06-16 14:59:52,341 Epoch  57 Step:     2800 Batch Loss:     0.443431 Tokens per Sec:    15515, Lr: 0.000300
2020-06-16 14:59:57,499 Epoch  57: total training loss 35.33
2020-06-16 14:59:57,500 EPOCH 58
2020-06-16 15:00:05,726 Epoch  58: total training loss 33.47
2020-06-16 15:00:05,727 EPOCH 59
2020-06-16 15:00:09,023 Epoch  59 Step:     2900 Batch Loss:     1.041415 Tokens per Sec:    15809, Lr: 0.000300
2020-06-16 15:00:13,967 Epoch  59: total training loss 33.24
2020-06-16 15:00:13,967 EPOCH 60
2020-06-16 15:00:22,354 Epoch  60: total training loss 33.85
2020-06-16 15:00:22,355 EPOCH 61
2020-06-16 15:00:25,707 Epoch  61 Step:     3000 Batch Loss:     1.278463 Tokens per Sec:    15631, Lr: 0.000300
2020-06-16 15:01:00,437 Hooray! New best validation result [eval_metric]!
2020-06-16 15:01:00,437 Saving new checkpoint.
2020-06-16 15:01:02,440 Example #0
2020-06-16 15:01:02,440 	Raw source:     ['hallo', ',']
2020-06-16 15:01:02,440 	Raw hypothesis: ['hi', 'there', '.']
2020-06-16 15:01:02,440 	Source:     hallo ,
2020-06-16 15:01:02,440 	Reference:  hello .
2020-06-16 15:01:02,440 	Hypothesis: hi there .
2020-06-16 15:01:02,440 Example #1
2020-06-16 15:01:02,440 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-16 15:01:02,440 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-16 15:01:02,440 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-16 15:01:02,440 	Reference:  hi , how can i help you ?
2020-06-16 15:01:02,440 	Hypothesis: hi , how can i help you ?
2020-06-16 15:01:02,440 Example #2
2020-06-16 15:01:02,440 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-16 15:01:02,440 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'a', 'lunch', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-16 15:01:02,440 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-16 15:01:02,440 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-16 15:01:02,440 	Hypothesis: hi , i &apos;m looking for a restaurant at a lunch inside the arden fair mall in san francisco , california .
2020-06-16 15:01:02,440 Example #3
2020-06-16 15:01:02,440 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-16 15:01:02,440 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-16 15:01:02,440 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-16 15:01:02,440 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-16 15:01:02,440 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-16 15:01:02,440 Example #4
2020-06-16 15:01:02,441 	Raw source:     ['klar', ',', 'geben', 'sie', 'mir', 'eine', 'sekunde', ',', 'ich', 'bin', 'gleich', 'zur@@', 'ü@@', 'ck', 'mit', 'ein', 'paar', 'optionen', '.']
2020-06-16 15:01:02,441 	Raw hypothesis: ['sure', ',', 'give', 'me', 'a', 'second', 'i', '&apos;ll', 'be', 'the', 'fare', ',', 'i', '&apos;m', 'at', 'first', 'options', '.']
2020-06-16 15:01:02,441 	Source:     klar , geben sie mir eine sekunde , ich bin gleich zurück mit ein paar optionen .
2020-06-16 15:01:02,441 	Reference:  sure , give me a second , i will be right back with a couple of options .
2020-06-16 15:01:02,441 	Hypothesis: sure , give me a second i &apos;ll be the fare , i &apos;m at first options .
2020-06-16 15:01:02,441 Validation result (greedy) at epoch  61, step     3000: bleu:  39.68, loss: 43479.2422, ppl:   8.2241, duration: 36.7330s
2020-06-16 15:01:07,176 Epoch  61: total training loss 35.69
2020-06-16 15:01:07,177 EPOCH 62
2020-06-16 15:01:15,326 Epoch  62: total training loss 33.50
2020-06-16 15:01:15,326 EPOCH 63
2020-06-16 15:01:18,651 Epoch  63 Step:     3100 Batch Loss:     0.569354 Tokens per Sec:    15972, Lr: 0.000300
2020-06-16 15:01:23,614 Epoch  63: total training loss 32.10
2020-06-16 15:01:23,616 EPOCH 64
2020-06-16 15:01:31,937 Epoch  64: total training loss 30.74
2020-06-16 15:01:31,937 EPOCH 65
2020-06-16 15:01:35,280 Epoch  65 Step:     3200 Batch Loss:     0.594266 Tokens per Sec:    15857, Lr: 0.000300
2020-06-16 15:01:40,267 Epoch  65: total training loss 30.49
2020-06-16 15:01:40,267 EPOCH 66
2020-06-16 15:01:48,617 Epoch  66: total training loss 29.91
2020-06-16 15:01:48,617 EPOCH 67
2020-06-16 15:01:52,077 Epoch  67 Step:     3300 Batch Loss:     0.443545 Tokens per Sec:    15729, Lr: 0.000300
2020-06-16 15:01:56,919 Epoch  67: total training loss 28.48
2020-06-16 15:01:56,920 EPOCH 68
2020-06-16 15:02:05,233 Epoch  68: total training loss 27.49
2020-06-16 15:02:05,233 EPOCH 69
2020-06-16 15:02:08,897 Epoch  69 Step:     3400 Batch Loss:     0.708889 Tokens per Sec:    15528, Lr: 0.000300
2020-06-16 15:02:13,559 Epoch  69: total training loss 27.99
2020-06-16 15:02:13,560 EPOCH 70
2020-06-16 15:02:21,923 Epoch  70: total training loss 27.56
2020-06-16 15:02:21,923 EPOCH 71
2020-06-16 15:02:25,595 Epoch  71 Step:     3500 Batch Loss:     0.662075 Tokens per Sec:    15633, Lr: 0.000300
2020-06-16 15:02:30,200 Epoch  71: total training loss 26.85
2020-06-16 15:02:30,200 EPOCH 72
2020-06-16 15:02:38,473 Epoch  72: total training loss 26.22
2020-06-16 15:02:38,473 EPOCH 73
2020-06-16 15:02:42,462 Epoch  73 Step:     3600 Batch Loss:     0.424412 Tokens per Sec:    15712, Lr: 0.000300
2020-06-16 15:02:46,734 Epoch  73: total training loss 26.16
2020-06-16 15:02:46,735 EPOCH 74
2020-06-16 15:02:54,984 Epoch  74: total training loss 26.18
2020-06-16 15:02:54,984 EPOCH 75
2020-06-16 15:02:59,142 Epoch  75 Step:     3700 Batch Loss:     0.388710 Tokens per Sec:    15594, Lr: 0.000300
2020-06-16 15:03:03,271 Epoch  75: total training loss 25.34
2020-06-16 15:03:03,271 EPOCH 76
2020-06-16 15:03:11,503 Epoch  76: total training loss 24.55
2020-06-16 15:03:11,504 EPOCH 77
2020-06-16 15:03:15,887 Epoch  77 Step:     3800 Batch Loss:     0.396233 Tokens per Sec:    15254, Lr: 0.000300
2020-06-16 15:03:19,886 Epoch  77: total training loss 24.79
2020-06-16 15:03:19,886 EPOCH 78
2020-06-16 15:03:28,123 Epoch  78: total training loss 23.66
2020-06-16 15:03:28,123 EPOCH 79
2020-06-16 15:03:32,735 Epoch  79 Step:     3900 Batch Loss:     0.407661 Tokens per Sec:    15778, Lr: 0.000300
2020-06-16 15:03:36,413 Epoch  79: total training loss 24.28
2020-06-16 15:03:36,414 EPOCH 80
2020-06-16 15:03:44,650 Epoch  80: total training loss 23.61
2020-06-16 15:03:44,650 EPOCH 81
2020-06-16 15:03:49,358 Epoch  81 Step:     4000 Batch Loss:     0.400016 Tokens per Sec:    15600, Lr: 0.000300
2020-06-16 15:04:14,753 Hooray! New best validation result [eval_metric]!
2020-06-16 15:04:14,753 Saving new checkpoint.
2020-06-16 15:04:16,927 Example #0
2020-06-16 15:04:16,928 	Raw source:     ['hallo', ',']
2020-06-16 15:04:16,928 	Raw hypothesis: ['hi', 'there', '?']
2020-06-16 15:04:16,928 	Source:     hallo ,
2020-06-16 15:04:16,928 	Reference:  hello .
2020-06-16 15:04:16,928 	Hypothesis: hi there ?
2020-06-16 15:04:16,928 Example #1
2020-06-16 15:04:16,928 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-16 15:04:16,928 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-16 15:04:16,928 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-16 15:04:16,928 	Reference:  hi , how can i help you ?
2020-06-16 15:04:16,928 	Hypothesis: hi , how can i help you ?
2020-06-16 15:04:16,928 Example #2
2020-06-16 15:04:16,928 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-16 15:04:16,928 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'at', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-16 15:04:16,928 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-16 15:04:16,928 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-16 15:04:16,928 	Hypothesis: hi , i &apos;m looking for a restaurant at a restaurant in the arden fair mall in san francisco , california .
2020-06-16 15:04:16,928 Example #3
2020-06-16 15:04:16,928 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-16 15:04:16,928 	Raw hypothesis: ['ok', '.', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-16 15:04:16,928 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-16 15:04:16,928 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-16 15:04:16,928 	Hypothesis: ok . what type of restaurant are you looking for ?
2020-06-16 15:04:16,928 Example #4
2020-06-16 15:04:16,928 	Raw source:     ['klar', ',', 'geben', 'sie', 'mir', 'eine', 'sekunde', ',', 'ich', 'bin', 'gleich', 'zur@@', 'ü@@', 'ck', 'mit', 'ein', 'paar', 'optionen', '.']
2020-06-16 15:04:16,928 	Raw hypothesis: ['sure', ',', 'give', 'me', 'a', 'second', 'i', '&apos;m', 'okay', ',', 'the', 'fare', 'with', 'the', 'first', '.']
2020-06-16 15:04:16,928 	Source:     klar , geben sie mir eine sekunde , ich bin gleich zurück mit ein paar optionen .
2020-06-16 15:04:16,928 	Reference:  sure , give me a second , i will be right back with a couple of options .
2020-06-16 15:04:16,928 	Hypothesis: sure , give me a second i &apos;m okay , the fare with the first .
2020-06-16 15:04:16,928 Validation result (greedy) at epoch  81, step     4000: bleu:  42.18, loss: 43478.7773, ppl:   8.2239, duration: 27.5702s
2020-06-16 15:04:20,290 Epoch  81: total training loss 23.75
2020-06-16 15:04:20,290 EPOCH 82
2020-06-16 15:04:28,339 Epoch  82: total training loss 23.68
2020-06-16 15:04:28,340 EPOCH 83
2020-06-16 15:04:32,973 Epoch  83 Step:     4100 Batch Loss:     0.528908 Tokens per Sec:    15898, Lr: 0.000300
2020-06-16 15:04:36,605 Epoch  83: total training loss 23.13
2020-06-16 15:04:36,606 EPOCH 84
2020-06-16 15:04:44,844 Epoch  84: total training loss 22.65
2020-06-16 15:04:44,845 EPOCH 85
2020-06-16 15:04:49,712 Epoch  85 Step:     4200 Batch Loss:     0.456541 Tokens per Sec:    15528, Lr: 0.000300
2020-06-16 15:04:53,119 Epoch  85: total training loss 21.71
2020-06-16 15:04:53,119 EPOCH 86
2020-06-16 15:05:01,449 Epoch  86: total training loss 21.89
2020-06-16 15:05:01,449 EPOCH 87
2020-06-16 15:05:06,652 Epoch  87 Step:     4300 Batch Loss:     0.455254 Tokens per Sec:    15641, Lr: 0.000300
2020-06-16 15:05:09,795 Epoch  87: total training loss 21.01
2020-06-16 15:05:09,795 EPOCH 88
2020-06-16 15:05:18,014 Epoch  88: total training loss 21.19
2020-06-16 15:05:18,015 EPOCH 89
2020-06-16 15:05:23,410 Epoch  89 Step:     4400 Batch Loss:     0.382118 Tokens per Sec:    15782, Lr: 0.000300
2020-06-16 15:05:26,183 Epoch  89: total training loss 20.44
2020-06-16 15:05:26,183 EPOCH 90
2020-06-16 15:05:34,364 Epoch  90: total training loss 19.70
2020-06-16 15:05:34,364 EPOCH 91
2020-06-16 15:05:40,209 Epoch  91 Step:     4500 Batch Loss:     0.415388 Tokens per Sec:    15676, Lr: 0.000300
2020-06-16 15:05:42,660 Epoch  91: total training loss 20.12
2020-06-16 15:05:42,660 EPOCH 92
2020-06-16 15:05:51,041 Epoch  92: total training loss 20.18
2020-06-16 15:05:51,042 EPOCH 93
2020-06-16 15:05:56,949 Epoch  93 Step:     4600 Batch Loss:     0.346511 Tokens per Sec:    15877, Lr: 0.000300
2020-06-16 15:05:59,279 Epoch  93: total training loss 19.54
2020-06-16 15:05:59,279 EPOCH 94
2020-06-16 15:06:07,568 Epoch  94: total training loss 19.19
2020-06-16 15:06:07,568 EPOCH 95
2020-06-16 15:06:13,838 Epoch  95 Step:     4700 Batch Loss:     0.357351 Tokens per Sec:    15623, Lr: 0.000300
2020-06-16 15:06:15,836 Epoch  95: total training loss 18.87
2020-06-16 15:06:15,836 EPOCH 96
2020-06-16 15:06:24,116 Epoch  96: total training loss 18.72
2020-06-16 15:06:24,116 EPOCH 97
2020-06-16 15:06:30,595 Epoch  97 Step:     4800 Batch Loss:     0.376834 Tokens per Sec:    15567, Lr: 0.000300
2020-06-16 15:06:32,433 Epoch  97: total training loss 18.79
2020-06-16 15:06:32,434 EPOCH 98
2020-06-16 15:06:40,726 Epoch  98: total training loss 18.29
2020-06-16 15:06:40,727 EPOCH 99
2020-06-16 15:06:47,456 Epoch  99 Step:     4900 Batch Loss:     0.395352 Tokens per Sec:    15542, Lr: 0.000300
2020-06-16 15:06:49,089 Epoch  99: total training loss 18.57
2020-06-16 15:06:49,089 EPOCH 100
2020-06-16 15:06:57,450 Epoch 100: total training loss 18.59
2020-06-16 15:06:57,450 Training ended after 100 epochs.
2020-06-16 15:06:57,450 Best validation result (greedy) at step     4000:  42.18 eval_metric.
2020-06-16 15:07:17,297  dev bleu:  43.76 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-16 15:07:17,302 Translations saved to: models/transformer_iwslt14_deen/00004000.hyps.dev
2020-06-16 15:07:32,653 test bleu:  40.83 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-16 15:07:32,658 Translations saved to: models/transformer_iwslt14_deen/00004000.hyps.test
