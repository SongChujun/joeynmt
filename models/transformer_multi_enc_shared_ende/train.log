2020-06-09 15:13:10,990 Hello! This is Joey-NMT.
2020-06-09 15:13:21,754 Total params: 69450753
2020-06-09 15:13:21,757 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.0.layer_norm.bias', 'encoder_2.0.layer_norm.weight', 'encoder_2.0.layers.0.feed_forward.layer_norm.bias', 'encoder_2.0.layers.0.feed_forward.layer_norm.weight', 'encoder_2.0.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.0.layer_norm.bias', 'encoder_2.0.layers.0.layer_norm.weight', 'encoder_2.0.layers.0.src_src_att.k_layer.bias', 'encoder_2.0.layers.0.src_src_att.k_layer.weight', 'encoder_2.0.layers.0.src_src_att.output_layer.bias', 'encoder_2.0.layers.0.src_src_att.output_layer.weight', 'encoder_2.0.layers.0.src_src_att.q_layer.bias', 'encoder_2.0.layers.0.src_src_att.q_layer.weight', 'encoder_2.0.layers.0.src_src_att.v_layer.bias', 'encoder_2.0.layers.0.src_src_att.v_layer.weight', 'encoder_2.0.layers.1.feed_forward.layer_norm.bias', 'encoder_2.0.layers.1.feed_forward.layer_norm.weight', 'encoder_2.0.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.1.layer_norm.bias', 'encoder_2.0.layers.1.layer_norm.weight', 'encoder_2.0.layers.1.src_src_att.k_layer.bias', 'encoder_2.0.layers.1.src_src_att.k_layer.weight', 'encoder_2.0.layers.1.src_src_att.output_layer.bias', 'encoder_2.0.layers.1.src_src_att.output_layer.weight', 'encoder_2.0.layers.1.src_src_att.q_layer.bias', 'encoder_2.0.layers.1.src_src_att.q_layer.weight', 'encoder_2.0.layers.1.src_src_att.v_layer.bias', 'encoder_2.0.layers.1.src_src_att.v_layer.weight', 'encoder_2.0.layers.2.feed_forward.layer_norm.bias', 'encoder_2.0.layers.2.feed_forward.layer_norm.weight', 'encoder_2.0.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.2.layer_norm.bias', 'encoder_2.0.layers.2.layer_norm.weight', 'encoder_2.0.layers.2.src_src_att.k_layer.bias', 'encoder_2.0.layers.2.src_src_att.k_layer.weight', 'encoder_2.0.layers.2.src_src_att.output_layer.bias', 'encoder_2.0.layers.2.src_src_att.output_layer.weight', 'encoder_2.0.layers.2.src_src_att.q_layer.bias', 'encoder_2.0.layers.2.src_src_att.q_layer.weight', 'encoder_2.0.layers.2.src_src_att.v_layer.bias', 'encoder_2.0.layers.2.src_src_att.v_layer.weight', 'encoder_2.0.layers.3.feed_forward.layer_norm.bias', 'encoder_2.0.layers.3.feed_forward.layer_norm.weight', 'encoder_2.0.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.3.layer_norm.bias', 'encoder_2.0.layers.3.layer_norm.weight', 'encoder_2.0.layers.3.src_src_att.k_layer.bias', 'encoder_2.0.layers.3.src_src_att.k_layer.weight', 'encoder_2.0.layers.3.src_src_att.output_layer.bias', 'encoder_2.0.layers.3.src_src_att.output_layer.weight', 'encoder_2.0.layers.3.src_src_att.q_layer.bias', 'encoder_2.0.layers.3.src_src_att.q_layer.weight', 'encoder_2.0.layers.3.src_src_att.v_layer.bias', 'encoder_2.0.layers.3.src_src_att.v_layer.weight', 'encoder_2.0.layers.4.feed_forward.layer_norm.bias', 'encoder_2.0.layers.4.feed_forward.layer_norm.weight', 'encoder_2.0.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.4.layer_norm.bias', 'encoder_2.0.layers.4.layer_norm.weight', 'encoder_2.0.layers.4.src_src_att.k_layer.bias', 'encoder_2.0.layers.4.src_src_att.k_layer.weight', 'encoder_2.0.layers.4.src_src_att.output_layer.bias', 'encoder_2.0.layers.4.src_src_att.output_layer.weight', 'encoder_2.0.layers.4.src_src_att.q_layer.bias', 'encoder_2.0.layers.4.src_src_att.q_layer.weight', 'encoder_2.0.layers.4.src_src_att.v_layer.bias', 'encoder_2.0.layers.4.src_src_att.v_layer.weight', 'encoder_2.0.layers.5.feed_forward.layer_norm.bias', 'encoder_2.0.layers.5.feed_forward.layer_norm.weight', 'encoder_2.0.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.5.layer_norm.bias', 'encoder_2.0.layers.5.layer_norm.weight', 'encoder_2.0.layers.5.src_src_att.k_layer.bias', 'encoder_2.0.layers.5.src_src_att.k_layer.weight', 'encoder_2.0.layers.5.src_src_att.output_layer.bias', 'encoder_2.0.layers.5.src_src_att.output_layer.weight', 'encoder_2.0.layers.5.src_src_att.q_layer.bias', 'encoder_2.0.layers.5.src_src_att.q_layer.weight', 'encoder_2.0.layers.5.src_src_att.v_layer.bias', 'encoder_2.0.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-09 15:13:27,479 cfg.name                           : transformer_multi_enc_ende
2020-06-09 15:13:27,480 cfg.data.src                       : en
2020-06-09 15:13:27,480 cfg.data.trg                       : de
2020-06-09 15:13:27,481 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-09 15:13:27,481 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-09 15:13:27,481 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-09 15:13:27,481 cfg.data.level                     : bpe
2020-06-09 15:13:27,481 cfg.data.lowercase                 : True
2020-06-09 15:13:27,481 cfg.data.max_sent_length           : 100
2020-06-09 15:13:27,481 cfg.testing.beam_size              : 5
2020-06-09 15:13:27,481 cfg.testing.alpha                  : 1.0
2020-06-09 15:13:27,482 cfg.training.random_seed           : 42
2020-06-09 15:13:27,482 cfg.training.optimizer             : adam
2020-06-09 15:13:27,482 cfg.training.normalization         : tokens
2020-06-09 15:13:27,482 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-09 15:13:27,482 cfg.training.scheduling            : plateau
2020-06-09 15:13:27,482 cfg.training.patience              : 8
2020-06-09 15:13:27,482 cfg.training.decrease_factor       : 0.7
2020-06-09 15:13:27,482 cfg.training.loss                  : crossentropy
2020-06-09 15:13:27,483 cfg.training.learning_rate         : 0.0002
2020-06-09 15:13:27,483 cfg.training.learning_rate_min     : 1e-08
2020-06-09 15:13:27,483 cfg.training.weight_decay          : 0.0
2020-06-09 15:13:27,483 cfg.training.label_smoothing       : 0.1
2020-06-09 15:13:27,483 cfg.training.batch_size            : 4096
2020-06-09 15:13:27,483 cfg.training.batch_type            : token
2020-06-09 15:13:27,483 cfg.training.eval_batch_size       : 3600
2020-06-09 15:13:27,483 cfg.training.eval_batch_type       : token
2020-06-09 15:13:27,484 cfg.training.batch_multiplier      : 1
2020-06-09 15:13:27,484 cfg.training.early_stopping_metric : ppl
2020-06-09 15:13:27,484 cfg.training.epochs                : 100
2020-06-09 15:13:27,484 cfg.training.validation_freq       : 1000
2020-06-09 15:13:27,484 cfg.training.logging_freq          : 100
2020-06-09 15:13:27,484 cfg.training.eval_metric           : bleu
2020-06-09 15:13:27,484 cfg.training.model_dir             : models/transformer_multi_enc_shared_ende
2020-06-09 15:13:27,484 cfg.training.overwrite             : True
2020-06-09 15:13:27,485 cfg.training.shuffle               : True
2020-06-09 15:13:27,485 cfg.training.use_cuda              : True
2020-06-09 15:13:27,485 cfg.training.max_output_length     : 100
2020-06-09 15:13:27,485 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-09 15:13:27,485 cfg.training.keep_last_ckpts       : 3
2020-06-09 15:13:27,485 cfg.model.initializer              : xavier
2020-06-09 15:13:27,485 cfg.model.bias_initializer         : zeros
2020-06-09 15:13:27,485 cfg.model.init_gain                : 1.0
2020-06-09 15:13:27,486 cfg.model.embed_initializer        : xavier
2020-06-09 15:13:27,486 cfg.model.embed_init_gain          : 1.0
2020-06-09 15:13:27,486 cfg.model.tied_embeddings          : False
2020-06-09 15:13:27,486 cfg.model.tied_softmax             : True
2020-06-09 15:13:27,486 cfg.model.encoder.type             : transformer
2020-06-09 15:13:27,486 cfg.model.encoder.num_layers       : 6
2020-06-09 15:13:27,486 cfg.model.encoder.num_heads        : 8
2020-06-09 15:13:27,486 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-09 15:13:27,487 cfg.model.encoder.embeddings.scale : True
2020-06-09 15:13:27,487 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-09 15:13:27,487 cfg.model.encoder.hidden_size      : 512
2020-06-09 15:13:27,487 cfg.model.encoder.ff_size          : 2048
2020-06-09 15:13:27,487 cfg.model.encoder.dropout          : 0.1
2020-06-09 15:13:27,487 cfg.model.encoder.freeze           : False
2020-06-09 15:13:27,487 cfg.model.encoder.multi_encoder    : True
2020-06-09 15:13:27,487 cfg.model.encoder.share_ctx_encoder : True
2020-06-09 15:13:27,487 cfg.model.decoder.type             : transformer
2020-06-09 15:13:27,488 cfg.model.decoder.num_layers       : 6
2020-06-09 15:13:27,488 cfg.model.decoder.num_heads        : 8
2020-06-09 15:13:27,488 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-09 15:13:27,488 cfg.model.decoder.embeddings.scale : True
2020-06-09 15:13:27,488 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-09 15:13:27,488 cfg.model.decoder.hidden_size      : 512
2020-06-09 15:13:27,488 cfg.model.decoder.ff_size          : 2048
2020-06-09 15:13:27,488 cfg.model.decoder.dropout          : 0.1
2020-06-09 15:13:27,489 cfg.model.decoder.freeze           : False
2020-06-09 15:13:27,489 Data set sizes: 
	train 9765,
	valid 1524,
	test 1190
2020-06-09 15:13:27,489 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-09 15:13:27,489 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-09 15:13:27,489 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-09 15:13:27,490 Number of Src words (types): 4559
2020-06-09 15:13:27,490 Number of Trg words (types): 5874
2020-06-09 15:13:27,490 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4559),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5874))
2020-06-09 15:13:27,506 EPOCH 1
2020-06-09 15:13:59,331 Epoch   1: total training loss 293.12
2020-06-09 15:13:59,332 EPOCH 2
2020-06-09 15:14:25,094 Epoch   2 Step:      100 Batch Loss:     4.658690 Tokens per Sec:     4285, Lr: 0.000200
2020-06-09 15:14:30,404 Epoch   2: total training loss 264.78
2020-06-09 15:14:30,405 EPOCH 3
2020-06-09 15:15:01,133 Epoch   3: total training loss 233.94
2020-06-09 15:15:01,134 EPOCH 4
2020-06-09 15:15:21,702 Epoch   4 Step:      200 Batch Loss:     3.255641 Tokens per Sec:     4142, Lr: 0.000200
2020-06-09 15:15:32,393 Epoch   4: total training loss 226.73
2020-06-09 15:15:32,394 EPOCH 5
2020-06-09 15:16:03,428 Epoch   5: total training loss 206.23
2020-06-09 15:16:03,429 EPOCH 6
2020-06-09 15:16:17,524 Epoch   6 Step:      300 Batch Loss:     3.541682 Tokens per Sec:     4238, Lr: 0.000200
2020-06-09 15:16:34,349 Epoch   6: total training loss 190.96
2020-06-09 15:16:34,350 EPOCH 7
2020-06-09 15:17:05,181 Epoch   7: total training loss 176.97
2020-06-09 15:17:05,182 EPOCH 8
2020-06-09 15:17:13,896 Epoch   8 Step:      400 Batch Loss:     3.103115 Tokens per Sec:     4305, Lr: 0.000200
2020-06-09 15:17:36,523 Epoch   8: total training loss 168.76
2020-06-09 15:17:36,524 EPOCH 9
2020-06-09 15:18:07,289 Epoch   9: total training loss 158.03
2020-06-09 15:18:07,290 EPOCH 10
2020-06-09 15:18:09,496 Epoch  10 Step:      500 Batch Loss:     3.305931 Tokens per Sec:     4608, Lr: 0.000200
2020-06-09 15:18:38,516 Epoch  10: total training loss 147.86
2020-06-09 15:18:38,518 EPOCH 11
2020-06-09 15:19:06,759 Epoch  11 Step:      600 Batch Loss:     3.493137 Tokens per Sec:     4205, Lr: 0.000200
2020-06-09 15:19:09,614 Epoch  11: total training loss 139.34
2020-06-09 15:19:09,614 EPOCH 12
2020-06-09 15:19:40,550 Epoch  12: total training loss 128.29
2020-06-09 15:19:40,551 EPOCH 13
2020-06-09 15:20:02,741 Epoch  13 Step:      700 Batch Loss:     3.601228 Tokens per Sec:     4237, Lr: 0.000200
2020-06-09 15:20:11,398 Epoch  13: total training loss 117.12
2020-06-09 15:20:11,399 EPOCH 14
2020-06-09 15:20:42,528 Epoch  14: total training loss 113.59
2020-06-09 15:20:42,529 EPOCH 15
2020-06-09 15:20:58,296 Epoch  15 Step:      800 Batch Loss:     2.472340 Tokens per Sec:     4297, Lr: 0.000200
2020-06-09 15:21:13,250 Epoch  15: total training loss 104.37
2020-06-09 15:21:13,251 EPOCH 16
2020-06-09 15:21:44,308 Epoch  16: total training loss 92.15
2020-06-09 15:21:44,309 EPOCH 17
2020-06-09 15:21:55,040 Epoch  17 Step:      900 Batch Loss:     0.979539 Tokens per Sec:     4007, Lr: 0.000200
2020-06-09 15:22:15,806 Epoch  17: total training loss 92.66
2020-06-09 15:22:15,808 EPOCH 18
2020-06-09 15:22:46,481 Epoch  18: total training loss 82.06
2020-06-09 15:22:46,483 EPOCH 19
2020-06-09 15:22:50,238 Epoch  19 Step:     1000 Batch Loss:     0.953831 Tokens per Sec:     5230, Lr: 0.000200
2020-06-09 15:23:39,690 Hooray! New best validation result [ppl]!
2020-06-09 15:23:39,691 Saving new checkpoint.
2020-06-09 15:23:51,118 Example #0
2020-06-09 15:23:51,119 	Raw source:     ['hello', '.']
2020-06-09 15:23:51,119 	Raw hypothesis: ['hallo', '.']
2020-06-09 15:23:51,119 	Source:     hello .
2020-06-09 15:23:51,119 	Reference:  hallo ,
2020-06-09 15:23:51,119 	Hypothesis: hallo .
2020-06-09 15:23:51,119 Example #1
2020-06-09 15:23:51,119 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 15:23:51,119 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 15:23:51,119 	Source:     hi , how can i help you ?
2020-06-09 15:23:51,119 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 15:23:51,119 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 15:23:51,119 Example #2
2020-06-09 15:23:51,119 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 15:23:51,119 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'san', 'francisco', ',', 'der', 'suche', 'nach', 'einem', 'ort', 'zum', 'mittagessen', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 15:23:51,120 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 15:23:51,120 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 15:23:51,120 	Hypothesis: hallo , ich suche ein restaurant in san francisco , der suche nach einem ort zum mittagessen in san francisco , kalifornien .
2020-06-09 15:23:51,120 Example #3
2020-06-09 15:23:51,120 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 15:23:51,120 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'haben', 'sie', '?']
2020-06-09 15:23:51,120 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 15:23:51,120 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 15:23:51,120 	Hypothesis: ok , welche art von restaurant haben sie ?
2020-06-09 15:23:51,120 Validation result (greedy) at epoch  19, step     1000: bleu:  24.22, loss: 50444.5391, ppl:  10.9302, duration: 60.8805s
2020-06-09 15:24:18,604 Epoch  19: total training loss 75.56
2020-06-09 15:24:18,605 EPOCH 20
2020-06-09 15:24:48,191 Epoch  20 Step:     1100 Batch Loss:     1.614845 Tokens per Sec:     4232, Lr: 0.000200
2020-06-09 15:24:49,381 Epoch  20: total training loss 72.07
2020-06-09 15:24:49,382 EPOCH 21
2020-06-09 15:25:20,676 Epoch  21: total training loss 64.72
2020-06-09 15:25:20,677 EPOCH 22
2020-06-09 15:25:46,039 Epoch  22 Step:     1200 Batch Loss:     1.987199 Tokens per Sec:     4161, Lr: 0.000200
2020-06-09 15:25:51,768 Epoch  22: total training loss 60.22
2020-06-09 15:25:51,770 EPOCH 23
2020-06-09 15:26:22,728 Epoch  23: total training loss 62.55
2020-06-09 15:26:22,729 EPOCH 24
2020-06-09 15:26:42,184 Epoch  24 Step:     1300 Batch Loss:     0.849573 Tokens per Sec:     4120, Lr: 0.000200
2020-06-09 15:26:54,062 Epoch  24: total training loss 53.70
2020-06-09 15:26:54,063 EPOCH 25
2020-06-09 15:27:24,998 Epoch  25: total training loss 50.40
2020-06-09 15:27:24,999 EPOCH 26
2020-06-09 15:27:37,639 Epoch  26 Step:     1400 Batch Loss:     0.940393 Tokens per Sec:     4263, Lr: 0.000200
2020-06-09 15:27:56,588 Epoch  26: total training loss 46.28
2020-06-09 15:27:56,589 EPOCH 27
2020-06-09 15:28:27,910 Epoch  27: total training loss 39.90
2020-06-09 15:28:27,912 EPOCH 28
2020-06-09 15:28:35,893 Epoch  28 Step:     1500 Batch Loss:     0.777357 Tokens per Sec:     4113, Lr: 0.000200
2020-06-09 15:28:59,296 Epoch  28: total training loss 38.25
2020-06-09 15:28:59,298 EPOCH 29
2020-06-09 15:29:30,609 Epoch  29: total training loss 36.05
2020-06-09 15:29:30,610 EPOCH 30
2020-06-09 15:29:32,335 Epoch  30 Step:     1600 Batch Loss:     0.736986 Tokens per Sec:     6114, Lr: 0.000200
2020-06-09 15:30:01,293 Epoch  30: total training loss 35.36
2020-06-09 15:30:01,294 EPOCH 31
2020-06-09 15:30:29,454 Epoch  31 Step:     1700 Batch Loss:     0.337225 Tokens per Sec:     4133, Lr: 0.000200
2020-06-09 15:30:32,654 Epoch  31: total training loss 32.34
2020-06-09 15:30:32,655 EPOCH 32
2020-06-09 15:31:03,562 Epoch  32: total training loss 28.04
2020-06-09 15:31:03,563 EPOCH 33
2020-06-09 15:31:25,296 Epoch  33 Step:     1800 Batch Loss:     0.484842 Tokens per Sec:     4279, Lr: 0.000200
2020-06-09 15:31:34,726 Epoch  33: total training loss 24.71
2020-06-09 15:31:34,728 EPOCH 34
2020-06-09 15:32:05,308 Epoch  34: total training loss 22.94
2020-06-09 15:32:05,309 EPOCH 35
2020-06-09 15:32:21,198 Epoch  35 Step:     1900 Batch Loss:     0.339346 Tokens per Sec:     4264, Lr: 0.000200
2020-06-09 15:32:36,101 Epoch  35: total training loss 21.07
2020-06-09 15:32:36,103 EPOCH 36
2020-06-09 15:33:06,911 Epoch  36: total training loss 19.62
2020-06-09 15:33:06,912 EPOCH 37
2020-06-09 15:33:16,639 Epoch  37 Step:     2000 Batch Loss:     0.457245 Tokens per Sec:     4429, Lr: 0.000200
2020-06-09 15:33:45,945 Hooray! New best validation result [ppl]!
2020-06-09 15:33:45,946 Saving new checkpoint.
2020-06-09 15:33:57,285 Example #0
2020-06-09 15:33:57,286 	Raw source:     ['hello', '.']
2020-06-09 15:33:57,286 	Raw hypothesis: ['hallo', '.']
2020-06-09 15:33:57,286 	Source:     hello .
2020-06-09 15:33:57,286 	Reference:  hallo ,
2020-06-09 15:33:57,286 	Hypothesis: hallo .
2020-06-09 15:33:57,286 Example #1
2020-06-09 15:33:57,286 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 15:33:57,286 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 15:33:57,286 	Source:     hi , how can i help you ?
2020-06-09 15:33:57,286 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 15:33:57,286 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 15:33:57,286 Example #2
2020-06-09 15:33:57,286 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 15:33:57,286 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'der', 'arden', 'fair', 'mall', '.']
2020-06-09 15:33:57,286 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 15:33:57,286 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 15:33:57,287 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in der arden fair mall .
2020-06-09 15:33:57,287 Example #3
2020-06-09 15:33:57,287 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 15:33:57,287 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 15:33:57,287 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 15:33:57,287 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 15:33:57,287 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 15:33:57,287 Validation result (greedy) at epoch  37, step     2000: bleu:  34.01, loss: 48954.7344, ppl:  10.1848, duration: 40.6467s
2020-06-09 15:34:18,803 Epoch  37: total training loss 18.51
2020-06-09 15:34:18,803 EPOCH 38
2020-06-09 15:34:49,797 Epoch  38: total training loss 16.88
2020-06-09 15:34:49,798 EPOCH 39
2020-06-09 15:34:55,547 Epoch  39 Step:     2100 Batch Loss:     0.163546 Tokens per Sec:     3698, Lr: 0.000200
2020-06-09 15:35:20,681 Epoch  39: total training loss 16.25
2020-06-09 15:35:20,682 EPOCH 40
2020-06-09 15:35:50,394 Epoch  40 Step:     2200 Batch Loss:     0.242196 Tokens per Sec:     4238, Lr: 0.000200
2020-06-09 15:35:51,651 Epoch  40: total training loss 16.93
2020-06-09 15:35:51,652 EPOCH 41
2020-06-09 15:36:22,340 Epoch  41: total training loss 15.21
2020-06-09 15:36:22,341 EPOCH 42
2020-06-09 15:36:46,394 Epoch  42 Step:     2300 Batch Loss:     0.151064 Tokens per Sec:     4276, Lr: 0.000200
2020-06-09 15:36:52,804 Epoch  42: total training loss 14.40
2020-06-09 15:36:52,804 EPOCH 43
2020-06-09 15:37:23,126 Epoch  43: total training loss 13.08
2020-06-09 15:37:23,127 EPOCH 44
2020-06-09 15:37:40,825 Epoch  44 Step:     2400 Batch Loss:     0.312468 Tokens per Sec:     4583, Lr: 0.000200
2020-06-09 15:37:53,214 Epoch  44: total training loss 14.03
2020-06-09 15:37:53,215 EPOCH 45
2020-06-09 15:38:23,004 Epoch  45: total training loss 12.96
2020-06-09 15:38:23,005 EPOCH 46
2020-06-09 15:38:35,457 Epoch  46 Step:     2500 Batch Loss:     0.227463 Tokens per Sec:     4503, Lr: 0.000200
2020-06-09 15:38:53,381 Epoch  46: total training loss 11.88
2020-06-09 15:38:53,383 EPOCH 47
2020-06-09 15:39:23,544 Epoch  47: total training loss 10.91
2020-06-09 15:39:23,545 EPOCH 48
2020-06-09 15:39:30,889 Epoch  48 Step:     2600 Batch Loss:     0.264211 Tokens per Sec:     4123, Lr: 0.000200
2020-06-09 15:39:53,503 Epoch  48: total training loss 10.93
2020-06-09 15:39:53,504 EPOCH 49
2020-06-09 15:40:23,320 Epoch  49: total training loss 10.08
2020-06-09 15:40:23,321 EPOCH 50
2020-06-09 15:40:25,364 Epoch  50 Step:     2700 Batch Loss:     0.188970 Tokens per Sec:     5207, Lr: 0.000200
2020-06-09 15:40:53,665 Epoch  50: total training loss 10.13
2020-06-09 15:40:53,666 EPOCH 51
2020-06-09 15:41:20,004 Epoch  51 Step:     2800 Batch Loss:     0.169078 Tokens per Sec:     4368, Lr: 0.000200
2020-06-09 15:41:23,520 Epoch  51: total training loss 9.48
2020-06-09 15:41:23,520 EPOCH 52
2020-06-09 15:41:53,712 Epoch  52: total training loss 9.07
2020-06-09 15:41:53,712 EPOCH 53
2020-06-09 15:42:14,845 Epoch  53 Step:     2900 Batch Loss:     0.161636 Tokens per Sec:     4509, Lr: 0.000200
2020-06-09 15:42:23,798 Epoch  53: total training loss 8.96
2020-06-09 15:42:23,799 EPOCH 54
2020-06-09 15:42:54,569 Epoch  54: total training loss 8.61
2020-06-09 15:42:54,571 EPOCH 55
2020-06-09 15:43:12,344 Epoch  55 Step:     3000 Batch Loss:     0.150268 Tokens per Sec:     4193, Lr: 0.000200
2020-06-09 15:43:38,404 Example #0
2020-06-09 15:43:38,405 	Raw source:     ['hello', '.']
2020-06-09 15:43:38,405 	Raw hypothesis: ['hallo', '.']
2020-06-09 15:43:38,406 	Source:     hello .
2020-06-09 15:43:38,406 	Reference:  hallo ,
2020-06-09 15:43:38,406 	Hypothesis: hallo .
2020-06-09 15:43:38,406 Example #1
2020-06-09 15:43:38,406 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 15:43:38,406 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 15:43:38,406 	Source:     hi , how can i help you ?
2020-06-09 15:43:38,406 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 15:43:38,407 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 15:43:38,407 Example #2
2020-06-09 15:43:38,407 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 15:43:38,407 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'der', 'arden', 'fair', 'mall', '.']
2020-06-09 15:43:38,407 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 15:43:38,407 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 15:43:38,407 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in der arden fair mall .
2020-06-09 15:43:38,407 Example #3
2020-06-09 15:43:38,407 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 15:43:38,408 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 15:43:38,408 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 15:43:38,408 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 15:43:38,408 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 15:43:38,408 Validation result (greedy) at epoch  55, step     3000: bleu:  35.35, loss: 49871.5586, ppl:  10.6373, duration: 26.0621s
2020-06-09 15:43:50,850 Epoch  55: total training loss 8.26
2020-06-09 15:43:50,852 EPOCH 56
2020-06-09 15:44:20,970 Epoch  56: total training loss 8.28
2020-06-09 15:44:20,971 EPOCH 57
2020-06-09 15:44:33,103 Epoch  57 Step:     3100 Batch Loss:     0.209845 Tokens per Sec:     4197, Lr: 0.000200
2020-06-09 15:44:51,081 Epoch  57: total training loss 10.08
2020-06-09 15:44:51,082 EPOCH 58
2020-06-09 15:45:20,983 Epoch  58: total training loss 9.37
2020-06-09 15:45:20,985 EPOCH 59
2020-06-09 15:45:27,465 Epoch  59 Step:     3200 Batch Loss:     0.136758 Tokens per Sec:     4342, Lr: 0.000200
2020-06-09 15:45:50,721 Epoch  59: total training loss 8.36
2020-06-09 15:45:50,722 EPOCH 60
2020-06-09 15:46:20,924 Epoch  60: total training loss 7.90
2020-06-09 15:46:20,925 EPOCH 61
2020-06-09 15:46:21,728 Epoch  61 Step:     3300 Batch Loss:     0.140447 Tokens per Sec:     1986, Lr: 0.000200
2020-06-09 15:46:51,510 Epoch  61: total training loss 8.17
2020-06-09 15:46:51,511 EPOCH 62
2020-06-09 15:47:17,028 Epoch  62 Step:     3400 Batch Loss:     0.088362 Tokens per Sec:     4258, Lr: 0.000200
2020-06-09 15:47:21,588 Epoch  62: total training loss 7.51
2020-06-09 15:47:21,589 EPOCH 63
2020-06-09 15:47:51,445 Epoch  63: total training loss 7.13
2020-06-09 15:47:51,446 EPOCH 64
2020-06-09 15:48:11,512 Epoch  64 Step:     3500 Batch Loss:     0.120683 Tokens per Sec:     4384, Lr: 0.000200
2020-06-09 15:48:21,526 Epoch  64: total training loss 7.11
2020-06-09 15:48:21,528 EPOCH 65
2020-06-09 15:48:51,357 Epoch  65: total training loss 6.85
2020-06-09 15:48:51,358 EPOCH 66
2020-06-09 15:49:07,729 Epoch  66 Step:     3600 Batch Loss:     0.126524 Tokens per Sec:     4181, Lr: 0.000200
2020-06-09 15:49:21,088 Epoch  66: total training loss 6.92
2020-06-09 15:49:21,089 EPOCH 67
2020-06-09 15:49:51,476 Epoch  67: total training loss 7.00
2020-06-09 15:49:51,477 EPOCH 68
2020-06-09 15:50:02,963 Epoch  68 Step:     3700 Batch Loss:     0.120683 Tokens per Sec:     4264, Lr: 0.000200
2020-06-09 15:50:21,422 Epoch  68: total training loss 7.05
2020-06-09 15:50:21,423 EPOCH 69
2020-06-09 15:50:51,616 Epoch  69: total training loss 6.85
2020-06-09 15:50:51,617 EPOCH 70
2020-06-09 15:50:57,380 Epoch  70 Step:     3800 Batch Loss:     0.121897 Tokens per Sec:     4945, Lr: 0.000200
2020-06-09 15:51:22,057 Epoch  70: total training loss 6.57
2020-06-09 15:51:22,059 EPOCH 71
2020-06-09 15:51:53,267 Epoch  71: total training loss 6.88
2020-06-09 15:51:53,268 EPOCH 72
2020-06-09 15:51:53,750 Epoch  72 Step:     3900 Batch Loss:     0.120387 Tokens per Sec:     5133, Lr: 0.000200
2020-06-09 15:52:23,122 Epoch  72: total training loss 6.44
2020-06-09 15:52:23,123 EPOCH 73
2020-06-09 15:52:50,043 Epoch  73 Step:     4000 Batch Loss:     0.115842 Tokens per Sec:     4077, Lr: 0.000200
2020-06-09 15:53:17,921 Example #0
2020-06-09 15:53:17,922 	Raw source:     ['hello', '.']
2020-06-09 15:53:17,922 	Raw hypothesis: ['hallo', '.']
2020-06-09 15:53:17,922 	Source:     hello .
2020-06-09 15:53:17,922 	Reference:  hallo ,
2020-06-09 15:53:17,922 	Hypothesis: hallo .
2020-06-09 15:53:17,922 Example #1
2020-06-09 15:53:17,922 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 15:53:17,923 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 15:53:17,923 	Source:     hi , how can i help you ?
2020-06-09 15:53:17,923 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 15:53:17,923 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 15:53:17,923 Example #2
2020-06-09 15:53:17,923 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 15:53:17,923 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'der', 'arden', 'fair', 'mall', '.']
2020-06-09 15:53:17,923 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 15:53:17,923 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 15:53:17,923 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in der arden fair mall .
2020-06-09 15:53:17,923 Example #3
2020-06-09 15:53:17,923 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 15:53:17,923 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 15:53:17,923 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 15:53:17,923 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 15:53:17,923 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 15:53:17,923 Validation result (greedy) at epoch  73, step     4000: bleu:  35.24, loss: 50174.0352, ppl:  10.7909, duration: 27.8779s
2020-06-09 15:53:22,106 Epoch  73: total training loss 6.23
2020-06-09 15:53:22,106 EPOCH 74
2020-06-09 15:53:52,443 Epoch  74: total training loss 6.02
2020-06-09 15:53:52,444 EPOCH 75
2020-06-09 15:54:12,536 Epoch  75 Step:     4100 Batch Loss:     0.110146 Tokens per Sec:     4452, Lr: 0.000200
2020-06-09 15:54:22,182 Epoch  75: total training loss 6.01
2020-06-09 15:54:22,183 EPOCH 76
2020-06-09 15:54:52,493 Epoch  76: total training loss 6.14
2020-06-09 15:54:52,494 EPOCH 77
