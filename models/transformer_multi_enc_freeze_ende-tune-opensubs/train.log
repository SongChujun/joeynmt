2020-06-28 09:06:44,316 Hello! This is Joey-NMT.
2020-06-28 09:06:49,065 Total params: 67099137
2020-06-28 09:06:49,068 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-06-28 09:06:51,511 Loading model from models/wmt_ende_transformer/best.ckpt
2020-06-28 09:06:51,861 Reset optimizer.
2020-06-28 09:06:51,862 Reset scheduler.
2020-06-28 09:06:51,862 Reset tracking of the best checkpoint.
2020-06-28 09:06:51,867 cfg.name                           : transformer
2020-06-28 09:06:51,867 cfg.data.src                       : en
2020-06-28 09:06:51,867 cfg.data.trg                       : de
2020-06-28 09:06:51,867 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best_opensubs
2020-06-28 09:06:51,867 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-06-28 09:06:51,867 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-06-28 09:06:51,867 cfg.data.level                     : bpe
2020-06-28 09:06:51,867 cfg.data.lowercase                 : False
2020-06-28 09:06:51,867 cfg.data.max_sent_length           : 100
2020-06-28 09:06:51,867 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-06-28 09:06:51,867 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-06-28 09:06:51,867 cfg.testing.beam_size              : 5
2020-06-28 09:06:51,867 cfg.testing.alpha                  : 1.0
2020-06-28 09:06:51,867 cfg.training.random_seed           : 42
2020-06-28 09:06:51,867 cfg.training.optimizer             : adam
2020-06-28 09:06:51,867 cfg.training.normalization         : tokens
2020-06-28 09:06:51,867 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-28 09:06:51,867 cfg.training.scheduling            : plateau
2020-06-28 09:06:51,867 cfg.training.patience              : 8
2020-06-28 09:06:51,867 cfg.training.decrease_factor       : 0.7
2020-06-28 09:06:51,867 cfg.training.loss                  : crossentropy
2020-06-28 09:06:51,867 cfg.training.learning_rate         : 0.0002
2020-06-28 09:06:51,867 cfg.training.learning_rate_min     : 1e-08
2020-06-28 09:06:51,867 cfg.training.weight_decay          : 0.0
2020-06-28 09:06:51,867 cfg.training.label_smoothing       : 0.1
2020-06-28 09:06:51,867 cfg.training.batch_size            : 4096
2020-06-28 09:06:51,867 cfg.training.batch_type            : token
2020-06-28 09:06:51,867 cfg.training.eval_batch_size       : 3600
2020-06-28 09:06:51,867 cfg.training.eval_batch_type       : token
2020-06-28 09:06:51,867 cfg.training.batch_multiplier      : 1
2020-06-28 09:06:51,867 cfg.training.early_stopping_metric : ppl
2020-06-28 09:06:51,867 cfg.training.epochs                : 100
2020-06-28 09:06:51,867 cfg.training.validation_freq       : 1000
2020-06-28 09:06:51,867 cfg.training.logging_freq          : 100
2020-06-28 09:06:51,867 cfg.training.eval_metric           : bleu
2020-06-28 09:06:51,868 cfg.training.model_dir             : models/transformer_multi_enc_freeze_ende-tune-opensubs
2020-06-28 09:06:51,868 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-06-28 09:06:51,868 cfg.training.reset_best_ckpt       : True
2020-06-28 09:06:51,868 cfg.training.reset_scheduler       : True
2020-06-28 09:06:51,868 cfg.training.reset_optimizer       : True
2020-06-28 09:06:51,868 cfg.training.overwrite             : False
2020-06-28 09:06:51,868 cfg.training.shuffle               : True
2020-06-28 09:06:51,868 cfg.training.use_cuda              : True
2020-06-28 09:06:51,868 cfg.training.max_output_length     : 100
2020-06-28 09:06:51,868 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-28 09:06:51,868 cfg.training.keep_last_ckpts       : 3
2020-06-28 09:06:51,868 cfg.model.initializer              : xavier
2020-06-28 09:06:51,868 cfg.model.bias_initializer         : zeros
2020-06-28 09:06:51,868 cfg.model.init_gain                : 1.0
2020-06-28 09:06:51,868 cfg.model.embed_initializer        : xavier
2020-06-28 09:06:51,868 cfg.model.embed_init_gain          : 1.0
2020-06-28 09:06:51,868 cfg.model.tied_embeddings          : True
2020-06-28 09:06:51,868 cfg.model.tied_softmax             : True
2020-06-28 09:06:51,868 cfg.model.encoder.type             : transformer
2020-06-28 09:06:51,868 cfg.model.encoder.num_layers       : 6
2020-06-28 09:06:51,868 cfg.model.encoder.num_heads        : 8
2020-06-28 09:06:51,868 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-28 09:06:51,868 cfg.model.encoder.embeddings.scale : True
2020-06-28 09:06:51,868 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-28 09:06:51,868 cfg.model.encoder.hidden_size      : 512
2020-06-28 09:06:51,868 cfg.model.encoder.ff_size          : 2048
2020-06-28 09:06:51,868 cfg.model.encoder.dropout          : 0.1
2020-06-28 09:06:51,868 cfg.model.encoder.freeze           : True
2020-06-28 09:06:51,868 cfg.model.encoder.multi_encoder    : True
2020-06-28 09:06:51,868 cfg.model.decoder.type             : transformer
2020-06-28 09:06:51,868 cfg.model.decoder.num_layers       : 6
2020-06-28 09:06:51,868 cfg.model.decoder.num_heads        : 8
2020-06-28 09:06:51,868 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-28 09:06:51,868 cfg.model.decoder.embeddings.scale : True
2020-06-28 09:06:51,868 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-28 09:06:51,868 cfg.model.decoder.hidden_size      : 512
2020-06-28 09:06:51,868 cfg.model.decoder.ff_size          : 2048
2020-06-28 09:06:51,868 cfg.model.decoder.dropout          : 0.1
2020-06-28 09:06:51,868 Data set sizes: 
	train 14599,
	valid 1523,
	test 1186
2020-06-28 09:06:51,868 First training example:
	[SRC] H@@ i there@@ ! How can I hel@@ p@@ ?
	[TRG] Hal@@ lo@@ ! Wie kann ich hel@@ fen@@ ?
2020-06-28 09:06:51,869 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-28 09:06:51,869 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-28 09:06:51,869 Number of Src words (types): 36628
2020-06-28 09:06:51,869 Number of Trg words (types): 36628
2020-06-28 09:06:51,869 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-06-28 09:06:51,896 EPOCH 1
2020-06-28 09:07:31,820 Epoch   1 Step:  1360100 Batch Loss:     3.530659 Tokens per Sec:     5667, Lr: 0.000200
2020-06-28 09:07:32,557 Epoch   1: total training loss 498.21
2020-06-28 09:07:32,557 EPOCH 2
2020-06-28 09:08:11,215 Epoch   2 Step:  1360200 Batch Loss:     2.839695 Tokens per Sec:     5730, Lr: 0.000200
2020-06-28 09:08:12,609 Epoch   2: total training loss 269.90
2020-06-28 09:08:12,609 EPOCH 3
2020-06-28 09:08:50,823 Epoch   3 Step:  1360300 Batch Loss:     1.958625 Tokens per Sec:     5680, Lr: 0.000200
2020-06-28 09:08:52,855 Epoch   3: total training loss 203.77
2020-06-28 09:08:52,856 EPOCH 4
2020-06-28 09:09:31,644 Epoch   4 Step:  1360400 Batch Loss:     1.676771 Tokens per Sec:     5440, Lr: 0.000200
2020-06-28 09:09:34,625 Epoch   4: total training loss 171.99
2020-06-28 09:09:34,625 EPOCH 5
2020-06-28 09:10:13,473 Epoch   5 Step:  1360500 Batch Loss:     1.632641 Tokens per Sec:     5449, Lr: 0.000200
2020-06-28 09:10:17,071 Epoch   5: total training loss 148.63
2020-06-28 09:10:17,072 EPOCH 6
2020-06-28 09:10:55,596 Epoch   6 Step:  1360600 Batch Loss:     1.536139 Tokens per Sec:     5359, Lr: 0.000200
2020-06-28 09:10:59,902 Epoch   6: total training loss 136.11
2020-06-28 09:10:59,903 EPOCH 7
2020-06-28 09:11:37,650 Epoch   7 Step:  1360700 Batch Loss:     1.049855 Tokens per Sec:     5379, Lr: 0.000200
2020-06-28 09:11:42,555 Epoch   7: total training loss 123.45
2020-06-28 09:11:42,555 EPOCH 8
2020-06-28 09:12:20,282 Epoch   8 Step:  1360800 Batch Loss:     1.360663 Tokens per Sec:     5305, Lr: 0.000200
2020-06-28 09:12:25,720 Epoch   8: total training loss 114.91
2020-06-28 09:12:25,720 EPOCH 9
2020-06-28 09:13:01,960 Epoch   9 Step:  1360900 Batch Loss:     1.059574 Tokens per Sec:     5358, Lr: 0.000200
2020-06-28 09:13:09,141 Epoch   9: total training loss 107.37
2020-06-28 09:13:09,142 EPOCH 10
2020-06-28 09:13:43,866 Epoch  10 Step:  1361000 Batch Loss:     1.375888 Tokens per Sec:     5413, Lr: 0.000200
2020-06-28 09:14:24,909 Hooray! New best validation result [ppl]!
2020-06-28 09:14:24,910 Saving new checkpoint.
2020-06-28 09:14:33,965 Example #0
2020-06-28 09:14:33,966 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 09:14:33,966 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 09:14:33,966 	Source:     Hello.
2020-06-28 09:14:33,966 	Reference:  Hallo,
2020-06-28 09:14:33,966 	Hypothesis: Hallo.
2020-06-28 09:14:33,966 Example #1
2020-06-28 09:14:33,966 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 09:14:33,966 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 09:14:33,966 	Source:     Hi, how can I help you?
2020-06-28 09:14:33,966 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:14:33,966 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:14:33,966 Example #2
2020-06-28 09:14:33,966 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 09:14:33,966 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'im', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 09:14:33,966 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 09:14:33,966 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 09:14:33,966 	Hypothesis: Hallo, ich suche ein Restaurant im Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 09:14:33,966 Example #3
2020-06-28 09:14:33,966 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 09:14:33,966 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 09:14:33,966 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 09:14:33,966 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 09:14:33,966 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 09:14:33,966 Validation result (greedy) at epoch  10, step  1361000: bleu:  39.93, loss: 24199.4375, ppl:   2.5732, duration: 50.0994s
2020-06-28 09:14:41,906 Epoch  10: total training loss 100.99
2020-06-28 09:14:41,907 EPOCH 11
2020-06-28 09:15:15,507 Epoch  11 Step:  1361100 Batch Loss:     0.659208 Tokens per Sec:     5504, Lr: 0.000200
2020-06-28 09:15:24,171 Epoch  11: total training loss 94.07
2020-06-28 09:15:24,172 EPOCH 12
2020-06-28 09:15:58,239 Epoch  12 Step:  1361200 Batch Loss:     0.924210 Tokens per Sec:     5233, Lr: 0.000200
2020-06-28 09:16:07,285 Epoch  12: total training loss 88.92
2020-06-28 09:16:07,286 EPOCH 13
2020-06-28 09:16:39,893 Epoch  13 Step:  1361300 Batch Loss:     0.950081 Tokens per Sec:     5395, Lr: 0.000200
2020-06-28 09:16:49,699 Epoch  13: total training loss 84.61
2020-06-28 09:16:49,699 EPOCH 14
2020-06-28 09:17:19,382 Epoch  14 Step:  1361400 Batch Loss:     0.766697 Tokens per Sec:     5638, Lr: 0.000200
2020-06-28 09:17:32,226 Epoch  14: total training loss 79.54
2020-06-28 09:17:32,227 EPOCH 15
2020-06-28 09:18:02,684 Epoch  15 Step:  1361500 Batch Loss:     0.823894 Tokens per Sec:     5403, Lr: 0.000200
2020-06-28 09:18:14,832 Epoch  15: total training loss 74.06
2020-06-28 09:18:14,833 EPOCH 16
2020-06-28 09:18:44,252 Epoch  16 Step:  1361600 Batch Loss:     0.653148 Tokens per Sec:     5424, Lr: 0.000200
2020-06-28 09:18:57,396 Epoch  16: total training loss 71.03
2020-06-28 09:18:57,397 EPOCH 17
2020-06-28 09:19:26,957 Epoch  17 Step:  1361700 Batch Loss:     0.914564 Tokens per Sec:     5371, Lr: 0.000200
2020-06-28 09:19:40,129 Epoch  17: total training loss 66.56
2020-06-28 09:19:40,129 EPOCH 18
2020-06-28 09:20:09,657 Epoch  18 Step:  1361800 Batch Loss:     0.861492 Tokens per Sec:     5232, Lr: 0.000200
2020-06-28 09:20:23,000 Epoch  18: total training loss 63.69
2020-06-28 09:20:23,001 EPOCH 19
2020-06-28 09:20:50,827 Epoch  19 Step:  1361900 Batch Loss:     0.754226 Tokens per Sec:     5547, Lr: 0.000200
2020-06-28 09:21:05,580 Epoch  19: total training loss 60.50
2020-06-28 09:21:05,581 EPOCH 20
2020-06-28 09:21:33,117 Epoch  20 Step:  1362000 Batch Loss:     0.746422 Tokens per Sec:     5442, Lr: 0.000200
2020-06-28 09:22:20,072 Hooray! New best validation result [ppl]!
2020-06-28 09:22:20,072 Saving new checkpoint.
2020-06-28 09:22:29,133 Example #0
2020-06-28 09:22:29,133 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 09:22:29,133 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 09:22:29,133 	Source:     Hello.
2020-06-28 09:22:29,133 	Reference:  Hallo,
2020-06-28 09:22:29,134 	Hypothesis: Hallo.
2020-06-28 09:22:29,134 Example #1
2020-06-28 09:22:29,134 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 09:22:29,134 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 09:22:29,134 	Source:     Hi, how can I help you?
2020-06-28 09:22:29,134 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:22:29,134 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:22:29,134 Example #2
2020-06-28 09:22:29,134 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 09:22:29,134 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 09:22:29,134 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 09:22:29,134 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 09:22:29,134 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 09:22:29,134 Example #3
2020-06-28 09:22:29,134 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 09:22:29,134 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 09:22:29,134 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 09:22:29,134 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 09:22:29,134 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 09:22:29,134 Validation result (greedy) at epoch  20, step  1362000: bleu:  42.91, loss: 22039.1562, ppl:   2.3650, duration: 56.0160s
2020-06-28 09:22:43,173 Epoch  20: total training loss 57.82
2020-06-28 09:22:43,173 EPOCH 21
2020-06-28 09:23:09,232 Epoch  21 Step:  1362100 Batch Loss:     0.545619 Tokens per Sec:     5611, Lr: 0.000200
2020-06-28 09:23:25,571 Epoch  21: total training loss 54.64
2020-06-28 09:23:25,571 EPOCH 22
2020-06-28 09:23:52,849 Epoch  22 Step:  1362200 Batch Loss:     0.560539 Tokens per Sec:     5207, Lr: 0.000200
2020-06-28 09:24:08,475 Epoch  22: total training loss 52.91
2020-06-28 09:24:08,476 EPOCH 23
2020-06-28 09:24:34,779 Epoch  23 Step:  1362300 Batch Loss:     0.370105 Tokens per Sec:     5306, Lr: 0.000200
2020-06-28 09:24:51,478 Epoch  23: total training loss 49.65
2020-06-28 09:24:51,478 EPOCH 24
2020-06-28 09:25:16,498 Epoch  24 Step:  1362400 Batch Loss:     0.519882 Tokens per Sec:     5383, Lr: 0.000200
2020-06-28 09:25:33,984 Epoch  24: total training loss 47.66
2020-06-28 09:25:33,985 EPOCH 25
2020-06-28 09:25:58,588 Epoch  25 Step:  1362500 Batch Loss:     0.509310 Tokens per Sec:     5449, Lr: 0.000200
2020-06-28 09:26:16,552 Epoch  25: total training loss 45.60
2020-06-28 09:26:16,553 EPOCH 26
2020-06-28 09:26:40,789 Epoch  26 Step:  1362600 Batch Loss:     0.394511 Tokens per Sec:     5329, Lr: 0.000200
2020-06-28 09:26:59,033 Epoch  26: total training loss 43.13
2020-06-28 09:26:59,034 EPOCH 27
2020-06-28 09:27:21,740 Epoch  27 Step:  1362700 Batch Loss:     0.441941 Tokens per Sec:     5378, Lr: 0.000200
2020-06-28 09:27:41,475 Epoch  27: total training loss 42.21
2020-06-28 09:27:41,475 EPOCH 28
2020-06-28 09:28:03,576 Epoch  28 Step:  1362800 Batch Loss:     0.435744 Tokens per Sec:     5342, Lr: 0.000200
2020-06-28 09:28:23,965 Epoch  28: total training loss 39.97
2020-06-28 09:28:23,966 EPOCH 29
2020-06-28 09:28:46,266 Epoch  29 Step:  1362900 Batch Loss:     0.394072 Tokens per Sec:     5101, Lr: 0.000200
2020-06-28 09:29:06,906 Epoch  29: total training loss 38.09
2020-06-28 09:29:06,906 EPOCH 30
2020-06-28 09:29:27,890 Epoch  30 Step:  1363000 Batch Loss:     0.389387 Tokens per Sec:     5320, Lr: 0.000200
2020-06-28 09:30:16,424 Hooray! New best validation result [ppl]!
2020-06-28 09:30:16,425 Saving new checkpoint.
2020-06-28 09:30:25,313 Example #0
2020-06-28 09:30:25,314 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 09:30:25,314 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 09:30:25,314 	Source:     Hello.
2020-06-28 09:30:25,314 	Reference:  Hallo,
2020-06-28 09:30:25,314 	Hypothesis: Hallo.
2020-06-28 09:30:25,314 Example #1
2020-06-28 09:30:25,314 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 09:30:25,314 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 09:30:25,314 	Source:     Hi, how can I help you?
2020-06-28 09:30:25,314 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:30:25,315 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:30:25,315 Example #2
2020-06-28 09:30:25,315 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 09:30:25,315 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 09:30:25,315 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 09:30:25,315 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 09:30:25,315 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 09:30:25,315 Example #3
2020-06-28 09:30:25,315 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 09:30:25,315 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 09:30:25,315 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 09:30:25,315 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 09:30:25,315 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 09:30:25,315 Validation result (greedy) at epoch  30, step  1363000: bleu:  43.32, loss: 21933.1621, ppl:   2.3552, duration: 57.4244s
2020-06-28 09:30:46,269 Epoch  30: total training loss 35.84
2020-06-28 09:30:46,270 EPOCH 31
2020-06-28 09:31:04,962 Epoch  31 Step:  1363100 Batch Loss:     0.361855 Tokens per Sec:     5694, Lr: 0.000200
2020-06-28 09:31:27,634 Epoch  31: total training loss 34.71
2020-06-28 09:31:27,634 EPOCH 32
2020-06-28 09:31:45,979 Epoch  32 Step:  1363200 Batch Loss:     0.319346 Tokens per Sec:     5344, Lr: 0.000200
2020-06-28 09:32:09,662 Epoch  32: total training loss 33.72
2020-06-28 09:32:09,662 EPOCH 33
2020-06-28 09:32:26,577 Epoch  33 Step:  1363300 Batch Loss:     0.275134 Tokens per Sec:     5663, Lr: 0.000200
2020-06-28 09:32:51,522 Epoch  33: total training loss 31.78
2020-06-28 09:32:51,523 EPOCH 34
2020-06-28 09:33:08,680 Epoch  34 Step:  1363400 Batch Loss:     0.224929 Tokens per Sec:     5547, Lr: 0.000200
2020-06-28 09:33:33,301 Epoch  34: total training loss 31.05
2020-06-28 09:33:33,301 EPOCH 35
2020-06-28 09:33:50,018 Epoch  35 Step:  1363500 Batch Loss:     0.337438 Tokens per Sec:     5413, Lr: 0.000200
2020-06-28 09:34:14,953 Epoch  35: total training loss 29.76
2020-06-28 09:34:14,953 EPOCH 36
2020-06-28 09:34:31,020 Epoch  36 Step:  1363600 Batch Loss:     0.268658 Tokens per Sec:     5398, Lr: 0.000200
2020-06-28 09:34:57,589 Epoch  36: total training loss 28.69
2020-06-28 09:34:57,590 EPOCH 37
2020-06-28 09:35:12,884 Epoch  37 Step:  1363700 Batch Loss:     0.309384 Tokens per Sec:     5504, Lr: 0.000200
2020-06-28 09:35:40,072 Epoch  37: total training loss 27.32
2020-06-28 09:35:40,073 EPOCH 38
2020-06-28 09:35:53,832 Epoch  38 Step:  1363800 Batch Loss:     0.260926 Tokens per Sec:     5817, Lr: 0.000200
2020-06-28 09:36:22,926 Epoch  38: total training loss 26.39
2020-06-28 09:36:22,927 EPOCH 39
2020-06-28 09:36:35,714 Epoch  39 Step:  1363900 Batch Loss:     0.263859 Tokens per Sec:     5610, Lr: 0.000200
2020-06-28 09:37:05,017 Epoch  39: total training loss 25.31
2020-06-28 09:37:05,017 EPOCH 40
2020-06-28 09:37:17,589 Epoch  40 Step:  1364000 Batch Loss:     0.244257 Tokens per Sec:     5595, Lr: 0.000200
2020-06-28 09:38:05,428 Example #0
2020-06-28 09:38:05,428 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 09:38:05,428 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 09:38:05,428 	Source:     Hello.
2020-06-28 09:38:05,428 	Reference:  Hallo,
2020-06-28 09:38:05,428 	Hypothesis: Hallo.
2020-06-28 09:38:05,428 Example #1
2020-06-28 09:38:05,428 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 09:38:05,428 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 09:38:05,428 	Source:     Hi, how can I help you?
2020-06-28 09:38:05,428 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:38:05,428 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:38:05,428 Example #2
2020-06-28 09:38:05,428 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 09:38:05,428 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 09:38:05,428 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 09:38:05,428 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 09:38:05,429 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 09:38:05,429 Example #3
2020-06-28 09:38:05,429 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 09:38:05,429 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 09:38:05,429 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 09:38:05,429 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 09:38:05,429 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 09:38:05,429 Validation result (greedy) at epoch  40, step  1364000: bleu:  43.73, loss: 22317.7285, ppl:   2.3909, duration: 47.8393s
2020-06-28 09:38:35,869 Epoch  40: total training loss 24.28
2020-06-28 09:38:35,869 EPOCH 41
2020-06-28 09:38:48,681 Epoch  41 Step:  1364100 Batch Loss:     0.163127 Tokens per Sec:     5217, Lr: 0.000200
2020-06-28 09:39:18,664 Epoch  41: total training loss 23.55
2020-06-28 09:39:18,665 EPOCH 42
2020-06-28 09:39:30,691 Epoch  42 Step:  1364200 Batch Loss:     0.197332 Tokens per Sec:     5235, Lr: 0.000200
2020-06-28 09:40:01,080 Epoch  42: total training loss 22.68
2020-06-28 09:40:01,081 EPOCH 43
2020-06-28 09:40:12,741 Epoch  43 Step:  1364300 Batch Loss:     0.190783 Tokens per Sec:     5294, Lr: 0.000200
2020-06-28 09:40:43,331 Epoch  43: total training loss 21.84
2020-06-28 09:40:43,332 EPOCH 44
2020-06-28 09:40:55,130 Epoch  44 Step:  1364400 Batch Loss:     0.205576 Tokens per Sec:     4967, Lr: 0.000200
2020-06-28 09:41:25,917 Epoch  44: total training loss 21.17
2020-06-28 09:41:25,918 EPOCH 45
2020-06-28 09:41:35,382 Epoch  45 Step:  1364500 Batch Loss:     0.172203 Tokens per Sec:     5958, Lr: 0.000200
2020-06-28 09:42:08,506 Epoch  45: total training loss 20.60
2020-06-28 09:42:08,506 EPOCH 46
2020-06-28 09:42:18,383 Epoch  46 Step:  1364600 Batch Loss:     0.181922 Tokens per Sec:     5123, Lr: 0.000200
2020-06-28 09:42:51,813 Epoch  46: total training loss 20.10
2020-06-28 09:42:51,814 EPOCH 47
2020-06-28 09:43:01,220 Epoch  47 Step:  1364700 Batch Loss:     0.190307 Tokens per Sec:     5234, Lr: 0.000200
2020-06-28 09:43:34,662 Epoch  47: total training loss 19.26
2020-06-28 09:43:34,663 EPOCH 48
2020-06-28 09:43:42,784 Epoch  48 Step:  1364800 Batch Loss:     0.178217 Tokens per Sec:     5749, Lr: 0.000200
2020-06-28 09:44:17,099 Epoch  48: total training loss 19.29
2020-06-28 09:44:17,100 EPOCH 49
2020-06-28 09:44:24,774 Epoch  49 Step:  1364900 Batch Loss:     0.192402 Tokens per Sec:     5215, Lr: 0.000200
2020-06-28 09:44:59,717 Epoch  49: total training loss 18.41
2020-06-28 09:44:59,718 EPOCH 50
2020-06-28 09:45:07,268 Epoch  50 Step:  1365000 Batch Loss:     0.165496 Tokens per Sec:     4870, Lr: 0.000200
2020-06-28 09:45:50,639 Example #0
2020-06-28 09:45:50,639 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 09:45:50,639 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 09:45:50,639 	Source:     Hello.
2020-06-28 09:45:50,640 	Reference:  Hallo,
2020-06-28 09:45:50,640 	Hypothesis: Hallo.
2020-06-28 09:45:50,640 Example #1
2020-06-28 09:45:50,640 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 09:45:50,640 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 09:45:50,640 	Source:     Hi, how can I help you?
2020-06-28 09:45:50,640 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:45:50,640 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:45:50,640 Example #2
2020-06-28 09:45:50,640 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 09:45:50,640 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 09:45:50,640 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 09:45:50,640 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 09:45:50,640 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 09:45:50,640 Example #3
2020-06-28 09:45:50,640 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 09:45:50,640 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 09:45:50,640 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 09:45:50,640 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 09:45:50,640 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 09:45:50,640 Validation result (greedy) at epoch  50, step  1365000: bleu:  44.21, loss: 22763.0176, ppl:   2.4328, duration: 43.3709s
2020-06-28 09:46:25,895 Epoch  50: total training loss 17.98
2020-06-28 09:46:25,896 EPOCH 51
2020-06-28 09:46:32,802 Epoch  51 Step:  1365100 Batch Loss:     0.166004 Tokens per Sec:     5193, Lr: 0.000200
2020-06-28 09:47:07,480 Epoch  51: total training loss 17.65
2020-06-28 09:47:07,480 EPOCH 52
2020-06-28 09:47:13,592 Epoch  52 Step:  1365200 Batch Loss:     0.170243 Tokens per Sec:     5193, Lr: 0.000200
2020-06-28 09:47:49,023 Epoch  52: total training loss 17.22
2020-06-28 09:47:49,023 EPOCH 53
2020-06-28 09:47:53,881 Epoch  53 Step:  1365300 Batch Loss:     0.166670 Tokens per Sec:     6358, Lr: 0.000200
2020-06-28 09:48:30,822 Epoch  53: total training loss 16.91
2020-06-28 09:48:30,822 EPOCH 54
2020-06-28 09:48:34,846 Epoch  54 Step:  1365400 Batch Loss:     0.151617 Tokens per Sec:     5240, Lr: 0.000200
2020-06-28 09:49:12,802 Epoch  54: total training loss 16.91
2020-06-28 09:49:12,802 EPOCH 55
2020-06-28 09:49:14,879 Epoch  55 Step:  1365500 Batch Loss:     0.148541 Tokens per Sec:     7135, Lr: 0.000200
2020-06-28 09:49:54,614 Epoch  55: total training loss 16.18
2020-06-28 09:49:54,614 EPOCH 56
2020-06-28 09:49:56,416 Epoch  56 Step:  1365600 Batch Loss:     0.157784 Tokens per Sec:     4650, Lr: 0.000200
2020-06-28 09:50:36,896 Epoch  56: total training loss 16.01
2020-06-28 09:50:36,897 EPOCH 57
2020-06-28 09:50:37,817 Epoch  57 Step:  1365700 Batch Loss:     0.146633 Tokens per Sec:     5312, Lr: 0.000200
2020-06-28 09:51:19,661 Epoch  57 Step:  1365800 Batch Loss:     0.153094 Tokens per Sec:     5384, Lr: 0.000200
2020-06-28 09:51:19,662 Epoch  57: total training loss 15.82
2020-06-28 09:51:19,662 EPOCH 58
2020-06-28 09:52:01,291 Epoch  58 Step:  1365900 Batch Loss:     0.142453 Tokens per Sec:     5475, Lr: 0.000200
2020-06-28 09:52:01,935 Epoch  58: total training loss 14.94
2020-06-28 09:52:01,935 EPOCH 59
2020-06-28 09:52:43,609 Epoch  59 Step:  1366000 Batch Loss:     0.157069 Tokens per Sec:     5371, Lr: 0.000200
2020-06-28 09:53:27,208 Example #0
2020-06-28 09:53:27,209 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 09:53:27,209 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 09:53:27,209 	Source:     Hello.
2020-06-28 09:53:27,209 	Reference:  Hallo,
2020-06-28 09:53:27,209 	Hypothesis: Hallo.
2020-06-28 09:53:27,209 Example #1
2020-06-28 09:53:27,209 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 09:53:27,209 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 09:53:27,209 	Source:     Hi, how can I help you?
2020-06-28 09:53:27,209 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:53:27,209 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 09:53:27,209 Example #2
2020-06-28 09:53:27,210 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 09:53:27,210 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 09:53:27,210 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 09:53:27,210 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 09:53:27,210 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 09:53:27,210 Example #3
2020-06-28 09:53:27,210 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 09:53:27,210 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 09:53:27,210 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 09:53:27,210 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 09:53:27,210 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 09:53:27,210 Validation result (greedy) at epoch  59, step  1366000: bleu:  44.18, loss: 23128.2578, ppl:   2.4677, duration: 43.5999s
2020-06-28 09:53:28,399 Epoch  59: total training loss 15.13
2020-06-28 09:53:28,399 EPOCH 60
2020-06-28 09:54:09,181 Epoch  60 Step:  1366100 Batch Loss:     0.121671 Tokens per Sec:     5376, Lr: 0.000200
2020-06-28 09:54:11,194 Epoch  60: total training loss 14.53
2020-06-28 09:54:11,194 EPOCH 61
2020-06-28 09:54:51,614 Epoch  61 Step:  1366200 Batch Loss:     0.133627 Tokens per Sec:     5299, Lr: 0.000200
2020-06-28 09:54:54,115 Epoch  61: total training loss 14.36
2020-06-28 09:54:54,116 EPOCH 62
2020-06-28 09:55:34,643 Epoch  62 Step:  1366300 Batch Loss:     0.152003 Tokens per Sec:     5234, Lr: 0.000200
2020-06-28 09:55:37,135 Epoch  62: total training loss 13.92
2020-06-28 09:55:37,135 EPOCH 63
2020-06-28 09:56:15,484 Epoch  63 Step:  1366400 Batch Loss:     0.142635 Tokens per Sec:     5446, Lr: 0.000200
2020-06-28 09:56:19,982 Epoch  63: total training loss 13.59
2020-06-28 09:56:19,983 EPOCH 64
2020-06-28 09:56:57,721 Epoch  64 Step:  1366500 Batch Loss:     0.140120 Tokens per Sec:     5454, Lr: 0.000200
2020-06-28 09:57:02,132 Epoch  64: total training loss 13.26
2020-06-28 09:57:02,132 EPOCH 65
2020-06-28 09:57:40,626 Epoch  65 Step:  1366600 Batch Loss:     0.124883 Tokens per Sec:     5309, Lr: 0.000200
2020-06-28 09:57:44,993 Epoch  65: total training loss 13.01
2020-06-28 09:57:44,993 EPOCH 66
2020-06-28 09:58:21,756 Epoch  66 Step:  1366700 Batch Loss:     0.123307 Tokens per Sec:     5359, Lr: 0.000200
2020-06-28 09:58:27,743 Epoch  66: total training loss 13.02
2020-06-28 09:58:27,744 EPOCH 67
2020-06-28 09:59:03,864 Epoch  67 Step:  1366800 Batch Loss:     0.122103 Tokens per Sec:     5341, Lr: 0.000200
2020-06-28 09:59:10,419 Epoch  67: total training loss 12.68
2020-06-28 09:59:10,420 EPOCH 68
2020-06-28 09:59:45,312 Epoch  68 Step:  1366900 Batch Loss:     0.134013 Tokens per Sec:     5446, Lr: 0.000200
2020-06-28 09:59:53,324 Epoch  68: total training loss 12.48
2020-06-28 09:59:53,325 EPOCH 69
2020-06-28 10:00:27,055 Epoch  69 Step:  1367000 Batch Loss:     0.120034 Tokens per Sec:     5473, Lr: 0.000200
2020-06-28 10:01:10,037 Example #0
2020-06-28 10:01:10,037 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 10:01:10,038 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 10:01:10,038 	Source:     Hello.
2020-06-28 10:01:10,038 	Reference:  Hallo,
2020-06-28 10:01:10,038 	Hypothesis: Hallo.
2020-06-28 10:01:10,038 Example #1
2020-06-28 10:01:10,038 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 10:01:10,038 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 10:01:10,038 	Source:     Hi, how can I help you?
2020-06-28 10:01:10,038 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:01:10,038 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:01:10,038 Example #2
2020-06-28 10:01:10,038 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 10:01:10,038 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 10:01:10,038 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 10:01:10,038 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 10:01:10,038 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 10:01:10,038 Example #3
2020-06-28 10:01:10,038 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 10:01:10,038 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 10:01:10,038 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 10:01:10,038 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 10:01:10,038 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 10:01:10,038 Validation result (greedy) at epoch  69, step  1367000: bleu:  44.35, loss: 23632.0977, ppl:   2.5168, duration: 42.9822s
2020-06-28 10:01:19,560 Epoch  69: total training loss 12.46
2020-06-28 10:01:19,561 EPOCH 70
2020-06-28 10:01:53,461 Epoch  70 Step:  1367100 Batch Loss:     0.120482 Tokens per Sec:     5373, Lr: 0.000200
2020-06-28 10:02:02,677 Epoch  70: total training loss 12.07
2020-06-28 10:02:02,677 EPOCH 71
2020-06-28 10:02:36,371 Epoch  71 Step:  1367200 Batch Loss:     0.111991 Tokens per Sec:     5305, Lr: 0.000200
2020-06-28 10:02:45,534 Epoch  71: total training loss 11.86
2020-06-28 10:02:45,535 EPOCH 72
2020-06-28 10:03:16,692 Epoch  72 Step:  1367300 Batch Loss:     0.105468 Tokens per Sec:     5643, Lr: 0.000200
2020-06-28 10:03:27,351 Epoch  72: total training loss 11.75
2020-06-28 10:03:27,352 EPOCH 73
2020-06-28 10:03:57,062 Epoch  73 Step:  1367400 Batch Loss:     0.112925 Tokens per Sec:     5801, Lr: 0.000200
2020-06-28 10:04:08,353 Epoch  73: total training loss 11.31
2020-06-28 10:04:08,354 EPOCH 74
2020-06-28 10:04:39,778 Epoch  74 Step:  1367500 Batch Loss:     0.122936 Tokens per Sec:     5435, Lr: 0.000200
2020-06-28 10:04:50,277 Epoch  74: total training loss 11.31
2020-06-28 10:04:50,277 EPOCH 75
2020-06-28 10:05:20,217 Epoch  75 Step:  1367600 Batch Loss:     0.103272 Tokens per Sec:     5507, Lr: 0.000200
2020-06-28 10:05:32,461 Epoch  75: total training loss 11.37
2020-06-28 10:05:32,462 EPOCH 76
2020-06-28 10:06:01,949 Epoch  76 Step:  1367700 Batch Loss:     0.102336 Tokens per Sec:     5392, Lr: 0.000200
2020-06-28 10:06:15,157 Epoch  76: total training loss 11.04
2020-06-28 10:06:15,158 EPOCH 77
2020-06-28 10:06:43,420 Epoch  77 Step:  1367800 Batch Loss:     0.129729 Tokens per Sec:     5508, Lr: 0.000200
2020-06-28 10:06:57,850 Epoch  77: total training loss 11.03
2020-06-28 10:06:57,851 EPOCH 78
2020-06-28 10:07:26,248 Epoch  78 Step:  1367900 Batch Loss:     0.093745 Tokens per Sec:     5418, Lr: 0.000200
2020-06-28 10:07:40,635 Epoch  78: total training loss 10.65
2020-06-28 10:07:40,636 EPOCH 79
2020-06-28 10:08:08,380 Epoch  79 Step:  1368000 Batch Loss:     0.116573 Tokens per Sec:     5530, Lr: 0.000200
2020-06-28 10:08:51,586 Example #0
2020-06-28 10:08:51,586 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 10:08:51,587 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 10:08:51,587 	Source:     Hello.
2020-06-28 10:08:51,587 	Reference:  Hallo,
2020-06-28 10:08:51,587 	Hypothesis: Hallo.
2020-06-28 10:08:51,587 Example #1
2020-06-28 10:08:51,587 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 10:08:51,587 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 10:08:51,587 	Source:     Hi, how can I help you?
2020-06-28 10:08:51,587 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:08:51,587 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:08:51,587 Example #2
2020-06-28 10:08:51,587 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 10:08:51,587 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 10:08:51,587 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 10:08:51,587 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 10:08:51,587 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 10:08:51,587 Example #3
2020-06-28 10:08:51,587 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 10:08:51,587 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 10:08:51,587 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 10:08:51,587 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 10:08:51,587 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 10:08:51,587 Validation result (greedy) at epoch  79, step  1368000: bleu:  44.23, loss: 23990.6992, ppl:   2.5523, duration: 43.2062s
2020-06-28 10:09:06,495 Epoch  79: total training loss 10.60
2020-06-28 10:09:06,495 EPOCH 80
2020-06-28 10:09:33,467 Epoch  80 Step:  1368100 Batch Loss:     0.107447 Tokens per Sec:     5394, Lr: 0.000200
2020-06-28 10:09:49,767 Epoch  80: total training loss 10.61
2020-06-28 10:09:49,767 EPOCH 81
2020-06-28 10:10:15,374 Epoch  81 Step:  1368200 Batch Loss:     0.092403 Tokens per Sec:     5614, Lr: 0.000200
2020-06-28 10:10:32,124 Epoch  81: total training loss 10.33
2020-06-28 10:10:32,125 EPOCH 82
2020-06-28 10:10:58,349 Epoch  82 Step:  1368300 Batch Loss:     0.117439 Tokens per Sec:     5316, Lr: 0.000200
2020-06-28 10:11:13,978 Epoch  82: total training loss 10.35
2020-06-28 10:11:13,978 EPOCH 83
2020-06-28 10:11:37,887 Epoch  83 Step:  1368400 Batch Loss:     0.090734 Tokens per Sec:     5808, Lr: 0.000200
2020-06-28 10:11:55,004 Epoch  83: total training loss 10.02
2020-06-28 10:11:55,004 EPOCH 84
2020-06-28 10:12:20,306 Epoch  84 Step:  1368500 Batch Loss:     0.090596 Tokens per Sec:     5394, Lr: 0.000200
2020-06-28 10:12:37,382 Epoch  84: total training loss 10.00
2020-06-28 10:12:37,382 EPOCH 85
2020-06-28 10:13:01,680 Epoch  85 Step:  1368600 Batch Loss:     0.092112 Tokens per Sec:     5444, Lr: 0.000200
2020-06-28 10:13:19,350 Epoch  85: total training loss 9.71
2020-06-28 10:13:19,351 EPOCH 86
2020-06-28 10:13:43,543 Epoch  86 Step:  1368700 Batch Loss:     0.099350 Tokens per Sec:     5488, Lr: 0.000200
2020-06-28 10:14:02,190 Epoch  86: total training loss 9.67
2020-06-28 10:14:02,191 EPOCH 87
2020-06-28 10:14:25,508 Epoch  87 Step:  1368800 Batch Loss:     0.082532 Tokens per Sec:     5542, Lr: 0.000200
2020-06-28 10:14:44,709 Epoch  87: total training loss 9.35
2020-06-28 10:14:44,710 EPOCH 88
2020-06-28 10:15:07,482 Epoch  88 Step:  1368900 Batch Loss:     0.108417 Tokens per Sec:     5562, Lr: 0.000200
2020-06-28 10:15:27,292 Epoch  88: total training loss 9.57
2020-06-28 10:15:27,293 EPOCH 89
2020-06-28 10:15:50,366 Epoch  89 Step:  1369000 Batch Loss:     0.090986 Tokens per Sec:     5417, Lr: 0.000200
2020-06-28 10:16:34,616 Example #0
2020-06-28 10:16:34,616 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 10:16:34,616 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 10:16:34,616 	Source:     Hello.
2020-06-28 10:16:34,616 	Reference:  Hallo,
2020-06-28 10:16:34,616 	Hypothesis: Hallo.
2020-06-28 10:16:34,616 Example #1
2020-06-28 10:16:34,616 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 10:16:34,616 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 10:16:34,616 	Source:     Hi, how can I help you?
2020-06-28 10:16:34,616 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:16:34,616 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:16:34,616 Example #2
2020-06-28 10:16:34,616 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 10:16:34,616 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 10:16:34,616 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 10:16:34,616 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 10:16:34,616 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 10:16:34,616 Example #3
2020-06-28 10:16:34,616 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 10:16:34,616 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 10:16:34,616 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 10:16:34,617 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 10:16:34,617 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 10:16:34,617 Validation result (greedy) at epoch  89, step  1369000: bleu:  44.43, loss: 24454.2754, ppl:   2.5989, duration: 44.2492s
2020-06-28 10:16:54,583 Epoch  89: total training loss 9.41
2020-06-28 10:16:54,584 EPOCH 90
2020-06-28 10:17:16,866 Epoch  90 Step:  1369100 Batch Loss:     0.091061 Tokens per Sec:     5232, Lr: 0.000200
2020-06-28 10:17:36,955 Epoch  90: total training loss 9.29
2020-06-28 10:17:36,956 EPOCH 91
2020-06-28 10:17:58,414 Epoch  91 Step:  1369200 Batch Loss:     0.084019 Tokens per Sec:     5280, Lr: 0.000200
2020-06-28 10:18:19,520 Epoch  91: total training loss 9.08
2020-06-28 10:18:19,521 EPOCH 92
2020-06-28 10:18:39,886 Epoch  92 Step:  1369300 Batch Loss:     0.084503 Tokens per Sec:     5593, Lr: 0.000200
2020-06-28 10:19:02,057 Epoch  92: total training loss 9.06
2020-06-28 10:19:02,058 EPOCH 93
2020-06-28 10:19:21,574 Epoch  93 Step:  1369400 Batch Loss:     0.085681 Tokens per Sec:     5506, Lr: 0.000200
2020-06-28 10:19:44,640 Epoch  93: total training loss 9.47
2020-06-28 10:19:44,640 EPOCH 94
2020-06-28 10:20:04,062 Epoch  94 Step:  1369500 Batch Loss:     0.083036 Tokens per Sec:     5357, Lr: 0.000200
2020-06-28 10:20:27,144 Epoch  94: total training loss 9.00
2020-06-28 10:20:27,145 EPOCH 95
2020-06-28 10:20:45,146 Epoch  95 Step:  1369600 Batch Loss:     0.083922 Tokens per Sec:     5573, Lr: 0.000200
2020-06-28 10:21:09,815 Epoch  95: total training loss 9.01
2020-06-28 10:21:09,815 EPOCH 96
2020-06-28 10:21:27,438 Epoch  96 Step:  1369700 Batch Loss:     0.077854 Tokens per Sec:     5587, Lr: 0.000200
2020-06-28 10:21:51,632 Epoch  96: total training loss 9.53
2020-06-28 10:21:51,633 EPOCH 97
2020-06-28 10:22:08,776 Epoch  97 Step:  1369800 Batch Loss:     0.090165 Tokens per Sec:     5572, Lr: 0.000200
2020-06-28 10:22:33,345 Epoch  97: total training loss 9.21
2020-06-28 10:22:33,346 EPOCH 98
2020-06-28 10:22:50,030 Epoch  98 Step:  1369900 Batch Loss:     0.087077 Tokens per Sec:     5400, Lr: 0.000200
2020-06-28 10:23:15,738 Epoch  98: total training loss 8.88
2020-06-28 10:23:15,738 EPOCH 99
2020-06-28 10:23:32,055 Epoch  99 Step:  1370000 Batch Loss:     0.096238 Tokens per Sec:     5353, Lr: 0.000200
2020-06-28 10:24:15,384 Example #0
2020-06-28 10:24:15,384 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-28 10:24:15,384 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-28 10:24:15,384 	Source:     Hello.
2020-06-28 10:24:15,384 	Reference:  Hallo,
2020-06-28 10:24:15,384 	Hypothesis: Hallo.
2020-06-28 10:24:15,384 Example #1
2020-06-28 10:24:15,384 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-28 10:24:15,384 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-28 10:24:15,385 	Source:     Hi, how can I help you?
2020-06-28 10:24:15,385 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:24:15,385 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-28 10:24:15,385 Example #2
2020-06-28 10:24:15,385 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-28 10:24:15,385 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-28 10:24:15,385 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-28 10:24:15,385 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-28 10:24:15,385 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-28 10:24:15,385 Example #3
2020-06-28 10:24:15,385 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-28 10:24:15,385 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-28 10:24:15,385 	Source:     Ok, what type of restaurant are you looking for?
2020-06-28 10:24:15,385 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-28 10:24:15,385 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-28 10:24:15,385 Validation result (greedy) at epoch  99, step  1370000: bleu:  43.75, loss: 24406.7051, ppl:   2.5941, duration: 43.3292s
2020-06-28 10:24:41,712 Epoch  99: total training loss 8.78
2020-06-28 10:24:41,713 EPOCH 100
2020-06-28 10:24:54,992 Epoch 100 Step:  1370100 Batch Loss:     0.091516 Tokens per Sec:     5935, Lr: 0.000200
2020-06-28 10:25:24,037 Epoch 100: total training loss 8.73
2020-06-28 10:25:24,037 Training ended after 100 epochs.
2020-06-28 10:25:24,037 Best validation result (greedy) at step  1363000:   2.36 ppl.
2020-06-28 10:26:22,101  dev bleu:  45.62 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-28 10:26:22,105 Translations saved to: models/transformer_multi_enc_freeze_ende-tune-opensubs/01363000.hyps.dev
2020-06-28 10:26:47,148 test bleu:  40.48 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-28 10:26:47,152 Translations saved to: models/transformer_multi_enc_freeze_ende-tune-opensubs/01363000.hyps.test
