2020-06-21 04:25:11,539 Hello! This is Joey-NMT.
2020-06-21 04:25:18,027 Total params: 82862081
2020-06-21 04:25:18,029 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-06-21 04:25:27,653 Loading model from models/wmt_ende_transformer/best.ckpt
2020-06-21 04:25:28,462 Reset optimizer.
2020-06-21 04:25:28,462 Reset scheduler.
2020-06-21 04:25:28,462 Reset tracking of the best checkpoint.
2020-06-21 04:25:28,473 cfg.name                           : transformer
2020-06-21 04:25:28,473 cfg.data.src                       : en
2020-06-21 04:25:28,473 cfg.data.trg                       : de
2020-06-21 04:25:28,474 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best-backtranslations
2020-06-21 04:25:28,474 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-06-21 04:25:28,474 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-06-21 04:25:28,474 cfg.data.level                     : bpe
2020-06-21 04:25:28,474 cfg.data.lowercase                 : False
2020-06-21 04:25:28,474 cfg.data.max_sent_length           : 100
2020-06-21 04:25:28,474 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-06-21 04:25:28,475 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-06-21 04:25:28,475 cfg.testing.beam_size              : 5
2020-06-21 04:25:28,475 cfg.testing.alpha                  : 1.0
2020-06-21 04:25:28,475 cfg.training.random_seed           : 42
2020-06-21 04:25:28,475 cfg.training.optimizer             : adam
2020-06-21 04:25:28,475 cfg.training.normalization         : tokens
2020-06-21 04:25:28,475 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-21 04:25:28,476 cfg.training.scheduling            : plateau
2020-06-21 04:25:28,476 cfg.training.patience              : 8
2020-06-21 04:25:28,476 cfg.training.decrease_factor       : 0.7
2020-06-21 04:25:28,476 cfg.training.loss                  : crossentropy
2020-06-21 04:25:28,476 cfg.training.learning_rate         : 0.0002
2020-06-21 04:25:28,476 cfg.training.learning_rate_min     : 1e-08
2020-06-21 04:25:28,476 cfg.training.weight_decay          : 0.0
2020-06-21 04:25:28,476 cfg.training.label_smoothing       : 0.1
2020-06-21 04:25:28,477 cfg.training.batch_size            : 4096
2020-06-21 04:25:28,477 cfg.training.batch_type            : token
2020-06-21 04:25:28,477 cfg.training.eval_batch_size       : 3600
2020-06-21 04:25:28,477 cfg.training.eval_batch_type       : token
2020-06-21 04:25:28,477 cfg.training.batch_multiplier      : 1
2020-06-21 04:25:28,477 cfg.training.early_stopping_metric : ppl
2020-06-21 04:25:28,477 cfg.training.epochs                : 100
2020-06-21 04:25:28,477 cfg.training.validation_freq       : 1000
2020-06-21 04:25:28,478 cfg.training.logging_freq          : 100
2020-06-21 04:25:28,478 cfg.training.eval_metric           : bleu
2020-06-21 04:25:28,478 cfg.training.model_dir             : models/transformer_multi_enc_ende-tune-bt
2020-06-21 04:25:28,478 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-06-21 04:25:28,478 cfg.training.reset_best_ckpt       : True
2020-06-21 04:25:28,478 cfg.training.reset_scheduler       : True
2020-06-21 04:25:28,478 cfg.training.reset_optimizer       : True
2020-06-21 04:25:28,478 cfg.training.overwrite             : False
2020-06-21 04:25:28,478 cfg.training.shuffle               : True
2020-06-21 04:25:28,479 cfg.training.use_cuda              : True
2020-06-21 04:25:28,479 cfg.training.max_output_length     : 100
2020-06-21 04:25:28,479 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-21 04:25:28,479 cfg.training.keep_last_ckpts       : 3
2020-06-21 04:25:28,479 cfg.model.initializer              : xavier
2020-06-21 04:25:28,479 cfg.model.bias_initializer         : zeros
2020-06-21 04:25:28,479 cfg.model.init_gain                : 1.0
2020-06-21 04:25:28,479 cfg.model.embed_initializer        : xavier
2020-06-21 04:25:28,479 cfg.model.embed_init_gain          : 1.0
2020-06-21 04:25:28,480 cfg.model.tied_embeddings          : True
2020-06-21 04:25:28,480 cfg.model.tied_softmax             : True
2020-06-21 04:25:28,480 cfg.model.encoder.type             : transformer
2020-06-21 04:25:28,480 cfg.model.encoder.num_layers       : 6
2020-06-21 04:25:28,480 cfg.model.encoder.num_heads        : 8
2020-06-21 04:25:28,480 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-21 04:25:28,480 cfg.model.encoder.embeddings.scale : True
2020-06-21 04:25:28,480 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-21 04:25:28,480 cfg.model.encoder.hidden_size      : 512
2020-06-21 04:25:28,480 cfg.model.encoder.ff_size          : 2048
2020-06-21 04:25:28,481 cfg.model.encoder.dropout          : 0.1
2020-06-21 04:25:28,481 cfg.model.encoder.multi_encoder    : True
2020-06-21 04:25:28,481 cfg.model.decoder.type             : transformer
2020-06-21 04:25:28,481 cfg.model.decoder.num_layers       : 6
2020-06-21 04:25:28,481 cfg.model.decoder.num_heads        : 8
2020-06-21 04:25:28,481 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-21 04:25:28,481 cfg.model.decoder.embeddings.scale : True
2020-06-21 04:25:28,481 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-21 04:25:28,481 cfg.model.decoder.hidden_size      : 512
2020-06-21 04:25:28,481 cfg.model.decoder.ff_size          : 2048
2020-06-21 04:25:28,482 cfg.model.decoder.dropout          : 0.1
2020-06-21 04:25:28,482 Data set sizes: 
	train 11257,
	valid 1523,
	test 1186
2020-06-21 04:25:28,482 First training example:
	[SRC] H@@ i there@@ ! How can I hel@@ p@@ ?
	[TRG] Hal@@ lo@@ ! Wie kann ich hel@@ fen@@ ?
2020-06-21 04:25:28,482 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-21 04:25:28,482 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-06-21 04:25:28,482 Number of Src words (types): 36628
2020-06-21 04:25:28,483 Number of Trg words (types): 36628
2020-06-21 04:25:28,483 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-06-21 04:25:28,529 EPOCH 1
2020-06-21 04:26:04,136 Epoch   1: total training loss 393.71
2020-06-21 04:26:04,137 EPOCH 2
2020-06-21 04:26:16,136 Epoch   2 Step:  1360100 Batch Loss:     2.717628 Tokens per Sec:     5563, Lr: 0.000200
2020-06-21 04:26:37,931 Epoch   2: total training loss 192.89
2020-06-21 04:26:37,932 EPOCH 3
2020-06-21 04:27:02,256 Epoch   3 Step:  1360200 Batch Loss:     0.990837 Tokens per Sec:     5448, Lr: 0.000200
2020-06-21 04:27:11,704 Epoch   3: total training loss 128.09
2020-06-21 04:27:11,705 EPOCH 4
2020-06-21 04:27:45,681 Epoch   4: total training loss 104.46
2020-06-21 04:27:45,693 EPOCH 5
2020-06-21 04:27:47,683 Epoch   5 Step:  1360300 Batch Loss:     1.171139 Tokens per Sec:     6340, Lr: 0.000200
2020-06-21 04:28:19,146 Epoch   5: total training loss 89.43
2020-06-21 04:28:19,147 EPOCH 6
2020-06-21 04:28:33,018 Epoch   6 Step:  1360400 Batch Loss:     1.024426 Tokens per Sec:     5755, Lr: 0.000200
2020-06-21 04:28:52,232 Epoch   6: total training loss 79.90
2020-06-21 04:28:52,233 EPOCH 7
2020-06-21 04:29:18,624 Epoch   7 Step:  1360500 Batch Loss:     0.949028 Tokens per Sec:     5487, Lr: 0.000200
2020-06-21 04:29:25,324 Epoch   7: total training loss 71.36
2020-06-21 04:29:25,326 EPOCH 8
2020-06-21 04:29:58,753 Epoch   8: total training loss 68.15
2020-06-21 04:29:58,754 EPOCH 9
2020-06-21 04:30:03,671 Epoch   9 Step:  1360600 Batch Loss:     0.707243 Tokens per Sec:     4550, Lr: 0.000200
2020-06-21 04:30:32,357 Epoch   9: total training loss 60.94
2020-06-21 04:30:32,358 EPOCH 10
2020-06-21 04:30:49,222 Epoch  10 Step:  1360700 Batch Loss:     0.950298 Tokens per Sec:     5174, Lr: 0.000200
2020-06-21 04:31:06,205 Epoch  10: total training loss 57.82
2020-06-21 04:31:06,207 EPOCH 11
2020-06-21 04:31:34,783 Epoch  11 Step:  1360800 Batch Loss:     0.566129 Tokens per Sec:     5478, Lr: 0.000200
2020-06-21 04:31:39,417 Epoch  11: total training loss 53.08
2020-06-21 04:31:39,418 EPOCH 12
2020-06-21 04:32:13,671 Epoch  12: total training loss 50.48
2020-06-21 04:32:13,672 EPOCH 13
2020-06-21 04:32:21,205 Epoch  13 Step:  1360900 Batch Loss:     0.718222 Tokens per Sec:     5250, Lr: 0.000200
2020-06-21 04:32:46,669 Epoch  13: total training loss 47.24
2020-06-21 04:32:46,670 EPOCH 14
2020-06-21 04:33:06,280 Epoch  14 Step:  1361000 Batch Loss:     0.620738 Tokens per Sec:     5471, Lr: 0.000200
2020-06-21 04:33:46,998 Hooray! New best validation result [ppl]!
2020-06-21 04:33:46,999 Saving new checkpoint.
2020-06-21 04:34:00,359 Example #0
2020-06-21 04:34:00,360 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-21 04:34:00,360 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-21 04:34:00,360 	Source:     Hello.
2020-06-21 04:34:00,360 	Reference:  Hallo,
2020-06-21 04:34:00,360 	Hypothesis: Hallo.
2020-06-21 04:34:00,360 Example #1
2020-06-21 04:34:00,360 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-21 04:34:00,360 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-21 04:34:00,360 	Source:     Hi, how can I help you?
2020-06-21 04:34:00,360 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:34:00,360 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:34:00,360 Example #2
2020-06-21 04:34:00,360 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-21 04:34:00,360 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-21 04:34:00,360 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-21 04:34:00,360 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-21 04:34:00,360 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-21 04:34:00,360 Example #3
2020-06-21 04:34:00,360 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-21 04:34:00,360 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-21 04:34:00,360 	Source:     Ok, what type of restaurant are you looking for?
2020-06-21 04:34:00,360 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-21 04:34:00,360 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-21 04:34:00,360 Validation result (greedy) at epoch  14, step  1361000: bleu:  47.05, loss: 19112.6270, ppl:   2.1095, duration: 54.0795s
2020-06-21 04:34:14,306 Epoch  14: total training loss 44.84
2020-06-21 04:34:14,307 EPOCH 15
2020-06-21 04:34:46,950 Epoch  15 Step:  1361100 Batch Loss:     0.602250 Tokens per Sec:     5382, Lr: 0.000200
2020-06-21 04:34:48,271 Epoch  15: total training loss 43.08
2020-06-21 04:34:48,272 EPOCH 16
2020-06-21 04:35:21,406 Epoch  16: total training loss 40.40
2020-06-21 04:35:21,407 EPOCH 17
2020-06-21 04:35:32,026 Epoch  17 Step:  1361200 Batch Loss:     0.431216 Tokens per Sec:     5688, Lr: 0.000200
2020-06-21 04:35:54,427 Epoch  17: total training loss 38.91
2020-06-21 04:35:54,428 EPOCH 18
2020-06-21 04:36:17,014 Epoch  18 Step:  1361300 Batch Loss:     0.520964 Tokens per Sec:     5636, Lr: 0.000200
2020-06-21 04:36:27,567 Epoch  18: total training loss 36.69
2020-06-21 04:36:27,568 EPOCH 19
2020-06-21 04:37:01,336 Epoch  19: total training loss 35.45
2020-06-21 04:37:01,337 EPOCH 20
2020-06-21 04:37:02,739 Epoch  20 Step:  1361400 Batch Loss:     0.471273 Tokens per Sec:     5412, Lr: 0.000200
2020-06-21 04:37:35,114 Epoch  20: total training loss 34.01
2020-06-21 04:37:35,115 EPOCH 21
2020-06-21 04:37:48,048 Epoch  21 Step:  1361500 Batch Loss:     0.338084 Tokens per Sec:     5382, Lr: 0.000200
2020-06-21 04:38:08,187 Epoch  21: total training loss 32.40
2020-06-21 04:38:08,188 EPOCH 22
2020-06-21 04:38:33,680 Epoch  22 Step:  1361600 Batch Loss:     0.471576 Tokens per Sec:     5486, Lr: 0.000200
2020-06-21 04:38:41,805 Epoch  22: total training loss 31.06
2020-06-21 04:38:41,806 EPOCH 23
2020-06-21 04:39:15,204 Epoch  23: total training loss 29.95
2020-06-21 04:39:15,205 EPOCH 24
2020-06-21 04:39:18,606 Epoch  24 Step:  1361700 Batch Loss:     0.373423 Tokens per Sec:     4645, Lr: 0.000200
2020-06-21 04:39:48,896 Epoch  24: total training loss 28.41
2020-06-21 04:39:48,897 EPOCH 25
2020-06-21 04:40:04,336 Epoch  25 Step:  1361800 Batch Loss:     0.343517 Tokens per Sec:     5358, Lr: 0.000200
2020-06-21 04:40:21,967 Epoch  25: total training loss 26.73
2020-06-21 04:40:21,968 EPOCH 26
2020-06-21 04:40:49,393 Epoch  26 Step:  1361900 Batch Loss:     0.398129 Tokens per Sec:     5656, Lr: 0.000200
2020-06-21 04:40:55,301 Epoch  26: total training loss 25.90
2020-06-21 04:40:55,301 EPOCH 27
2020-06-21 04:41:29,003 Epoch  27: total training loss 25.44
2020-06-21 04:41:29,004 EPOCH 28
2020-06-21 04:41:35,041 Epoch  28 Step:  1362000 Batch Loss:     0.282111 Tokens per Sec:     6007, Lr: 0.000200
2020-06-21 04:42:15,490 Hooray! New best validation result [ppl]!
2020-06-21 04:42:15,491 Saving new checkpoint.
2020-06-21 04:42:29,060 Example #0
2020-06-21 04:42:29,061 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-21 04:42:29,061 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-21 04:42:29,061 	Source:     Hello.
2020-06-21 04:42:29,061 	Reference:  Hallo,
2020-06-21 04:42:29,061 	Hypothesis: Hallo.
2020-06-21 04:42:29,061 Example #1
2020-06-21 04:42:29,061 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-21 04:42:29,061 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-21 04:42:29,061 	Source:     Hi, how can I help you?
2020-06-21 04:42:29,061 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:42:29,061 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:42:29,061 Example #2
2020-06-21 04:42:29,061 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-21 04:42:29,061 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-21 04:42:29,061 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-21 04:42:29,061 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-21 04:42:29,061 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-21 04:42:29,061 Example #3
2020-06-21 04:42:29,061 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-21 04:42:29,061 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-21 04:42:29,061 	Source:     Ok, what type of restaurant are you looking for?
2020-06-21 04:42:29,061 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-21 04:42:29,061 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-21 04:42:29,061 Validation result (greedy) at epoch  28, step  1362000: bleu:  50.70, loss: 16555.7617, ppl:   1.9091, duration: 54.0191s
2020-06-21 04:42:55,949 Epoch  28: total training loss 23.92
2020-06-21 04:42:55,950 EPOCH 29
2020-06-21 04:43:14,871 Epoch  29 Step:  1362100 Batch Loss:     0.394686 Tokens per Sec:     5493, Lr: 0.000200
2020-06-21 04:43:28,752 Epoch  29: total training loss 23.06
2020-06-21 04:43:28,753 EPOCH 30
2020-06-21 04:44:00,074 Epoch  30 Step:  1362200 Batch Loss:     0.310327 Tokens per Sec:     5465, Lr: 0.000200
2020-06-21 04:44:02,006 Epoch  30: total training loss 22.00
2020-06-21 04:44:02,007 EPOCH 31
2020-06-21 04:44:35,406 Epoch  31: total training loss 21.31
2020-06-21 04:44:35,407 EPOCH 32
2020-06-21 04:44:46,099 Epoch  32 Step:  1362300 Batch Loss:     0.247459 Tokens per Sec:     5242, Lr: 0.000200
2020-06-21 04:45:08,619 Epoch  32: total training loss 20.48
2020-06-21 04:45:08,620 EPOCH 33
2020-06-21 04:45:31,341 Epoch  33 Step:  1362400 Batch Loss:     0.295663 Tokens per Sec:     5376, Lr: 0.000200
2020-06-21 04:45:42,665 Epoch  33: total training loss 20.08
2020-06-21 04:45:42,666 EPOCH 34
2020-06-21 04:46:16,415 Epoch  34: total training loss 19.40
2020-06-21 04:46:16,416 EPOCH 35
2020-06-21 04:46:16,580 Epoch  35 Step:  1362500 Batch Loss:     0.221776 Tokens per Sec:     5191, Lr: 0.000200
2020-06-21 04:46:49,706 Epoch  35: total training loss 18.56
2020-06-21 04:46:49,707 EPOCH 36
2020-06-21 04:47:02,171 Epoch  36 Step:  1362600 Batch Loss:     0.278559 Tokens per Sec:     5701, Lr: 0.000200
2020-06-21 04:47:23,140 Epoch  36: total training loss 17.92
2020-06-21 04:47:23,141 EPOCH 37
2020-06-21 04:47:48,158 Epoch  37 Step:  1362700 Batch Loss:     0.247070 Tokens per Sec:     5539, Lr: 0.000200
2020-06-21 04:47:56,850 Epoch  37: total training loss 17.47
2020-06-21 04:47:56,851 EPOCH 38
2020-06-21 04:48:30,212 Epoch  38: total training loss 16.70
2020-06-21 04:48:30,213 EPOCH 39
2020-06-21 04:48:34,238 Epoch  39 Step:  1362800 Batch Loss:     0.216426 Tokens per Sec:     4923, Lr: 0.000200
2020-06-21 04:49:03,512 Epoch  39: total training loss 16.31
2020-06-21 04:49:03,513 EPOCH 40
2020-06-21 04:49:18,979 Epoch  40 Step:  1362900 Batch Loss:     0.184276 Tokens per Sec:     5402, Lr: 0.000200
2020-06-21 04:49:37,273 Epoch  40: total training loss 16.20
2020-06-21 04:49:37,274 EPOCH 41
2020-06-21 04:50:04,253 Epoch  41 Step:  1363000 Batch Loss:     0.188374 Tokens per Sec:     5483, Lr: 0.000200
2020-06-21 04:50:48,549 Hooray! New best validation result [ppl]!
2020-06-21 04:50:48,550 Saving new checkpoint.
2020-06-21 04:51:02,065 Example #0
2020-06-21 04:51:02,065 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-21 04:51:02,065 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-21 04:51:02,065 	Source:     Hello.
2020-06-21 04:51:02,065 	Reference:  Hallo,
2020-06-21 04:51:02,065 	Hypothesis: Hallo.
2020-06-21 04:51:02,066 Example #1
2020-06-21 04:51:02,066 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-21 04:51:02,066 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-21 04:51:02,066 	Source:     Hi, how can I help you?
2020-06-21 04:51:02,066 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:51:02,066 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:51:02,066 Example #2
2020-06-21 04:51:02,066 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-21 04:51:02,066 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-21 04:51:02,067 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-21 04:51:02,067 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-21 04:51:02,067 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-21 04:51:02,067 Example #3
2020-06-21 04:51:02,067 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-21 04:51:02,067 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-21 04:51:02,067 	Source:     Ok, what type of restaurant are you looking for?
2020-06-21 04:51:02,067 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-21 04:51:02,067 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-21 04:51:02,068 Validation result (greedy) at epoch  41, step  1363000: bleu:  51.67, loss: 15906.2051, ppl:   1.8612, duration: 57.8120s
2020-06-21 04:51:08,836 Epoch  41: total training loss 15.55
2020-06-21 04:51:08,837 EPOCH 42
2020-06-21 04:51:42,227 Epoch  42: total training loss 14.92
2020-06-21 04:51:42,228 EPOCH 43
2020-06-21 04:51:47,457 Epoch  43 Step:  1363100 Batch Loss:     0.216932 Tokens per Sec:     6180, Lr: 0.000200
2020-06-21 04:52:15,846 Epoch  43: total training loss 14.66
2020-06-21 04:52:15,847 EPOCH 44
2020-06-21 04:52:33,007 Epoch  44 Step:  1363200 Batch Loss:     0.185845 Tokens per Sec:     5396, Lr: 0.000200
2020-06-21 04:52:49,450 Epoch  44: total training loss 14.30
2020-06-21 04:52:49,451 EPOCH 45
2020-06-21 04:53:18,887 Epoch  45 Step:  1363300 Batch Loss:     0.178544 Tokens per Sec:     5400, Lr: 0.000200
2020-06-21 04:53:23,354 Epoch  45: total training loss 13.85
2020-06-21 04:53:23,355 EPOCH 46
2020-06-21 04:53:56,980 Epoch  46: total training loss 13.38
2020-06-21 04:53:56,981 EPOCH 47
2020-06-21 04:54:04,281 Epoch  47 Step:  1363400 Batch Loss:     0.172115 Tokens per Sec:     5321, Lr: 0.000200
2020-06-21 04:54:30,233 Epoch  47: total training loss 12.91
2020-06-21 04:54:30,234 EPOCH 48
2020-06-21 04:54:49,631 Epoch  48 Step:  1363500 Batch Loss:     0.173206 Tokens per Sec:     5505, Lr: 0.000200
2020-06-21 04:55:03,785 Epoch  48: total training loss 12.36
2020-06-21 04:55:03,786 EPOCH 49
2020-06-21 04:55:34,141 Epoch  49 Step:  1363600 Batch Loss:     0.162984 Tokens per Sec:     5692, Lr: 0.000200
2020-06-21 04:55:36,476 Epoch  49: total training loss 12.20
2020-06-21 04:55:36,477 EPOCH 50
2020-06-21 04:56:10,121 Epoch  50: total training loss 11.93
2020-06-21 04:56:10,122 EPOCH 51
2020-06-21 04:56:20,193 Epoch  51 Step:  1363700 Batch Loss:     0.160758 Tokens per Sec:     5402, Lr: 0.000200
2020-06-21 04:56:43,329 Epoch  51: total training loss 11.71
2020-06-21 04:56:43,330 EPOCH 52
2020-06-21 04:57:04,664 Epoch  52 Step:  1363800 Batch Loss:     0.146164 Tokens per Sec:     5575, Lr: 0.000200
2020-06-21 04:57:16,326 Epoch  52: total training loss 11.16
2020-06-21 04:57:16,327 EPOCH 53
2020-06-21 04:57:49,840 Epoch  53 Step:  1363900 Batch Loss:     0.147640 Tokens per Sec:     5511, Lr: 0.000200
2020-06-21 04:57:49,842 Epoch  53: total training loss 11.14
2020-06-21 04:57:49,842 EPOCH 54
2020-06-21 04:58:22,971 Epoch  54: total training loss 10.68
2020-06-21 04:58:22,973 EPOCH 55
2020-06-21 04:58:35,615 Epoch  55 Step:  1364000 Batch Loss:     0.130817 Tokens per Sec:     5192, Lr: 0.000200
2020-06-21 04:59:17,391 Hooray! New best validation result [ppl]!
2020-06-21 04:59:17,393 Saving new checkpoint.
2020-06-21 04:59:30,838 Example #0
2020-06-21 04:59:30,839 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-21 04:59:30,839 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-21 04:59:30,839 	Source:     Hello.
2020-06-21 04:59:30,839 	Reference:  Hallo,
2020-06-21 04:59:30,839 	Hypothesis: Hallo.
2020-06-21 04:59:30,839 Example #1
2020-06-21 04:59:30,839 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-21 04:59:30,839 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-21 04:59:30,839 	Source:     Hi, how can I help you?
2020-06-21 04:59:30,839 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:59:30,840 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-21 04:59:30,840 Example #2
2020-06-21 04:59:30,840 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-21 04:59:30,840 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-21 04:59:30,840 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-21 04:59:30,840 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-21 04:59:30,840 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-21 04:59:30,840 Example #3
2020-06-21 04:59:30,840 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-21 04:59:30,840 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-21 04:59:30,840 	Source:     Ok, what type of restaurant are you looking for?
2020-06-21 04:59:30,840 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-21 04:59:30,840 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-21 04:59:30,840 Validation result (greedy) at epoch  55, step  1364000: bleu:  53.71, loss: 15689.9268, ppl:   1.8456, duration: 55.2216s
2020-06-21 04:59:51,438 Epoch  55: total training loss 10.46
2020-06-21 04:59:51,439 EPOCH 56
2020-06-21 05:00:16,251 Epoch  56 Step:  1364100 Batch Loss:     0.135264 Tokens per Sec:     5447, Lr: 0.000200
2020-06-21 05:00:24,733 Epoch  56: total training loss 10.22
2020-06-21 05:00:24,734 EPOCH 57
2020-06-21 05:00:57,780 Epoch  57: total training loss 9.93
2020-06-21 05:00:57,781 EPOCH 58
2020-06-21 05:01:00,333 Epoch  58 Step:  1364200 Batch Loss:     0.134686 Tokens per Sec:     7727, Lr: 0.000200
2020-06-21 05:01:31,576 Epoch  58: total training loss 9.60
2020-06-21 05:01:31,577 EPOCH 59
2020-06-21 05:01:48,171 Epoch  59 Step:  1364300 Batch Loss:     0.114714 Tokens per Sec:     5126, Lr: 0.000200
2020-06-21 05:02:06,017 Epoch  59: total training loss 9.61
2020-06-21 05:02:06,018 EPOCH 60
2020-06-21 05:02:33,675 Epoch  60 Step:  1364400 Batch Loss:     0.125250 Tokens per Sec:     5495, Lr: 0.000200
2020-06-21 05:02:39,379 Epoch  60: total training loss 9.33
2020-06-21 05:02:39,380 EPOCH 61
2020-06-21 05:03:12,709 Epoch  61: total training loss 9.06
2020-06-21 05:03:12,710 EPOCH 62
2020-06-21 05:03:19,155 Epoch  62 Step:  1364500 Batch Loss:     0.124184 Tokens per Sec:     5401, Lr: 0.000200
2020-06-21 05:03:45,379 Epoch  62: total training loss 8.84
2020-06-21 05:03:45,380 EPOCH 63
2020-06-21 05:04:04,267 Epoch  63 Step:  1364600 Batch Loss:     0.117033 Tokens per Sec:     5617, Lr: 0.000200
2020-06-21 05:04:18,755 Epoch  63: total training loss 8.85
2020-06-21 05:04:18,756 EPOCH 64
2020-06-21 05:04:49,540 Epoch  64 Step:  1364700 Batch Loss:     0.122276 Tokens per Sec:     5428, Lr: 0.000200
2020-06-21 05:04:51,781 Epoch  64: total training loss 8.56
2020-06-21 05:04:51,782 EPOCH 65
2020-06-21 05:05:25,278 Epoch  65: total training loss 8.47
2020-06-21 05:05:25,279 EPOCH 66
2020-06-21 05:05:35,109 Epoch  66 Step:  1364800 Batch Loss:     0.119921 Tokens per Sec:     4881, Lr: 0.000200
2020-06-21 05:05:58,794 Epoch  66: total training loss 8.36
2020-06-21 05:05:58,795 EPOCH 67
2020-06-21 05:06:20,138 Epoch  67 Step:  1364900 Batch Loss:     0.111044 Tokens per Sec:     5617, Lr: 0.000200
2020-06-21 05:06:32,075 Epoch  67: total training loss 8.17
2020-06-21 05:06:32,076 EPOCH 68
2020-06-21 05:07:05,196 Epoch  68: total training loss 7.81
2020-06-21 05:07:05,197 EPOCH 69
2020-06-21 05:07:05,525 Epoch  69 Step:  1365000 Batch Loss:     0.103765 Tokens per Sec:     8412, Lr: 0.000200
2020-06-21 05:07:47,455 Example #0
2020-06-21 05:07:47,456 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-21 05:07:47,456 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-21 05:07:47,456 	Source:     Hello.
2020-06-21 05:07:47,456 	Reference:  Hallo,
2020-06-21 05:07:47,456 	Hypothesis: Hallo.
2020-06-21 05:07:47,457 Example #1
2020-06-21 05:07:47,457 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-21 05:07:47,457 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-21 05:07:47,457 	Source:     Hi, how can I help you?
2020-06-21 05:07:47,457 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-21 05:07:47,457 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-21 05:07:47,457 Example #2
2020-06-21 05:07:47,457 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-21 05:07:47,457 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-21 05:07:47,458 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-21 05:07:47,458 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-21 05:07:47,458 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-21 05:07:47,458 Example #3
2020-06-21 05:07:47,458 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-21 05:07:47,458 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-21 05:07:47,458 	Source:     Ok, what type of restaurant are you looking for?
2020-06-21 05:07:47,458 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-21 05:07:47,459 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-21 05:07:47,459 Validation result (greedy) at epoch  69, step  1365000: bleu:  53.81, loss: 15834.3984, ppl:   1.8560, duration: 41.9331s
2020-06-21 05:08:20,510 Epoch  69: total training loss 7.87
2020-06-21 05:08:20,511 EPOCH 70
2020-06-21 05:08:32,974 Epoch  70 Step:  1365100 Batch Loss:     0.100473 Tokens per Sec:     5417, Lr: 0.000200
2020-06-21 05:08:53,972 Epoch  70: total training loss 7.74
2020-06-21 05:08:53,973 EPOCH 71
2020-06-21 05:09:18,683 Epoch  71 Step:  1365200 Batch Loss:     0.120755 Tokens per Sec:     5476, Lr: 0.000200
2020-06-21 05:09:27,828 Epoch  71: total training loss 7.54
2020-06-21 05:09:27,828 EPOCH 72
2020-06-21 05:10:01,410 Epoch  72: total training loss 7.50
2020-06-21 05:10:01,411 EPOCH 73
2020-06-21 05:10:04,133 Epoch  73 Step:  1365300 Batch Loss:     0.095482 Tokens per Sec:     5943, Lr: 0.000200
2020-06-21 05:10:34,017 Epoch  73: total training loss 7.70
2020-06-21 05:10:34,018 EPOCH 74
2020-06-21 05:10:48,320 Epoch  74 Step:  1365400 Batch Loss:     0.108361 Tokens per Sec:     5538, Lr: 0.000200
2020-06-21 05:11:07,268 Epoch  74: total training loss 7.39
2020-06-21 05:11:07,269 EPOCH 75
2020-06-21 05:11:33,653 Epoch  75 Step:  1365500 Batch Loss:     0.097460 Tokens per Sec:     5524, Lr: 0.000200
2020-06-21 05:11:40,457 Epoch  75: total training loss 7.07
2020-06-21 05:11:40,457 EPOCH 76
2020-06-21 05:12:13,761 Epoch  76: total training loss 7.00
2020-06-21 05:12:13,761 EPOCH 77
2020-06-21 05:12:19,553 Epoch  77 Step:  1365600 Batch Loss:     0.088742 Tokens per Sec:     5291, Lr: 0.000200
2020-06-21 05:12:47,059 Epoch  77: total training loss 6.81
2020-06-21 05:12:47,060 EPOCH 78
2020-06-21 05:13:05,863 Epoch  78 Step:  1365700 Batch Loss:     0.104245 Tokens per Sec:     5244, Lr: 0.000200
2020-06-21 05:13:21,013 Epoch  78: total training loss 6.87
2020-06-21 05:13:21,014 EPOCH 79
2020-06-21 05:13:49,843 Epoch  79 Step:  1365800 Batch Loss:     0.085787 Tokens per Sec:     5725, Lr: 0.000200
2020-06-21 05:13:54,195 Epoch  79: total training loss 6.65
2020-06-21 05:13:54,196 EPOCH 80
2020-06-21 05:14:27,536 Epoch  80: total training loss 6.60
2020-06-21 05:14:27,537 EPOCH 81
2020-06-21 05:14:35,374 Epoch  81 Step:  1365900 Batch Loss:     0.085629 Tokens per Sec:     5988, Lr: 0.000200
2020-06-21 05:15:00,349 Epoch  81: total training loss 6.46
2020-06-21 05:15:00,350 EPOCH 82
2020-06-21 05:15:21,354 Epoch  82 Step:  1366000 Batch Loss:     0.077876 Tokens per Sec:     5459, Lr: 0.000200
2020-06-21 05:16:03,134 Example #0
2020-06-21 05:16:03,135 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-21 05:16:03,135 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-21 05:16:03,135 	Source:     Hello.
2020-06-21 05:16:03,135 	Reference:  Hallo,
2020-06-21 05:16:03,135 	Hypothesis: Hallo.
2020-06-21 05:16:03,135 Example #1
2020-06-21 05:16:03,135 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-21 05:16:03,135 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-21 05:16:03,135 	Source:     Hi, how can I help you?
2020-06-21 05:16:03,135 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-21 05:16:03,135 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-21 05:16:03,135 Example #2
2020-06-21 05:16:03,135 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-21 05:16:03,135 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-21 05:16:03,135 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-21 05:16:03,135 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-21 05:16:03,135 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-21 05:16:03,135 Example #3
2020-06-21 05:16:03,135 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-21 05:16:03,136 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-21 05:16:03,136 	Source:     Ok, what type of restaurant are you looking for?
2020-06-21 05:16:03,136 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-21 05:16:03,136 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-21 05:16:03,136 Validation result (greedy) at epoch  82, step  1366000: bleu:  54.63, loss: 16135.4141, ppl:   1.8780, duration: 41.7796s
2020-06-21 05:16:15,464 Epoch  82: total training loss 6.53
2020-06-21 05:16:15,464 EPOCH 83
2020-06-21 05:16:47,586 Epoch  83 Step:  1366100 Batch Loss:     0.081520 Tokens per Sec:     5525, Lr: 0.000200
2020-06-21 05:16:49,089 Epoch  83: total training loss 6.42
2020-06-21 05:16:49,089 EPOCH 84
2020-06-21 05:17:22,300 Epoch  84: total training loss 6.25
2020-06-21 05:17:22,301 EPOCH 85
2020-06-21 05:17:33,452 Epoch  85 Step:  1366200 Batch Loss:     0.074669 Tokens per Sec:     5606, Lr: 0.000200
2020-06-21 05:17:55,855 Epoch  85: total training loss 6.19
2020-06-21 05:17:55,856 EPOCH 86
2020-06-21 05:18:19,643 Epoch  86 Step:  1366300 Batch Loss:     0.078298 Tokens per Sec:     5459, Lr: 0.000200
2020-06-21 05:18:29,048 Epoch  86: total training loss 6.11
2020-06-21 05:18:29,049 EPOCH 87
2020-06-21 05:19:02,530 Epoch  87: total training loss 6.09
2020-06-21 05:19:02,531 EPOCH 88
2020-06-21 05:19:03,910 Epoch  88 Step:  1366400 Batch Loss:     0.092200 Tokens per Sec:     8062, Lr: 0.000200
2020-06-21 05:19:35,968 Epoch  88: total training loss 6.14
2020-06-21 05:19:35,969 EPOCH 89
2020-06-21 05:19:48,933 Epoch  89 Step:  1366500 Batch Loss:     0.107798 Tokens per Sec:     5654, Lr: 0.000200
2020-06-21 05:20:09,141 Epoch  89: total training loss 5.97
2020-06-21 05:20:09,142 EPOCH 90
2020-06-21 05:20:34,281 Epoch  90 Step:  1366600 Batch Loss:     0.099460 Tokens per Sec:     5522, Lr: 0.000200
2020-06-21 05:20:42,446 Epoch  90: total training loss 5.85
2020-06-21 05:20:42,451 EPOCH 91
2020-06-21 05:21:16,027 Epoch  91: total training loss 5.70
2020-06-21 05:21:16,028 EPOCH 92
2020-06-21 05:21:20,286 Epoch  92 Step:  1366700 Batch Loss:     0.080797 Tokens per Sec:     5753, Lr: 0.000200
2020-06-21 05:21:49,309 Epoch  92: total training loss 5.66
2020-06-21 05:21:49,309 EPOCH 93
2020-06-21 05:22:04,705 Epoch  93 Step:  1366800 Batch Loss:     0.072801 Tokens per Sec:     5793, Lr: 0.000200
2020-06-21 05:22:22,775 Epoch  93: total training loss 5.68
2020-06-21 05:22:22,776 EPOCH 94
2020-06-21 05:22:50,818 Epoch  94 Step:  1366900 Batch Loss:     0.079742 Tokens per Sec:     5401, Lr: 0.000200
2020-06-21 05:22:56,161 Epoch  94: total training loss 5.57
2020-06-21 05:22:56,162 EPOCH 95
2020-06-21 05:23:29,219 Epoch  95: total training loss 5.47
2020-06-21 05:23:29,220 EPOCH 96
2020-06-21 05:23:35,427 Epoch  96 Step:  1367000 Batch Loss:     0.063165 Tokens per Sec:     5567, Lr: 0.000200
2020-06-21 05:24:16,838 Example #0
2020-06-21 05:24:16,839 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-06-21 05:24:16,839 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-06-21 05:24:16,840 	Source:     Hello.
2020-06-21 05:24:16,840 	Reference:  Hallo,
2020-06-21 05:24:16,840 	Hypothesis: Hallo.
2020-06-21 05:24:16,840 Example #1
2020-06-21 05:24:16,840 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-06-21 05:24:16,840 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-06-21 05:24:16,840 	Source:     Hi, how can I help you?
2020-06-21 05:24:16,840 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-06-21 05:24:16,840 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-06-21 05:24:16,841 Example #2
2020-06-21 05:24:16,841 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-06-21 05:24:16,841 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-06-21 05:24:16,841 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-06-21 05:24:16,841 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-06-21 05:24:16,841 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-06-21 05:24:16,841 Example #3
2020-06-21 05:24:16,841 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-06-21 05:24:16,841 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-06-21 05:24:16,842 	Source:     Ok, what type of restaurant are you looking for?
2020-06-21 05:24:16,842 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-06-21 05:24:16,842 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-06-21 05:24:16,842 Validation result (greedy) at epoch  96, step  1367000: bleu:  54.68, loss: 16129.2168, ppl:   1.8775, duration: 41.4132s
2020-06-21 05:24:44,040 Epoch  96: total training loss 5.47
2020-06-21 05:24:44,041 EPOCH 97
2020-06-21 05:25:02,924 Epoch  97 Step:  1367100 Batch Loss:     0.064357 Tokens per Sec:     5361, Lr: 0.000200
2020-06-21 05:25:17,608 Epoch  97: total training loss 5.35
2020-06-21 05:25:17,609 EPOCH 98
2020-06-21 05:25:47,633 Epoch  98 Step:  1367200 Batch Loss:     0.064691 Tokens per Sec:     5648, Lr: 0.000200
2020-06-21 05:25:50,872 Epoch  98: total training loss 5.30
2020-06-21 05:25:50,872 EPOCH 99
2020-06-21 05:26:25,141 Epoch  99: total training loss 5.41
2020-06-21 05:26:25,142 EPOCH 100
2020-06-21 05:26:34,495 Epoch 100 Step:  1367300 Batch Loss:     0.064625 Tokens per Sec:     5498, Lr: 0.000200
2020-06-21 05:26:58,256 Epoch 100: total training loss 5.33
2020-06-21 05:26:58,257 Training ended after 100 epochs.
2020-06-21 05:26:58,257 Best validation result (greedy) at step  1364000:   1.85 ppl.
2020-06-21 05:28:19,067  dev bleu:  54.89 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-21 05:28:19,076 Translations saved to: models/transformer_multi_enc_ende-tune-bt/01364000.hyps.dev
2020-06-21 05:28:49,317 test bleu:  42.15 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-21 05:28:49,324 Translations saved to: models/transformer_multi_enc_ende-tune-bt/01364000.hyps.test
