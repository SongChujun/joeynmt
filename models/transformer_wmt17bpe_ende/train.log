2020-07-02 03:52:34,268 Hello! This is Joey-NMT.
2020-07-02 03:52:38,999 Total params: 49382400
2020-07-02 03:52:39,000 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-07-02 03:52:40,617 cfg.name                           : transformer
2020-07-02 03:52:40,617 cfg.data.src                       : en
2020-07-02 03:52:40,617 cfg.data.trg                       : de
2020-07-02 03:52:40,617 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-02 03:52:40,617 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-02 03:52:40,617 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-02 03:52:40,617 cfg.data.level                     : bpe
2020-07-02 03:52:40,618 cfg.data.lowercase                 : False
2020-07-02 03:52:40,618 cfg.data.max_sent_length           : 100
2020-07-02 03:52:40,618 cfg.testing.beam_size              : 5
2020-07-02 03:52:40,618 cfg.testing.alpha                  : 1.0
2020-07-02 03:52:40,618 cfg.training.random_seed           : 42
2020-07-02 03:52:40,618 cfg.training.optimizer             : adam
2020-07-02 03:52:40,618 cfg.training.normalization         : tokens
2020-07-02 03:52:40,618 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-02 03:52:40,618 cfg.training.scheduling            : plateau
2020-07-02 03:52:40,618 cfg.training.patience              : 8
2020-07-02 03:52:40,618 cfg.training.decrease_factor       : 0.7
2020-07-02 03:52:40,618 cfg.training.loss                  : crossentropy
2020-07-02 03:52:40,618 cfg.training.learning_rate         : 0.0002
2020-07-02 03:52:40,618 cfg.training.learning_rate_min     : 1e-08
2020-07-02 03:52:40,618 cfg.training.weight_decay          : 0.0
2020-07-02 03:52:40,618 cfg.training.label_smoothing       : 0.1
2020-07-02 03:52:40,618 cfg.training.batch_size            : 4096
2020-07-02 03:52:40,619 cfg.training.batch_type            : token
2020-07-02 03:52:40,619 cfg.training.batch_multiplier      : 1
2020-07-02 03:52:40,619 cfg.training.early_stopping_metric : ppl
2020-07-02 03:52:40,619 cfg.training.epochs                : 100
2020-07-02 03:52:40,619 cfg.training.validation_freq       : 1000
2020-07-02 03:52:40,619 cfg.training.logging_freq          : 100
2020-07-02 03:52:40,619 cfg.training.eval_metric           : bleu
2020-07-02 03:52:40,619 cfg.training.model_dir             : models/transformer_wmt17bpe_ende
2020-07-02 03:52:40,619 cfg.training.overwrite             : True
2020-07-02 03:52:40,619 cfg.training.shuffle               : True
2020-07-02 03:52:40,619 cfg.training.use_cuda              : True
2020-07-02 03:52:40,619 cfg.training.max_output_length     : 100
2020-07-02 03:52:40,619 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-02 03:52:40,619 cfg.training.keep_last_ckpts       : 3
2020-07-02 03:52:40,619 cfg.model.initializer              : xavier
2020-07-02 03:52:40,619 cfg.model.bias_initializer         : zeros
2020-07-02 03:52:40,619 cfg.model.init_gain                : 1.0
2020-07-02 03:52:40,619 cfg.model.embed_initializer        : xavier
2020-07-02 03:52:40,619 cfg.model.embed_init_gain          : 1.0
2020-07-02 03:52:40,619 cfg.model.tied_embeddings          : False
2020-07-02 03:52:40,619 cfg.model.tied_softmax             : True
2020-07-02 03:52:40,619 cfg.model.encoder.type             : transformer
2020-07-02 03:52:40,619 cfg.model.encoder.num_layers       : 6
2020-07-02 03:52:40,619 cfg.model.encoder.num_heads        : 8
2020-07-02 03:52:40,620 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-02 03:52:40,620 cfg.model.encoder.embeddings.scale : True
2020-07-02 03:52:40,620 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-02 03:52:40,620 cfg.model.encoder.hidden_size      : 512
2020-07-02 03:52:40,620 cfg.model.encoder.ff_size          : 2048
2020-07-02 03:52:40,620 cfg.model.encoder.freeze           : False
2020-07-02 03:52:40,620 cfg.model.encoder.dropout          : 0.1
2020-07-02 03:52:40,620 cfg.model.decoder.type             : transformer
2020-07-02 03:52:40,620 cfg.model.decoder.num_layers       : 6
2020-07-02 03:52:40,620 cfg.model.decoder.num_heads        : 8
2020-07-02 03:52:40,620 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-02 03:52:40,620 cfg.model.decoder.embeddings.scale : True
2020-07-02 03:52:40,620 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-02 03:52:40,620 cfg.model.decoder.hidden_size      : 512
2020-07-02 03:52:40,620 cfg.model.decoder.ff_size          : 2048
2020-07-02 03:52:40,620 cfg.model.decoder.freeze           : False
2020-07-02 03:52:40,620 cfg.model.decoder.dropout          : 0.1
2020-07-02 03:52:40,620 Data set sizes: 
	train 9771,
	valid 1525,
	test 1165
2020-07-02 03:52:40,620 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-02 03:52:40,620 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) &@@ (9) a@@
2020-07-02 03:52:40,620 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) Sie (8) ich (9) das
2020-07-02 03:52:40,621 Number of Src words (types): 4442
2020-07-02 03:52:40,621 Number of Trg words (types): 5796
2020-07-02 03:52:40,621 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4442),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5796))
2020-07-02 03:52:40,629 EPOCH 1
2020-07-02 03:53:02,815 Epoch   1: total training loss 375.01
2020-07-02 03:53:02,816 EPOCH 2
2020-07-02 03:53:14,308 Epoch   2 Step:      100 Batch Loss:     4.275163 Tokens per Sec:     6951, Lr: 0.000200
2020-07-02 03:53:26,383 Epoch   2: total training loss 332.82
2020-07-02 03:53:26,383 EPOCH 3
2020-07-02 03:53:50,783 Epoch   3 Step:      200 Batch Loss:     3.944503 Tokens per Sec:     6101, Lr: 0.000200
2020-07-02 03:53:51,526 Epoch   3: total training loss 311.74
2020-07-02 03:53:51,526 EPOCH 4
2020-07-02 03:54:16,503 Epoch   4: total training loss 271.48
2020-07-02 03:54:16,504 EPOCH 5
2020-07-02 03:54:27,331 Epoch   5 Step:      300 Batch Loss:     2.292799 Tokens per Sec:     6146, Lr: 0.000200
2020-07-02 03:54:41,625 Epoch   5: total training loss 242.81
2020-07-02 03:54:41,625 EPOCH 6
2020-07-02 03:55:04,022 Epoch   6 Step:      400 Batch Loss:     2.635741 Tokens per Sec:     6191, Lr: 0.000200
2020-07-02 03:55:06,460 Epoch   6: total training loss 210.78
2020-07-02 03:55:06,460 EPOCH 7
2020-07-02 03:55:31,521 Epoch   7: total training loss 186.32
2020-07-02 03:55:31,522 EPOCH 8
2020-07-02 03:55:40,957 Epoch   8 Step:      500 Batch Loss:     2.251626 Tokens per Sec:     6291, Lr: 0.000200
2020-07-02 03:55:56,635 Epoch   8: total training loss 160.12
2020-07-02 03:55:56,635 EPOCH 9
2020-07-02 03:56:18,065 Epoch   9 Step:      600 Batch Loss:     2.478430 Tokens per Sec:     6208, Lr: 0.000200
2020-07-02 03:56:21,370 Epoch   9: total training loss 145.21
2020-07-02 03:56:21,370 EPOCH 10
2020-07-02 03:56:46,182 Epoch  10: total training loss 127.22
2020-07-02 03:56:46,182 EPOCH 11
2020-07-02 03:56:55,210 Epoch  11 Step:      700 Batch Loss:     1.101788 Tokens per Sec:     6176, Lr: 0.000200
2020-07-02 03:57:11,077 Epoch  11: total training loss 115.50
2020-07-02 03:57:11,077 EPOCH 12
2020-07-02 03:57:31,999 Epoch  12 Step:      800 Batch Loss:     1.754041 Tokens per Sec:     6208, Lr: 0.000200
2020-07-02 03:57:36,039 Epoch  12: total training loss 108.54
2020-07-02 03:57:36,039 EPOCH 13
2020-07-02 03:58:00,855 Epoch  13: total training loss 93.61
2020-07-02 03:58:00,856 EPOCH 14
2020-07-02 03:58:08,914 Epoch  14 Step:      900 Batch Loss:     0.952189 Tokens per Sec:     6067, Lr: 0.000200
2020-07-02 03:58:25,874 Epoch  14: total training loss 87.47
2020-07-02 03:58:25,875 EPOCH 15
2020-07-02 03:58:45,766 Epoch  15 Step:     1000 Batch Loss:     0.990985 Tokens per Sec:     6204, Lr: 0.000200
2020-07-02 04:00:23,166 Hooray! New best validation result [ppl]!
2020-07-02 04:00:23,167 Saving new checkpoint.
2020-07-02 04:00:29,441 Example #0
2020-07-02 04:00:29,441 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:00:29,441 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:00:29,441 	Source:     Hello .
2020-07-02 04:00:29,441 	Reference:  Hallo ,
2020-07-02 04:00:29,441 	Hypothesis: Hallo .
2020-07-02 04:00:29,442 Example #1
2020-07-02 04:00:29,442 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:00:29,442 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:00:29,442 	Source:     Hi , how can I help you ?
2020-07-02 04:00:29,442 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:00:29,442 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:00:29,442 Example #2
2020-07-02 04:00:29,442 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:00:29,442 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:00:29,442 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:00:29,442 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:00:29,442 	Hypothesis: Hallo , ich suche ein Restaurant in San Francisco , Kalifornien , Kalifornien .
2020-07-02 04:00:29,442 Example #3
2020-07-02 04:00:29,442 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:00:29,442 	Raw hypothesis: ['O@@', 'k', ',', 'welche', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:00:29,442 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:00:29,442 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:00:29,442 	Hypothesis: Ok , welche Art von Restaurant suchen Sie ?
2020-07-02 04:00:29,442 Validation result (greedy) at epoch  15, step     1000: bleu:  29.99, loss: 47848.3906, ppl:   7.1104, duration: 103.6756s
2020-07-02 04:00:34,343 Epoch  15: total training loss 83.76
2020-07-02 04:00:34,343 EPOCH 16
2020-07-02 04:00:59,274 Epoch  16: total training loss 71.21
2020-07-02 04:00:59,275 EPOCH 17
2020-07-02 04:01:05,925 Epoch  17 Step:     1100 Batch Loss:     0.562077 Tokens per Sec:     5910, Lr: 0.000200
2020-07-02 04:01:24,120 Epoch  17: total training loss 64.75
2020-07-02 04:01:24,121 EPOCH 18
2020-07-02 04:01:43,254 Epoch  18 Step:     1200 Batch Loss:     0.673562 Tokens per Sec:     6178, Lr: 0.000200
2020-07-02 04:01:49,306 Epoch  18: total training loss 57.21
2020-07-02 04:01:49,306 EPOCH 19
2020-07-02 04:02:14,544 Epoch  19: total training loss 52.01
2020-07-02 04:02:14,545 EPOCH 20
2020-07-02 04:02:19,937 Epoch  20 Step:     1300 Batch Loss:     0.528873 Tokens per Sec:     6216, Lr: 0.000200
2020-07-02 04:02:39,467 Epoch  20: total training loss 48.26
2020-07-02 04:02:39,468 EPOCH 21
2020-07-02 04:02:57,099 Epoch  21 Step:     1400 Batch Loss:     0.559982 Tokens per Sec:     6197, Lr: 0.000200
2020-07-02 04:03:04,503 Epoch  21: total training loss 42.28
2020-07-02 04:03:04,504 EPOCH 22
2020-07-02 04:03:29,432 Epoch  22: total training loss 37.89
2020-07-02 04:03:29,433 EPOCH 23
2020-07-02 04:03:34,209 Epoch  23 Step:     1500 Batch Loss:     0.424868 Tokens per Sec:     6140, Lr: 0.000200
2020-07-02 04:03:54,518 Epoch  23: total training loss 34.91
2020-07-02 04:03:54,519 EPOCH 24
2020-07-02 04:04:11,505 Epoch  24 Step:     1600 Batch Loss:     0.749670 Tokens per Sec:     6160, Lr: 0.000200
2020-07-02 04:04:19,610 Epoch  24: total training loss 32.10
2020-07-02 04:04:19,610 EPOCH 25
2020-07-02 04:04:44,829 Epoch  25: total training loss 28.42
2020-07-02 04:04:44,829 EPOCH 26
2020-07-02 04:04:48,915 Epoch  26 Step:     1700 Batch Loss:     0.391840 Tokens per Sec:     6079, Lr: 0.000200
2020-07-02 04:05:09,989 Epoch  26: total training loss 25.84
2020-07-02 04:05:09,990 EPOCH 27
2020-07-02 04:05:26,055 Epoch  27 Step:     1800 Batch Loss:     0.376957 Tokens per Sec:     6108, Lr: 0.000200
2020-07-02 04:05:35,148 Epoch  27: total training loss 24.38
2020-07-02 04:05:35,148 EPOCH 28
2020-07-02 04:06:00,424 Epoch  28: total training loss 22.67
2020-07-02 04:06:00,425 EPOCH 29
2020-07-02 04:06:02,622 Epoch  29 Step:     1900 Batch Loss:     0.309569 Tokens per Sec:     5500, Lr: 0.000200
2020-07-02 04:06:25,628 Epoch  29: total training loss 22.35
2020-07-02 04:06:25,629 EPOCH 30
2020-07-02 04:06:39,816 Epoch  30 Step:     2000 Batch Loss:     0.210376 Tokens per Sec:     6186, Lr: 0.000200
2020-07-02 04:08:13,385 Hooray! New best validation result [ppl]!
2020-07-02 04:08:13,386 Saving new checkpoint.
2020-07-02 04:08:19,734 Example #0
2020-07-02 04:08:19,735 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:08:19,735 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:08:19,735 	Source:     Hello .
2020-07-02 04:08:19,735 	Reference:  Hallo ,
2020-07-02 04:08:19,735 	Hypothesis: Hallo .
2020-07-02 04:08:19,735 Example #1
2020-07-02 04:08:19,736 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:08:19,736 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:08:19,736 	Source:     Hi , how can I help you ?
2020-07-02 04:08:19,736 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:08:19,736 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:08:19,736 Example #2
2020-07-02 04:08:19,736 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:08:19,736 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:08:19,736 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:08:19,736 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:08:19,736 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:08:19,736 Example #3
2020-07-02 04:08:19,736 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:08:19,736 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:08:19,736 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:08:19,736 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:08:19,736 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:08:19,737 Validation result (greedy) at epoch  30, step     2000: bleu:  35.99, loss: 45501.9844, ppl:   6.4583, duration: 99.9193s
2020-07-02 04:08:30,185 Epoch  30: total training loss 18.36
2020-07-02 04:08:30,185 EPOCH 31
2020-07-02 04:08:55,319 Epoch  31: total training loss 16.81
2020-07-02 04:08:55,319 EPOCH 32
2020-07-02 04:08:56,367 Epoch  32 Step:     2100 Batch Loss:     0.324859 Tokens per Sec:     6238, Lr: 0.000200
2020-07-02 04:09:20,264 Epoch  32: total training loss 15.92
2020-07-02 04:09:20,265 EPOCH 33
2020-07-02 04:09:33,490 Epoch  33 Step:     2200 Batch Loss:     0.235182 Tokens per Sec:     6195, Lr: 0.000200
2020-07-02 04:09:45,425 Epoch  33: total training loss 14.96
2020-07-02 04:09:45,425 EPOCH 34
2020-07-02 04:10:10,633 Epoch  34: total training loss 13.90
2020-07-02 04:10:10,633 EPOCH 35
2020-07-02 04:10:11,018 Epoch  35 Step:     2300 Batch Loss:     0.157019 Tokens per Sec:     5365, Lr: 0.000200
2020-07-02 04:10:35,571 Epoch  35: total training loss 12.88
2020-07-02 04:10:35,572 EPOCH 36
2020-07-02 04:10:48,568 Epoch  36 Step:     2400 Batch Loss:     0.188362 Tokens per Sec:     6220, Lr: 0.000200
2020-07-02 04:11:00,759 Epoch  36: total training loss 12.75
2020-07-02 04:11:00,760 EPOCH 37
2020-07-02 04:11:25,463 Epoch  37 Step:     2500 Batch Loss:     0.179717 Tokens per Sec:     6104, Lr: 0.000200
2020-07-02 04:11:25,870 Epoch  37: total training loss 12.13
2020-07-02 04:11:25,870 EPOCH 38
2020-07-02 04:11:51,269 Epoch  38: total training loss 11.80
2020-07-02 04:11:51,269 EPOCH 39
2020-07-02 04:12:02,955 Epoch  39 Step:     2600 Batch Loss:     0.169914 Tokens per Sec:     6277, Lr: 0.000200
2020-07-02 04:12:16,402 Epoch  39: total training loss 11.43
2020-07-02 04:12:16,402 EPOCH 40
2020-07-02 04:12:40,129 Epoch  40 Step:     2700 Batch Loss:     0.170668 Tokens per Sec:     6097, Lr: 0.000200
2020-07-02 04:12:41,645 Epoch  40: total training loss 10.98
2020-07-02 04:12:41,646 EPOCH 41
2020-07-02 04:13:06,869 Epoch  41: total training loss 10.52
2020-07-02 04:13:06,870 EPOCH 42
2020-07-02 04:13:17,228 Epoch  42 Step:     2800 Batch Loss:     0.136513 Tokens per Sec:     6164, Lr: 0.000200
2020-07-02 04:13:32,068 Epoch  42: total training loss 10.63
2020-07-02 04:13:32,069 EPOCH 43
2020-07-02 04:13:54,400 Epoch  43 Step:     2900 Batch Loss:     0.150308 Tokens per Sec:     6107, Lr: 0.000200
2020-07-02 04:13:57,267 Epoch  43: total training loss 10.27
2020-07-02 04:13:57,267 EPOCH 44
2020-07-02 04:14:22,550 Epoch  44: total training loss 9.63
2020-07-02 04:14:22,551 EPOCH 45
2020-07-02 04:14:31,824 Epoch  45 Step:     3000 Batch Loss:     0.135182 Tokens per Sec:     6090, Lr: 0.000200
2020-07-02 04:15:59,020 Example #0
2020-07-02 04:15:59,020 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:15:59,020 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:15:59,020 	Source:     Hello .
2020-07-02 04:15:59,020 	Reference:  Hallo ,
2020-07-02 04:15:59,020 	Hypothesis: Hallo .
2020-07-02 04:15:59,021 Example #1
2020-07-02 04:15:59,021 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:15:59,021 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:15:59,021 	Source:     Hi , how can I help you ?
2020-07-02 04:15:59,021 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:15:59,021 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:15:59,021 Example #2
2020-07-02 04:15:59,021 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:15:59,021 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', '.']
2020-07-02 04:15:59,021 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:15:59,021 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:15:59,021 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall .
2020-07-02 04:15:59,021 Example #3
2020-07-02 04:15:59,021 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:15:59,021 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:15:59,021 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:15:59,021 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:15:59,021 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:15:59,021 Validation result (greedy) at epoch  45, step     3000: bleu:  36.01, loss: 46466.6055, ppl:   6.7188, duration: 87.1967s
2020-07-02 04:16:14,752 Epoch  45: total training loss 9.38
2020-07-02 04:16:14,753 EPOCH 46
2020-07-02 04:16:36,270 Epoch  46 Step:     3100 Batch Loss:     0.152705 Tokens per Sec:     6114, Lr: 0.000200
2020-07-02 04:16:39,762 Epoch  46: total training loss 9.04
2020-07-02 04:16:39,763 EPOCH 47
2020-07-02 04:17:04,832 Epoch  47: total training loss 8.93
2020-07-02 04:17:04,833 EPOCH 48
2020-07-02 04:17:13,457 Epoch  48 Step:     3200 Batch Loss:     0.140552 Tokens per Sec:     6016, Lr: 0.000200
2020-07-02 04:17:29,938 Epoch  48: total training loss 8.97
2020-07-02 04:17:29,938 EPOCH 49
2020-07-02 04:17:50,486 Epoch  49 Step:     3300 Batch Loss:     0.125080 Tokens per Sec:     6236, Lr: 0.000200
2020-07-02 04:17:54,760 Epoch  49: total training loss 8.99
2020-07-02 04:17:54,760 EPOCH 50
2020-07-02 04:18:19,877 Epoch  50: total training loss 9.37
2020-07-02 04:18:19,877 EPOCH 51
2020-07-02 04:18:27,338 Epoch  51 Step:     3400 Batch Loss:     0.144413 Tokens per Sec:     6123, Lr: 0.000200
2020-07-02 04:18:45,127 Epoch  51: total training loss 9.30
2020-07-02 04:18:45,127 EPOCH 52
2020-07-02 04:19:04,308 Epoch  52 Step:     3500 Batch Loss:     0.121832 Tokens per Sec:     6087, Lr: 0.000200
2020-07-02 04:19:10,545 Epoch  52: total training loss 8.95
2020-07-02 04:19:10,546 EPOCH 53
2020-07-02 04:19:35,837 Epoch  53: total training loss 8.86
2020-07-02 04:19:35,837 EPOCH 54
2020-07-02 04:19:41,103 Epoch  54 Step:     3600 Batch Loss:     0.115684 Tokens per Sec:     5899, Lr: 0.000200
2020-07-02 04:20:01,253 Epoch  54: total training loss 8.34
2020-07-02 04:20:01,254 EPOCH 55
2020-07-02 04:20:18,313 Epoch  55 Step:     3700 Batch Loss:     0.109228 Tokens per Sec:     6119, Lr: 0.000200
2020-07-02 04:20:26,520 Epoch  55: total training loss 8.31
2020-07-02 04:20:26,520 EPOCH 56
2020-07-02 04:20:51,770 Epoch  56: total training loss 8.39
2020-07-02 04:20:51,771 EPOCH 57
2020-07-02 04:20:55,045 Epoch  57 Step:     3800 Batch Loss:     0.113329 Tokens per Sec:     6261, Lr: 0.000200
2020-07-02 04:21:16,893 Epoch  57: total training loss 8.61
2020-07-02 04:21:16,893 EPOCH 58
2020-07-02 04:21:31,957 Epoch  58 Step:     3900 Batch Loss:     0.280888 Tokens per Sec:     6148, Lr: 0.000200
2020-07-02 04:21:42,031 Epoch  58: total training loss 10.36
2020-07-02 04:21:42,032 EPOCH 59
2020-07-02 04:22:07,103 Epoch  59: total training loss 10.29
2020-07-02 04:22:07,104 EPOCH 60
2020-07-02 04:22:09,279 Epoch  60 Step:     4000 Batch Loss:     0.136295 Tokens per Sec:     6007, Lr: 0.000200
2020-07-02 04:23:52,822 Example #0
2020-07-02 04:23:52,822 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:23:52,822 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:23:52,822 	Source:     Hello .
2020-07-02 04:23:52,822 	Reference:  Hallo ,
2020-07-02 04:23:52,822 	Hypothesis: Hallo .
2020-07-02 04:23:52,822 Example #1
2020-07-02 04:23:52,822 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:23:52,822 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:23:52,822 	Source:     Hi , how can I help you ?
2020-07-02 04:23:52,822 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:23:52,822 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:23:52,822 Example #2
2020-07-02 04:23:52,823 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:23:52,823 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:23:52,823 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:23:52,823 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:23:52,823 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:23:52,823 Example #3
2020-07-02 04:23:52,823 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:23:52,823 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:23:52,823 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:23:52,823 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:23:52,823 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:23:52,823 Validation result (greedy) at epoch  60, step     4000: bleu:  36.54, loss: 47118.1836, ppl:   6.9007, duration: 103.5436s
2020-07-02 04:24:15,852 Epoch  60: total training loss 8.91
2020-07-02 04:24:15,852 EPOCH 61
2020-07-02 04:24:29,962 Epoch  61 Step:     4100 Batch Loss:     0.098172 Tokens per Sec:     6125, Lr: 0.000200
2020-07-02 04:24:41,112 Epoch  61: total training loss 7.65
2020-07-02 04:24:41,113 EPOCH 62
2020-07-02 04:25:06,235 Epoch  62: total training loss 7.26
2020-07-02 04:25:06,236 EPOCH 63
2020-07-02 04:25:06,945 Epoch  63 Step:     4200 Batch Loss:     0.101190 Tokens per Sec:     4425, Lr: 0.000200
2020-07-02 04:25:31,297 Epoch  63: total training loss 6.94
2020-07-02 04:25:31,297 EPOCH 64
2020-07-02 04:25:43,987 Epoch  64 Step:     4300 Batch Loss:     0.097934 Tokens per Sec:     6016, Lr: 0.000200
2020-07-02 04:25:56,295 Epoch  64: total training loss 6.67
2020-07-02 04:25:56,296 EPOCH 65
2020-07-02 04:26:21,043 Epoch  65 Step:     4400 Batch Loss:     0.094576 Tokens per Sec:     6116, Lr: 0.000200
2020-07-02 04:26:21,423 Epoch  65: total training loss 6.67
2020-07-02 04:26:21,423 EPOCH 66
2020-07-02 04:26:46,635 Epoch  66: total training loss 6.54
2020-07-02 04:26:46,635 EPOCH 67
2020-07-02 04:26:58,260 Epoch  67 Step:     4500 Batch Loss:     0.079678 Tokens per Sec:     6171, Lr: 0.000200
2020-07-02 04:27:11,824 Epoch  67: total training loss 6.28
2020-07-02 04:27:11,824 EPOCH 68
2020-07-02 04:27:36,026 Epoch  68 Step:     4600 Batch Loss:     0.096481 Tokens per Sec:     6091, Lr: 0.000200
2020-07-02 04:27:37,072 Epoch  68: total training loss 6.14
2020-07-02 04:27:37,072 EPOCH 69
2020-07-02 04:28:02,309 Epoch  69: total training loss 6.22
2020-07-02 04:28:02,310 EPOCH 70
2020-07-02 04:28:13,513 Epoch  70 Step:     4700 Batch Loss:     0.079514 Tokens per Sec:     6104, Lr: 0.000200
2020-07-02 04:28:27,454 Epoch  70: total training loss 6.33
2020-07-02 04:28:27,455 EPOCH 71
2020-07-02 04:28:50,472 Epoch  71 Step:     4800 Batch Loss:     0.094056 Tokens per Sec:     6151, Lr: 0.000200
2020-07-02 04:28:52,402 Epoch  71: total training loss 6.09
2020-07-02 04:28:52,402 EPOCH 72
2020-07-02 04:29:17,662 Epoch  72: total training loss 6.18
2020-07-02 04:29:17,663 EPOCH 73
2020-07-02 04:29:27,628 Epoch  73 Step:     4900 Batch Loss:     0.085561 Tokens per Sec:     6185, Lr: 0.000200
2020-07-02 04:29:42,734 Epoch  73: total training loss 6.07
2020-07-02 04:29:42,735 EPOCH 74
2020-07-02 04:30:04,711 Epoch  74 Step:     5000 Batch Loss:     0.093376 Tokens per Sec:     6165, Lr: 0.000200
2020-07-02 04:31:24,991 Example #0
2020-07-02 04:31:24,992 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:31:24,992 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:31:24,992 	Source:     Hello .
2020-07-02 04:31:24,992 	Reference:  Hallo ,
2020-07-02 04:31:24,992 	Hypothesis: Hallo .
2020-07-02 04:31:24,992 Example #1
2020-07-02 04:31:24,992 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:31:24,992 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:31:24,992 	Source:     Hi , how can I help you ?
2020-07-02 04:31:24,992 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:31:24,992 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:31:24,992 Example #2
2020-07-02 04:31:24,992 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:31:24,992 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:31:24,992 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:31:24,992 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:31:24,992 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:31:24,992 Example #3
2020-07-02 04:31:24,993 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:31:24,993 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:31:24,993 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:31:24,993 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:31:24,993 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:31:24,993 Validation result (greedy) at epoch  74, step     5000: bleu:  36.91, loss: 46725.4102, ppl:   6.7905, duration: 80.2807s
2020-07-02 04:31:28,025 Epoch  74: total training loss 5.79
2020-07-02 04:31:28,025 EPOCH 75
2020-07-02 04:31:53,191 Epoch  75: total training loss 5.90
2020-07-02 04:31:53,191 EPOCH 76
2020-07-02 04:32:02,370 Epoch  76 Step:     5100 Batch Loss:     0.087760 Tokens per Sec:     6070, Lr: 0.000200
2020-07-02 04:32:18,214 Epoch  76: total training loss 5.85
2020-07-02 04:32:18,215 EPOCH 77
2020-07-02 04:32:39,584 Epoch  77 Step:     5200 Batch Loss:     0.091630 Tokens per Sec:     6091, Lr: 0.000200
2020-07-02 04:32:43,446 Epoch  77: total training loss 5.96
2020-07-02 04:32:43,446 EPOCH 78
2020-07-02 04:33:08,615 Epoch  78: total training loss 5.90
2020-07-02 04:33:08,616 EPOCH 79
2020-07-02 04:33:16,947 Epoch  79 Step:     5300 Batch Loss:     0.082458 Tokens per Sec:     6006, Lr: 0.000200
2020-07-02 04:33:33,883 Epoch  79: total training loss 6.05
2020-07-02 04:33:33,883 EPOCH 80
2020-07-02 04:33:54,011 Epoch  80 Step:     5400 Batch Loss:     0.088233 Tokens per Sec:     6174, Lr: 0.000200
2020-07-02 04:33:58,938 Epoch  80: total training loss 6.41
2020-07-02 04:33:58,938 EPOCH 81
2020-07-02 04:34:24,239 Epoch  81: total training loss 6.19
2020-07-02 04:34:24,239 EPOCH 82
2020-07-02 04:34:31,356 Epoch  82 Step:     5500 Batch Loss:     0.102512 Tokens per Sec:     6023, Lr: 0.000200
2020-07-02 04:34:49,299 Epoch  82: total training loss 6.21
2020-07-02 04:34:49,300 EPOCH 83
2020-07-02 04:35:08,881 Epoch  83 Step:     5600 Batch Loss:     0.083651 Tokens per Sec:     6145, Lr: 0.000200
2020-07-02 04:35:14,457 Epoch  83: total training loss 6.15
2020-07-02 04:35:14,457 EPOCH 84
2020-07-02 04:35:39,611 Epoch  84: total training loss 5.99
2020-07-02 04:35:39,611 EPOCH 85
2020-07-02 04:35:46,045 Epoch  85 Step:     5700 Batch Loss:     0.085069 Tokens per Sec:     6249, Lr: 0.000200
2020-07-02 04:36:04,697 Epoch  85: total training loss 7.51
2020-07-02 04:36:04,698 EPOCH 86
2020-07-02 04:36:23,545 Epoch  86 Step:     5800 Batch Loss:     0.105548 Tokens per Sec:     6148, Lr: 0.000200
2020-07-02 04:36:29,860 Epoch  86: total training loss 7.14
2020-07-02 04:36:29,860 EPOCH 87
2020-07-02 04:36:55,104 Epoch  87: total training loss 5.98
2020-07-02 04:36:55,104 EPOCH 88
2020-07-02 04:37:00,940 Epoch  88 Step:     5900 Batch Loss:     0.084794 Tokens per Sec:     6073, Lr: 0.000200
2020-07-02 04:37:20,651 Epoch  88: total training loss 6.23
2020-07-02 04:37:20,654 EPOCH 89
2020-07-02 04:37:38,683 Epoch  89 Step:     6000 Batch Loss:     0.087408 Tokens per Sec:     6099, Lr: 0.000200
2020-07-02 04:39:06,984 Example #0
2020-07-02 04:39:06,984 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:39:06,984 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:39:06,984 	Source:     Hello .
2020-07-02 04:39:06,984 	Reference:  Hallo ,
2020-07-02 04:39:06,985 	Hypothesis: Hallo .
2020-07-02 04:39:06,985 Example #1
2020-07-02 04:39:06,985 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:39:06,985 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:39:06,985 	Source:     Hi , how can I help you ?
2020-07-02 04:39:06,985 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:39:06,985 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:39:06,985 Example #2
2020-07-02 04:39:06,985 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:39:06,985 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:39:06,985 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:39:06,985 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:39:06,985 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:39:06,985 Example #3
2020-07-02 04:39:06,985 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:39:06,985 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:39:06,985 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:39:06,985 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:39:06,985 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:39:06,986 Validation result (greedy) at epoch  89, step     6000: bleu:  37.21, loss: 47208.3398, ppl:   6.9263, duration: 88.3016s
2020-07-02 04:39:14,376 Epoch  89: total training loss 6.04
2020-07-02 04:39:14,376 EPOCH 90
2020-07-02 04:39:39,609 Epoch  90: total training loss 5.63
2020-07-02 04:39:39,609 EPOCH 91
2020-07-02 04:39:44,556 Epoch  91 Step:     6100 Batch Loss:     0.077546 Tokens per Sec:     6112, Lr: 0.000200
2020-07-02 04:40:04,921 Epoch  91: total training loss 5.33
2020-07-02 04:40:04,922 EPOCH 92
2020-07-02 04:40:22,254 Epoch  92 Step:     6200 Batch Loss:     0.075025 Tokens per Sec:     6039, Lr: 0.000200
2020-07-02 04:40:30,289 Epoch  92: total training loss 5.22
2020-07-02 04:40:30,290 EPOCH 93
2020-07-02 04:40:55,762 Epoch  93: total training loss 5.30
2020-07-02 04:40:55,763 EPOCH 94
2020-07-02 04:40:59,848 Epoch  94 Step:     6300 Batch Loss:     0.066224 Tokens per Sec:     6022, Lr: 0.000200
2020-07-02 04:41:21,197 Epoch  94: total training loss 5.38
2020-07-02 04:41:21,198 EPOCH 95
2020-07-02 04:41:37,201 Epoch  95 Step:     6400 Batch Loss:     0.089974 Tokens per Sec:     6024, Lr: 0.000200
2020-07-02 04:41:46,810 Epoch  95: total training loss 5.19
2020-07-02 04:41:46,810 EPOCH 96
2020-07-02 04:42:11,951 Epoch  96: total training loss 5.08
2020-07-02 04:42:11,952 EPOCH 97
2020-07-02 04:42:14,617 Epoch  97 Step:     6500 Batch Loss:     0.066693 Tokens per Sec:     6048, Lr: 0.000200
2020-07-02 04:42:37,237 Epoch  97: total training loss 4.97
2020-07-02 04:42:37,238 EPOCH 98
2020-07-02 04:42:51,533 Epoch  98 Step:     6600 Batch Loss:     0.064960 Tokens per Sec:     6052, Lr: 0.000200
2020-07-02 04:43:02,415 Epoch  98: total training loss 4.91
2020-07-02 04:43:02,416 EPOCH 99
2020-07-02 04:43:27,651 Epoch  99: total training loss 4.86
2020-07-02 04:43:27,651 EPOCH 100
2020-07-02 04:43:29,128 Epoch 100 Step:     6700 Batch Loss:     0.098279 Tokens per Sec:     5284, Lr: 0.000200
2020-07-02 04:43:53,098 Epoch 100: total training loss 4.82
2020-07-02 04:43:53,099 Training ended after 100 epochs.
2020-07-02 04:43:53,099 Best validation result (greedy) at step     2000:   6.46 ppl.
2020-07-02 04:44:53,438  dev bleu:  37.19 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-02 04:44:53,443 Translations saved to: models/transformer_wmt17bpe_ende/00002000.hyps.dev
2020-07-02 04:45:36,926 test bleu:  34.65 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-02 04:45:36,930 Translations saved to: models/transformer_wmt17bpe_ende/00002000.hyps.test
