2020-07-08 01:50:14,634 Hello! This is Joey-NMT.
2020-07-08 01:50:21,238 Total params: 62894080
2020-07-08 01:50:21,239 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-07-08 01:50:27,397 cfg.name                           : transformer
2020-07-08 01:50:27,397 cfg.data.src                       : en
2020-07-08 01:50:27,398 cfg.data.trg                       : de
2020-07-08 01:50:27,398 cfg.data.train                     : chatnmt/official_split/wmt17bpe/train.tags.bpe.wmt-ende-best
2020-07-08 01:50:27,398 cfg.data.dev                       : chatnmt/official_split/wmt17bpe/dev.tags.bpe.wmt-ende-best
2020-07-08 01:50:27,398 cfg.data.test                      : chatnmt/official_split/wmt17bpe/test.tags.bpe.wmt-ende-best
2020-07-08 01:50:27,398 cfg.data.level                     : bpe
2020-07-08 01:50:27,398 cfg.data.lowercase                 : False
2020-07-08 01:50:27,398 cfg.data.max_sent_length           : 100
2020-07-08 01:50:27,399 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-08 01:50:27,399 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-08 01:50:27,399 cfg.testing.beam_size              : 5
2020-07-08 01:50:27,399 cfg.testing.alpha                  : 1.0
2020-07-08 01:50:27,399 cfg.training.random_seed           : 42
2020-07-08 01:50:27,399 cfg.training.optimizer             : adam
2020-07-08 01:50:27,399 cfg.training.normalization         : tokens
2020-07-08 01:50:27,399 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-08 01:50:27,400 cfg.training.scheduling            : plateau
2020-07-08 01:50:27,400 cfg.training.patience              : 8
2020-07-08 01:50:27,400 cfg.training.decrease_factor       : 0.7
2020-07-08 01:50:27,400 cfg.training.loss                  : crossentropy
2020-07-08 01:50:27,400 cfg.training.learning_rate         : 0.0002
2020-07-08 01:50:27,400 cfg.training.learning_rate_min     : 1e-08
2020-07-08 01:50:27,400 cfg.training.weight_decay          : 0.0
2020-07-08 01:50:27,400 cfg.training.label_smoothing       : 0.1
2020-07-08 01:50:27,401 cfg.training.batch_size            : 4096
2020-07-08 01:50:27,401 cfg.training.batch_type            : token
2020-07-08 01:50:27,401 cfg.training.batch_multiplier      : 1
2020-07-08 01:50:27,401 cfg.training.early_stopping_metric : ppl
2020-07-08 01:50:27,401 cfg.training.epochs                : 50
2020-07-08 01:50:27,401 cfg.training.validation_freq       : 1000
2020-07-08 01:50:27,401 cfg.training.logging_freq          : 100
2020-07-08 01:50:27,401 cfg.training.eval_metric           : bleu
2020-07-08 01:50:27,401 cfg.training.model_dir             : models/wmt20/tfm_b4096_ende/
2020-07-08 01:50:27,402 cfg.training.overwrite             : True
2020-07-08 01:50:27,402 cfg.training.shuffle               : True
2020-07-08 01:50:27,402 cfg.training.use_cuda              : True
2020-07-08 01:50:27,402 cfg.training.max_output_length     : 100
2020-07-08 01:50:27,402 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-08 01:50:27,402 cfg.training.keep_last_ckpts       : 3
2020-07-08 01:50:27,402 cfg.model.initializer              : xavier
2020-07-08 01:50:27,402 cfg.model.bias_initializer         : zeros
2020-07-08 01:50:27,403 cfg.model.init_gain                : 1.0
2020-07-08 01:50:27,403 cfg.model.embed_initializer        : xavier
2020-07-08 01:50:27,403 cfg.model.embed_init_gain          : 1.0
2020-07-08 01:50:27,403 cfg.model.tied_embeddings          : True
2020-07-08 01:50:27,403 cfg.model.tied_softmax             : True
2020-07-08 01:50:27,403 cfg.model.encoder.type             : transformer
2020-07-08 01:50:27,403 cfg.model.encoder.num_layers       : 6
2020-07-08 01:50:27,403 cfg.model.encoder.num_heads        : 8
2020-07-08 01:50:27,403 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-08 01:50:27,404 cfg.model.encoder.embeddings.scale : True
2020-07-08 01:50:27,404 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-08 01:50:27,404 cfg.model.encoder.hidden_size      : 512
2020-07-08 01:50:27,404 cfg.model.encoder.ff_size          : 2048
2020-07-08 01:50:27,404 cfg.model.encoder.freeze           : False
2020-07-08 01:50:27,404 cfg.model.encoder.dropout          : 0.1
2020-07-08 01:50:27,404 cfg.model.decoder.type             : transformer
2020-07-08 01:50:27,404 cfg.model.decoder.num_layers       : 6
2020-07-08 01:50:27,405 cfg.model.decoder.num_heads        : 8
2020-07-08 01:50:27,405 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-08 01:50:27,405 cfg.model.decoder.embeddings.scale : True
2020-07-08 01:50:27,405 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-08 01:50:27,405 cfg.model.decoder.hidden_size      : 512
2020-07-08 01:50:27,405 cfg.model.decoder.ff_size          : 2048
2020-07-08 01:50:27,405 cfg.model.decoder.freeze           : False
2020-07-08 01:50:27,405 cfg.model.decoder.dropout          : 0.1
2020-07-08 01:50:27,406 Data set sizes: 
	train 10939,
	valid 776,
	test 0
2020-07-08 01:50:27,406 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-08 01:50:27,406 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-08 01:50:27,406 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-08 01:50:27,406 Number of Src words (types): 36628
2020-07-08 01:50:27,407 Number of Trg words (types): 36628
2020-07-08 01:50:27,407 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-08 01:50:27,439 EPOCH 1
2020-07-08 01:51:37,954 Epoch   1: total training loss 439.38
2020-07-08 01:51:37,955 EPOCH 2
2020-07-08 01:52:10,431 Epoch   2 Step:      100 Batch Loss:     5.766905 Tokens per Sec:     2471, Lr: 0.000200
2020-07-08 01:52:46,983 Epoch   2: total training loss 363.05
2020-07-08 01:52:46,983 EPOCH 3
2020-07-08 01:53:50,987 Epoch   3 Step:      200 Batch Loss:     6.604062 Tokens per Sec:     2477, Lr: 0.000200
2020-07-08 01:53:56,277 Epoch   3: total training loss 351.19
2020-07-08 01:53:56,278 EPOCH 4
2020-07-08 01:55:05,255 Epoch   4: total training loss 327.75
2020-07-08 01:55:05,255 EPOCH 5
2020-07-08 01:55:31,879 Epoch   5 Step:      300 Batch Loss:     5.064141 Tokens per Sec:     2544, Lr: 0.000200
2020-07-08 01:56:14,536 Epoch   5: total training loss 302.62
2020-07-08 01:56:14,536 EPOCH 6
2020-07-08 01:57:12,349 Epoch   6 Step:      400 Batch Loss:     4.146019 Tokens per Sec:     2462, Lr: 0.000200
2020-07-08 01:57:23,943 Epoch   6: total training loss 282.83
2020-07-08 01:57:23,943 EPOCH 7
2020-07-08 01:58:33,316 Epoch   7: total training loss 264.22
2020-07-08 01:58:33,316 EPOCH 8
2020-07-08 01:58:53,544 Epoch   8 Step:      500 Batch Loss:     4.191285 Tokens per Sec:     2464, Lr: 0.000200
2020-07-08 01:59:43,119 Epoch   8: total training loss 248.74
2020-07-08 01:59:43,119 EPOCH 9
2020-07-08 02:00:34,715 Epoch   9 Step:      600 Batch Loss:     3.487727 Tokens per Sec:     2490, Lr: 0.000200
2020-07-08 02:00:52,478 Epoch   9: total training loss 231.35
2020-07-08 02:00:52,478 EPOCH 10
2020-07-08 02:02:01,984 Epoch  10: total training loss 212.61
2020-07-08 02:02:01,984 EPOCH 11
2020-07-08 02:02:15,379 Epoch  11 Step:      700 Batch Loss:     3.334935 Tokens per Sec:     2442, Lr: 0.000200
2020-07-08 02:03:11,664 Epoch  11: total training loss 198.25
2020-07-08 02:03:11,664 EPOCH 12
2020-07-08 02:03:56,354 Epoch  12 Step:      800 Batch Loss:     2.617477 Tokens per Sec:     2449, Lr: 0.000200
2020-07-08 02:04:21,395 Epoch  12: total training loss 181.63
2020-07-08 02:04:21,395 EPOCH 13
2020-07-08 02:05:31,072 Epoch  13: total training loss 167.81
2020-07-08 02:05:31,072 EPOCH 14
2020-07-08 02:05:38,037 Epoch  14 Step:      900 Batch Loss:     1.583335 Tokens per Sec:     2691, Lr: 0.000200
2020-07-08 02:06:40,532 Epoch  14: total training loss 152.75
2020-07-08 02:06:40,532 EPOCH 15
2020-07-08 02:07:20,863 Epoch  15 Step:     1000 Batch Loss:     1.446793 Tokens per Sec:     2453, Lr: 0.000200
2020-07-08 02:10:44,488 Hooray! New best validation result [ppl]!
2020-07-08 02:10:44,489 Saving new checkpoint.
2020-07-08 02:10:45,391 Example #0
2020-07-08 02:10:45,391 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-08 02:10:45,391 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-08 02:10:45,391 	Source:     Hi , how can I help you ?
2020-07-08 02:10:45,392 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-08 02:10:45,392 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-08 02:10:45,392 Example #1
2020-07-08 02:10:45,392 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-08 02:10:45,392 	Raw hypothesis: ['O@@', 'k', ',', 'welche', 'Art', 'von', 'Essen', 'möchten', 'Sie', '?']
2020-07-08 02:10:45,392 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-08 02:10:45,392 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-08 02:10:45,392 	Hypothesis: Ok , welche Art von Essen möchten Sie ?
2020-07-08 02:10:45,393 Example #2
2020-07-08 02:10:45,393 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-08 02:10:45,393 	Raw hypothesis: ['Si@@', 'cher', ',', 'lassen', 'Sie', 'mich', 'das', 'für', 'Sie', 'finden', ',', 'ich', 'habe', 'ein', 'paar', 'Optionen', 'für', 'ein', 'paar', 'Optionen', 'gefunden', '.']
2020-07-08 02:10:45,393 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-08 02:10:45,393 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-08 02:10:45,393 	Hypothesis: Sicher , lassen Sie mich das für Sie finden , ich habe ein paar Optionen für ein paar Optionen gefunden .
2020-07-08 02:10:45,393 Example #3
2020-07-08 02:10:45,393 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-08 02:10:45,393 	Raw hypothesis: ['Ich', 'habe', 'ein', 'paar', 'Optionen', 'gefunden', ',', 'die', 'die', 'erste', 'heißt', 'H@@', 'üh@@', 'n@@', 'chen', ',', 'die', 'die', 'andere', 'Option', 'ist', '.', 'Die', 'erste', 'heißt', 'andere', 'heißt', 'Sal@@', 'ri@@', 'ka', '.', 'Die', 'zweite', 'heißt', 'andere', 'Option', 'ist', 'mit', 'einer', 'zweite', 'heißt', 'Sal@@', 'ri@@', 'ri@@', 'ka', '.', '‖', 'Die', 'zweite', 'heißt', 'andere', 'andere', 'Option', ',', 'die', 'zweite', 'heißt', 'andere', 'andere', 'andere', 'andere', 'andere', 'Option', 'ist', 'ein', 'zweite', 'heißt', 'andere', 'andere', 'andere', 'andere', 'andere', 'andere', 'andere', 'Option', '.', '‖', 'Die', 'zweite', 'heißt', 'andere', 'andere', 'andere', 'andere', 'Option', '.']
2020-07-08 02:10:45,394 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-08 02:10:45,394 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-08 02:10:45,394 	Hypothesis: Ich habe ein paar Optionen gefunden , die die erste heißt Hühnchen , die die andere Option ist . Die erste heißt andere heißt Salrika . Die zweite heißt andere Option ist mit einer zweite heißt Salririka . ‖ Die zweite heißt andere andere Option , die zweite heißt andere andere andere andere andere Option ist ein zweite heißt andere andere andere andere andere andere andere Option . ‖ Die zweite heißt andere andere andere andere Option .
2020-07-08 02:10:45,394 Validation result (greedy) at epoch  15, step     1000: bleu:  16.51, loss: 38468.0000, ppl:  13.1404, duration: 204.5299s
2020-07-08 02:11:14,417 Epoch  15: total training loss 138.58
2020-07-08 02:11:14,417 EPOCH 16
2020-07-08 02:12:23,470 Epoch  16: total training loss 134.22
2020-07-08 02:12:23,470 EPOCH 17
2020-07-08 02:12:27,740 Epoch  17 Step:     1100 Batch Loss:     2.213821 Tokens per Sec:     2561, Lr: 0.000200
2020-07-08 02:13:32,485 Epoch  17: total training loss 122.73
2020-07-08 02:13:32,485 EPOCH 18
2020-07-08 02:14:09,044 Epoch  18 Step:     1200 Batch Loss:     0.963664 Tokens per Sec:     2475, Lr: 0.000200
2020-07-08 02:14:41,975 Epoch  18: total training loss 117.93
2020-07-08 02:14:41,976 EPOCH 19
2020-07-08 02:15:50,840 Epoch  19 Step:     1300 Batch Loss:     1.535227 Tokens per Sec:     2498, Lr: 0.000200
2020-07-08 02:15:50,841 Epoch  19: total training loss 103.59
2020-07-08 02:15:50,842 EPOCH 20
2020-07-08 02:16:59,798 Epoch  20: total training loss 100.22
2020-07-08 02:16:59,799 EPOCH 21
2020-07-08 02:17:32,075 Epoch  21 Step:     1400 Batch Loss:     0.863174 Tokens per Sec:     2476, Lr: 0.000200
2020-07-08 02:18:08,880 Epoch  21: total training loss 92.97
2020-07-08 02:18:08,880 EPOCH 22
2020-07-08 02:19:13,939 Epoch  22 Step:     1500 Batch Loss:     0.397101 Tokens per Sec:     2478, Lr: 0.000200
2020-07-08 02:19:18,232 Epoch  22: total training loss 84.15
2020-07-08 02:19:18,233 EPOCH 23
2020-07-08 02:20:27,005 Epoch  23: total training loss 78.64
2020-07-08 02:20:27,005 EPOCH 24
2020-07-08 02:20:54,720 Epoch  24 Step:     1600 Batch Loss:     1.952184 Tokens per Sec:     2495, Lr: 0.000200
2020-07-08 02:21:36,289 Epoch  24: total training loss 76.32
2020-07-08 02:21:36,289 EPOCH 25
2020-07-08 02:22:37,213 Epoch  25 Step:     1700 Batch Loss:     1.028219 Tokens per Sec:     2503, Lr: 0.000200
2020-07-08 02:22:45,191 Epoch  25: total training loss 68.29
2020-07-08 02:22:45,191 EPOCH 26
2020-07-08 02:23:54,750 Epoch  26: total training loss 60.86
2020-07-08 02:23:54,751 EPOCH 27
2020-07-08 02:24:19,555 Epoch  27 Step:     1800 Batch Loss:     0.488248 Tokens per Sec:     2446, Lr: 0.000200
2020-07-08 02:25:04,471 Epoch  27: total training loss 56.13
2020-07-08 02:25:04,471 EPOCH 28
2020-07-08 02:26:02,524 Epoch  28 Step:     1900 Batch Loss:     0.764811 Tokens per Sec:     2494, Lr: 0.000200
2020-07-08 02:26:13,761 Epoch  28: total training loss 52.19
2020-07-08 02:26:13,761 EPOCH 29
2020-07-08 02:27:22,507 Epoch  29: total training loss 49.50
2020-07-08 02:27:22,508 EPOCH 30
2020-07-08 02:27:43,465 Epoch  30 Step:     2000 Batch Loss:     0.422841 Tokens per Sec:     2487, Lr: 0.000200
2020-07-08 02:30:13,204 Hooray! New best validation result [ppl]!
2020-07-08 02:30:13,204 Saving new checkpoint.
2020-07-08 02:30:14,089 Example #0
2020-07-08 02:30:14,089 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-08 02:30:14,089 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-08 02:30:14,090 	Source:     Hi , how can I help you ?
2020-07-08 02:30:14,090 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-08 02:30:14,090 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-08 02:30:14,090 Example #1
2020-07-08 02:30:14,090 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-08 02:30:14,090 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-08 02:30:14,090 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-08 02:30:14,090 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-08 02:30:14,091 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-08 02:30:14,091 Example #2
2020-07-08 02:30:14,091 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-08 02:30:14,091 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'einen', 'Moment', ',', 'ich', 'habe', 'ein', 'paar', 'Optionen', 'für', 'ein', 'paar', 'Optionen', 'gefunden', '.']
2020-07-08 02:30:14,091 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-08 02:30:14,091 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-08 02:30:14,091 	Hypothesis: Klar , geben Sie mir einen Moment , ich habe ein paar Optionen für ein paar Optionen gefunden .
2020-07-08 02:30:14,091 Example #3
2020-07-08 02:30:14,092 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-08 02:30:14,092 	Raw hypothesis: ['Ich', 'habe', '"', '"', '"', '"', '"', 'gefunden', ',', 'ein', 'paar', 'P@@', 'izz@@', 'eria', '"', 'F@@', 'ast@@', '-F@@', 'oo@@', 'd-@@', 'Ort', 'mit', 'einer', 'Sal@@', 'ate', '&', 'Wein', '.', 'Ich', 'habe', 'eine', 'P@@', 'izza', ',', 'Sp@@', 'eck', 'und', 'Z@@', 'wi@@', 'eb@@', 'eln', '.', 'Ich', 'habe', 'eine', 'Sal@@', 'ate', 'mit', 'einer', 'umfangreichen', 'Speis@@', 'ek@@', 'arte', 'mit', 'einer', 'T@@', 'isch@@', 'ecken', 'auf', 'der', 'Terrasse', '.']
2020-07-08 02:30:14,092 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-08 02:30:14,092 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-08 02:30:14,092 	Hypothesis: Ich habe " " " " " gefunden , ein paar Pizzeria " Fast-Food-Ort mit einer Salate & Wein . Ich habe eine Pizza , Speck und Zwiebeln . Ich habe eine Salate mit einer umfangreichen Speisekarte mit einer Tischecken auf der Terrasse .
2020-07-08 02:30:14,092 Validation result (greedy) at epoch  30, step     2000: bleu:  36.10, loss: 29126.9961, ppl:   7.0304, duration: 150.6267s
2020-07-08 02:31:02,595 Epoch  30: total training loss 47.20
2020-07-08 02:31:02,596 EPOCH 31
2020-07-08 02:31:55,022 Epoch  31 Step:     2100 Batch Loss:     1.017046 Tokens per Sec:     2496, Lr: 0.000200
2020-07-08 02:32:11,508 Epoch  31: total training loss 43.32
2020-07-08 02:32:11,508 EPOCH 32
2020-07-08 02:33:21,048 Epoch  32: total training loss 41.36
2020-07-08 02:33:21,049 EPOCH 33
2020-07-08 02:33:34,975 Epoch  33 Step:     2200 Batch Loss:     0.358601 Tokens per Sec:     2392, Lr: 0.000200
2020-07-08 02:34:30,735 Epoch  33: total training loss 37.23
2020-07-08 02:34:30,735 EPOCH 34
2020-07-08 02:35:16,764 Epoch  34 Step:     2300 Batch Loss:     0.294752 Tokens per Sec:     2513, Lr: 0.000200
2020-07-08 02:35:39,733 Epoch  34: total training loss 32.63
2020-07-08 02:35:39,734 EPOCH 35
2020-07-08 02:36:49,052 Epoch  35: total training loss 31.78
2020-07-08 02:36:49,052 EPOCH 36
2020-07-08 02:36:56,875 Epoch  36 Step:     2400 Batch Loss:     0.335077 Tokens per Sec:     2355, Lr: 0.000200
2020-07-08 02:37:58,279 Epoch  36: total training loss 30.13
2020-07-08 02:37:58,279 EPOCH 37
2020-07-08 02:38:37,596 Epoch  37 Step:     2500 Batch Loss:     0.425761 Tokens per Sec:     2556, Lr: 0.000200
2020-07-08 02:39:06,858 Epoch  37: total training loss 25.51
2020-07-08 02:39:06,859 EPOCH 38
2020-07-08 02:40:15,859 Epoch  38: total training loss 23.48
2020-07-08 02:40:15,859 EPOCH 39
2020-07-08 02:40:18,855 Epoch  39 Step:     2600 Batch Loss:     0.223416 Tokens per Sec:     2413, Lr: 0.000200
2020-07-08 02:41:25,271 Epoch  39: total training loss 21.15
2020-07-08 02:41:25,272 EPOCH 40
2020-07-08 02:42:02,226 Epoch  40 Step:     2700 Batch Loss:     0.296055 Tokens per Sec:     2500, Lr: 0.000200
2020-07-08 02:42:34,502 Epoch  40: total training loss 19.76
2020-07-08 02:42:34,502 EPOCH 41
2020-07-08 02:43:42,988 Epoch  41 Step:     2800 Batch Loss:     0.299350 Tokens per Sec:     2476, Lr: 0.000200
2020-07-08 02:43:43,999 Epoch  41: total training loss 18.96
2020-07-08 02:43:43,999 EPOCH 42
2020-07-08 02:44:53,539 Epoch  42: total training loss 17.03
2020-07-08 02:44:53,539 EPOCH 43
2020-07-08 02:45:24,988 Epoch  43 Step:     2900 Batch Loss:     0.220744 Tokens per Sec:     2476, Lr: 0.000200
2020-07-08 02:46:03,420 Epoch  43: total training loss 15.99
2020-07-08 02:46:03,420 EPOCH 44
2020-07-08 02:47:06,501 Epoch  44 Step:     3000 Batch Loss:     0.181085 Tokens per Sec:     2467, Lr: 0.000200
2020-07-08 02:49:41,247 Example #0
2020-07-08 02:49:41,247 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-08 02:49:41,247 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-08 02:49:41,247 	Source:     Hi , how can I help you ?
2020-07-08 02:49:41,247 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-08 02:49:41,247 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-08 02:49:41,248 Example #1
2020-07-08 02:49:41,248 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-08 02:49:41,248 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-08 02:49:41,248 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-08 02:49:41,248 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-08 02:49:41,248 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-08 02:49:41,248 Example #2
2020-07-08 02:49:41,248 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-08 02:49:41,248 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'bitte', 'einen', 'Moment', ',', 'ich', 'werde', 'ein', 'paar', 'Optionen', 'mit', 'ein', 'paar', 'Optionen', 'heraus@@', 'finden', '.']
2020-07-08 02:49:41,249 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-08 02:49:41,249 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-08 02:49:41,249 	Hypothesis: Klar , geben Sie mir bitte einen Moment , ich werde ein paar Optionen mit ein paar Optionen herausfinden .
2020-07-08 02:49:41,249 Example #3
2020-07-08 02:49:41,249 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-08 02:49:41,249 	Raw hypothesis: ['Ich', 'habe', 'folgendes', 'gefunden', ':', '"', 'Se@@', 'as@@', 'ons', '52', '"', ',', 'eine', 'sa@@', 'ison@@', 'alen', 'amerikanischen', 'Gerichten', 'sowie', 'internationalen', 'W@@', 'einen', 'in', 'geho@@', 'ben@@', 'em', 'Rahmen', '.', 'Ich', 'habe', 'auch', ':', '"', ',', 'eine', 'umfangreiche', 'Speis@@', 'ek@@', 'arte', 'mit', 'sa@@', 'ison@@', 'alen', ',', 'lokalen', 'Zutaten', 'anbietet', '.']
2020-07-08 02:49:41,249 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-08 02:49:41,249 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-08 02:49:41,249 	Hypothesis: Ich habe folgendes gefunden : " Seasons 52 " , eine saisonalen amerikanischen Gerichten sowie internationalen Weinen in gehobenem Rahmen . Ich habe auch : " , eine umfangreiche Speisekarte mit saisonalen , lokalen Zutaten anbietet .
2020-07-08 02:49:41,250 Validation result (greedy) at epoch  44, step     3000: bleu:  39.15, loss: 29629.6270, ppl:   7.2711, duration: 154.7476s
2020-07-08 02:49:47,600 Epoch  44: total training loss 14.91
2020-07-08 02:49:47,601 EPOCH 45
2020-07-08 02:50:56,726 Epoch  45: total training loss 13.74
2020-07-08 02:50:56,726 EPOCH 46
2020-07-08 02:51:23,584 Epoch  46 Step:     3100 Batch Loss:     0.252405 Tokens per Sec:     2520, Lr: 0.000200
2020-07-08 02:52:05,505 Epoch  46: total training loss 12.92
2020-07-08 02:52:05,506 EPOCH 47
2020-07-08 02:53:04,733 Epoch  47 Step:     3200 Batch Loss:     0.191559 Tokens per Sec:     2477, Lr: 0.000200
2020-07-08 02:53:15,103 Epoch  47: total training loss 13.22
2020-07-08 02:53:15,103 EPOCH 48
2020-07-08 02:54:24,459 Epoch  48: total training loss 14.62
2020-07-08 02:54:24,459 EPOCH 49
2020-07-08 02:54:46,363 Epoch  49 Step:     3300 Batch Loss:     0.243024 Tokens per Sec:     2453, Lr: 0.000200
2020-07-08 02:55:33,684 Epoch  49: total training loss 13.14
2020-07-08 02:55:33,685 EPOCH 50
2020-07-08 02:56:28,579 Epoch  50 Step:     3400 Batch Loss:     0.198717 Tokens per Sec:     2510, Lr: 0.000200
2020-07-08 02:56:42,746 Epoch  50: total training loss 11.30
2020-07-08 02:56:42,746 Training ended after  50 epochs.
2020-07-08 02:56:42,746 Best validation result (greedy) at step     2000:   7.03 ppl.
2020-07-08 02:58:29,926  dev bleu:  37.41 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-08 02:58:29,928 Translations saved to: models/wmt20/tfm_b4096_ende/00002000.hyps.dev
2020-07-08 02:58:29,929 test bleu:  -1.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-08 02:58:29,930 Translations saved to: models/wmt20/tfm_b4096_ende/00002000.hyps.test
