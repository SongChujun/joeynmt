2020-06-17 10:51:17,498 Hello! This is Joey-NMT.
2020-06-17 10:51:22,271 Total params: 14786561
2020-06-17 10:51:22,274 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-17 10:51:24,233 cfg.name                           : transformer_arch-iwslt14_multi_enc_shared_deen
2020-06-17 10:51:24,233 cfg.data.src                       : de
2020-06-17 10:51:24,233 cfg.data.trg                       : en
2020-06-17 10:51:24,233 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-17 10:51:24,233 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-17 10:51:24,233 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-17 10:51:24,233 cfg.data.level                     : bpe
2020-06-17 10:51:24,233 cfg.data.lowercase                 : True
2020-06-17 10:51:24,233 cfg.data.max_sent_length           : 100
2020-06-17 10:51:24,233 cfg.testing.beam_size              : 5
2020-06-17 10:51:24,233 cfg.testing.alpha                  : 1.0
2020-06-17 10:51:24,234 cfg.training.random_seed           : 42
2020-06-17 10:51:24,234 cfg.training.optimizer             : adam
2020-06-17 10:51:24,234 cfg.training.normalization         : tokens
2020-06-17 10:51:24,234 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-17 10:51:24,234 cfg.training.scheduling            : plateau
2020-06-17 10:51:24,234 cfg.training.patience              : 5
2020-06-17 10:51:24,234 cfg.training.decrease_factor       : 0.7
2020-06-17 10:51:24,234 cfg.training.loss                  : crossentropy
2020-06-17 10:51:24,234 cfg.training.learning_rate         : 0.0003
2020-06-17 10:51:24,234 cfg.training.learning_rate_min     : 1e-08
2020-06-17 10:51:24,234 cfg.training.weight_decay          : 0.0
2020-06-17 10:51:24,234 cfg.training.label_smoothing       : 0.1
2020-06-17 10:51:24,234 cfg.training.batch_size            : 4096
2020-06-17 10:51:24,234 cfg.training.batch_type            : token
2020-06-17 10:51:24,234 cfg.training.eval_batch_size       : 3600
2020-06-17 10:51:24,234 cfg.training.eval_batch_type       : token
2020-06-17 10:51:24,234 cfg.training.batch_multiplier      : 1
2020-06-17 10:51:24,234 cfg.training.early_stopping_metric : eval_metric
2020-06-17 10:51:24,234 cfg.training.epochs                : 100
2020-06-17 10:51:24,234 cfg.training.validation_freq       : 1000
2020-06-17 10:51:24,234 cfg.training.logging_freq          : 100
2020-06-17 10:51:24,234 cfg.training.eval_metric           : bleu
2020-06-17 10:51:24,234 cfg.training.model_dir             : models/transformer_arch-iwslt14_multi_enc_shared_deen
2020-06-17 10:51:24,235 cfg.training.overwrite             : True
2020-06-17 10:51:24,235 cfg.training.shuffle               : True
2020-06-17 10:51:24,235 cfg.training.use_cuda              : True
2020-06-17 10:51:24,235 cfg.training.max_output_length     : 100
2020-06-17 10:51:24,235 cfg.training.print_valid_sents     : [0, 1, 2, 3, 4]
2020-06-17 10:51:24,235 cfg.training.keep_last_ckpts       : 5
2020-06-17 10:51:24,235 cfg.model.initializer              : xavier
2020-06-17 10:51:24,235 cfg.model.bias_initializer         : zeros
2020-06-17 10:51:24,235 cfg.model.init_gain                : 1.0
2020-06-17 10:51:24,235 cfg.model.embed_initializer        : xavier
2020-06-17 10:51:24,235 cfg.model.embed_init_gain          : 1.0
2020-06-17 10:51:24,235 cfg.model.tied_embeddings          : False
2020-06-17 10:51:24,235 cfg.model.tied_softmax             : True
2020-06-17 10:51:24,235 cfg.model.encoder.type             : transformer
2020-06-17 10:51:24,235 cfg.model.encoder.num_layers       : 6
2020-06-17 10:51:24,235 cfg.model.encoder.num_heads        : 4
2020-06-17 10:51:24,235 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-17 10:51:24,235 cfg.model.encoder.embeddings.scale : True
2020-06-17 10:51:24,235 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-17 10:51:24,235 cfg.model.encoder.hidden_size      : 256
2020-06-17 10:51:24,235 cfg.model.encoder.ff_size          : 1024
2020-06-17 10:51:24,235 cfg.model.encoder.freeze           : False
2020-06-17 10:51:24,235 cfg.model.encoder.multi_encoder    : True
2020-06-17 10:51:24,235 cfg.model.encoder.share_encoder    : True
2020-06-17 10:51:24,235 cfg.model.encoder.dropout          : 0.3
2020-06-17 10:51:24,236 cfg.model.decoder.type             : transformer
2020-06-17 10:51:24,236 cfg.model.decoder.num_layers       : 6
2020-06-17 10:51:24,236 cfg.model.decoder.num_heads        : 4
2020-06-17 10:51:24,236 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-17 10:51:24,236 cfg.model.decoder.embeddings.scale : True
2020-06-17 10:51:24,236 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-17 10:51:24,236 cfg.model.decoder.hidden_size      : 256
2020-06-17 10:51:24,236 cfg.model.decoder.ff_size          : 1024
2020-06-17 10:51:24,236 cfg.model.decoder.freeze           : False
2020-06-17 10:51:24,236 cfg.model.decoder.dropout          : 0.3
2020-06-17 10:51:24,236 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-17 10:51:24,236 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-17 10:51:24,236 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-17 10:51:24,236 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-17 10:51:24,236 Number of Src words (types): 5876
2020-06-17 10:51:24,236 Number of Trg words (types): 4561
2020-06-17 10:51:24,236 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=256, vocab_size=5876),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4561))
2020-06-17 10:51:24,245 EPOCH 1
2020-06-17 10:51:36,037 Epoch   1: total training loss 300.25
2020-06-17 10:51:36,037 EPOCH 2
2020-06-17 10:51:46,112 Epoch   2 Step:      100 Batch Loss:     4.916021 Tokens per Sec:    11502, Lr: 0.000300
2020-06-17 10:51:47,344 Epoch   2: total training loss 251.77
2020-06-17 10:51:47,344 EPOCH 3
2020-06-17 10:51:58,747 Epoch   3: total training loss 247.42
2020-06-17 10:51:58,748 EPOCH 4
2020-06-17 10:52:07,172 Epoch   4 Step:      200 Batch Loss:     5.019435 Tokens per Sec:    11412, Lr: 0.000300
2020-06-17 10:52:10,137 Epoch   4: total training loss 238.54
2020-06-17 10:52:10,137 EPOCH 5
2020-06-17 10:52:21,488 Epoch   5: total training loss 226.36
2020-06-17 10:52:21,489 EPOCH 6
2020-06-17 10:52:28,174 Epoch   6 Step:      300 Batch Loss:     4.342203 Tokens per Sec:    11347, Lr: 0.000300
2020-06-17 10:52:33,180 Epoch   6: total training loss 209.04
2020-06-17 10:52:33,181 EPOCH 7
2020-06-17 10:52:44,829 Epoch   7: total training loss 203.88
2020-06-17 10:52:44,829 EPOCH 8
2020-06-17 10:52:49,740 Epoch   8 Step:      400 Batch Loss:     2.735342 Tokens per Sec:    11694, Lr: 0.000300
2020-06-17 10:52:56,254 Epoch   8: total training loss 192.19
2020-06-17 10:52:56,255 EPOCH 9
2020-06-17 10:53:07,682 Epoch   9: total training loss 176.52
2020-06-17 10:53:07,683 EPOCH 10
2020-06-17 10:53:11,328 Epoch  10 Step:      500 Batch Loss:     3.921876 Tokens per Sec:    11544, Lr: 0.000300
2020-06-17 10:53:19,134 Epoch  10: total training loss 176.56
2020-06-17 10:53:19,135 EPOCH 11
2020-06-17 10:53:30,584 Epoch  11: total training loss 167.97
2020-06-17 10:53:30,584 EPOCH 12
2020-06-17 10:53:32,596 Epoch  12 Step:      600 Batch Loss:     2.312406 Tokens per Sec:    11305, Lr: 0.000300
2020-06-17 10:53:41,955 Epoch  12: total training loss 157.46
2020-06-17 10:53:41,956 EPOCH 13
2020-06-17 10:53:53,411 Epoch  13: total training loss 154.31
2020-06-17 10:53:53,412 EPOCH 14
2020-06-17 10:53:53,875 Epoch  14 Step:      700 Batch Loss:     3.145554 Tokens per Sec:     9588, Lr: 0.000300
2020-06-17 10:54:04,802 Epoch  14: total training loss 146.77
2020-06-17 10:54:04,803 EPOCH 15
2020-06-17 10:54:15,193 Epoch  15 Step:      800 Batch Loss:     2.470381 Tokens per Sec:    11623, Lr: 0.000300
2020-06-17 10:54:16,245 Epoch  15: total training loss 135.79
2020-06-17 10:54:16,245 EPOCH 16
2020-06-17 10:54:27,775 Epoch  16: total training loss 135.56
2020-06-17 10:54:27,775 EPOCH 17
2020-06-17 10:54:36,953 Epoch  17 Step:      900 Batch Loss:     2.975961 Tokens per Sec:    11424, Lr: 0.000300
2020-06-17 10:54:39,099 Epoch  17: total training loss 125.99
2020-06-17 10:54:39,099 EPOCH 18
2020-06-17 10:54:50,387 Epoch  18: total training loss 124.46
2020-06-17 10:54:50,387 EPOCH 19
2020-06-17 10:54:57,464 Epoch  19 Step:     1000 Batch Loss:     1.709106 Tokens per Sec:    11980, Lr: 0.000300
2020-06-17 10:55:21,120 Hooray! New best validation result [eval_metric]!
2020-06-17 10:55:21,121 Saving new checkpoint.
2020-06-17 10:55:23,227 Example #0
2020-06-17 10:55:23,227 	Raw source:     ['hallo', ',']
2020-06-17 10:55:23,228 	Raw hypothesis: ['hi', '.']
2020-06-17 10:55:23,228 	Source:     hallo ,
2020-06-17 10:55:23,228 	Reference:  hello .
2020-06-17 10:55:23,228 	Hypothesis: hi .
2020-06-17 10:55:23,228 Example #1
2020-06-17 10:55:23,228 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:55:23,228 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:55:23,228 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:55:23,228 	Reference:  hi , how can i help you ?
2020-06-17 10:55:23,228 	Hypothesis: hi , how can i help you ?
2020-06-17 10:55:23,228 Example #2
2020-06-17 10:55:23,228 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:55:23,228 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:55:23,228 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:55:23,228 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:55:23,228 	Hypothesis: hi , i &apos;m looking for a restaurant in san francisco , california .
2020-06-17 10:55:23,228 Example #3
2020-06-17 10:55:23,228 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:55:23,228 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'food', 'would', 'you', 'like', '?']
2020-06-17 10:55:23,228 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:55:23,228 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:55:23,228 	Hypothesis: ok , what kind of food would you like ?
2020-06-17 10:55:23,228 Example #4
2020-06-17 10:55:23,228 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 10:55:23,228 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'a', 'restaurant', '.']
2020-06-17 10:55:23,228 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 10:55:23,228 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 10:55:23,228 	Hypothesis: i &apos;m looking for a restaurant .
2020-06-17 10:55:23,228 Validation result (greedy) at epoch  19, step     1000: bleu:  15.94, loss: 54433.0625, ppl:  13.3604, duration: 25.7637s
2020-06-17 10:55:27,569 Epoch  19: total training loss 118.84
2020-06-17 10:55:27,570 EPOCH 20
2020-06-17 10:55:38,983 Epoch  20: total training loss 116.20
2020-06-17 10:55:38,984 EPOCH 21
2020-06-17 10:55:44,745 Epoch  21 Step:     1100 Batch Loss:     1.410925 Tokens per Sec:    10891, Lr: 0.000300
2020-06-17 10:55:50,662 Epoch  21: total training loss 115.03
2020-06-17 10:55:50,662 EPOCH 22
2020-06-17 10:56:02,021 Epoch  22: total training loss 105.58
2020-06-17 10:56:02,023 EPOCH 23
2020-06-17 10:56:05,892 Epoch  23 Step:     1200 Batch Loss:     2.971022 Tokens per Sec:    11342, Lr: 0.000300
2020-06-17 10:56:13,572 Epoch  23: total training loss 104.84
2020-06-17 10:56:13,573 EPOCH 24
2020-06-17 10:56:24,936 Epoch  24: total training loss 100.94
2020-06-17 10:56:24,936 EPOCH 25
2020-06-17 10:56:27,102 Epoch  25 Step:     1300 Batch Loss:     1.334225 Tokens per Sec:    11441, Lr: 0.000300
2020-06-17 10:56:36,359 Epoch  25: total training loss 101.30
2020-06-17 10:56:36,360 EPOCH 26
2020-06-17 10:56:47,993 Epoch  26: total training loss 94.15
2020-06-17 10:56:47,993 EPOCH 27
2020-06-17 10:56:48,227 Epoch  27 Step:     1400 Batch Loss:     1.199271 Tokens per Sec:    11562, Lr: 0.000300
2020-06-17 10:56:59,576 Epoch  27: total training loss 93.28
2020-06-17 10:56:59,577 EPOCH 28
2020-06-17 10:57:09,382 Epoch  28 Step:     1500 Batch Loss:     1.064291 Tokens per Sec:    11708, Lr: 0.000300
2020-06-17 10:57:10,941 Epoch  28: total training loss 89.95
2020-06-17 10:57:10,941 EPOCH 29
2020-06-17 10:57:22,346 Epoch  29: total training loss 83.34
2020-06-17 10:57:22,347 EPOCH 30
2020-06-17 10:57:30,908 Epoch  30 Step:     1600 Batch Loss:     1.459010 Tokens per Sec:    11502, Lr: 0.000300
2020-06-17 10:57:33,829 Epoch  30: total training loss 81.42
2020-06-17 10:57:33,829 EPOCH 31
2020-06-17 10:57:45,192 Epoch  31: total training loss 79.83
2020-06-17 10:57:45,193 EPOCH 32
2020-06-17 10:57:52,322 Epoch  32 Step:     1700 Batch Loss:     0.976814 Tokens per Sec:    11425, Lr: 0.000300
2020-06-17 10:57:56,627 Epoch  32: total training loss 77.90
2020-06-17 10:57:56,627 EPOCH 33
2020-06-17 10:58:08,154 Epoch  33: total training loss 76.00
2020-06-17 10:58:08,154 EPOCH 34
2020-06-17 10:58:13,501 Epoch  34 Step:     1800 Batch Loss:     0.978868 Tokens per Sec:    11341, Lr: 0.000300
2020-06-17 10:58:19,674 Epoch  34: total training loss 76.91
2020-06-17 10:58:19,675 EPOCH 35
2020-06-17 10:58:31,257 Epoch  35: total training loss 73.62
2020-06-17 10:58:31,257 EPOCH 36
2020-06-17 10:58:34,522 Epoch  36 Step:     1900 Batch Loss:     1.036314 Tokens per Sec:    12275, Lr: 0.000300
2020-06-17 10:58:42,864 Epoch  36: total training loss 71.59
2020-06-17 10:58:42,864 EPOCH 37
2020-06-17 10:58:54,289 Epoch  37: total training loss 66.38
2020-06-17 10:58:54,290 EPOCH 38
2020-06-17 10:58:56,132 Epoch  38 Step:     2000 Batch Loss:     1.695451 Tokens per Sec:    11501, Lr: 0.000300
2020-06-17 10:59:16,124 Hooray! New best validation result [eval_metric]!
2020-06-17 10:59:16,124 Saving new checkpoint.
2020-06-17 10:59:18,191 Example #0
2020-06-17 10:59:18,191 	Raw source:     ['hallo', ',']
2020-06-17 10:59:18,191 	Raw hypothesis: ['hi', 'there', '.']
2020-06-17 10:59:18,192 	Source:     hallo ,
2020-06-17 10:59:18,192 	Reference:  hello .
2020-06-17 10:59:18,192 	Hypothesis: hi there .
2020-06-17 10:59:18,192 Example #1
2020-06-17 10:59:18,192 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:59:18,192 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:59:18,192 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:59:18,192 	Reference:  hi , how can i help you ?
2020-06-17 10:59:18,192 	Hypothesis: hi , how can i help you ?
2020-06-17 10:59:18,192 Example #2
2020-06-17 10:59:18,192 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:59:18,192 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:59:18,192 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:59:18,192 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:59:18,192 	Hypothesis: hi , i &apos;m looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:59:18,192 Example #3
2020-06-17 10:59:18,192 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:59:18,192 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:59:18,192 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:59:18,192 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:59:18,192 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-17 10:59:18,192 Example #4
2020-06-17 10:59:18,192 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 10:59:18,192 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'a', 'restaurant', '.']
2020-06-17 10:59:18,192 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 10:59:18,192 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 10:59:18,192 	Hypothesis: i &apos;m looking for a restaurant .
2020-06-17 10:59:18,192 Validation result (greedy) at epoch  38, step     2000: bleu:  29.94, loss: 45099.9141, ppl:   8.5662, duration: 22.0602s
2020-06-17 10:59:27,709 Epoch  38: total training loss 67.08
2020-06-17 10:59:27,710 EPOCH 39
2020-06-17 10:59:39,277 Epoch  39: total training loss 64.47
2020-06-17 10:59:39,277 EPOCH 40
2020-06-17 10:59:39,468 Epoch  40 Step:     2100 Batch Loss:     1.311945 Tokens per Sec:    13529, Lr: 0.000300
2020-06-17 10:59:50,698 Epoch  40: total training loss 63.98
2020-06-17 10:59:50,699 EPOCH 41
2020-06-17 11:00:00,802 Epoch  41 Step:     2200 Batch Loss:     0.501504 Tokens per Sec:    11522, Lr: 0.000300
2020-06-17 11:00:02,172 Epoch  41: total training loss 62.62
2020-06-17 11:00:02,172 EPOCH 42
2020-06-17 11:00:13,787 Epoch  42: total training loss 59.93
2020-06-17 11:00:13,787 EPOCH 43
2020-06-17 11:00:21,784 Epoch  43 Step:     2300 Batch Loss:     1.471457 Tokens per Sec:    11924, Lr: 0.000300
2020-06-17 11:00:25,229 Epoch  43: total training loss 58.01
2020-06-17 11:00:25,230 EPOCH 44
2020-06-17 11:00:36,618 Epoch  44: total training loss 55.19
2020-06-17 11:00:36,619 EPOCH 45
2020-06-17 11:00:43,521 Epoch  45 Step:     2400 Batch Loss:     1.006538 Tokens per Sec:    11573, Lr: 0.000300
2020-06-17 11:00:48,035 Epoch  45: total training loss 54.34
2020-06-17 11:00:48,036 EPOCH 46
2020-06-17 11:00:59,484 Epoch  46: total training loss 53.30
2020-06-17 11:00:59,485 EPOCH 47
2020-06-17 11:01:05,087 Epoch  47 Step:     2500 Batch Loss:     0.646818 Tokens per Sec:    10974, Lr: 0.000300
2020-06-17 11:01:10,890 Epoch  47: total training loss 55.47
2020-06-17 11:01:10,891 EPOCH 48
2020-06-17 11:01:22,318 Epoch  48: total training loss 52.11
2020-06-17 11:01:22,318 EPOCH 49
2020-06-17 11:01:25,999 Epoch  49 Step:     2600 Batch Loss:     0.666156 Tokens per Sec:    11067, Lr: 0.000300
2020-06-17 11:01:33,927 Epoch  49: total training loss 51.39
2020-06-17 11:01:33,928 EPOCH 50
2020-06-17 11:01:45,431 Epoch  50: total training loss 51.58
2020-06-17 11:01:45,431 EPOCH 51
2020-06-17 11:01:47,222 Epoch  51 Step:     2700 Batch Loss:     0.653984 Tokens per Sec:    11477, Lr: 0.000300
2020-06-17 11:01:57,016 Epoch  51: total training loss 51.81
2020-06-17 11:01:57,017 EPOCH 52
2020-06-17 11:02:08,235 Epoch  52 Step:     2800 Batch Loss:     0.905678 Tokens per Sec:    11487, Lr: 0.000300
2020-06-17 11:02:08,465 Epoch  52: total training loss 48.97
2020-06-17 11:02:08,466 EPOCH 53
2020-06-17 11:02:19,977 Epoch  53: total training loss 46.20
2020-06-17 11:02:19,977 EPOCH 54
2020-06-17 11:02:29,620 Epoch  54 Step:     2900 Batch Loss:     1.024759 Tokens per Sec:    11198, Lr: 0.000300
2020-06-17 11:02:31,429 Epoch  54: total training loss 45.27
2020-06-17 11:02:31,430 EPOCH 55
2020-06-17 11:02:42,999 Epoch  55: total training loss 44.30
2020-06-17 11:02:42,999 EPOCH 56
2020-06-17 11:02:50,894 Epoch  56 Step:     3000 Batch Loss:     0.588594 Tokens per Sec:    11550, Lr: 0.000300
2020-06-17 11:03:09,400 Hooray! New best validation result [eval_metric]!
2020-06-17 11:03:09,401 Saving new checkpoint.
2020-06-17 11:03:11,487 Example #0
2020-06-17 11:03:11,487 	Raw source:     ['hallo', ',']
2020-06-17 11:03:11,487 	Raw hypothesis: ['hi', '.']
2020-06-17 11:03:11,487 	Source:     hallo ,
2020-06-17 11:03:11,487 	Reference:  hello .
2020-06-17 11:03:11,488 	Hypothesis: hi .
2020-06-17 11:03:11,488 Example #1
2020-06-17 11:03:11,488 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 11:03:11,488 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 11:03:11,488 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 11:03:11,488 	Reference:  hi , how can i help you ?
2020-06-17 11:03:11,488 	Hypothesis: hi , how can i help you ?
2020-06-17 11:03:11,488 Example #2
2020-06-17 11:03:11,488 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 11:03:11,488 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 11:03:11,488 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 11:03:11,488 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 11:03:11,488 	Hypothesis: hi , i &apos;m looking for a restaurant for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 11:03:11,488 Example #3
2020-06-17 11:03:11,488 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 11:03:11,488 	Raw hypothesis: ['ok', '.', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 11:03:11,488 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 11:03:11,488 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 11:03:11,489 	Hypothesis: ok . what kind of restaurant are you looking for ?
2020-06-17 11:03:11,489 Example #4
2020-06-17 11:03:11,489 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 11:03:11,489 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'a', 'fast', 'food', '.']
2020-06-17 11:03:11,489 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 11:03:11,489 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 11:03:11,489 	Hypothesis: i &apos;m looking for a fast food .
2020-06-17 11:03:11,489 Validation result (greedy) at epoch  56, step     3000: bleu:  36.44, loss: 43403.0469, ppl:   7.9012, duration: 20.5943s
2020-06-17 11:03:15,032 Epoch  56: total training loss 42.33
2020-06-17 11:03:15,033 EPOCH 57
2020-06-17 11:03:26,555 Epoch  57: total training loss 41.70
2020-06-17 11:03:26,556 EPOCH 58
2020-06-17 11:03:32,867 Epoch  58 Step:     3100 Batch Loss:     0.510494 Tokens per Sec:    11471, Lr: 0.000300
2020-06-17 11:03:38,091 Epoch  58: total training loss 39.94
2020-06-17 11:03:38,092 EPOCH 59
2020-06-17 11:03:49,609 Epoch  59: total training loss 41.25
2020-06-17 11:03:49,609 EPOCH 60
2020-06-17 11:03:54,317 Epoch  60 Step:     3200 Batch Loss:     1.011243 Tokens per Sec:    11467, Lr: 0.000300
2020-06-17 11:04:01,119 Epoch  60: total training loss 38.94
2020-06-17 11:04:01,119 EPOCH 61
2020-06-17 11:04:12,640 Epoch  61: total training loss 38.84
2020-06-17 11:04:12,640 EPOCH 62
2020-06-17 11:04:15,590 Epoch  62 Step:     3300 Batch Loss:     0.765787 Tokens per Sec:    12196, Lr: 0.000300
2020-06-17 11:04:24,018 Epoch  62: total training loss 38.18
2020-06-17 11:04:24,019 EPOCH 63
2020-06-17 11:04:35,420 Epoch  63: total training loss 37.05
2020-06-17 11:04:35,421 EPOCH 64
2020-06-17 11:04:36,692 Epoch  64 Step:     3400 Batch Loss:     0.527962 Tokens per Sec:    13916, Lr: 0.000300
2020-06-17 11:04:46,771 Epoch  64: total training loss 35.81
2020-06-17 11:04:46,772 EPOCH 65
2020-06-17 11:04:58,283 Epoch  65: total training loss 35.17
2020-06-17 11:04:58,284 EPOCH 66
2020-06-17 11:04:58,443 Epoch  66 Step:     3500 Batch Loss:     0.821098 Tokens per Sec:    17220, Lr: 0.000300
2020-06-17 11:05:09,750 Epoch  66: total training loss 35.53
2020-06-17 11:05:09,751 EPOCH 67
2020-06-17 11:05:19,953 Epoch  67 Step:     3600 Batch Loss:     0.601480 Tokens per Sec:    11456, Lr: 0.000300
2020-06-17 11:05:21,309 Epoch  67: total training loss 35.06
2020-06-17 11:05:21,310 EPOCH 68
2020-06-17 11:05:32,869 Epoch  68: total training loss 32.83
2020-06-17 11:05:32,870 EPOCH 69
2020-06-17 11:05:41,406 Epoch  69 Step:     3700 Batch Loss:     0.528407 Tokens per Sec:    11661, Lr: 0.000300
2020-06-17 11:05:44,214 Epoch  69: total training loss 32.85
2020-06-17 11:05:44,214 EPOCH 70
2020-06-17 11:05:55,727 Epoch  70: total training loss 32.65
2020-06-17 11:05:55,728 EPOCH 71
2020-06-17 11:06:02,289 Epoch  71 Step:     3800 Batch Loss:     0.473678 Tokens per Sec:    12523, Lr: 0.000300
2020-06-17 11:06:07,199 Epoch  71: total training loss 32.44
2020-06-17 11:06:07,199 EPOCH 72
2020-06-17 11:06:18,638 Epoch  72: total training loss 31.72
2020-06-17 11:06:18,639 EPOCH 73
2020-06-17 11:06:23,979 Epoch  73 Step:     3900 Batch Loss:     0.738223 Tokens per Sec:    11438, Lr: 0.000300
2020-06-17 11:06:30,140 Epoch  73: total training loss 32.57
2020-06-17 11:06:30,141 EPOCH 74
2020-06-17 11:06:41,738 Epoch  74: total training loss 31.39
2020-06-17 11:06:41,738 EPOCH 75
2020-06-17 11:06:45,297 Epoch  75 Step:     4000 Batch Loss:     0.415841 Tokens per Sec:    12220, Lr: 0.000300
2020-06-17 11:07:01,209 Hooray! New best validation result [eval_metric]!
2020-06-17 11:07:01,209 Saving new checkpoint.
2020-06-17 11:07:03,274 Example #0
2020-06-17 11:07:03,275 	Raw source:     ['hallo', ',']
2020-06-17 11:07:03,275 	Raw hypothesis: ['hello', '.']
2020-06-17 11:07:03,275 	Source:     hallo ,
2020-06-17 11:07:03,275 	Reference:  hello .
2020-06-17 11:07:03,275 	Hypothesis: hello .
2020-06-17 11:07:03,275 Example #1
2020-06-17 11:07:03,275 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 11:07:03,275 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 11:07:03,275 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 11:07:03,275 	Reference:  hi , how can i help you ?
2020-06-17 11:07:03,275 	Hypothesis: hi , how can i help you ?
2020-06-17 11:07:03,275 Example #2
2020-06-17 11:07:03,275 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 11:07:03,275 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 11:07:03,275 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 11:07:03,275 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 11:07:03,275 	Hypothesis: hi , i &apos;m looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 11:07:03,275 Example #3
2020-06-17 11:07:03,275 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 11:07:03,275 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 11:07:03,275 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 11:07:03,275 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 11:07:03,275 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 11:07:03,275 Example #4
2020-06-17 11:07:03,275 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 11:07:03,275 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'something', 'good', 'reviews', '.']
2020-06-17 11:07:03,275 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 11:07:03,275 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 11:07:03,275 	Hypothesis: i &apos;m looking for something good reviews .
2020-06-17 11:07:03,275 Validation result (greedy) at epoch  75, step     4000: bleu:  38.06, loss: 44560.1914, ppl:   8.3488, duration: 17.9774s
2020-06-17 11:07:11,198 Epoch  75: total training loss 30.79
2020-06-17 11:07:11,199 EPOCH 76
2020-06-17 11:07:22,588 Epoch  76: total training loss 29.62
2020-06-17 11:07:22,588 EPOCH 77
2020-06-17 11:07:24,745 Epoch  77 Step:     4100 Batch Loss:     0.452923 Tokens per Sec:    11639, Lr: 0.000300
2020-06-17 11:07:34,020 Epoch  77: total training loss 28.37
2020-06-17 11:07:34,020 EPOCH 78
2020-06-17 11:07:45,429 Epoch  78: total training loss 28.15
2020-06-17 11:07:45,430 EPOCH 79
2020-06-17 11:07:46,039 Epoch  79 Step:     4200 Batch Loss:     0.382103 Tokens per Sec:    12178, Lr: 0.000300
2020-06-17 11:07:56,989 Epoch  79: total training loss 28.38
2020-06-17 11:07:56,990 EPOCH 80
2020-06-17 11:08:07,784 Epoch  80 Step:     4300 Batch Loss:     0.841081 Tokens per Sec:    11102, Lr: 0.000300
2020-06-17 11:08:08,796 Epoch  80: total training loss 28.25
2020-06-17 11:08:08,796 EPOCH 81
2020-06-17 11:08:20,258 Epoch  81: total training loss 27.40
2020-06-17 11:08:20,259 EPOCH 82
2020-06-17 11:08:29,394 Epoch  82 Step:     4400 Batch Loss:     0.545844 Tokens per Sec:    11354, Lr: 0.000300
2020-06-17 11:08:31,750 Epoch  82: total training loss 26.94
2020-06-17 11:08:31,751 EPOCH 83
2020-06-17 11:08:43,089 Epoch  83: total training loss 26.94
2020-06-17 11:08:43,090 EPOCH 84
2020-06-17 11:08:50,225 Epoch  84 Step:     4500 Batch Loss:     0.572164 Tokens per Sec:    11715, Lr: 0.000300
2020-06-17 11:08:54,467 Epoch  84: total training loss 26.35
2020-06-17 11:08:54,468 EPOCH 85
2020-06-17 11:09:05,953 Epoch  85: total training loss 25.14
2020-06-17 11:09:05,953 EPOCH 86
2020-06-17 11:09:11,857 Epoch  86 Step:     4600 Batch Loss:     0.481581 Tokens per Sec:    11452, Lr: 0.000300
2020-06-17 11:09:17,486 Epoch  86: total training loss 25.24
2020-06-17 11:09:17,487 EPOCH 87
2020-06-17 11:09:29,007 Epoch  87: total training loss 26.35
2020-06-17 11:09:29,008 EPOCH 88
2020-06-17 11:09:32,741 Epoch  88 Step:     4700 Batch Loss:     0.665335 Tokens per Sec:    11911, Lr: 0.000300
2020-06-17 11:09:40,462 Epoch  88: total training loss 25.40
2020-06-17 11:09:40,463 EPOCH 89
2020-06-17 11:09:51,836 Epoch  89: total training loss 24.56
2020-06-17 11:09:51,836 EPOCH 90
2020-06-17 11:09:54,619 Epoch  90 Step:     4800 Batch Loss:     0.542742 Tokens per Sec:     9436, Lr: 0.000300
2020-06-17 11:10:03,253 Epoch  90: total training loss 24.22
2020-06-17 11:10:03,253 EPOCH 91
2020-06-17 11:10:14,798 Epoch  91: total training loss 24.24
2020-06-17 11:10:14,798 EPOCH 92
2020-06-17 11:10:15,759 Epoch  92 Step:     4900 Batch Loss:     0.365488 Tokens per Sec:     6668, Lr: 0.000300
2020-06-17 11:10:26,416 Epoch  92: total training loss 23.88
2020-06-17 11:10:26,416 EPOCH 93
2020-06-17 11:10:36,722 Epoch  93 Step:     5000 Batch Loss:     0.362553 Tokens per Sec:    11680, Lr: 0.000300
2020-06-17 11:10:50,975 Hooray! New best validation result [eval_metric]!
2020-06-17 11:10:50,975 Saving new checkpoint.
2020-06-17 11:10:53,153 Example #0
2020-06-17 11:10:53,154 	Raw source:     ['hallo', ',']
2020-06-17 11:10:53,154 	Raw hypothesis: ['hello', '.']
2020-06-17 11:10:53,154 	Source:     hallo ,
2020-06-17 11:10:53,154 	Reference:  hello .
2020-06-17 11:10:53,154 	Hypothesis: hello .
2020-06-17 11:10:53,154 Example #1
2020-06-17 11:10:53,155 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 11:10:53,155 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 11:10:53,155 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 11:10:53,155 	Reference:  hi , how can i help you ?
2020-06-17 11:10:53,155 	Hypothesis: hi , how can i help you ?
2020-06-17 11:10:53,155 Example #2
2020-06-17 11:10:53,155 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 11:10:53,156 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 11:10:53,156 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 11:10:53,156 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 11:10:53,156 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 11:10:53,156 Example #3
2020-06-17 11:10:53,156 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 11:10:53,156 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 11:10:53,157 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 11:10:53,157 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 11:10:53,157 	Hypothesis: ok , what kind of restaurant are you looking for ?
2020-06-17 11:10:53,157 Example #4
2020-06-17 11:10:53,157 	Raw source:     ['ich', 'suche', 'ein', 'günsti@@', 'ges', 'fastfood-@@', 'restaurant', '.']
2020-06-17 11:10:53,157 	Raw hypothesis: ['i', '&apos;m', 'looking', 'for', 'some', 'fast', 'food', 'restaurant', '.']
2020-06-17 11:10:53,157 	Source:     ich suche ein günstiges fastfood-restaurant .
2020-06-17 11:10:53,158 	Reference:  i &apos;m looking for a cheap fast food restaurant .
2020-06-17 11:10:53,158 	Hypothesis: i &apos;m looking for some fast food restaurant .
2020-06-17 11:10:53,158 Validation result (greedy) at epoch  93, step     5000: bleu:  40.88, loss: 44635.6602, ppl:   8.3788, duration: 16.4352s
2020-06-17 11:10:54,162 Epoch  93: total training loss 23.00
2020-06-17 11:10:54,162 EPOCH 94
2020-06-17 11:11:05,539 Epoch  94: total training loss 23.00
2020-06-17 11:11:05,540 EPOCH 95
2020-06-17 11:11:14,715 Epoch  95 Step:     5100 Batch Loss:     0.391647 Tokens per Sec:    10999, Lr: 0.000300
2020-06-17 11:11:17,040 Epoch  95: total training loss 23.12
2020-06-17 11:11:17,040 EPOCH 96
2020-06-17 11:11:28,599 Epoch  96: total training loss 22.25
2020-06-17 11:11:28,599 EPOCH 97
2020-06-17 11:11:35,582 Epoch  97 Step:     5200 Batch Loss:     0.403147 Tokens per Sec:    11500, Lr: 0.000300
2020-06-17 11:11:40,136 Epoch  97: total training loss 22.19
2020-06-17 11:11:40,136 EPOCH 98
2020-06-17 11:11:51,612 Epoch  98: total training loss 21.79
2020-06-17 11:11:51,612 EPOCH 99
2020-06-17 11:11:57,201 Epoch  99 Step:     5300 Batch Loss:     0.383133 Tokens per Sec:    11690, Lr: 0.000300
2020-06-17 11:12:03,168 Epoch  99: total training loss 21.87
2020-06-17 11:12:03,170 EPOCH 100
2020-06-17 11:12:14,494 Epoch 100: total training loss 20.84
2020-06-17 11:12:14,495 Training ended after 100 epochs.
2020-06-17 11:12:14,495 Best validation result (greedy) at step     5000:  40.88 eval_metric.
2020-06-17 11:12:29,001  dev bleu:  42.51 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 11:12:29,006 Translations saved to: models/transformer_arch-iwslt14_multi_enc_shared_deen/00005000.hyps.dev
2020-06-17 11:12:40,044 test bleu:  37.74 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 11:12:40,049 Translations saved to: models/transformer_arch-iwslt14_multi_enc_shared_deen/00005000.hyps.test
