2020-06-15 15:53:47,744 Hello! This is Joey-NMT.
2020-06-15 15:54:02,823 Total params: 55759366
2020-06-15 15:54:02,824 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.W_g.weight', 'decoder.layers.0.b_g', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_prev_trg_att.k_layer.bias', 'decoder.layers.0.src_prev_trg_att.k_layer.weight', 'decoder.layers.0.src_prev_trg_att.output_layer.bias', 'decoder.layers.0.src_prev_trg_att.output_layer.weight', 'decoder.layers.0.src_prev_trg_att.q_layer.bias', 'decoder.layers.0.src_prev_trg_att.q_layer.weight', 'decoder.layers.0.src_prev_trg_att.v_layer.bias', 'decoder.layers.0.src_prev_trg_att.v_layer.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.W_g.weight', 'decoder.layers.1.b_g', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_prev_trg_att.k_layer.bias', 'decoder.layers.1.src_prev_trg_att.k_layer.weight', 'decoder.layers.1.src_prev_trg_att.output_layer.bias', 'decoder.layers.1.src_prev_trg_att.output_layer.weight', 'decoder.layers.1.src_prev_trg_att.q_layer.bias', 'decoder.layers.1.src_prev_trg_att.q_layer.weight', 'decoder.layers.1.src_prev_trg_att.v_layer.bias', 'decoder.layers.1.src_prev_trg_att.v_layer.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.W_g.weight', 'decoder.layers.2.b_g', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_prev_trg_att.k_layer.bias', 'decoder.layers.2.src_prev_trg_att.k_layer.weight', 'decoder.layers.2.src_prev_trg_att.output_layer.bias', 'decoder.layers.2.src_prev_trg_att.output_layer.weight', 'decoder.layers.2.src_prev_trg_att.q_layer.bias', 'decoder.layers.2.src_prev_trg_att.q_layer.weight', 'decoder.layers.2.src_prev_trg_att.v_layer.bias', 'decoder.layers.2.src_prev_trg_att.v_layer.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.W_g.weight', 'decoder.layers.3.b_g', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_prev_trg_att.k_layer.bias', 'decoder.layers.3.src_prev_trg_att.k_layer.weight', 'decoder.layers.3.src_prev_trg_att.output_layer.bias', 'decoder.layers.3.src_prev_trg_att.output_layer.weight', 'decoder.layers.3.src_prev_trg_att.q_layer.bias', 'decoder.layers.3.src_prev_trg_att.q_layer.weight', 'decoder.layers.3.src_prev_trg_att.v_layer.bias', 'decoder.layers.3.src_prev_trg_att.v_layer.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.W_g.weight', 'decoder.layers.4.b_g', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_prev_trg_att.k_layer.bias', 'decoder.layers.4.src_prev_trg_att.k_layer.weight', 'decoder.layers.4.src_prev_trg_att.output_layer.bias', 'decoder.layers.4.src_prev_trg_att.output_layer.weight', 'decoder.layers.4.src_prev_trg_att.q_layer.bias', 'decoder.layers.4.src_prev_trg_att.q_layer.weight', 'decoder.layers.4.src_prev_trg_att.v_layer.bias', 'decoder.layers.4.src_prev_trg_att.v_layer.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.W_g.weight', 'decoder.layers.5.b_g', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_prev_trg_att.k_layer.bias', 'decoder.layers.5.src_prev_trg_att.k_layer.weight', 'decoder.layers.5.src_prev_trg_att.output_layer.bias', 'decoder.layers.5.src_prev_trg_att.output_layer.weight', 'decoder.layers.5.src_prev_trg_att.q_layer.bias', 'decoder.layers.5.src_prev_trg_att.q_layer.weight', 'decoder.layers.5.src_prev_trg_att.v_layer.bias', 'decoder.layers.5.src_prev_trg_att.v_layer.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-15 15:54:09,614 cfg.name                           : transformer
2020-06-15 15:54:09,614 cfg.data.src                       : de
2020-06-15 15:54:09,615 cfg.data.trg                       : en
2020-06-15 15:54:09,615 cfg.data.train                     : chatnmt/prep/train.tags.bpe.10000
2020-06-15 15:54:09,615 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.10000
2020-06-15 15:54:09,615 cfg.data.test                      : chatnmt/prep/test.tags.bpe.10000
2020-06-15 15:54:09,615 cfg.data.level                     : bpe
2020-06-15 15:54:09,615 cfg.data.lowercase                 : True
2020-06-15 15:54:09,615 cfg.data.max_sent_length           : 100
2020-06-15 15:54:09,615 cfg.testing.beam_size              : 5
2020-06-15 15:54:09,615 cfg.testing.alpha                  : 1.0
2020-06-15 15:54:09,615 cfg.training.random_seed           : 42
2020-06-15 15:54:09,615 cfg.training.optimizer             : adam
2020-06-15 15:54:09,615 cfg.training.normalization         : tokens
2020-06-15 15:54:09,615 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-15 15:54:09,615 cfg.training.scheduling            : plateau
2020-06-15 15:54:09,615 cfg.training.patience              : 8
2020-06-15 15:54:09,615 cfg.training.decrease_factor       : 0.7
2020-06-15 15:54:09,615 cfg.training.loss                  : crossentropy
2020-06-15 15:54:09,615 cfg.training.learning_rate         : 0.0002
2020-06-15 15:54:09,615 cfg.training.learning_rate_min     : 1e-08
2020-06-15 15:54:09,615 cfg.training.weight_decay          : 0.0
2020-06-15 15:54:09,615 cfg.training.label_smoothing       : 0.1
2020-06-15 15:54:09,615 cfg.training.batch_size            : 4096
2020-06-15 15:54:09,615 cfg.training.batch_type            : token
2020-06-15 15:54:09,615 cfg.training.eval_batch_size       : 3600
2020-06-15 15:54:09,615 cfg.training.eval_batch_type       : token
2020-06-15 15:54:09,615 cfg.training.batch_multiplier      : 1
2020-06-15 15:54:09,615 cfg.training.early_stopping_metric : ppl
2020-06-15 15:54:09,615 cfg.training.epochs                : 100
2020-06-15 15:54:09,615 cfg.training.validation_freq       : 1000
2020-06-15 15:54:09,615 cfg.training.logging_freq          : 100
2020-06-15 15:54:09,615 cfg.training.eval_metric           : bleu
2020-06-15 15:54:09,615 cfg.training.model_dir             : models/transformer_deen
2020-06-15 15:54:09,615 cfg.training.overwrite             : True
2020-06-15 15:54:09,616 cfg.training.shuffle               : True
2020-06-15 15:54:09,616 cfg.training.use_cuda              : True
2020-06-15 15:54:09,616 cfg.training.max_output_length     : 100
2020-06-15 15:54:09,616 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-15 15:54:09,616 cfg.training.keep_last_ckpts       : 3
2020-06-15 15:54:09,616 cfg.model.initializer              : xavier
2020-06-15 15:54:09,616 cfg.model.bias_initializer         : zeros
2020-06-15 15:54:09,616 cfg.model.init_gain                : 1.0
2020-06-15 15:54:09,616 cfg.model.embed_initializer        : xavier
2020-06-15 15:54:09,616 cfg.model.embed_init_gain          : 1.0
2020-06-15 15:54:09,616 cfg.model.tied_embeddings          : False
2020-06-15 15:54:09,616 cfg.model.tied_softmax             : True
2020-06-15 15:54:09,616 cfg.model.encoder.type             : transformer
2020-06-15 15:54:09,616 cfg.model.encoder.num_layers       : 6
2020-06-15 15:54:09,616 cfg.model.encoder.num_heads        : 8
2020-06-15 15:54:09,616 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-15 15:54:09,616 cfg.model.encoder.embeddings.scale : True
2020-06-15 15:54:09,616 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-15 15:54:09,616 cfg.model.encoder.hidden_size      : 512
2020-06-15 15:54:09,616 cfg.model.encoder.ff_size          : 2048
2020-06-15 15:54:09,616 cfg.model.encoder.freeze           : False
2020-06-15 15:54:09,616 cfg.model.encoder.dropout          : 0.1
2020-06-15 15:54:09,616 cfg.model.decoder.type             : transformer
2020-06-15 15:54:09,616 cfg.model.decoder.num_layers       : 6
2020-06-15 15:54:09,616 cfg.model.decoder.num_heads        : 8
2020-06-15 15:54:09,616 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-15 15:54:09,616 cfg.model.decoder.embeddings.scale : True
2020-06-15 15:54:09,616 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-15 15:54:09,616 cfg.model.decoder.hidden_size      : 512
2020-06-15 15:54:09,616 cfg.model.decoder.ff_size          : 2048
2020-06-15 15:54:09,616 cfg.model.decoder.freeze           : False
2020-06-15 15:54:09,616 cfg.model.decoder.dropout          : 0.1
2020-06-15 15:54:09,616 Data set sizes: 
	train 9467,
	valid 1476,
	test 1163
2020-06-15 15:54:09,616 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-15 15:54:09,616 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-15 15:54:09,617 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-15 15:54:09,617 Number of Src words (types): 5833
2020-06-15 15:54:09,617 Number of Trg words (types): 4536
2020-06-15 15:54:09,617 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=5833),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4536))
2020-06-15 15:54:09,626 EPOCH 1
2020-06-15 15:54:27,684 Epoch   1: total training loss 263.72
2020-06-15 15:54:27,685 EPOCH 2
2020-06-15 15:54:44,317 Epoch   2 Step:      100 Batch Loss:     4.581603 Tokens per Sec:     7690, Lr: 0.000200
2020-06-15 15:54:44,319 Epoch   2: total training loss 225.23
2020-06-15 15:54:44,319 EPOCH 3
2020-06-15 15:55:02,687 Epoch   3: total training loss 207.51
2020-06-15 15:55:02,688 EPOCH 4
2020-06-15 15:55:21,480 Epoch   4 Step:      200 Batch Loss:     5.334261 Tokens per Sec:     6806, Lr: 0.000200
2020-06-15 15:55:21,480 Epoch   4: total training loss 186.31
2020-06-15 15:55:21,480 EPOCH 5
2020-06-15 15:55:40,130 Epoch   5: total training loss 170.86
2020-06-15 15:55:40,131 EPOCH 6
2020-06-15 15:55:58,825 Epoch   6: total training loss 150.12
2020-06-15 15:55:58,826 EPOCH 7
2020-06-15 15:55:59,224 Epoch   7 Step:      300 Batch Loss:     3.262703 Tokens per Sec:     6906, Lr: 0.000200
2020-06-15 15:56:17,569 Epoch   7: total training loss 133.06
2020-06-15 15:56:17,570 EPOCH 8
2020-06-15 15:56:36,202 Epoch   8: total training loss 119.80
2020-06-15 15:56:36,202 EPOCH 9
2020-06-15 15:56:37,228 Epoch   9 Step:      400 Batch Loss:     3.953438 Tokens per Sec:     6535, Lr: 0.000200
2020-06-15 15:56:55,053 Epoch   9: total training loss 111.68
2020-06-15 15:56:55,054 EPOCH 10
2020-06-15 15:57:13,717 Epoch  10: total training loss 96.65
2020-06-15 15:57:13,718 EPOCH 11
2020-06-15 15:57:15,297 Epoch  11 Step:      500 Batch Loss:     2.449464 Tokens per Sec:     7223, Lr: 0.000200
2020-06-15 15:57:32,374 Epoch  11: total training loss 92.31
2020-06-15 15:57:32,375 EPOCH 12
2020-06-15 15:57:51,189 Epoch  12: total training loss 85.48
2020-06-15 15:57:51,190 EPOCH 13
2020-06-15 15:57:53,119 Epoch  13 Step:      600 Batch Loss:     0.539289 Tokens per Sec:     6272, Lr: 0.000200
2020-06-15 15:58:10,031 Epoch  13: total training loss 78.95
2020-06-15 15:58:10,032 EPOCH 14
2020-06-15 15:58:28,711 Epoch  14: total training loss 77.30
2020-06-15 15:58:28,711 EPOCH 15
2020-06-15 15:58:30,613 Epoch  15 Step:      700 Batch Loss:     1.122270 Tokens per Sec:     6825, Lr: 0.000200
2020-06-15 15:58:47,469 Epoch  15: total training loss 64.01
2020-06-15 15:58:47,470 EPOCH 16
2020-06-15 15:59:06,257 Epoch  16: total training loss 56.59
2020-06-15 15:59:06,258 EPOCH 17
2020-06-15 15:59:08,984 Epoch  17 Step:      800 Batch Loss:     1.208293 Tokens per Sec:     7095, Lr: 0.000200
2020-06-15 15:59:24,974 Epoch  17: total training loss 56.10
2020-06-15 15:59:24,975 EPOCH 18
2020-06-15 15:59:43,664 Epoch  18: total training loss 56.16
2020-06-15 15:59:43,664 EPOCH 19
2020-06-15 15:59:46,422 Epoch  19 Step:      900 Batch Loss:     1.226281 Tokens per Sec:     7204, Lr: 0.000200
2020-06-15 16:00:02,408 Epoch  19: total training loss 47.93
2020-06-15 16:00:02,409 EPOCH 20
2020-06-15 16:00:21,203 Epoch  20: total training loss 45.22
2020-06-15 16:00:21,203 EPOCH 21
2020-06-15 16:00:24,295 Epoch  21 Step:     1000 Batch Loss:     2.025252 Tokens per Sec:     6815, Lr: 0.000200
2020-06-15 16:01:05,725 Hooray! New best validation result [ppl]!
2020-06-15 16:01:05,726 Saving new checkpoint.
2020-06-15 16:01:13,314 Example #0
2020-06-15 16:01:13,315 	Raw source:     ['hallo', ',']
2020-06-15 16:01:13,315 	Raw hypothesis: ['hello', '?']
2020-06-15 16:01:13,315 	Source:     hallo ,
2020-06-15 16:01:13,315 	Reference:  hello .
2020-06-15 16:01:13,315 	Hypothesis: hello ?
2020-06-15 16:01:13,315 Example #1
2020-06-15 16:01:13,315 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-15 16:01:13,315 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-15 16:01:13,315 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-15 16:01:13,315 	Reference:  hi , how can i help you ?
2020-06-15 16:01:13,315 	Hypothesis: hi , how can i help you ?
2020-06-15 16:01:13,315 Example #2
2020-06-15 16:01:13,315 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-15 16:01:13,315 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'to', 'eat', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-15 16:01:13,315 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-15 16:01:13,315 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-15 16:01:13,315 	Hypothesis: hi , i &apos;m looking for a restaurant to eat at the arden fair mall in san francisco , california .
2020-06-15 16:01:13,315 Example #3
2020-06-15 16:01:13,315 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-15 16:01:13,315 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-15 16:01:13,315 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-15 16:01:13,316 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-15 16:01:13,316 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-15 16:01:13,316 Validation result (greedy) at epoch  21, step     1000: bleu:  34.29, loss: 41595.2305, ppl:   7.5064, duration: 49.0200s
2020-06-15 16:01:28,081 Epoch  21: total training loss 41.45
2020-06-15 16:01:28,082 EPOCH 22
2020-06-15 16:01:46,875 Epoch  22: total training loss 36.94
2020-06-15 16:01:46,876 EPOCH 23
2020-06-15 16:01:50,437 Epoch  23 Step:     1100 Batch Loss:     0.535389 Tokens per Sec:     7017, Lr: 0.000200
2020-06-15 16:02:05,649 Epoch  23: total training loss 32.40
2020-06-15 16:02:05,649 EPOCH 24
2020-06-15 16:02:24,367 Epoch  24: total training loss 29.25
2020-06-15 16:02:24,368 EPOCH 25
2020-06-15 16:02:28,617 Epoch  25 Step:     1200 Batch Loss:     0.424136 Tokens per Sec:     6938, Lr: 0.000200
2020-06-15 16:02:43,095 Epoch  25: total training loss 28.91
2020-06-15 16:02:43,095 EPOCH 26
2020-06-15 16:03:01,810 Epoch  26: total training loss 27.82
2020-06-15 16:03:01,810 EPOCH 27
2020-06-15 16:03:06,079 Epoch  27 Step:     1300 Batch Loss:     0.326672 Tokens per Sec:     6855, Lr: 0.000200
2020-06-15 16:03:20,452 Epoch  27: total training loss 27.36
2020-06-15 16:03:20,453 EPOCH 28
2020-06-15 16:03:39,344 Epoch  28: total training loss 24.64
2020-06-15 16:03:39,345 EPOCH 29
2020-06-15 16:03:43,375 Epoch  29 Step:     1400 Batch Loss:     0.441646 Tokens per Sec:     6943, Lr: 0.000200
2020-06-15 16:03:58,108 Epoch  29: total training loss 21.68
2020-06-15 16:03:58,108 EPOCH 30
2020-06-15 16:04:16,933 Epoch  30: total training loss 20.06
2020-06-15 16:04:16,934 EPOCH 31
2020-06-15 16:04:21,083 Epoch  31 Step:     1500 Batch Loss:     0.611756 Tokens per Sec:     7150, Lr: 0.000200
2020-06-15 16:04:35,660 Epoch  31: total training loss 16.98
2020-06-15 16:04:35,660 EPOCH 32
2020-06-15 16:04:54,426 Epoch  32: total training loss 15.54
2020-06-15 16:04:54,427 EPOCH 33
2020-06-15 16:04:58,789 Epoch  33 Step:     1600 Batch Loss:     0.227009 Tokens per Sec:     7081, Lr: 0.000200
2020-06-15 16:05:13,190 Epoch  33: total training loss 15.70
2020-06-15 16:05:13,191 EPOCH 34
2020-06-15 16:05:31,994 Epoch  34: total training loss 14.07
2020-06-15 16:05:31,994 EPOCH 35
2020-06-15 16:05:36,373 Epoch  35 Step:     1700 Batch Loss:     0.233873 Tokens per Sec:     6991, Lr: 0.000200
2020-06-15 16:05:50,660 Epoch  35: total training loss 13.28
2020-06-15 16:05:50,661 EPOCH 36
2020-06-15 16:06:09,447 Epoch  36: total training loss 12.33
2020-06-15 16:06:09,448 EPOCH 37
2020-06-15 16:06:14,162 Epoch  37 Step:     1800 Batch Loss:     0.373535 Tokens per Sec:     7050, Lr: 0.000200
2020-06-15 16:06:28,238 Epoch  37: total training loss 11.60
2020-06-15 16:06:28,239 EPOCH 38
2020-06-15 16:06:46,971 Epoch  38: total training loss 10.83
2020-06-15 16:06:46,971 EPOCH 39
2020-06-15 16:06:52,044 Epoch  39 Step:     1900 Batch Loss:     0.218874 Tokens per Sec:     6860, Lr: 0.000200
2020-06-15 16:07:05,786 Epoch  39: total training loss 10.02
2020-06-15 16:07:05,787 EPOCH 40
2020-06-15 16:07:24,492 Epoch  40: total training loss 9.46
2020-06-15 16:07:24,493 EPOCH 41
2020-06-15 16:07:30,213 Epoch  41 Step:     2000 Batch Loss:     0.183295 Tokens per Sec:     7169, Lr: 0.000200
2020-06-15 16:08:32,967 Hooray! New best validation result [ppl]!
2020-06-15 16:08:32,968 Saving new checkpoint.
2020-06-15 16:08:40,216 Example #0
2020-06-15 16:08:40,217 	Raw source:     ['hallo', ',']
2020-06-15 16:08:40,217 	Raw hypothesis: ['hello', '?']
2020-06-15 16:08:40,217 	Source:     hallo ,
2020-06-15 16:08:40,217 	Reference:  hello .
2020-06-15 16:08:40,217 	Hypothesis: hello ?
2020-06-15 16:08:40,217 Example #1
2020-06-15 16:08:40,217 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-15 16:08:40,217 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-15 16:08:40,217 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-15 16:08:40,217 	Reference:  hi , how can i help you ?
2020-06-15 16:08:40,217 	Hypothesis: hi , how can i help you ?
2020-06-15 16:08:40,217 Example #2
2020-06-15 16:08:40,217 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-15 16:08:40,217 	Raw hypothesis: ['hey', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'to', 'eat', 'lunch', 'at', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-15 16:08:40,217 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-15 16:08:40,217 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-15 16:08:40,217 	Hypothesis: hey , i was looking for a restaurant to eat lunch at the arden fair mall in san francisco , california .
2020-06-15 16:08:40,217 Example #3
2020-06-15 16:08:40,217 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-15 16:08:40,217 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-15 16:08:40,217 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-15 16:08:40,217 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-15 16:08:40,218 	Hypothesis: ok , what kind of restaurant are you looking for ?
2020-06-15 16:08:40,218 Validation result (greedy) at epoch  41, step     2000: bleu:  41.03, loss: 39531.0195, ppl:   6.7919, duration: 70.0043s
2020-06-15 16:08:52,298 Epoch  41: total training loss 9.10
2020-06-15 16:08:52,299 EPOCH 42
2020-06-15 16:09:10,989 Epoch  42: total training loss 9.05
2020-06-15 16:09:10,989 EPOCH 43
2020-06-15 16:09:16,544 Epoch  43 Step:     2100 Batch Loss:     0.265915 Tokens per Sec:     7038, Lr: 0.000200
2020-06-15 16:09:29,728 Epoch  43: total training loss 11.52
2020-06-15 16:09:29,728 EPOCH 44
2020-06-15 16:09:48,438 Epoch  44: total training loss 9.75
2020-06-15 16:09:48,438 EPOCH 45
2020-06-15 16:09:54,091 Epoch  45 Step:     2200 Batch Loss:     0.206174 Tokens per Sec:     6973, Lr: 0.000200
2020-06-15 16:10:07,049 Epoch  45: total training loss 8.97
2020-06-15 16:10:07,050 EPOCH 46
2020-06-15 16:10:25,790 Epoch  46: total training loss 9.14
2020-06-15 16:10:25,790 EPOCH 47
2020-06-15 16:10:31,383 Epoch  47 Step:     2300 Batch Loss:     0.181469 Tokens per Sec:     6860, Lr: 0.000200
2020-06-15 16:10:44,503 Epoch  47: total training loss 7.96
2020-06-15 16:10:44,504 EPOCH 48
2020-06-15 16:11:03,248 Epoch  48: total training loss 7.53
2020-06-15 16:11:03,248 EPOCH 49
2020-06-15 16:11:08,743 Epoch  49 Step:     2400 Batch Loss:     0.189558 Tokens per Sec:     6807, Lr: 0.000200
2020-06-15 16:11:21,955 Epoch  49: total training loss 7.67
2020-06-15 16:11:21,955 EPOCH 50
2020-06-15 16:11:40,654 Epoch  50: total training loss 6.88
2020-06-15 16:11:40,654 EPOCH 51
2020-06-15 16:11:46,434 Epoch  51 Step:     2500 Batch Loss:     0.140922 Tokens per Sec:     7014, Lr: 0.000200
2020-06-15 16:11:59,358 Epoch  51: total training loss 6.61
2020-06-15 16:11:59,359 EPOCH 52
2020-06-15 16:12:17,970 Epoch  52: total training loss 6.34
2020-06-15 16:12:17,970 EPOCH 53
2020-06-15 16:12:24,465 Epoch  53 Step:     2600 Batch Loss:     0.115823 Tokens per Sec:     6854, Lr: 0.000200
2020-06-15 16:12:36,706 Epoch  53: total training loss 6.33
2020-06-15 16:12:36,706 EPOCH 54
2020-06-15 16:12:55,427 Epoch  54: total training loss 5.99
2020-06-15 16:12:55,428 EPOCH 55
2020-06-15 16:13:02,258 Epoch  55 Step:     2700 Batch Loss:     0.119850 Tokens per Sec:     6991, Lr: 0.000200
2020-06-15 16:13:14,110 Epoch  55: total training loss 6.00
2020-06-15 16:13:14,110 EPOCH 56
2020-06-15 16:13:32,737 Epoch  56: total training loss 5.87
2020-06-15 16:13:32,738 EPOCH 57
2020-06-15 16:13:39,997 Epoch  57 Step:     2800 Batch Loss:     0.112310 Tokens per Sec:     6914, Lr: 0.000200
2020-06-15 16:13:51,554 Epoch  57: total training loss 5.81
2020-06-15 16:13:51,554 EPOCH 58
2020-06-15 16:14:10,237 Epoch  58: total training loss 5.46
2020-06-15 16:14:10,237 EPOCH 59
2020-06-15 16:14:17,762 Epoch  59 Step:     2900 Batch Loss:     0.110959 Tokens per Sec:     6923, Lr: 0.000200
2020-06-15 16:14:28,928 Epoch  59: total training loss 5.54
2020-06-15 16:14:28,929 EPOCH 60
2020-06-15 16:14:47,797 Epoch  60: total training loss 5.61
2020-06-15 16:14:47,798 EPOCH 61
2020-06-15 16:14:55,337 Epoch  61 Step:     3000 Batch Loss:     0.151020 Tokens per Sec:     6950, Lr: 0.000200
2020-06-15 16:15:57,891 Example #0
2020-06-15 16:15:57,892 	Raw source:     ['hallo', ',']
2020-06-15 16:15:57,892 	Raw hypothesis: ['hello', '?']
2020-06-15 16:15:57,892 	Source:     hallo ,
2020-06-15 16:15:57,892 	Reference:  hello .
2020-06-15 16:15:57,892 	Hypothesis: hello ?
2020-06-15 16:15:57,892 Example #1
2020-06-15 16:15:57,892 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-15 16:15:57,892 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-15 16:15:57,892 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-15 16:15:57,892 	Reference:  hi , how can i help you ?
2020-06-15 16:15:57,892 	Hypothesis: hi , how can i help you ?
2020-06-15 16:15:57,892 Example #2
2020-06-15 16:15:57,892 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-15 16:15:57,893 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-15 16:15:57,893 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-15 16:15:57,893 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-15 16:15:57,893 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-15 16:15:57,893 Example #3
2020-06-15 16:15:57,893 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-15 16:15:57,893 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-15 16:15:57,893 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-15 16:15:57,893 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-15 16:15:57,893 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-15 16:15:57,893 Validation result (greedy) at epoch  61, step     3000: bleu:  42.08, loss: 40444.1211, ppl:   7.0992, duration: 62.5549s
2020-06-15 16:16:09,085 Epoch  61: total training loss 5.96
2020-06-15 16:16:09,086 EPOCH 62
2020-06-15 16:16:27,757 Epoch  62: total training loss 5.89
2020-06-15 16:16:27,758 EPOCH 63
2020-06-15 16:16:35,284 Epoch  63 Step:     3100 Batch Loss:     0.096934 Tokens per Sec:     7056, Lr: 0.000200
2020-06-15 16:16:46,461 Epoch  63: total training loss 5.65
2020-06-15 16:16:46,462 EPOCH 64
2020-06-15 16:17:05,258 Epoch  64: total training loss 5.39
2020-06-15 16:17:05,258 EPOCH 65
2020-06-15 16:17:12,793 Epoch  65 Step:     3200 Batch Loss:     0.092252 Tokens per Sec:     7034, Lr: 0.000200
2020-06-15 16:17:23,973 Epoch  65: total training loss 5.26
2020-06-15 16:17:23,973 EPOCH 66
2020-06-15 16:17:42,759 Epoch  66: total training loss 5.22
2020-06-15 16:17:42,760 EPOCH 67
2020-06-15 16:17:50,593 Epoch  67 Step:     3300 Batch Loss:     0.111359 Tokens per Sec:     6946, Lr: 0.000200
2020-06-15 16:18:01,538 Epoch  67: total training loss 4.95
2020-06-15 16:18:01,538 EPOCH 68
2020-06-15 16:18:20,291 Epoch  68: total training loss 4.85
2020-06-15 16:18:20,291 EPOCH 69
2020-06-15 16:18:28,571 Epoch  69 Step:     3400 Batch Loss:     0.100207 Tokens per Sec:     6870, Lr: 0.000200
2020-06-15 16:18:39,020 Epoch  69: total training loss 4.90
2020-06-15 16:18:39,021 EPOCH 70
2020-06-15 16:18:57,784 Epoch  70: total training loss 4.91
2020-06-15 16:18:57,784 EPOCH 71
2020-06-15 16:19:06,098 Epoch  71 Step:     3500 Batch Loss:     0.093825 Tokens per Sec:     6904, Lr: 0.000200
2020-06-15 16:19:16,539 Epoch  71: total training loss 4.80
2020-06-15 16:19:16,540 EPOCH 72
2020-06-15 16:19:35,210 Epoch  72: total training loss 4.68
2020-06-15 16:19:35,210 EPOCH 73
2020-06-15 16:19:44,248 Epoch  73 Step:     3600 Batch Loss:     0.099080 Tokens per Sec:     6933, Lr: 0.000200
2020-06-15 16:19:54,015 Epoch  73: total training loss 4.80
2020-06-15 16:19:54,015 EPOCH 74
2020-06-15 16:20:12,785 Epoch  74: total training loss 5.41
2020-06-15 16:20:12,786 EPOCH 75
2020-06-15 16:20:22,179 Epoch  75 Step:     3700 Batch Loss:     0.137981 Tokens per Sec:     6902, Lr: 0.000200
2020-06-15 16:20:31,484 Epoch  75: total training loss 5.89
2020-06-15 16:20:31,484 EPOCH 76
2020-06-15 16:20:50,048 Epoch  76: total training loss 5.15
2020-06-15 16:20:50,049 EPOCH 77
2020-06-15 16:20:59,863 Epoch  77 Step:     3800 Batch Loss:     0.105977 Tokens per Sec:     6812, Lr: 0.000200
2020-06-15 16:21:08,912 Epoch  77: total training loss 4.88
2020-06-15 16:21:08,912 EPOCH 78
2020-06-15 16:21:27,520 Epoch  78: total training loss 4.59
2020-06-15 16:21:27,521 EPOCH 79
2020-06-15 16:21:37,911 Epoch  79 Step:     3900 Batch Loss:     0.102100 Tokens per Sec:     7002, Lr: 0.000200
2020-06-15 16:21:46,240 Epoch  79: total training loss 4.52
2020-06-15 16:21:46,241 EPOCH 80
2020-06-15 16:22:04,938 Epoch  80: total training loss 4.27
2020-06-15 16:22:04,939 EPOCH 81
2020-06-15 16:22:15,490 Epoch  81 Step:     4000 Batch Loss:     0.091596 Tokens per Sec:     6960, Lr: 0.000200
2020-06-15 16:23:18,562 Example #0
2020-06-15 16:23:18,562 	Raw source:     ['hallo', ',']
2020-06-15 16:23:18,562 	Raw hypothesis: ['hello', '.']
2020-06-15 16:23:18,562 	Source:     hallo ,
2020-06-15 16:23:18,563 	Reference:  hello .
2020-06-15 16:23:18,563 	Hypothesis: hello .
2020-06-15 16:23:18,563 Example #1
2020-06-15 16:23:18,563 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-15 16:23:18,563 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-15 16:23:18,563 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-15 16:23:18,563 	Reference:  hi , how can i help you ?
2020-06-15 16:23:18,563 	Hypothesis: hi , how can i help you ?
2020-06-15 16:23:18,563 Example #2
2020-06-15 16:23:18,563 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-15 16:23:18,563 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-15 16:23:18,563 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-15 16:23:18,563 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-15 16:23:18,563 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-15 16:23:18,563 Example #3
2020-06-15 16:23:18,563 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-15 16:23:18,563 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-15 16:23:18,563 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-15 16:23:18,563 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-15 16:23:18,563 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-15 16:23:18,563 Validation result (greedy) at epoch  81, step     4000: bleu:  41.87, loss: 40474.1016, ppl:   7.1095, duration: 63.0728s
2020-06-15 16:23:26,700 Epoch  81: total training loss 4.40
2020-06-15 16:23:26,701 EPOCH 82
2020-06-15 16:23:45,374 Epoch  82: total training loss 4.34
2020-06-15 16:23:45,375 EPOCH 83
2020-06-15 16:23:55,945 Epoch  83 Step:     4100 Batch Loss:     0.118213 Tokens per Sec:     6967, Lr: 0.000200
2020-06-15 16:24:04,191 Epoch  83: total training loss 4.37
2020-06-15 16:24:04,191 EPOCH 84
2020-06-15 16:24:22,741 Epoch  84: total training loss 4.22
2020-06-15 16:24:22,742 EPOCH 85
2020-06-15 16:24:33,686 Epoch  85 Step:     4200 Batch Loss:     0.101246 Tokens per Sec:     6905, Lr: 0.000200
2020-06-15 16:24:41,399 Epoch  85: total training loss 4.01
2020-06-15 16:24:41,400 EPOCH 86
2020-06-15 16:25:00,048 Epoch  86: total training loss 4.08
2020-06-15 16:25:00,049 EPOCH 87
2020-06-15 16:25:11,737 Epoch  87 Step:     4300 Batch Loss:     0.078839 Tokens per Sec:     6962, Lr: 0.000200
2020-06-15 16:25:18,744 Epoch  87: total training loss 4.18
2020-06-15 16:25:18,744 EPOCH 88
2020-06-15 16:25:37,322 Epoch  88: total training loss 4.10
2020-06-15 16:25:37,323 EPOCH 89
2020-06-15 16:25:49,607 Epoch  89 Step:     4400 Batch Loss:     0.064559 Tokens per Sec:     6931, Lr: 0.000200
2020-06-15 16:25:55,954 Epoch  89: total training loss 3.92
2020-06-15 16:25:55,955 EPOCH 90
2020-06-15 16:26:14,620 Epoch  90: total training loss 3.74
2020-06-15 16:26:14,621 EPOCH 91
2020-06-15 16:26:27,745 Epoch  91 Step:     4500 Batch Loss:     0.070342 Tokens per Sec:     6980, Lr: 0.000200
2020-06-15 16:26:33,259 Epoch  91: total training loss 3.88
2020-06-15 16:26:33,259 EPOCH 92
2020-06-15 16:26:52,072 Epoch  92: total training loss 3.81
2020-06-15 16:26:52,073 EPOCH 93
2020-06-15 16:27:05,366 Epoch  93 Step:     4600 Batch Loss:     0.077409 Tokens per Sec:     7054, Lr: 0.000200
2020-06-15 16:27:10,634 Epoch  93: total training loss 3.92
2020-06-15 16:27:10,635 EPOCH 94
2020-06-15 16:27:29,334 Epoch  94: total training loss 3.89
2020-06-15 16:27:29,334 EPOCH 95
2020-06-15 16:27:43,414 Epoch  95 Step:     4700 Batch Loss:     0.095023 Tokens per Sec:     6956, Lr: 0.000200
2020-06-15 16:27:47,942 Epoch  95: total training loss 3.86
2020-06-15 16:27:47,942 EPOCH 96
2020-06-15 16:28:06,591 Epoch  96: total training loss 3.69
2020-06-15 16:28:06,592 EPOCH 97
2020-06-15 16:28:21,201 Epoch  97 Step:     4800 Batch Loss:     0.073837 Tokens per Sec:     6904, Lr: 0.000200
2020-06-15 16:28:25,400 Epoch  97: total training loss 3.80
2020-06-15 16:28:25,400 EPOCH 98
2020-06-15 16:28:44,153 Epoch  98: total training loss 3.77
2020-06-15 16:28:44,153 EPOCH 99
2020-06-15 16:28:59,191 Epoch  99 Step:     4900 Batch Loss:     0.077543 Tokens per Sec:     6954, Lr: 0.000200
2020-06-15 16:29:02,853 Epoch  99: total training loss 3.76
2020-06-15 16:29:02,853 EPOCH 100
2020-06-15 16:29:21,573 Epoch 100: total training loss 3.78
2020-06-15 16:29:21,574 Training ended after 100 epochs.
2020-06-15 16:29:21,574 Best validation result (greedy) at step     2000:   6.79 ppl.
2020-06-15 16:30:07,646  dev bleu:  43.84 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-15 16:30:07,651 Translations saved to: models/transformer_deen/00002000.hyps.dev
2020-06-15 16:30:41,798 test bleu:  41.36 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-15 16:30:41,804 Translations saved to: models/transformer_deen/00002000.hyps.test
