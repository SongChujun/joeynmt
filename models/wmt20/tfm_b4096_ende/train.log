2020-07-05 15:04:27,175 Hello! This is Joey-NMT.
2020-07-05 15:04:31,713 Total params: 62894080
2020-07-05 15:04:31,714 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-07-05 15:04:33,344 cfg.name                           : transformer
2020-07-05 15:04:33,344 cfg.data.src                       : en
2020-07-05 15:04:33,344 cfg.data.trg                       : de
2020-07-05 15:04:33,344 cfg.data.train                     : chatnmt/official_split/wmt17bpe/train.tags.bpe.wmt-ende-best
2020-07-05 15:04:33,344 cfg.data.dev                       : chatnmt/official_split/wmt17bpe/dev.tags.bpe.wmt-ende-best
2020-07-05 15:04:33,344 cfg.data.test                      : chatnmt/official_split/wmt17bpe/test.tags.bpe.wmt-ende-best
2020-07-05 15:04:33,344 cfg.data.level                     : bpe
2020-07-05 15:04:33,344 cfg.data.lowercase                 : False
2020-07-05 15:04:33,345 cfg.data.max_sent_length           : 100
2020-07-05 15:04:33,345 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-05 15:04:33,345 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-05 15:04:33,345 cfg.testing.beam_size              : 5
2020-07-05 15:04:33,345 cfg.testing.alpha                  : 1.0
2020-07-05 15:04:33,345 cfg.training.random_seed           : 42
2020-07-05 15:04:33,345 cfg.training.optimizer             : adam
2020-07-05 15:04:33,345 cfg.training.normalization         : tokens
2020-07-05 15:04:33,345 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-05 15:04:33,345 cfg.training.scheduling            : plateau
2020-07-05 15:04:33,345 cfg.training.patience              : 8
2020-07-05 15:04:33,345 cfg.training.decrease_factor       : 0.7
2020-07-05 15:04:33,345 cfg.training.loss                  : crossentropy
2020-07-05 15:04:33,345 cfg.training.learning_rate         : 0.0002
2020-07-05 15:04:33,345 cfg.training.learning_rate_min     : 1e-08
2020-07-05 15:04:33,345 cfg.training.weight_decay          : 0.0
2020-07-05 15:04:33,345 cfg.training.label_smoothing       : 0.1
2020-07-05 15:04:33,345 cfg.training.batch_size            : 4096
2020-07-05 15:04:33,345 cfg.training.batch_type            : token
2020-07-05 15:04:33,345 cfg.training.batch_multiplier      : 1
2020-07-05 15:04:33,345 cfg.training.early_stopping_metric : ppl
2020-07-05 15:04:33,346 cfg.training.epochs                : 50
2020-07-05 15:04:33,346 cfg.training.validation_freq       : 1000
2020-07-05 15:04:33,346 cfg.training.logging_freq          : 100
2020-07-05 15:04:33,346 cfg.training.eval_metric           : bleu
2020-07-05 15:04:33,346 cfg.training.model_dir             : models/wmt20/tfm_b4096_ende/
2020-07-05 15:04:33,346 cfg.training.overwrite             : False
2020-07-05 15:04:33,346 cfg.training.shuffle               : True
2020-07-05 15:04:33,346 cfg.training.use_cuda              : True
2020-07-05 15:04:33,346 cfg.training.max_output_length     : 100
2020-07-05 15:04:33,346 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-05 15:04:33,346 cfg.training.keep_last_ckpts       : 3
2020-07-05 15:04:33,346 cfg.model.initializer              : xavier
2020-07-05 15:04:33,346 cfg.model.bias_initializer         : zeros
2020-07-05 15:04:33,346 cfg.model.init_gain                : 1.0
2020-07-05 15:04:33,346 cfg.model.embed_initializer        : xavier
2020-07-05 15:04:33,346 cfg.model.embed_init_gain          : 1.0
2020-07-05 15:04:33,346 cfg.model.tied_embeddings          : True
2020-07-05 15:04:33,346 cfg.model.tied_softmax             : True
2020-07-05 15:04:33,346 cfg.model.encoder.type             : transformer
2020-07-05 15:04:33,346 cfg.model.encoder.num_layers       : 6
2020-07-05 15:04:33,346 cfg.model.encoder.num_heads        : 8
2020-07-05 15:04:33,346 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-05 15:04:33,346 cfg.model.encoder.embeddings.scale : True
2020-07-05 15:04:33,346 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-05 15:04:33,347 cfg.model.encoder.hidden_size      : 512
2020-07-05 15:04:33,347 cfg.model.encoder.ff_size          : 2048
2020-07-05 15:04:33,347 cfg.model.encoder.freeze           : False
2020-07-05 15:04:33,347 cfg.model.encoder.dropout          : 0.1
2020-07-05 15:04:33,347 cfg.model.decoder.type             : transformer
2020-07-05 15:04:33,347 cfg.model.decoder.num_layers       : 6
2020-07-05 15:04:33,347 cfg.model.decoder.num_heads        : 8
2020-07-05 15:04:33,347 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-05 15:04:33,347 cfg.model.decoder.embeddings.scale : True
2020-07-05 15:04:33,347 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-05 15:04:33,347 cfg.model.decoder.hidden_size      : 512
2020-07-05 15:04:33,347 cfg.model.decoder.ff_size          : 2048
2020-07-05 15:04:33,347 cfg.model.decoder.freeze           : False
2020-07-05 15:04:33,347 cfg.model.decoder.dropout          : 0.1
2020-07-05 15:04:33,347 Data set sizes: 
	train 10938,
	valid 776,
	test 0
2020-07-05 15:04:33,347 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-05 15:04:33,347 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-05 15:04:33,347 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-05 15:04:33,347 Number of Src words (types): 36628
2020-07-05 15:04:33,347 Number of Trg words (types): 36628
2020-07-05 15:04:33,348 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-05 15:04:33,376 EPOCH 1
2020-07-05 15:05:04,909 Epoch   1: total training loss 443.90
2020-07-05 15:05:04,910 EPOCH 2
2020-07-05 15:05:19,953 Epoch   2 Step:      100 Batch Loss:     5.695699 Tokens per Sec:     5347, Lr: 0.000200
2020-07-05 15:05:38,223 Epoch   2: total training loss 369.51
2020-07-05 15:05:38,224 EPOCH 3
2020-07-05 15:06:10,386 Epoch   3 Step:      200 Batch Loss:     4.880442 Tokens per Sec:     4892, Lr: 0.000200
2020-07-05 15:06:13,001 Epoch   3: total training loss 355.17
2020-07-05 15:06:13,001 EPOCH 4
2020-07-05 15:06:47,732 Epoch   4: total training loss 334.75
2020-07-05 15:06:47,733 EPOCH 5
2020-07-05 15:07:00,941 Epoch   5 Step:      300 Batch Loss:     5.136628 Tokens per Sec:     5064, Lr: 0.000200
2020-07-05 15:07:22,304 Epoch   5: total training loss 309.27
2020-07-05 15:07:22,304 EPOCH 6
2020-07-05 15:07:51,761 Epoch   6 Step:      400 Batch Loss:     5.293697 Tokens per Sec:     4884, Lr: 0.000200
2020-07-05 15:07:57,025 Epoch   6: total training loss 287.96
2020-07-05 15:07:57,026 EPOCH 7
2020-07-05 15:08:31,441 Epoch   7: total training loss 266.09
2020-07-05 15:08:31,441 EPOCH 8
2020-07-05 15:08:42,894 Epoch   8 Step:      500 Batch Loss:     3.018682 Tokens per Sec:     4989, Lr: 0.000200
2020-07-05 15:09:05,961 Epoch   8: total training loss 258.59
2020-07-05 15:09:05,961 EPOCH 9
2020-07-05 15:09:33,310 Epoch   9 Step:      600 Batch Loss:     4.049275 Tokens per Sec:     4904, Lr: 0.000200
2020-07-05 15:09:40,751 Epoch   9: total training loss 240.83
2020-07-05 15:09:40,751 EPOCH 10
2020-07-05 15:10:15,497 Epoch  10: total training loss 221.47
2020-07-05 15:10:15,498 EPOCH 11
2020-07-05 15:10:24,296 Epoch  11 Step:      700 Batch Loss:     3.323156 Tokens per Sec:     4907, Lr: 0.000200
2020-07-05 15:10:50,220 Epoch  11: total training loss 207.74
2020-07-05 15:10:50,221 EPOCH 12
2020-07-05 15:11:15,269 Epoch  12 Step:      800 Batch Loss:     2.854541 Tokens per Sec:     4947, Lr: 0.000200
2020-07-05 15:11:24,692 Epoch  12: total training loss 190.69
2020-07-05 15:11:24,692 EPOCH 13
2020-07-05 15:11:59,334 Epoch  13: total training loss 184.16
2020-07-05 15:11:59,335 EPOCH 14
2020-07-05 15:12:05,617 Epoch  14 Step:      900 Batch Loss:     2.996264 Tokens per Sec:     5087, Lr: 0.000200
2020-07-05 15:12:33,953 Epoch  14: total training loss 161.01
2020-07-05 15:12:33,954 EPOCH 15
2020-07-05 15:12:57,029 Epoch  15 Step:     1000 Batch Loss:     2.537602 Tokens per Sec:     4937, Lr: 0.000200
2020-07-05 15:14:17,886 Hooray! New best validation result [ppl]!
2020-07-05 15:14:17,886 Saving new checkpoint.
2020-07-05 15:14:25,955 Example #0
2020-07-05 15:14:25,955 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-05 15:14:25,955 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-05 15:14:25,955 	Source:     Hi , how can I help you ?
2020-07-05 15:14:25,955 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:14:25,955 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:14:25,955 Example #1
2020-07-05 15:14:25,955 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-05 15:14:25,955 	Raw hypothesis: ['O@@', 'k', ',', 'welche', 'Art', 'von', 'Essen', 'möchten', 'Sie', '?']
2020-07-05 15:14:25,955 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-05 15:14:25,956 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-05 15:14:25,956 	Hypothesis: Ok , welche Art von Essen möchten Sie ?
2020-07-05 15:14:25,956 Example #2
2020-07-05 15:14:25,956 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-05 15:14:25,956 	Raw hypothesis: ['Si@@', 'cher', ',', 'lassen', 'Sie', 'mich', 'das', 'für', 'Sie', 'finden', ',', 'ich', 'habe', 'ein', 'paar', 'Optionen', 'für', 'Sie', 'finden', '.']
2020-07-05 15:14:25,956 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-05 15:14:25,956 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-05 15:14:25,956 	Hypothesis: Sicher , lassen Sie mich das für Sie finden , ich habe ein paar Optionen für Sie finden .
2020-07-05 15:14:25,956 Example #3
2020-07-05 15:14:25,956 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-05 15:14:25,956 	Raw hypothesis: ['Ich', 'habe', 'ein', 'paar', 'Restaurant', 'gefunden', '.', 'Die', 'erste', 'ist', 'ein', 'Restaurant', 'mit', 'einem', 'Restaurant', 'mit', 'einem', 'Restaurant', 'mit', 'einem', 'Restaurant', 'mit', 'einem', 'Restaurant', 'mit', 'einem', 'Restaurant', 'mit', 'einem', 'Speis@@', 'ek@@', 'arte', '.', 'Ich', 'habe', 'ein', 'paar', 'Restaurant', 'mit', 'einem', 'Restaurant', '.']
2020-07-05 15:14:25,956 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-05 15:14:25,956 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-05 15:14:25,956 	Hypothesis: Ich habe ein paar Restaurant gefunden . Die erste ist ein Restaurant mit einem Restaurant mit einem Restaurant mit einem Restaurant mit einem Restaurant mit einem Restaurant mit einem Speisekarte . Ich habe ein paar Restaurant mit einem Restaurant .
2020-07-05 15:14:25,956 Validation result (greedy) at epoch  15, step     1000: bleu:  15.54, loss: 39670.1367, ppl:  14.4965, duration: 88.9266s
2020-07-05 15:14:36,521 Epoch  15: total training loss 148.09
2020-07-05 15:14:36,521 EPOCH 16
2020-07-05 15:15:11,144 Epoch  16: total training loss 138.57
2020-07-05 15:15:11,145 EPOCH 17
2020-07-05 15:15:16,861 Epoch  17 Step:     1100 Batch Loss:     2.557591 Tokens per Sec:     5039, Lr: 0.000200
2020-07-05 15:15:45,622 Epoch  17: total training loss 134.78
2020-07-05 15:15:45,623 EPOCH 18
2020-07-05 15:16:07,272 Epoch  18 Step:     1200 Batch Loss:     2.379969 Tokens per Sec:     4939, Lr: 0.000200
2020-07-05 15:16:20,104 Epoch  18: total training loss 124.09
2020-07-05 15:16:20,105 EPOCH 19
2020-07-05 15:16:54,552 Epoch  19: total training loss 112.09
2020-07-05 15:16:54,552 EPOCH 20
2020-07-05 15:16:58,218 Epoch  20 Step:     1300 Batch Loss:     1.610344 Tokens per Sec:     4857, Lr: 0.000200
2020-07-05 15:17:29,066 Epoch  20: total training loss 103.98
2020-07-05 15:17:29,067 EPOCH 21
2020-07-05 15:17:49,616 Epoch  21 Step:     1400 Batch Loss:     1.388511 Tokens per Sec:     4952, Lr: 0.000200
2020-07-05 15:18:03,563 Epoch  21: total training loss 97.63
2020-07-05 15:18:03,563 EPOCH 22
2020-07-05 15:18:38,219 Epoch  22: total training loss 91.40
2020-07-05 15:18:38,220 EPOCH 23
2020-07-05 15:18:40,292 Epoch  23 Step:     1500 Batch Loss:     1.307855 Tokens per Sec:     4342, Lr: 0.000200
2020-07-05 15:19:12,895 Epoch  23: total training loss 87.97
2020-07-05 15:19:12,896 EPOCH 24
2020-07-05 15:19:30,635 Epoch  24 Step:     1600 Batch Loss:     0.722096 Tokens per Sec:     4927, Lr: 0.000200
2020-07-05 15:19:47,693 Epoch  24: total training loss 79.56
2020-07-05 15:19:47,693 EPOCH 25
2020-07-05 15:20:21,885 Epoch  25 Step:     1700 Batch Loss:     1.750332 Tokens per Sec:     4918, Lr: 0.000200
2020-07-05 15:20:22,407 Epoch  25: total training loss 72.24
2020-07-05 15:20:22,407 EPOCH 26
2020-07-05 15:20:57,076 Epoch  26: total training loss 66.28
2020-07-05 15:20:57,077 EPOCH 27
2020-07-05 15:21:12,951 Epoch  27 Step:     1800 Batch Loss:     0.436710 Tokens per Sec:     4868, Lr: 0.000200
2020-07-05 15:21:31,548 Epoch  27: total training loss 59.33
2020-07-05 15:21:31,549 EPOCH 28
2020-07-05 15:22:04,067 Epoch  28 Step:     1900 Batch Loss:     0.751104 Tokens per Sec:     4935, Lr: 0.000200
2020-07-05 15:22:06,160 Epoch  28: total training loss 58.68
2020-07-05 15:22:06,161 EPOCH 29
2020-07-05 15:22:40,786 Epoch  29: total training loss 52.95
2020-07-05 15:22:40,787 EPOCH 30
2020-07-05 15:22:54,990 Epoch  30 Step:     2000 Batch Loss:     0.213649 Tokens per Sec:     4898, Lr: 0.000200
2020-07-05 15:24:26,434 Hooray! New best validation result [ppl]!
2020-07-05 15:24:26,434 Saving new checkpoint.
2020-07-05 15:24:34,497 Example #0
2020-07-05 15:24:34,498 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-05 15:24:34,498 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-05 15:24:34,498 	Source:     Hi , how can I help you ?
2020-07-05 15:24:34,498 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:24:34,498 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:24:34,498 Example #1
2020-07-05 15:24:34,498 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-05 15:24:34,498 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-05 15:24:34,498 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-05 15:24:34,498 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-05 15:24:34,498 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-05 15:24:34,498 Example #2
2020-07-05 15:24:34,498 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-05 15:24:34,498 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'bitte', 'einen', 'Moment', ',', 'ich', 'habe', 'ein', 'paar', 'Optionen', 'gefunden', '.']
2020-07-05 15:24:34,498 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-05 15:24:34,498 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-05 15:24:34,498 	Hypothesis: Klar , geben Sie mir bitte einen Moment , ich habe ein paar Optionen gefunden .
2020-07-05 15:24:34,498 Example #3
2020-07-05 15:24:34,498 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-05 15:24:34,498 	Raw hypothesis: ['Ich', 'habe', '19', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'eine', 'Sitz@@', 'gelegenheiten', 'im', 'Freien', ',', 'eine', 'Sitz@@', 'gelegenheiten', 'im', 'Freien', '&', 'Sitz@@', 'gelegenheiten', 'im', 'Freien', '&', 'Sitz@@', 'gelegenheiten', 'im', 'Freien', '.', 'Ich', 'habe', 'auf', 'einer', 'Sal@@', 'ate', '&', 'Innen@@', '-', 'und', 'im', 'Frei@@', 'en.@@', 'Ein', 'stil@@', 'voller', ',', 'Bur@@', 'ger', '.', 'Ich', 'habe', 'auch', 'Sal@@', 'ate', '&', 'Sal@@', 'ate', '&', 'Sal@@', 'ate', '&', 'Sal@@', 'ate', '&', 'Sitz@@', 'gelegenheiten', 'im', 'Freien', '.']
2020-07-05 15:24:34,498 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-05 15:24:34,499 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-05 15:24:34,499 	Hypothesis: Ich habe 19 : Seasons 52 , eine Sitzgelegenheiten im Freien , eine Sitzgelegenheiten im Freien & Sitzgelegenheiten im Freien & Sitzgelegenheiten im Freien . Ich habe auf einer Salate & Innen- und im Freien.Ein stilvoller , Burger . Ich habe auch Salate & Salate & Salate & Salate & Sitzgelegenheiten im Freien .
2020-07-05 15:24:34,499 Validation result (greedy) at epoch  30, step     2000: bleu:  32.67, loss: 29843.5898, ppl:   7.4750, duration: 99.5074s
2020-07-05 15:24:54,281 Epoch  30: total training loss 48.33
2020-07-05 15:24:54,282 EPOCH 31
2020-07-05 15:25:25,074 Epoch  31 Step:     2100 Batch Loss:     0.789535 Tokens per Sec:     4978, Lr: 0.000200
2020-07-05 15:25:28,838 Epoch  31: total training loss 44.05
2020-07-05 15:25:28,838 EPOCH 32
2020-07-05 15:26:03,295 Epoch  32: total training loss 42.60
2020-07-05 15:26:03,296 EPOCH 33
2020-07-05 15:26:15,107 Epoch  33 Step:     2200 Batch Loss:     0.841376 Tokens per Sec:     4862, Lr: 0.000200
2020-07-05 15:26:38,036 Epoch  33: total training loss 43.47
2020-07-05 15:26:38,037 EPOCH 34
2020-07-05 15:27:05,839 Epoch  34 Step:     2300 Batch Loss:     0.212408 Tokens per Sec:     4927, Lr: 0.000200
2020-07-05 15:27:12,483 Epoch  34: total training loss 36.43
2020-07-05 15:27:12,483 EPOCH 35
2020-07-05 15:27:47,326 Epoch  35: total training loss 32.29
2020-07-05 15:27:47,326 EPOCH 36
2020-07-05 15:27:56,489 Epoch  36 Step:     2400 Batch Loss:     0.648497 Tokens per Sec:     4937, Lr: 0.000200
2020-07-05 15:28:21,715 Epoch  36: total training loss 28.98
2020-07-05 15:28:21,716 EPOCH 37
2020-07-05 15:28:46,964 Epoch  37 Step:     2500 Batch Loss:     0.419793 Tokens per Sec:     5000, Lr: 0.000200
2020-07-05 15:28:56,180 Epoch  37: total training loss 26.67
2020-07-05 15:28:56,181 EPOCH 38
2020-07-05 15:29:30,816 Epoch  38: total training loss 24.19
2020-07-05 15:29:30,817 EPOCH 39
2020-07-05 15:29:38,196 Epoch  39 Step:     2600 Batch Loss:     0.229921 Tokens per Sec:     5069, Lr: 0.000200
2020-07-05 15:30:05,479 Epoch  39: total training loss 21.74
2020-07-05 15:30:05,480 EPOCH 40
2020-07-05 15:30:28,976 Epoch  40 Step:     2700 Batch Loss:     0.250594 Tokens per Sec:     5031, Lr: 0.000200
2020-07-05 15:30:39,889 Epoch  40: total training loss 20.64
2020-07-05 15:30:39,890 EPOCH 41
2020-07-05 15:31:14,543 Epoch  41: total training loss 21.35
2020-07-05 15:31:14,544 EPOCH 42
2020-07-05 15:31:19,166 Epoch  42 Step:     2800 Batch Loss:     0.430043 Tokens per Sec:     4368, Lr: 0.000200
2020-07-05 15:31:49,142 Epoch  42: total training loss 17.90
2020-07-05 15:31:49,143 EPOCH 43
2020-07-05 15:32:10,897 Epoch  43 Step:     2900 Batch Loss:     0.164222 Tokens per Sec:     4953, Lr: 0.000200
2020-07-05 15:32:23,696 Epoch  43: total training loss 16.68
2020-07-05 15:32:23,696 EPOCH 44
2020-07-05 15:32:58,353 Epoch  44: total training loss 15.44
2020-07-05 15:32:58,354 EPOCH 45
2020-07-05 15:33:02,051 Epoch  45 Step:     3000 Batch Loss:     0.183539 Tokens per Sec:     4928, Lr: 0.000200
2020-07-05 15:34:24,756 Example #0
2020-07-05 15:34:24,756 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-05 15:34:24,757 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-05 15:34:24,757 	Source:     Hi , how can I help you ?
2020-07-05 15:34:24,757 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:34:24,757 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:34:24,757 Example #1
2020-07-05 15:34:24,757 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-05 15:34:24,757 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-05 15:34:24,757 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-05 15:34:24,757 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-05 15:34:24,757 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-05 15:34:24,757 Example #2
2020-07-05 15:34:24,757 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-05 15:34:24,757 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'einen', 'zweiten', ',', 'ich', 'habe', 'ein', 'paar', 'Optionen', 'mit', 'ein', 'paar', 'Optionen', '.']
2020-07-05 15:34:24,757 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-05 15:34:24,757 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-05 15:34:24,757 	Hypothesis: Klar , geben Sie mir einen zweiten , ich habe ein paar Optionen mit ein paar Optionen .
2020-07-05 15:34:24,757 Example #3
2020-07-05 15:34:24,757 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-05 15:34:24,757 	Raw hypothesis: ['Ich', 'habe', 'folgendes', 'gefunden', ':', '"', 'Se@@', 'as@@', 'ons', '52', '"', ',', 'eine', 'familien@@', 'freundliche', 'K@@', 'ette', ',', 'im', 'geho@@', 'ben@@', 'er', 'chinesische', 'Küche', 'in', 'geho@@', 'ben@@', 'er', 'chin@@', 'es@@', 'isches', 'Restaurant', '.', 'Ich', 'habe', 'auch', 'wechsel@@', 'n@@', 'des', 'Menü', 'mit', 'sa@@', 'ison@@', 'alen', ',', 'Sand@@', 'wich@@', 'ich', 'auf', 'Au@@', 'ß@@', 'enter@@', 'rasse', '&', 'Sal@@', 'ate', '&', 'Sal@@', 'ate', '&', 'Sal@@', 'ate', 'sowie', 'den', 'Se@@', 'as@@', 'ons', '52', 'gefunden', '.']
2020-07-05 15:34:24,757 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-05 15:34:24,758 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-05 15:34:24,758 	Hypothesis: Ich habe folgendes gefunden : " Seasons 52 " , eine familienfreundliche Kette , im gehobener chinesische Küche in gehobener chinesisches Restaurant . Ich habe auch wechselndes Menü mit saisonalen , Sandwichich auf Außenterrasse & Salate & Salate & Salate sowie den Seasons 52 gefunden .
2020-07-05 15:34:24,758 Validation result (greedy) at epoch  45, step     3000: bleu:  36.82, loss: 30210.0430, ppl:   7.6619, duration: 82.7059s
2020-07-05 15:34:55,535 Epoch  45: total training loss 14.14
2020-07-05 15:34:55,535 EPOCH 46
2020-07-05 15:35:15,921 Epoch  46 Step:     3100 Batch Loss:     0.176487 Tokens per Sec:     4980, Lr: 0.000200
2020-07-05 15:35:30,239 Epoch  46: total training loss 13.44
2020-07-05 15:35:30,240 EPOCH 47
2020-07-05 15:36:05,024 Epoch  47: total training loss 12.73
2020-07-05 15:36:05,025 EPOCH 48
2020-07-05 15:36:07,042 Epoch  48 Step:     3200 Batch Loss:     0.167685 Tokens per Sec:     4656, Lr: 0.000200
2020-07-05 15:36:39,443 Epoch  48: total training loss 12.15
2020-07-05 15:36:39,443 EPOCH 49
2020-07-05 15:36:58,127 Epoch  49 Step:     3300 Batch Loss:     0.162480 Tokens per Sec:     4901, Lr: 0.000200
2020-07-05 15:37:14,109 Epoch  49: total training loss 11.40
2020-07-05 15:37:14,110 EPOCH 50
2020-07-05 15:37:48,816 Epoch  50: total training loss 10.91
2020-07-05 15:37:48,817 Training ended after  50 epochs.
2020-07-05 15:37:48,817 Best validation result (greedy) at step     2000:   7.48 ppl.
2020-07-05 15:38:52,242  dev bleu:  35.88 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-05 15:38:52,247 Translations saved to: models/wmt20/tfm_b4096_ende/00002000.hyps.dev
2020-07-05 15:38:52,248 test bleu:  -1.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-05 15:38:52,249 Translations saved to: models/wmt20/tfm_b4096_ende/00002000.hyps.test
