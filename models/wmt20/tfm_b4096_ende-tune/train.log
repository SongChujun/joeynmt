2020-07-05 15:20:32,252 Hello! This is Joey-NMT.
2020-07-05 15:20:39,155 Total params: 62894080
2020-07-05 15:20:39,158 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-07-05 15:20:40,775 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-05 15:20:41,017 Reset optimizer.
2020-07-05 15:20:41,017 Reset scheduler.
2020-07-05 15:20:41,017 Reset tracking of the best checkpoint.
2020-07-05 15:20:41,024 cfg.name                           : transformer
2020-07-05 15:20:41,024 cfg.data.src                       : en
2020-07-05 15:20:41,024 cfg.data.trg                       : de
2020-07-05 15:20:41,024 cfg.data.train                     : chatnmt/official_split/wmt17bpe/train.tags.bpe.wmt-ende-best
2020-07-05 15:20:41,024 cfg.data.dev                       : chatnmt/official_split/wmt17bpe/dev.tags.bpe.wmt-ende-best
2020-07-05 15:20:41,024 cfg.data.test                      : chatnmt/official_split/wmt17bpe/test.tags.bpe.wmt-ende-best
2020-07-05 15:20:41,024 cfg.data.level                     : bpe
2020-07-05 15:20:41,024 cfg.data.lowercase                 : False
2020-07-05 15:20:41,024 cfg.data.max_sent_length           : 100
2020-07-05 15:20:41,024 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-05 15:20:41,024 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-05 15:20:41,024 cfg.testing.beam_size              : 5
2020-07-05 15:20:41,025 cfg.testing.alpha                  : 1.0
2020-07-05 15:20:41,025 cfg.training.random_seed           : 42
2020-07-05 15:20:41,025 cfg.training.optimizer             : adam
2020-07-05 15:20:41,025 cfg.training.normalization         : tokens
2020-07-05 15:20:41,025 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-05 15:20:41,025 cfg.training.scheduling            : plateau
2020-07-05 15:20:41,025 cfg.training.patience              : 8
2020-07-05 15:20:41,025 cfg.training.decrease_factor       : 0.7
2020-07-05 15:20:41,025 cfg.training.loss                  : crossentropy
2020-07-05 15:20:41,025 cfg.training.learning_rate         : 0.0002
2020-07-05 15:20:41,025 cfg.training.learning_rate_min     : 1e-08
2020-07-05 15:20:41,025 cfg.training.weight_decay          : 0.0
2020-07-05 15:20:41,025 cfg.training.label_smoothing       : 0.1
2020-07-05 15:20:41,025 cfg.training.batch_size            : 4096
2020-07-05 15:20:41,025 cfg.training.batch_type            : token
2020-07-05 15:20:41,025 cfg.training.batch_multiplier      : 1
2020-07-05 15:20:41,025 cfg.training.early_stopping_metric : ppl
2020-07-05 15:20:41,025 cfg.training.epochs                : 50
2020-07-05 15:20:41,025 cfg.training.validation_freq       : 1000
2020-07-05 15:20:41,025 cfg.training.logging_freq          : 100
2020-07-05 15:20:41,025 cfg.training.eval_metric           : bleu
2020-07-05 15:20:41,025 cfg.training.model_dir             : models/wmt20/tfm_b4096_ende-tune
2020-07-05 15:20:41,025 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-05 15:20:41,025 cfg.training.reset_best_ckpt       : True
2020-07-05 15:20:41,025 cfg.training.reset_scheduler       : True
2020-07-05 15:20:41,025 cfg.training.reset_optimizer       : True
2020-07-05 15:20:41,025 cfg.training.overwrite             : False
2020-07-05 15:20:41,026 cfg.training.shuffle               : True
2020-07-05 15:20:41,026 cfg.training.use_cuda              : True
2020-07-05 15:20:41,026 cfg.training.max_output_length     : 100
2020-07-05 15:20:41,026 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-05 15:20:41,026 cfg.training.keep_last_ckpts       : 3
2020-07-05 15:20:41,026 cfg.model.initializer              : xavier
2020-07-05 15:20:41,026 cfg.model.bias_initializer         : zeros
2020-07-05 15:20:41,026 cfg.model.init_gain                : 1.0
2020-07-05 15:20:41,026 cfg.model.embed_initializer        : xavier
2020-07-05 15:20:41,026 cfg.model.embed_init_gain          : 1.0
2020-07-05 15:20:41,026 cfg.model.tied_embeddings          : True
2020-07-05 15:20:41,026 cfg.model.tied_softmax             : True
2020-07-05 15:20:41,026 cfg.model.encoder.type             : transformer
2020-07-05 15:20:41,026 cfg.model.encoder.num_layers       : 6
2020-07-05 15:20:41,026 cfg.model.encoder.num_heads        : 8
2020-07-05 15:20:41,026 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-05 15:20:41,026 cfg.model.encoder.embeddings.scale : True
2020-07-05 15:20:41,026 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-05 15:20:41,026 cfg.model.encoder.hidden_size      : 512
2020-07-05 15:20:41,026 cfg.model.encoder.ff_size          : 2048
2020-07-05 15:20:41,026 cfg.model.encoder.dropout          : 0.1
2020-07-05 15:20:41,026 cfg.model.decoder.type             : transformer
2020-07-05 15:20:41,026 cfg.model.decoder.num_layers       : 6
2020-07-05 15:20:41,026 cfg.model.decoder.num_heads        : 8
2020-07-05 15:20:41,026 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-05 15:20:41,026 cfg.model.decoder.embeddings.scale : True
2020-07-05 15:20:41,026 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-05 15:20:41,027 cfg.model.decoder.hidden_size      : 512
2020-07-05 15:20:41,027 cfg.model.decoder.ff_size          : 2048
2020-07-05 15:20:41,027 cfg.model.decoder.dropout          : 0.1
2020-07-05 15:20:41,027 Data set sizes: 
	train 10938,
	valid 776,
	test 0
2020-07-05 15:20:41,027 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-05 15:20:41,027 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-05 15:20:41,027 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-05 15:20:41,027 Number of Src words (types): 36628
2020-07-05 15:20:41,027 Number of Trg words (types): 36628
2020-07-05 15:20:41,027 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-05 15:20:41,055 EPOCH 1
2020-07-05 15:21:12,359 Epoch   1: total training loss 66.43
2020-07-05 15:21:12,360 EPOCH 2
2020-07-05 15:21:27,478 Epoch   2 Step:  1360100 Batch Loss:     0.803533 Tokens per Sec:     5321, Lr: 0.000200
2020-07-05 15:21:45,675 Epoch   2: total training loss 49.12
2020-07-05 15:21:45,675 EPOCH 3
2020-07-05 15:22:17,773 Epoch   3 Step:  1360200 Batch Loss:     0.536506 Tokens per Sec:     4902, Lr: 0.000200
2020-07-05 15:22:20,385 Epoch   3: total training loss 42.68
2020-07-05 15:22:20,385 EPOCH 4
2020-07-05 15:22:55,078 Epoch   4: total training loss 38.38
2020-07-05 15:22:55,079 EPOCH 5
2020-07-05 15:23:08,273 Epoch   5 Step:  1360300 Batch Loss:     0.599877 Tokens per Sec:     5069, Lr: 0.000200
2020-07-05 15:23:29,605 Epoch   5: total training loss 34.20
2020-07-05 15:23:29,606 EPOCH 6
2020-07-05 15:23:59,000 Epoch   6 Step:  1360400 Batch Loss:     0.532428 Tokens per Sec:     4895, Lr: 0.000200
2020-07-05 15:24:04,251 Epoch   6: total training loss 31.15
2020-07-05 15:24:04,252 EPOCH 7
2020-07-05 15:24:38,593 Epoch   7: total training loss 28.39
2020-07-05 15:24:38,593 EPOCH 8
2020-07-05 15:24:50,038 Epoch   8 Step:  1360500 Batch Loss:     0.359945 Tokens per Sec:     4992, Lr: 0.000200
2020-07-05 15:25:13,071 Epoch   8: total training loss 26.94
2020-07-05 15:25:13,071 EPOCH 9
2020-07-05 15:25:40,377 Epoch   9 Step:  1360600 Batch Loss:     0.401033 Tokens per Sec:     4912, Lr: 0.000200
2020-07-05 15:25:47,805 Epoch   9: total training loss 25.13
2020-07-05 15:25:47,806 EPOCH 10
2020-07-05 15:26:22,487 Epoch  10: total training loss 23.40
2020-07-05 15:26:22,488 EPOCH 11
2020-07-05 15:26:31,278 Epoch  11 Step:  1360700 Batch Loss:     0.311912 Tokens per Sec:     4912, Lr: 0.000200
2020-07-05 15:26:57,169 Epoch  11: total training loss 21.94
2020-07-05 15:26:57,170 EPOCH 12
2020-07-05 15:27:22,214 Epoch  12 Step:  1360800 Batch Loss:     0.319269 Tokens per Sec:     4947, Lr: 0.000200
2020-07-05 15:27:31,624 Epoch  12: total training loss 20.39
2020-07-05 15:27:31,625 EPOCH 13
2020-07-05 15:28:06,213 Epoch  13: total training loss 19.68
2020-07-05 15:28:06,214 EPOCH 14
2020-07-05 15:28:12,486 Epoch  14 Step:  1360900 Batch Loss:     0.324748 Tokens per Sec:     5097, Lr: 0.000200
2020-07-05 15:28:40,783 Epoch  14: total training loss 18.10
2020-07-05 15:28:40,784 EPOCH 15
2020-07-05 15:29:03,826 Epoch  15 Step:  1361000 Batch Loss:     0.288024 Tokens per Sec:     4944, Lr: 0.000200
2020-07-05 15:30:19,711 Hooray! New best validation result [ppl]!
2020-07-05 15:30:19,711 Saving new checkpoint.
2020-07-05 15:30:27,867 Example #0
2020-07-05 15:30:27,867 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-05 15:30:27,867 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-05 15:30:27,867 	Source:     Hi , how can I help you ?
2020-07-05 15:30:27,867 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:30:27,867 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:30:27,868 Example #1
2020-07-05 15:30:27,868 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-05 15:30:27,868 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-05 15:30:27,868 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-05 15:30:27,868 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-05 15:30:27,868 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-05 15:30:27,868 Example #2
2020-07-05 15:30:27,868 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-05 15:30:27,868 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'eine', 'Sek@@', 'unde', ',', 'ich', 'werde', 'gleich', 'wieder', 'ein', 'paar', 'Optionen', 'haben', '.']
2020-07-05 15:30:27,868 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-05 15:30:27,868 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-05 15:30:27,868 	Hypothesis: Klar , geben Sie mir eine Sekunde , ich werde gleich wieder ein paar Optionen haben .
2020-07-05 15:30:27,868 Example #3
2020-07-05 15:30:27,868 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-05 15:30:27,868 	Raw hypothesis: ['Ich', 'habe', 'Folgendes', 'gefunden', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'eine', 'wechsel@@', 'nde', 'Speis@@', 'ek@@', 'arte', 'mit', 'sa@@', 'ison@@', 'alen', 'amerikanischen', 'Gerichten', 'zusammen', 'mit', 'internationalen', 'W@@', 'einen', 'in', 'geho@@', 'ben@@', 'em', 'Rahmen', '.', 'Ich', 'habe', 'auch', 'Folgendes', 'gefunden', ':', 'P@@', 'lu@@', 'to', "'", 's', ':', 'eine', 'lokale', 'K@@', 'ette', 'mit', 'haus@@', 'eigenen', 'Sal@@', 'aten', '&', 'Sand@@', 'wich@@', 'es', 'in', 'lässi@@', 'gem', ',', 'moder@@', 'nem', 'Ambiente', '.']
2020-07-05 15:30:27,868 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-05 15:30:27,868 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-05 15:30:27,868 	Hypothesis: Ich habe Folgendes gefunden : Seasons 52 , eine wechselnde Speisekarte mit saisonalen amerikanischen Gerichten zusammen mit internationalen Weinen in gehobenem Rahmen . Ich habe auch Folgendes gefunden : Pluto ' s : eine lokale Kette mit hauseigenen Salaten & Sandwiches in lässigem , modernem Ambiente .
2020-07-05 15:30:27,869 Validation result (greedy) at epoch  15, step  1361000: bleu:  55.42, loss: 9741.6162, ppl:   1.9283, duration: 84.0413s
2020-07-05 15:30:38,214 Epoch  15: total training loss 17.09
2020-07-05 15:30:38,215 EPOCH 16
2020-07-05 15:31:12,567 Epoch  16: total training loss 16.29
2020-07-05 15:31:12,568 EPOCH 17
2020-07-05 15:31:18,272 Epoch  17 Step:  1361100 Batch Loss:     0.249579 Tokens per Sec:     5049, Lr: 0.000200
2020-07-05 15:31:46,812 Epoch  17: total training loss 15.48
2020-07-05 15:31:46,813 EPOCH 18
2020-07-05 15:32:08,389 Epoch  18 Step:  1361200 Batch Loss:     0.260753 Tokens per Sec:     4956, Lr: 0.000200
2020-07-05 15:32:21,183 Epoch  18: total training loss 15.01
2020-07-05 15:32:21,184 EPOCH 19
2020-07-05 15:32:55,599 Epoch  19: total training loss 14.30
2020-07-05 15:32:55,599 EPOCH 20
2020-07-05 15:32:59,267 Epoch  20 Step:  1361300 Batch Loss:     0.203503 Tokens per Sec:     4854, Lr: 0.000200
2020-07-05 15:33:30,104 Epoch  20: total training loss 13.50
2020-07-05 15:33:30,105 EPOCH 21
2020-07-05 15:33:50,625 Epoch  21 Step:  1361400 Batch Loss:     0.212993 Tokens per Sec:     4959, Lr: 0.000200
2020-07-05 15:34:04,544 Epoch  21: total training loss 12.89
2020-07-05 15:34:04,544 EPOCH 22
2020-07-05 15:34:39,060 Epoch  22: total training loss 12.62
2020-07-05 15:34:39,060 EPOCH 23
2020-07-05 15:34:41,117 Epoch  23 Step:  1361500 Batch Loss:     0.172661 Tokens per Sec:     4375, Lr: 0.000200
2020-07-05 15:35:13,547 Epoch  23: total training loss 12.34
2020-07-05 15:35:13,548 EPOCH 24
2020-07-05 15:35:31,230 Epoch  24 Step:  1361600 Batch Loss:     0.197205 Tokens per Sec:     4943, Lr: 0.000200
2020-07-05 15:35:48,253 Epoch  24: total training loss 11.70
2020-07-05 15:35:48,254 EPOCH 25
2020-07-05 15:36:22,338 Epoch  25 Step:  1361700 Batch Loss:     0.161445 Tokens per Sec:     4934, Lr: 0.000200
2020-07-05 15:36:22,858 Epoch  25: total training loss 11.16
2020-07-05 15:36:22,858 EPOCH 26
2020-07-05 15:36:57,384 Epoch  26: total training loss 10.79
2020-07-05 15:36:57,385 EPOCH 27
2020-07-05 15:37:13,194 Epoch  27 Step:  1361800 Batch Loss:     0.147399 Tokens per Sec:     4888, Lr: 0.000200
2020-07-05 15:37:31,736 Epoch  27: total training loss 10.35
2020-07-05 15:37:31,737 EPOCH 28
2020-07-05 15:38:04,184 Epoch  28 Step:  1361900 Batch Loss:     0.157541 Tokens per Sec:     4946, Lr: 0.000200
2020-07-05 15:38:06,272 Epoch  28: total training loss 10.04
2020-07-05 15:38:06,273 EPOCH 29
2020-07-05 15:38:40,819 Epoch  29: total training loss 9.74
2020-07-05 15:38:40,819 EPOCH 30
2020-07-05 15:38:54,983 Epoch  30 Step:  1362000 Batch Loss:     0.113908 Tokens per Sec:     4912, Lr: 0.000200
2020-07-05 15:40:15,014 Example #0
2020-07-05 15:40:15,014 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-05 15:40:15,014 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-05 15:40:15,014 	Source:     Hi , how can I help you ?
2020-07-05 15:40:15,014 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:40:15,015 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:40:15,015 Example #1
2020-07-05 15:40:15,015 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-05 15:40:15,015 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-05 15:40:15,015 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-05 15:40:15,015 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-05 15:40:15,015 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-05 15:40:15,015 Example #2
2020-07-05 15:40:15,015 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-05 15:40:15,015 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'eine', 'Sek@@', 'unde', ',', 'ich', 'werde', 'gleich', 'wieder', 'ein', 'paar', 'Optionen', 'haben', '.']
2020-07-05 15:40:15,015 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-05 15:40:15,015 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-05 15:40:15,015 	Hypothesis: Klar , geben Sie mir eine Sekunde , ich werde gleich wieder ein paar Optionen haben .
2020-07-05 15:40:15,015 Example #3
2020-07-05 15:40:15,015 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-05 15:40:15,015 	Raw hypothesis: ['Ich', 'habe', 'Folgendes', 'gefunden', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'ein', 'wechsel@@', 'n@@', 'des', 'Menü', 'mit', 'sa@@', 'ison@@', 'alen', 'amerikanischen', 'Gerichten', 'sowie', 'internationalen', 'W@@', 'einen', 'in', 'geho@@', 'ben@@', 'em', 'Rahmen', '.', 'Ich', 'habe', 'auch', 'Folgendes', 'gefunden', ':', 'P@@', 'lu@@', 'to', "'", 's', ':', 'eine', 'lokale', 'K@@', 'ette', 'mit', 'Gegen@@', 'service', ',', 'einschließlich', 'haus@@', 'eigenen', 'Sal@@', 'aten', '&', 'Sand@@', 'wich@@', 'es', 'in', 'lässi@@', 'gem', ',', 'moder@@', 'nem', 'Ambiente', '.']
2020-07-05 15:40:15,015 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-05 15:40:15,016 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-05 15:40:15,016 	Hypothesis: Ich habe Folgendes gefunden : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten sowie internationalen Weinen in gehobenem Rahmen . Ich habe auch Folgendes gefunden : Pluto ' s : eine lokale Kette mit Gegenservice , einschließlich hauseigenen Salaten & Sandwiches in lässigem , modernem Ambiente .
2020-07-05 15:40:15,016 Validation result (greedy) at epoch  30, step  1362000: bleu:  54.74, loss: 10933.7227, ppl:   2.0896, duration: 80.0319s
2020-07-05 15:40:35,502 Epoch  30: total training loss 9.40
2020-07-05 15:40:35,502 EPOCH 31
2020-07-05 15:41:06,272 Epoch  31 Step:  1362100 Batch Loss:     0.150472 Tokens per Sec:     4981, Lr: 0.000200
2020-07-05 15:41:10,033 Epoch  31: total training loss 9.12
2020-07-05 15:41:10,034 EPOCH 32
2020-07-05 15:41:44,170 Epoch  32: total training loss 8.83
2020-07-05 15:41:44,170 EPOCH 33
2020-07-05 15:41:55,964 Epoch  33 Step:  1362200 Batch Loss:     0.132840 Tokens per Sec:     4869, Lr: 0.000200
2020-07-05 15:42:18,867 Epoch  33: total training loss 8.82
2020-07-05 15:42:18,867 EPOCH 34
2020-07-05 15:42:46,617 Epoch  34 Step:  1362300 Batch Loss:     0.111208 Tokens per Sec:     4937, Lr: 0.000200
2020-07-05 15:42:53,247 Epoch  34: total training loss 8.45
2020-07-05 15:42:53,248 EPOCH 35
2020-07-05 15:43:27,988 Epoch  35: total training loss 8.27
2020-07-05 15:43:27,988 EPOCH 36
2020-07-05 15:43:37,137 Epoch  36 Step:  1362400 Batch Loss:     0.119283 Tokens per Sec:     4945, Lr: 0.000200
2020-07-05 15:44:02,332 Epoch  36: total training loss 7.87
2020-07-05 15:44:02,332 EPOCH 37
2020-07-05 15:44:27,577 Epoch  37 Step:  1362500 Batch Loss:     0.115880 Tokens per Sec:     5001, Lr: 0.000200
2020-07-05 15:44:36,796 Epoch  37: total training loss 7.73
2020-07-05 15:44:36,796 EPOCH 38
2020-07-05 15:45:11,411 Epoch  38: total training loss 7.54
2020-07-05 15:45:11,412 EPOCH 39
2020-07-05 15:45:18,787 Epoch  39 Step:  1362600 Batch Loss:     0.108628 Tokens per Sec:     5072, Lr: 0.000200
2020-07-05 15:45:46,031 Epoch  39: total training loss 7.30
2020-07-05 15:45:46,032 EPOCH 40
2020-07-05 15:46:09,475 Epoch  40 Step:  1362700 Batch Loss:     0.108121 Tokens per Sec:     5043, Lr: 0.000200
2020-07-05 15:46:20,356 Epoch  40: total training loss 7.55
2020-07-05 15:46:20,357 EPOCH 41
2020-07-05 15:46:54,913 Epoch  41: total training loss 7.54
2020-07-05 15:46:54,914 EPOCH 42
2020-07-05 15:46:59,529 Epoch  42 Step:  1362800 Batch Loss:     0.105693 Tokens per Sec:     4374, Lr: 0.000200
2020-07-05 15:47:29,424 Epoch  42: total training loss 6.84
2020-07-05 15:47:29,424 EPOCH 43
2020-07-05 15:47:51,132 Epoch  43 Step:  1362900 Batch Loss:     0.106486 Tokens per Sec:     4963, Lr: 0.000200
2020-07-05 15:48:03,938 Epoch  43: total training loss 6.67
2020-07-05 15:48:03,938 EPOCH 44
2020-07-05 15:48:38,565 Epoch  44: total training loss 6.57
2020-07-05 15:48:38,566 EPOCH 45
2020-07-05 15:48:42,264 Epoch  45 Step:  1363000 Batch Loss:     0.093309 Tokens per Sec:     4928, Lr: 0.000200
2020-07-05 15:49:53,872 Example #0
2020-07-05 15:49:53,873 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-05 15:49:53,873 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-05 15:49:53,873 	Source:     Hi , how can I help you ?
2020-07-05 15:49:53,873 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:49:53,873 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-05 15:49:53,873 Example #1
2020-07-05 15:49:53,873 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-05 15:49:53,873 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-05 15:49:53,873 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-05 15:49:53,873 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-05 15:49:53,873 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-05 15:49:53,873 Example #2
2020-07-05 15:49:53,873 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-05 15:49:53,873 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'eine', 'Sek@@', 'unde', ',', 'ich', 'werde', 'gleich', 'wieder', 'ein', 'paar', 'Optionen', 'haben', '.']
2020-07-05 15:49:53,873 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-05 15:49:53,874 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-05 15:49:53,874 	Hypothesis: Klar , geben Sie mir eine Sekunde , ich werde gleich wieder ein paar Optionen haben .
2020-07-05 15:49:53,874 Example #3
2020-07-05 15:49:53,874 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-05 15:49:53,874 	Raw hypothesis: ['Ich', 'habe', 'Folgendes', 'gefunden', ':', '"', 'Se@@', 'as@@', 'ons', '52', '"', ',', 'ein', 'wechsel@@', 'n@@', 'des', 'Menü', 'mit', 'sa@@', 'ison@@', 'alen', 'amerikanischen', 'Gerichten', 'sowie', 'internationalen', 'W@@', 'einen', 'in', 'geho@@', 'ben@@', 'em', 'Rahmen', '.', 'Ich', 'habe', 'auch', 'Folgendes', 'gefunden', ':', '"', 'P@@', 'lu@@', 'to', "'", 's', '"', ':', 'eine', 'lokale', 'Co@@', 'unter@@', '-@@', 'Ser@@', 've-@@', 'K@@', 'ette', 'mit', 'haus@@', 'eigenen', 'Sal@@', 'aten', '&', 'Sand@@', 'wich@@', 'es', 'in', 'lässi@@', 'gem', ',', 'moder@@', 'nem', 'Ambiente', '.']
2020-07-05 15:49:53,874 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-05 15:49:53,874 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-05 15:49:53,874 	Hypothesis: Ich habe Folgendes gefunden : " Seasons 52 " , ein wechselndes Menü mit saisonalen amerikanischen Gerichten sowie internationalen Weinen in gehobenem Rahmen . Ich habe auch Folgendes gefunden : " Pluto ' s " : eine lokale Counter-Serve-Kette mit hauseigenen Salaten & Sandwiches in lässigem , modernem Ambiente .
2020-07-05 15:49:53,874 Validation result (greedy) at epoch  45, step  1363000: bleu:  53.99, loss: 11946.5576, ppl:   2.2372, duration: 71.6099s
2020-07-05 15:50:24,646 Epoch  45: total training loss 6.30
2020-07-05 15:50:24,646 EPOCH 46
2020-07-05 15:50:45,039 Epoch  46 Step:  1363100 Batch Loss:     0.110769 Tokens per Sec:     4978, Lr: 0.000200
2020-07-05 15:50:59,348 Epoch  46: total training loss 6.25
2020-07-05 15:50:59,349 EPOCH 47
2020-07-05 15:51:34,061 Epoch  47: total training loss 6.20
2020-07-05 15:51:34,062 EPOCH 48
2020-07-05 15:51:36,071 Epoch  48 Step:  1363200 Batch Loss:     0.075020 Tokens per Sec:     4675, Lr: 0.000200
2020-07-05 15:52:08,422 Epoch  48: total training loss 6.04
2020-07-05 15:52:08,422 EPOCH 49
2020-07-05 15:52:27,076 Epoch  49 Step:  1363300 Batch Loss:     0.101917 Tokens per Sec:     4909, Lr: 0.000200
2020-07-05 15:52:43,044 Epoch  49: total training loss 5.83
2020-07-05 15:52:43,044 EPOCH 50
2020-07-05 15:53:17,709 Epoch  50: total training loss 5.70
2020-07-05 15:53:17,710 Training ended after  50 epochs.
2020-07-05 15:53:17,710 Best validation result (greedy) at step  1361000:   1.93 ppl.
2020-07-05 15:54:25,944  dev bleu:  56.03 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-05 15:54:25,948 Translations saved to: models/wmt20/tfm_b4096_ende-tune/01361000.hyps.dev
2020-07-05 15:54:25,949 test bleu:  -1.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-05 15:54:25,951 Translations saved to: models/wmt20/tfm_b4096_ende-tune/01361000.hyps.test
