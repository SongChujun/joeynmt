2020-06-09 17:29:19,890 Hello! This is Joey-NMT.
2020-06-09 17:29:27,890 Total params: 53688833
2020-06-09 17:29:28,084 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.0.layer_norm.bias', 'encoder_2.0.layer_norm.weight', 'encoder_2.0.layers.5.feed_forward.layer_norm.bias', 'encoder_2.0.layers.5.feed_forward.layer_norm.weight', 'encoder_2.0.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.0.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.0.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.0.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.0.layers.5.layer_norm.bias', 'encoder_2.0.layers.5.layer_norm.weight', 'encoder_2.0.layers.5.src_src_att.k_layer.bias', 'encoder_2.0.layers.5.src_src_att.k_layer.weight', 'encoder_2.0.layers.5.src_src_att.output_layer.bias', 'encoder_2.0.layers.5.src_src_att.output_layer.weight', 'encoder_2.0.layers.5.src_src_att.q_layer.bias', 'encoder_2.0.layers.5.src_src_att.q_layer.weight', 'encoder_2.0.layers.5.src_src_att.v_layer.bias', 'encoder_2.0.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-09 17:29:33,400 cfg.name                           : transformer_multi_enc_shared_all_ende
2020-06-09 17:29:33,400 cfg.data.src                       : en
2020-06-09 17:29:33,400 cfg.data.trg                       : de
2020-06-09 17:29:33,400 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-09 17:29:33,400 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-09 17:29:33,401 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-09 17:29:33,401 cfg.data.level                     : bpe
2020-06-09 17:29:33,401 cfg.data.lowercase                 : True
2020-06-09 17:29:33,401 cfg.data.max_sent_length           : 100
2020-06-09 17:29:33,401 cfg.testing.beam_size              : 5
2020-06-09 17:29:33,401 cfg.testing.alpha                  : 1.0
2020-06-09 17:29:33,401 cfg.training.random_seed           : 42
2020-06-09 17:29:33,401 cfg.training.optimizer             : adam
2020-06-09 17:29:33,402 cfg.training.normalization         : tokens
2020-06-09 17:29:33,402 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-09 17:29:33,402 cfg.training.scheduling            : plateau
2020-06-09 17:29:33,402 cfg.training.patience              : 8
2020-06-09 17:29:33,402 cfg.training.decrease_factor       : 0.7
2020-06-09 17:29:33,402 cfg.training.loss                  : crossentropy
2020-06-09 17:29:33,402 cfg.training.learning_rate         : 0.0002
2020-06-09 17:29:33,402 cfg.training.learning_rate_min     : 1e-08
2020-06-09 17:29:33,403 cfg.training.weight_decay          : 0.0
2020-06-09 17:29:33,403 cfg.training.label_smoothing       : 0.1
2020-06-09 17:29:33,403 cfg.training.batch_size            : 4096
2020-06-09 17:29:33,403 cfg.training.batch_type            : token
2020-06-09 17:29:33,403 cfg.training.eval_batch_size       : 3600
2020-06-09 17:29:33,403 cfg.training.eval_batch_type       : token
2020-06-09 17:29:33,403 cfg.training.batch_multiplier      : 1
2020-06-09 17:29:33,403 cfg.training.early_stopping_metric : ppl
2020-06-09 17:29:33,404 cfg.training.epochs                : 100
2020-06-09 17:29:33,404 cfg.training.validation_freq       : 1000
2020-06-09 17:29:33,404 cfg.training.logging_freq          : 100
2020-06-09 17:29:33,404 cfg.training.eval_metric           : bleu
2020-06-09 17:29:33,404 cfg.training.model_dir             : models/transformer_multi_enc_shared_all_ende
2020-06-09 17:29:33,404 cfg.training.overwrite             : True
2020-06-09 17:29:33,404 cfg.training.shuffle               : True
2020-06-09 17:29:33,404 cfg.training.use_cuda              : True
2020-06-09 17:29:33,404 cfg.training.max_output_length     : 100
2020-06-09 17:29:33,405 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-09 17:29:33,405 cfg.training.keep_last_ckpts       : 3
2020-06-09 17:29:33,405 cfg.model.initializer              : xavier
2020-06-09 17:29:33,405 cfg.model.bias_initializer         : zeros
2020-06-09 17:29:33,405 cfg.model.init_gain                : 1.0
2020-06-09 17:29:33,405 cfg.model.embed_initializer        : xavier
2020-06-09 17:29:33,405 cfg.model.embed_init_gain          : 1.0
2020-06-09 17:29:33,405 cfg.model.tied_embeddings          : False
2020-06-09 17:29:33,405 cfg.model.tied_softmax             : True
2020-06-09 17:29:33,405 cfg.model.encoder.type             : transformer
2020-06-09 17:29:33,405 cfg.model.encoder.num_layers       : 6
2020-06-09 17:29:33,406 cfg.model.encoder.num_heads        : 8
2020-06-09 17:29:33,406 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-09 17:29:33,406 cfg.model.encoder.embeddings.scale : True
2020-06-09 17:29:33,406 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-09 17:29:33,406 cfg.model.encoder.hidden_size      : 512
2020-06-09 17:29:33,406 cfg.model.encoder.ff_size          : 2048
2020-06-09 17:29:33,406 cfg.model.encoder.dropout          : 0.1
2020-06-09 17:29:33,406 cfg.model.encoder.freeze           : False
2020-06-09 17:29:33,406 cfg.model.encoder.multi_encoder    : True
2020-06-09 17:29:33,406 cfg.model.encoder.share_ctx_encoder : True
2020-06-09 17:29:33,406 cfg.model.encoder.share_encoder    : True
2020-06-09 17:29:33,407 cfg.model.decoder.type             : transformer
2020-06-09 17:29:33,407 cfg.model.decoder.num_layers       : 6
2020-06-09 17:29:33,407 cfg.model.decoder.num_heads        : 8
2020-06-09 17:29:33,407 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-09 17:29:33,407 cfg.model.decoder.embeddings.scale : True
2020-06-09 17:29:33,407 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-09 17:29:33,407 cfg.model.decoder.hidden_size      : 512
2020-06-09 17:29:33,407 cfg.model.decoder.ff_size          : 2048
2020-06-09 17:29:33,407 cfg.model.decoder.dropout          : 0.1
2020-06-09 17:29:33,407 cfg.model.decoder.freeze           : False
2020-06-09 17:29:33,407 Data set sizes: 
	train 9765,
	valid 1524,
	test 1190
2020-06-09 17:29:33,408 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-09 17:29:33,408 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-09 17:29:33,408 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-09 17:29:33,408 Number of Src words (types): 4559
2020-06-09 17:29:33,408 Number of Trg words (types): 5874
2020-06-09 17:29:33,408 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4559),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5874))
2020-06-09 17:29:33,426 EPOCH 1
2020-06-09 17:30:04,620 Epoch   1: total training loss 294.05
2020-06-09 17:30:04,621 EPOCH 2
2020-06-09 17:30:29,515 Epoch   2 Step:      100 Batch Loss:     4.678598 Tokens per Sec:     4434, Lr: 0.000200
2020-06-09 17:30:34,459 Epoch   2: total training loss 265.26
2020-06-09 17:30:34,460 EPOCH 3
2020-06-09 17:31:03,949 Epoch   3: total training loss 235.58
2020-06-09 17:31:03,950 EPOCH 4
2020-06-09 17:31:23,644 Epoch   4 Step:      200 Batch Loss:     3.325485 Tokens per Sec:     4326, Lr: 0.000200
2020-06-09 17:31:33,780 Epoch   4: total training loss 230.50
2020-06-09 17:31:33,781 EPOCH 5
2020-06-09 17:32:03,415 Epoch   5: total training loss 211.11
2020-06-09 17:32:03,416 EPOCH 6
2020-06-09 17:32:16,953 Epoch   6 Step:      300 Batch Loss:     3.532578 Tokens per Sec:     4413, Lr: 0.000200
2020-06-09 17:32:32,887 Epoch   6: total training loss 191.37
2020-06-09 17:32:32,888 EPOCH 7
2020-06-09 17:33:02,549 Epoch   7: total training loss 179.31
2020-06-09 17:33:02,550 EPOCH 8
2020-06-09 17:33:10,863 Epoch   8 Step:      400 Batch Loss:     3.125143 Tokens per Sec:     4512, Lr: 0.000200
2020-06-09 17:33:32,291 Epoch   8: total training loss 169.16
2020-06-09 17:33:32,293 EPOCH 9
2020-06-09 17:34:01,660 Epoch   9: total training loss 158.24
2020-06-09 17:34:01,661 EPOCH 10
2020-06-09 17:34:03,672 Epoch  10 Step:      500 Batch Loss:     3.303524 Tokens per Sec:     5049, Lr: 0.000200
2020-06-09 17:34:30,989 Epoch  10: total training loss 146.05
2020-06-09 17:34:30,990 EPOCH 11
2020-06-09 17:34:57,769 Epoch  11 Step:      600 Batch Loss:     3.501712 Tokens per Sec:     4434, Lr: 0.000200
2020-06-09 17:35:00,515 Epoch  11: total training loss 137.97
2020-06-09 17:35:00,516 EPOCH 12
2020-06-09 17:35:29,525 Epoch  12: total training loss 126.00
2020-06-09 17:35:29,525 EPOCH 13
2020-06-09 17:35:50,636 Epoch  13 Step:      700 Batch Loss:     3.695563 Tokens per Sec:     4454, Lr: 0.000200
2020-06-09 17:35:58,741 Epoch  13: total training loss 117.28
2020-06-09 17:35:58,742 EPOCH 14
2020-06-09 17:36:28,242 Epoch  14: total training loss 118.02
2020-06-09 17:36:28,243 EPOCH 15
2020-06-09 17:36:43,384 Epoch  15 Step:      800 Batch Loss:     2.470199 Tokens per Sec:     4475, Lr: 0.000200
2020-06-09 17:36:57,802 Epoch  15: total training loss 106.62
2020-06-09 17:36:57,803 EPOCH 16
2020-06-09 17:37:27,425 Epoch  16: total training loss 94.06
2020-06-09 17:37:27,426 EPOCH 17
2020-06-09 17:37:37,861 Epoch  17 Step:      900 Batch Loss:     0.960557 Tokens per Sec:     4121, Lr: 0.000200
2020-06-09 17:37:57,273 Epoch  17: total training loss 93.19
2020-06-09 17:37:57,274 EPOCH 18
2020-06-09 17:38:26,639 Epoch  18: total training loss 83.21
2020-06-09 17:38:26,640 EPOCH 19
2020-06-09 17:38:30,288 Epoch  19 Step:     1000 Batch Loss:     1.007244 Tokens per Sec:     5384, Lr: 0.000200
2020-06-09 17:39:11,404 Hooray! New best validation result [ppl]!
2020-06-09 17:39:11,405 Saving new checkpoint.
2020-06-09 17:39:19,929 Example #0
2020-06-09 17:39:19,930 	Raw source:     ['hello', '.']
2020-06-09 17:39:19,930 	Raw hypothesis: ['hallo', '.']
2020-06-09 17:39:19,930 	Source:     hello .
2020-06-09 17:39:19,930 	Reference:  hallo ,
2020-06-09 17:39:19,930 	Hypothesis: hallo .
2020-06-09 17:39:19,930 Example #1
2020-06-09 17:39:19,930 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 17:39:19,930 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 17:39:19,930 	Source:     hi , how can i help you ?
2020-06-09 17:39:19,930 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 17:39:19,930 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 17:39:19,930 Example #2
2020-06-09 17:39:19,930 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 17:39:19,930 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'san', 'francisco', ',', 'kalifornien', ',', 'in', 'der', 'arden', 'fair', 'mall', '.']
2020-06-09 17:39:19,930 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 17:39:19,930 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 17:39:19,930 	Hypothesis: hallo , ich suche ein restaurant in san francisco , kalifornien , in der arden fair mall .
2020-06-09 17:39:19,930 Example #3
2020-06-09 17:39:19,931 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 17:39:19,931 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 17:39:19,931 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 17:39:19,931 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 17:39:19,931 	Hypothesis: ok , welche art von restaurant suchen sie ?
2020-06-09 17:39:19,931 Validation result (greedy) at epoch  19, step     1000: bleu:  25.26, loss: 50902.7070, ppl:  11.1702, duration: 49.6411s
2020-06-09 17:39:46,206 Epoch  19: total training loss 76.97
2020-06-09 17:39:46,206 EPOCH 20
2020-06-09 17:40:14,405 Epoch  20 Step:     1100 Batch Loss:     1.624174 Tokens per Sec:     4439, Lr: 0.000200
2020-06-09 17:40:15,570 Epoch  20: total training loss 75.86
2020-06-09 17:40:15,570 EPOCH 21
2020-06-09 17:40:45,239 Epoch  21: total training loss 66.59
2020-06-09 17:40:45,240 EPOCH 22
2020-06-09 17:41:09,570 Epoch  22 Step:     1200 Batch Loss:     1.875407 Tokens per Sec:     4337, Lr: 0.000200
2020-06-09 17:41:14,982 Epoch  22: total training loss 61.65
2020-06-09 17:41:14,983 EPOCH 23
2020-06-09 17:41:44,771 Epoch  23: total training loss 60.03
2020-06-09 17:41:44,773 EPOCH 24
2020-06-09 17:42:03,070 Epoch  24 Step:     1300 Batch Loss:     0.880543 Tokens per Sec:     4381, Lr: 0.000200
2020-06-09 17:42:14,153 Epoch  24: total training loss 55.68
2020-06-09 17:42:14,153 EPOCH 25
2020-06-09 17:42:43,661 Epoch  25: total training loss 53.44
2020-06-09 17:42:43,662 EPOCH 26
2020-06-09 17:42:55,711 Epoch  26 Step:     1400 Batch Loss:     0.996261 Tokens per Sec:     4472, Lr: 0.000200
2020-06-09 17:43:13,359 Epoch  26: total training loss 48.13
2020-06-09 17:43:13,360 EPOCH 27
2020-06-09 17:43:42,877 Epoch  27: total training loss 41.25
2020-06-09 17:43:42,878 EPOCH 28
2020-06-09 17:43:50,463 Epoch  28 Step:     1500 Batch Loss:     0.778977 Tokens per Sec:     4326, Lr: 0.000200
2020-06-09 17:44:13,100 Epoch  28: total training loss 38.86
2020-06-09 17:44:13,101 EPOCH 29
2020-06-09 17:44:43,013 Epoch  29: total training loss 37.07
2020-06-09 17:44:43,015 EPOCH 30
2020-06-09 17:44:44,715 Epoch  30 Step:     1600 Batch Loss:     0.759934 Tokens per Sec:     6204, Lr: 0.000200
2020-06-09 17:45:12,575 Epoch  30: total training loss 34.95
2020-06-09 17:45:12,576 EPOCH 31
2020-06-09 17:45:39,205 Epoch  31 Step:     1700 Batch Loss:     0.343471 Tokens per Sec:     4371, Lr: 0.000200
2020-06-09 17:45:42,368 Epoch  31: total training loss 32.26
2020-06-09 17:45:42,369 EPOCH 32
2020-06-09 17:46:11,998 Epoch  32: total training loss 29.09
2020-06-09 17:46:11,999 EPOCH 33
2020-06-09 17:46:32,746 Epoch  33 Step:     1800 Batch Loss:     0.570378 Tokens per Sec:     4482, Lr: 0.000200
2020-06-09 17:46:41,714 Epoch  33: total training loss 26.78
2020-06-09 17:46:41,715 EPOCH 34
2020-06-09 17:47:11,245 Epoch  34: total training loss 27.52
2020-06-09 17:47:11,246 EPOCH 35
2020-06-09 17:47:26,021 Epoch  35 Step:     1900 Batch Loss:     0.380925 Tokens per Sec:     4587, Lr: 0.000200
2020-06-09 17:47:40,291 Epoch  35: total training loss 23.89
2020-06-09 17:47:40,293 EPOCH 36
2020-06-09 17:48:10,167 Epoch  36: total training loss 21.28
2020-06-09 17:48:10,168 EPOCH 37
2020-06-09 17:48:19,367 Epoch  37 Step:     2000 Batch Loss:     0.505930 Tokens per Sec:     4683, Lr: 0.000200
2020-06-09 17:48:49,651 Hooray! New best validation result [ppl]!
2020-06-09 17:48:49,652 Saving new checkpoint.
2020-06-09 17:48:58,338 Example #0
2020-06-09 17:48:58,339 	Raw source:     ['hello', '.']
2020-06-09 17:48:58,339 	Raw hypothesis: ['hallo', '.']
2020-06-09 17:48:58,339 	Source:     hello .
2020-06-09 17:48:58,339 	Reference:  hallo ,
2020-06-09 17:48:58,339 	Hypothesis: hallo .
2020-06-09 17:48:58,339 Example #1
2020-06-09 17:48:58,339 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 17:48:58,339 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 17:48:58,339 	Source:     hi , how can i help you ?
2020-06-09 17:48:58,340 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 17:48:58,340 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 17:48:58,340 Example #2
2020-06-09 17:48:58,340 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 17:48:58,340 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 17:48:58,340 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 17:48:58,340 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 17:48:58,340 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 17:48:58,340 Example #3
2020-06-09 17:48:58,340 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 17:48:58,340 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 17:48:58,340 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 17:48:58,340 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 17:48:58,340 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 17:48:58,340 Validation result (greedy) at epoch  37, step     2000: bleu:  33.77, loss: 49492.3203, ppl:  10.4477, duration: 38.9721s
2020-06-09 17:49:18,623 Epoch  37: total training loss 19.95
2020-06-09 17:49:18,624 EPOCH 38
2020-06-09 17:49:48,159 Epoch  38: total training loss 17.71
2020-06-09 17:49:48,160 EPOCH 39
2020-06-09 17:49:53,400 Epoch  39 Step:     2100 Batch Loss:     0.162257 Tokens per Sec:     4055, Lr: 0.000200
2020-06-09 17:50:17,504 Epoch  39: total training loss 16.80
2020-06-09 17:50:17,505 EPOCH 40
2020-06-09 17:50:45,543 Epoch  40 Step:     2200 Batch Loss:     0.236291 Tokens per Sec:     4491, Lr: 0.000200
2020-06-09 17:50:46,783 Epoch  40: total training loss 16.55
2020-06-09 17:50:46,783 EPOCH 41
2020-06-09 17:51:16,039 Epoch  41: total training loss 15.68
2020-06-09 17:51:16,040 EPOCH 42
2020-06-09 17:51:39,402 Epoch  42 Step:     2300 Batch Loss:     0.145723 Tokens per Sec:     4403, Lr: 0.000200
2020-06-09 17:51:45,576 Epoch  42: total training loss 14.78
2020-06-09 17:51:45,577 EPOCH 43
2020-06-09 17:52:15,324 Epoch  43: total training loss 13.83
2020-06-09 17:52:15,325 EPOCH 44
2020-06-09 17:52:32,615 Epoch  44 Step:     2400 Batch Loss:     0.363106 Tokens per Sec:     4691, Lr: 0.000200
2020-06-09 17:52:44,487 Epoch  44: total training loss 15.76
2020-06-09 17:52:44,488 EPOCH 45
2020-06-09 17:53:13,053 Epoch  45: total training loss 15.62
2020-06-09 17:53:13,055 EPOCH 46
2020-06-09 17:53:25,146 Epoch  46 Step:     2500 Batch Loss:     0.254537 Tokens per Sec:     4639, Lr: 0.000200
2020-06-09 17:53:42,239 Epoch  46: total training loss 13.25
2020-06-09 17:53:42,240 EPOCH 47
2020-06-09 17:54:11,465 Epoch  47: total training loss 11.69
2020-06-09 17:54:11,466 EPOCH 48
2020-06-09 17:54:18,594 Epoch  48 Step:     2600 Batch Loss:     0.262240 Tokens per Sec:     4248, Lr: 0.000200
2020-06-09 17:54:40,513 Epoch  48: total training loss 11.51
2020-06-09 17:54:40,514 EPOCH 49
2020-06-09 17:55:09,163 Epoch  49: total training loss 10.44
2020-06-09 17:55:09,165 EPOCH 50
2020-06-09 17:55:11,145 Epoch  50 Step:     2700 Batch Loss:     0.186773 Tokens per Sec:     5372, Lr: 0.000200
2020-06-09 17:55:38,651 Epoch  50: total training loss 10.65
2020-06-09 17:55:38,652 EPOCH 51
2020-06-09 17:56:04,096 Epoch  51 Step:     2800 Batch Loss:     0.169420 Tokens per Sec:     4520, Lr: 0.000200
2020-06-09 17:56:07,415 Epoch  51: total training loss 9.87
2020-06-09 17:56:07,415 EPOCH 52
2020-06-09 17:56:36,099 Epoch  52: total training loss 9.53
2020-06-09 17:56:36,100 EPOCH 53
2020-06-09 17:56:56,499 Epoch  53 Step:     2900 Batch Loss:     0.173898 Tokens per Sec:     4671, Lr: 0.000200
2020-06-09 17:57:05,339 Epoch  53: total training loss 9.28
2020-06-09 17:57:05,341 EPOCH 54
2020-06-09 17:57:34,825 Epoch  54: total training loss 8.80
2020-06-09 17:57:34,826 EPOCH 55
2020-06-09 17:57:52,238 Epoch  55 Step:     3000 Batch Loss:     0.154926 Tokens per Sec:     4281, Lr: 0.000200
2020-06-09 17:58:20,952 Example #0
2020-06-09 17:58:20,953 	Raw source:     ['hello', '.']
2020-06-09 17:58:20,953 	Raw hypothesis: ['hallo', '.']
2020-06-09 17:58:20,954 	Source:     hello .
2020-06-09 17:58:20,954 	Reference:  hallo ,
2020-06-09 17:58:20,954 	Hypothesis: hallo .
2020-06-09 17:58:20,954 Example #1
2020-06-09 17:58:20,954 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 17:58:20,954 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 17:58:20,954 	Source:     hi , how can i help you ?
2020-06-09 17:58:20,954 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 17:58:20,955 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 17:58:20,955 Example #2
2020-06-09 17:58:20,955 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 17:58:20,955 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'sacramento', ',', 'kalifornien', '.']
2020-06-09 17:58:20,955 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 17:58:20,955 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 17:58:20,955 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in sacramento , kalifornien .
2020-06-09 17:58:20,955 Example #3
2020-06-09 17:58:20,955 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 17:58:20,956 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 17:58:20,956 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 17:58:20,956 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 17:58:20,956 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 17:58:20,956 Validation result (greedy) at epoch  55, step     3000: bleu:  35.22, loss: 50505.7812, ppl:  10.9620, duration: 28.7159s
2020-06-09 17:58:32,978 Epoch  55: total training loss 8.45
2020-06-09 17:58:32,979 EPOCH 56
2020-06-09 17:59:02,648 Epoch  56: total training loss 8.53
2020-06-09 17:59:02,650 EPOCH 57
2020-06-09 17:59:14,694 Epoch  57 Step:     3100 Batch Loss:     0.170771 Tokens per Sec:     4229, Lr: 0.000200
2020-06-09 17:59:32,056 Epoch  57: total training loss 8.64
2020-06-09 17:59:32,057 EPOCH 58
2020-06-09 18:00:01,212 Epoch  58: total training loss 8.81
2020-06-09 18:00:01,213 EPOCH 59
2020-06-09 18:00:07,454 Epoch  59 Step:     3200 Batch Loss:     0.130677 Tokens per Sec:     4507, Lr: 0.000200
2020-06-09 18:00:30,360 Epoch  59: total training loss 8.32
2020-06-09 18:00:30,361 EPOCH 60
2020-06-09 18:00:59,864 Epoch  60: total training loss 8.50
2020-06-09 18:00:59,865 EPOCH 61
2020-06-09 18:01:00,654 Epoch  61 Step:     3300 Batch Loss:     0.263281 Tokens per Sec:     2019, Lr: 0.000200
2020-06-09 18:01:29,092 Epoch  61: total training loss 11.36
2020-06-09 18:01:29,093 EPOCH 62
2020-06-09 18:01:54,066 Epoch  62 Step:     3400 Batch Loss:     0.102675 Tokens per Sec:     4350, Lr: 0.000200
2020-06-09 18:01:58,504 Epoch  62: total training loss 9.27
2020-06-09 18:01:58,504 EPOCH 63
2020-06-09 18:02:27,871 Epoch  63: total training loss 7.88
2020-06-09 18:02:27,872 EPOCH 64
2020-06-09 18:02:47,504 Epoch  64 Step:     3500 Batch Loss:     0.132540 Tokens per Sec:     4481, Lr: 0.000200
2020-06-09 18:02:57,305 Epoch  64: total training loss 7.62
2020-06-09 18:02:57,306 EPOCH 65
2020-06-09 18:03:26,327 Epoch  65: total training loss 7.15
2020-06-09 18:03:26,329 EPOCH 66
2020-06-09 18:03:42,616 Epoch  66 Step:     3600 Batch Loss:     0.126297 Tokens per Sec:     4203, Lr: 0.000200
2020-06-09 18:03:55,701 Epoch  66: total training loss 7.07
2020-06-09 18:03:55,702 EPOCH 67
2020-06-09 18:04:25,572 Epoch  67: total training loss 6.95
2020-06-09 18:04:25,573 EPOCH 68
2020-06-09 18:04:36,818 Epoch  68 Step:     3700 Batch Loss:     0.113114 Tokens per Sec:     4356, Lr: 0.000200
2020-06-09 18:04:54,913 Epoch  68: total training loss 6.78
2020-06-09 18:04:54,914 EPOCH 69
2020-06-09 18:05:24,777 Epoch  69: total training loss 6.89
2020-06-09 18:05:24,778 EPOCH 70
2020-06-09 18:05:30,376 Epoch  70 Step:     3800 Batch Loss:     0.122619 Tokens per Sec:     5091, Lr: 0.000200
2020-06-09 18:05:53,886 Epoch  70: total training loss 6.59
2020-06-09 18:05:53,887 EPOCH 71
2020-06-09 18:06:24,845 Epoch  71: total training loss 6.82
2020-06-09 18:06:24,846 EPOCH 72
2020-06-09 18:06:25,295 Epoch  72 Step:     3900 Batch Loss:     0.116904 Tokens per Sec:     5479, Lr: 0.000200
2020-06-09 18:06:54,230 Epoch  72: total training loss 6.67
2020-06-09 18:06:54,231 EPOCH 73
2020-06-09 18:07:19,973 Epoch  73 Step:     4000 Batch Loss:     0.130736 Tokens per Sec:     4264, Lr: 0.000200
2020-06-09 18:07:43,467 Example #0
2020-06-09 18:07:43,467 	Raw source:     ['hello', '.']
2020-06-09 18:07:43,468 	Raw hypothesis: ['hallo', '.']
2020-06-09 18:07:43,468 	Source:     hello .
2020-06-09 18:07:43,468 	Reference:  hallo ,
2020-06-09 18:07:43,468 	Hypothesis: hallo .
2020-06-09 18:07:43,468 Example #1
2020-06-09 18:07:43,468 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 18:07:43,468 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 18:07:43,468 	Source:     hi , how can i help you ?
2020-06-09 18:07:43,468 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 18:07:43,469 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 18:07:43,469 Example #2
2020-06-09 18:07:43,469 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 18:07:43,469 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 18:07:43,469 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 18:07:43,469 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 18:07:43,469 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 18:07:43,469 Example #3
2020-06-09 18:07:43,470 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 18:07:43,470 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 18:07:43,470 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 18:07:43,470 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 18:07:43,470 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 18:07:43,470 Validation result (greedy) at epoch  73, step     4000: bleu:  35.77, loss: 51026.7031, ppl:  11.2361, duration: 23.4954s
2020-06-09 18:07:47,622 Epoch  73: total training loss 6.55
2020-06-09 18:07:47,623 EPOCH 74
2020-06-09 18:08:17,237 Epoch  74: total training loss 6.41
2020-06-09 18:08:17,239 EPOCH 75
2020-06-09 18:08:36,739 Epoch  75 Step:     4100 Batch Loss:     0.117060 Tokens per Sec:     4588, Lr: 0.000200
2020-06-09 18:08:46,230 Epoch  75: total training loss 6.32
2020-06-09 18:08:46,231 EPOCH 76
2020-06-09 18:09:15,835 Epoch  76: total training loss 6.39
2020-06-09 18:09:15,836 EPOCH 77
2020-06-09 18:09:29,919 Epoch  77 Step:     4200 Batch Loss:     0.111735 Tokens per Sec:     4598, Lr: 0.000200
2020-06-09 18:09:45,309 Epoch  77: total training loss 6.12
2020-06-09 18:09:45,310 EPOCH 78
2020-06-09 18:10:14,652 Epoch  78: total training loss 6.11
2020-06-09 18:10:14,653 EPOCH 79
2020-06-09 18:10:24,928 Epoch  79 Step:     4300 Batch Loss:     0.100771 Tokens per Sec:     4276, Lr: 0.000200
2020-06-09 18:10:44,467 Epoch  79: total training loss 5.93
2020-06-09 18:10:44,468 EPOCH 80
2020-06-09 18:11:14,124 Epoch  80: total training loss 6.13
2020-06-09 18:11:14,125 EPOCH 81
2020-06-09 18:11:18,714 Epoch  81 Step:     4400 Batch Loss:     0.111677 Tokens per Sec:     4252, Lr: 0.000200
2020-06-09 18:11:43,536 Epoch  81: total training loss 5.88
2020-06-09 18:11:43,537 EPOCH 82
2020-06-09 18:12:11,773 Epoch  82 Step:     4500 Batch Loss:     0.110060 Tokens per Sec:     4430, Lr: 0.000200
2020-06-09 18:12:12,852 Epoch  82: total training loss 5.85
2020-06-09 18:12:12,852 EPOCH 83
2020-06-09 18:12:42,105 Epoch  83: total training loss 5.85
2020-06-09 18:12:42,106 EPOCH 84
2020-06-09 18:13:05,793 Epoch  84 Step:     4600 Batch Loss:     0.121789 Tokens per Sec:     4504, Lr: 0.000200
2020-06-09 18:13:11,701 Epoch  84: total training loss 5.78
2020-06-09 18:13:11,702 EPOCH 85
2020-06-09 18:13:41,672 Epoch  85: total training loss 6.01
2020-06-09 18:13:41,673 EPOCH 86
2020-06-09 18:13:59,241 Epoch  86 Step:     4700 Batch Loss:     0.109840 Tokens per Sec:     4714, Lr: 0.000200
2020-06-09 18:14:11,056 Epoch  86: total training loss 5.75
2020-06-09 18:14:11,057 EPOCH 87
2020-06-09 18:14:39,753 Epoch  87: total training loss 8.34
2020-06-09 18:14:39,754 EPOCH 88
2020-06-09 18:14:52,980 Epoch  88 Step:     4800 Batch Loss:     0.123311 Tokens per Sec:     4344, Lr: 0.000200
2020-06-09 18:15:09,119 Epoch  88: total training loss 7.47
2020-06-09 18:15:09,120 EPOCH 89
2020-06-09 18:15:38,877 Epoch  89: total training loss 6.31
2020-06-09 18:15:38,878 EPOCH 90
2020-06-09 18:15:47,444 Epoch  90 Step:     4900 Batch Loss:     0.114071 Tokens per Sec:     3944, Lr: 0.000200
2020-06-09 18:16:08,524 Epoch  90: total training loss 6.03
2020-06-09 18:16:08,525 EPOCH 91
2020-06-09 18:16:37,860 Epoch  91: total training loss 5.45
2020-06-09 18:16:37,861 EPOCH 92
2020-06-09 18:16:41,549 Epoch  92 Step:     5000 Batch Loss:     0.089159 Tokens per Sec:     3556, Lr: 0.000200
2020-06-09 18:17:07,385 Example #0
2020-06-09 18:17:07,386 	Raw source:     ['hello', '.']
2020-06-09 18:17:07,387 	Raw hypothesis: ['hallo', '.']
2020-06-09 18:17:07,387 	Source:     hello .
2020-06-09 18:17:07,387 	Reference:  hallo ,
2020-06-09 18:17:07,387 	Hypothesis: hallo .
2020-06-09 18:17:07,387 Example #1
2020-06-09 18:17:07,387 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-09 18:17:07,387 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-09 18:17:07,387 	Source:     hi , how can i help you ?
2020-06-09 18:17:07,388 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-09 18:17:07,388 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-09 18:17:07,388 Example #2
2020-06-09 18:17:07,388 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-09 18:17:07,388 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-09 18:17:07,388 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-09 18:17:07,388 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-09 18:17:07,389 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-09 18:17:07,389 Example #3
2020-06-09 18:17:07,389 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-09 18:17:07,389 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-09 18:17:07,389 	Source:     ok , what type of restaurant are you looking for ?
2020-06-09 18:17:07,389 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-09 18:17:07,389 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-09 18:17:07,389 Validation result (greedy) at epoch  92, step     5000: bleu:  35.57, loss: 51161.6250, ppl:  11.3082, duration: 25.8395s
2020-06-09 18:17:33,689 Epoch  92: total training loss 5.52
2020-06-09 18:17:33,690 EPOCH 93
2020-06-09 18:18:00,379 Epoch  93 Step:     5100 Batch Loss:     0.092293 Tokens per Sec:     4528, Lr: 0.000200
2020-06-09 18:18:02,758 Epoch  93: total training loss 5.25
2020-06-09 18:18:02,759 EPOCH 94
2020-06-09 18:18:31,901 Epoch  94: total training loss 5.04
2020-06-09 18:18:31,902 EPOCH 95
2020-06-09 18:18:53,744 Epoch  95 Step:     5200 Batch Loss:     0.078767 Tokens per Sec:     4611, Lr: 0.000200
2020-06-09 18:19:01,097 Epoch  95: total training loss 5.05
2020-06-09 18:19:01,098 EPOCH 96
2020-06-09 18:19:30,362 Epoch  96: total training loss 5.22
2020-06-09 18:19:30,363 EPOCH 97
2020-06-09 18:19:48,306 Epoch  97 Step:     5300 Batch Loss:     0.097411 Tokens per Sec:     4245, Lr: 0.000200
2020-06-09 18:19:59,942 Epoch  97: total training loss 5.26
2020-06-09 18:19:59,944 EPOCH 98
2020-06-09 18:20:29,272 Epoch  98: total training loss 5.19
2020-06-09 18:20:29,273 EPOCH 99
2020-06-09 18:20:42,008 Epoch  99 Step:     5400 Batch Loss:     0.083560 Tokens per Sec:     4278, Lr: 0.000200
2020-06-09 18:20:58,555 Epoch  99: total training loss 5.17
2020-06-09 18:20:58,556 EPOCH 100
2020-06-09 18:21:27,920 Epoch 100: total training loss 5.10
2020-06-09 18:21:27,921 Training ended after 100 epochs.
2020-06-09 18:21:27,921 Best validation result (greedy) at step     2000:  10.45 ppl.
2020-06-09 18:21:58,140  dev bleu:  36.51 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-09 18:21:58,147 Translations saved to: models/transformer_multi_enc_shared_all_ende/00002000.hyps.dev
2020-06-09 18:22:18,230 test bleu:  33.36 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-09 18:22:18,238 Translations saved to: models/transformer_multi_enc_shared_all_ende/00002000.hyps.test
