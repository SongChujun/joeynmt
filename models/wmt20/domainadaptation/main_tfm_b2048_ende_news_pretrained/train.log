2020-07-13 01:50:40,758 Hello! This is Joey-NMT.
2020-07-13 01:50:40,777 Total params: 82862081
2020-07-13 01:50:40,779 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-07-13 01:50:58,155 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-13 01:51:05,885 Reset optimizer.
2020-07-13 01:51:05,885 Reset scheduler.
2020-07-13 01:51:05,885 Reset tracking of the best checkpoint.
2020-07-13 01:51:05,890 cfg.name                           : transformer
2020-07-13 01:51:05,890 cfg.data.src                       : en
2020-07-13 01:51:05,890 cfg.data.trg                       : de
2020-07-13 01:51:05,890 cfg.data.train                     : chatnmt/official_split_line_by_line/wmt17bpe__boundaries/news_bigrams_line_by_line
2020-07-13 01:51:05,890 cfg.data.dev                       : chatnmt/official_split_line_by_line/wmt17bpe__boundaries/train.1400.tags.bpe.wmt-ende-best
2020-07-13 01:51:05,890 cfg.data.test                      : chatnmt/official_split/wmt17bpe/test.tags.bpe.wmt-ende-best
2020-07-13 01:51:05,890 cfg.data.level                     : bpe
2020-07-13 01:51:05,890 cfg.data.lowercase                 : False
2020-07-13 01:51:05,890 cfg.data.max_sent_length           : 100
2020-07-13 01:51:05,891 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-13 01:51:05,891 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-13 01:51:05,891 cfg.testing.beam_size              : 5
2020-07-13 01:51:05,891 cfg.testing.alpha                  : 1.0
2020-07-13 01:51:05,891 cfg.training.random_seed           : 42
2020-07-13 01:51:05,891 cfg.training.optimizer             : adam
2020-07-13 01:51:05,891 cfg.training.normalization         : tokens
2020-07-13 01:51:05,891 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-13 01:51:05,891 cfg.training.scheduling            : plateau
2020-07-13 01:51:05,891 cfg.training.patience              : 8
2020-07-13 01:51:05,891 cfg.training.decrease_factor       : 0.7
2020-07-13 01:51:05,891 cfg.training.loss                  : crossentropy
2020-07-13 01:51:05,891 cfg.training.learning_rate         : 0.0002
2020-07-13 01:51:05,891 cfg.training.learning_rate_min     : 1e-08
2020-07-13 01:51:05,891 cfg.training.weight_decay          : 0.0
2020-07-13 01:51:05,892 cfg.training.label_smoothing       : 0.1
2020-07-13 01:51:05,892 cfg.training.batch_size            : 2048
2020-07-13 01:51:05,892 cfg.training.batch_type            : token
2020-07-13 01:51:05,892 cfg.training.batch_multiplier      : 1
2020-07-13 01:51:05,892 cfg.training.early_stopping_metric : ppl
2020-07-13 01:51:05,892 cfg.training.epochs                : 100
2020-07-13 01:51:05,892 cfg.training.validation_freq       : 750
2020-07-13 01:51:05,892 cfg.training.logging_freq          : 100
2020-07-13 01:51:05,892 cfg.training.eval_metric           : bleu
2020-07-13 01:51:05,892 cfg.training.model_dir             : models/wmt20/domainadaptation/main_tfm_b2048_ende_news_pretrained/
2020-07-13 01:51:05,892 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-13 01:51:05,892 cfg.training.reset_best_ckpt       : True
2020-07-13 01:51:05,892 cfg.training.reset_scheduler       : True
2020-07-13 01:51:05,892 cfg.training.reset_optimizer       : True
2020-07-13 01:51:05,892 cfg.training.overwrite             : True
2020-07-13 01:51:05,893 cfg.training.shuffle               : True
2020-07-13 01:51:05,893 cfg.training.use_cuda              : True
2020-07-13 01:51:05,893 cfg.training.max_output_length     : 100
2020-07-13 01:51:05,893 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-13 01:51:05,893 cfg.training.keep_last_ckpts       : 3
2020-07-13 01:51:05,893 cfg.model.initializer              : xavier
2020-07-13 01:51:05,893 cfg.model.bias_initializer         : zeros
2020-07-13 01:51:05,893 cfg.model.init_gain                : 1.0
2020-07-13 01:51:05,893 cfg.model.embed_initializer        : xavier
2020-07-13 01:51:05,893 cfg.model.embed_init_gain          : 1.0
2020-07-13 01:51:05,893 cfg.model.tied_embeddings          : True
2020-07-13 01:51:05,893 cfg.model.tied_softmax             : True
2020-07-13 01:51:05,893 cfg.model.encoder.type             : transformer
2020-07-13 01:51:05,893 cfg.model.encoder.num_layers       : 6
2020-07-13 01:51:05,893 cfg.model.encoder.num_heads        : 8
2020-07-13 01:51:05,894 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-13 01:51:05,894 cfg.model.encoder.embeddings.scale : True
2020-07-13 01:51:05,894 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-13 01:51:05,894 cfg.model.encoder.hidden_size      : 512
2020-07-13 01:51:05,894 cfg.model.encoder.ff_size          : 2048
2020-07-13 01:51:05,894 cfg.model.encoder.freeze           : False
2020-07-13 01:51:05,894 cfg.model.encoder.dropout          : 0.1
2020-07-13 01:51:05,894 cfg.model.encoder.multi_encoder    : True
2020-07-13 01:51:05,894 cfg.model.decoder.type             : transformer
2020-07-13 01:51:05,894 cfg.model.decoder.num_layers       : 6
2020-07-13 01:51:05,894 cfg.model.decoder.num_heads        : 8
2020-07-13 01:51:05,894 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-13 01:51:05,894 cfg.model.decoder.embeddings.scale : True
2020-07-13 01:51:05,894 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-13 01:51:05,894 cfg.model.decoder.hidden_size      : 512
2020-07-13 01:51:05,894 cfg.model.decoder.ff_size          : 2048
2020-07-13 01:51:05,894 cfg.model.decoder.freeze           : False
2020-07-13 01:51:05,894 cfg.model.decoder.dropout          : 0.1
2020-07-13 01:51:05,894 Data set sizes: 
	train 1303,
	valid 1347,
	test 0
2020-07-13 01:51:05,894 First training example:
	[SRC] Th@@ ak@@ sin and the L@@ ess@@ ons of Hong Kong
	[TRG] Th@@ ak@@ sin und die Lehren aus Hongkong
2020-07-13 01:51:05,894 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-13 01:51:05,895 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-13 01:51:05,895 Number of Src words (types): 36628
2020-07-13 01:51:05,895 Number of Trg words (types): 36628
2020-07-13 01:51:05,895 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-13 01:51:05,922 EPOCH 1
2020-07-13 01:51:10,803 Epoch   1: total training loss 139.78
2020-07-13 01:51:10,803 EPOCH 2
2020-07-13 01:51:14,505 Epoch   2: total training loss 107.57
2020-07-13 01:51:14,506 EPOCH 3
2020-07-13 01:51:18,217 Epoch   3: total training loss 81.63
2020-07-13 01:51:18,218 EPOCH 4
2020-07-13 01:51:21,899 Epoch   4: total training loss 61.77
2020-07-13 01:51:21,900 EPOCH 5
2020-07-13 01:51:25,410 Epoch   5 Step:  1360100 Batch Loss:     1.936708 Tokens per Sec:     7595, Lr: 0.000200
2020-07-13 01:51:25,594 Epoch   5: total training loss 46.83
2020-07-13 01:51:25,595 EPOCH 6
2020-07-13 01:51:29,381 Epoch   6: total training loss 39.14
2020-07-13 01:51:29,381 EPOCH 7
2020-07-13 01:51:33,182 Epoch   7: total training loss 32.69
2020-07-13 01:51:33,183 EPOCH 8
2020-07-13 01:51:36,873 Epoch   8: total training loss 26.24
2020-07-13 01:51:36,874 EPOCH 9
2020-07-13 01:51:40,735 Epoch   9: total training loss 24.45
2020-07-13 01:51:40,736 EPOCH 10
2020-07-13 01:51:43,688 Epoch  10 Step:  1360200 Batch Loss:     1.200581 Tokens per Sec:     7609, Lr: 0.000200
2020-07-13 01:51:44,524 Epoch  10: total training loss 21.91
2020-07-13 01:51:44,525 EPOCH 11
2020-07-13 01:51:48,307 Epoch  11: total training loss 19.85
2020-07-13 01:51:48,307 EPOCH 12
2020-07-13 01:51:52,024 Epoch  12: total training loss 17.55
2020-07-13 01:51:52,025 EPOCH 13
2020-07-13 01:51:55,858 Epoch  13: total training loss 17.02
2020-07-13 01:51:55,859 EPOCH 14
2020-07-13 01:51:59,682 Epoch  14: total training loss 16.44
2020-07-13 01:51:59,683 EPOCH 15
2020-07-13 01:52:01,935 Epoch  15 Step:  1360300 Batch Loss:     0.864211 Tokens per Sec:     7394, Lr: 0.000200
2020-07-13 01:52:03,490 Epoch  15: total training loss 14.71
2020-07-13 01:52:03,491 EPOCH 16
2020-07-13 01:52:07,218 Epoch  16: total training loss 13.15
2020-07-13 01:52:07,218 EPOCH 17
2020-07-13 01:52:11,010 Epoch  17: total training loss 12.88
2020-07-13 01:52:11,010 EPOCH 18
2020-07-13 01:52:14,853 Epoch  18: total training loss 12.32
2020-07-13 01:52:14,855 EPOCH 19
2020-07-13 01:52:18,687 Epoch  19: total training loss 12.24
2020-07-13 01:52:18,688 EPOCH 20
2020-07-13 01:52:20,297 Epoch  20 Step:  1360400 Batch Loss:     0.602203 Tokens per Sec:     6518, Lr: 0.000200
2020-07-13 01:52:22,495 Epoch  20: total training loss 10.93
2020-07-13 01:52:22,496 EPOCH 21
2020-07-13 01:52:26,230 Epoch  21: total training loss 9.99
2020-07-13 01:52:26,231 EPOCH 22
2020-07-13 01:52:29,992 Epoch  22: total training loss 9.50
2020-07-13 01:52:29,994 EPOCH 23
2020-07-13 01:52:33,798 Epoch  23: total training loss 8.86
2020-07-13 01:52:33,798 EPOCH 24
2020-07-13 01:52:37,617 Epoch  24: total training loss 8.56
2020-07-13 01:52:37,617 EPOCH 25
2020-07-13 01:52:38,386 Epoch  25 Step:  1360500 Batch Loss:     0.445861 Tokens per Sec:     7117, Lr: 0.000200
2020-07-13 01:53:00,958 Hooray! New best validation result [ppl]!
2020-07-13 01:53:00,958 Saving new checkpoint.
2020-07-13 01:53:02,097 Example #0
2020-07-13 01:53:02,097 	Raw source:     ['H@@', 'i', 'there', '!', 'How', 'can', 'I', 'help', '?']
2020-07-13 01:53:02,097 	Raw hypothesis: ['Wie', 'kann', 'ich', 'helfen', '?']
2020-07-13 01:53:02,097 	Source:     Hi there ! How can I help ?
2020-07-13 01:53:02,097 	Reference:  Hallo ! Wie kann ich helfen ?
2020-07-13 01:53:02,097 	Hypothesis: Wie kann ich helfen ?
2020-07-13 01:53:02,097 Example #1
2020-07-13 01:53:02,097 	Raw source:     ['H@@', 'ey', 'there', ',', 'I', 'need', 'to', 'take', 'my', 'car', 'to', 'mechan@@', 'ic', 'and', 'I', 'would', 'like', 'to', 'see', 'Intelli@@', 'gent', 'Auto', 'imports', '.']
2020-07-13 01:53:02,097 	Raw hypothesis: ['H@@', 'ey', ',', 'muss', 'ich', 'mein', 'Auto', 'zum', 'mechan@@', 'ischen', 'fahren', 'und', 'ich', 'würde', 'mir', 'auch', 'Intelli@@', 'gent', 'Auto', 'Im@@', 'porte', 'sehen', '.']
2020-07-13 01:53:02,097 	Source:     Hey there , I need to take my car to mechanic and I would like to see Intelligent Auto imports .
2020-07-13 01:53:02,097 	Reference:  Hey , ich muss mein Auto zum Mechaniker bringen und ich würde gerne Intelligent Auto Imports besuchen .
2020-07-13 01:53:02,098 	Hypothesis: Hey , muss ich mein Auto zum mechanischen fahren und ich würde mir auch Intelligent Auto Importe sehen .
2020-07-13 01:53:02,098 Example #2
2020-07-13 01:53:02,098 	Raw source:     ['S@@', 'ure', '!', 'what', 'type', 'of', 'car', 'is', 'it', '?']
2020-07-13 01:53:02,098 	Raw hypothesis: ['K@@', 'lar', ',', 'was', 'für', 'ein', 'Auto', 'ist', 'es', '?']
2020-07-13 01:53:02,098 	Source:     Sure ! what type of car is it ?
2020-07-13 01:53:02,098 	Reference:  Sicher ! Was für ein Auto ist das ?
2020-07-13 01:53:02,098 	Hypothesis: Klar , was für ein Auto ist es ?
2020-07-13 01:53:02,098 Example #3
2020-07-13 01:53:02,098 	Raw source:     ['about', '2011', 'Ne@@', 'as@@', 'on', 'Road', '.']
2020-07-13 01:53:02,098 	Raw hypothesis: ['über', '2011', 'Ne@@', 'as@@', 'on', 'Road']
2020-07-13 01:53:02,098 	Source:     about 2011 Neason Road .
2020-07-13 01:53:02,098 	Reference:  2011 Neason Road .
2020-07-13 01:53:02,098 	Hypothesis: über 2011 Neason Road
2020-07-13 01:53:02,098 Validation result (greedy) at epoch  25, step  1360500: bleu:  22.89, loss: 29520.6953, ppl:   5.4121, duration: 23.7111s
2020-07-13 01:53:05,053 Epoch  25: total training loss 7.80
2020-07-13 01:53:05,053 EPOCH 26
2020-07-13 01:53:08,728 Epoch  26: total training loss 7.34
2020-07-13 01:53:08,729 EPOCH 27
2020-07-13 01:53:12,429 Epoch  27: total training loss 7.01
2020-07-13 01:53:12,430 EPOCH 28
2020-07-13 01:53:16,185 Epoch  28: total training loss 7.02
2020-07-13 01:53:16,185 EPOCH 29
2020-07-13 01:53:19,990 Epoch  29: total training loss 6.74
2020-07-13 01:53:19,991 EPOCH 30
2020-07-13 01:53:20,361 Epoch  30 Step:  1360600 Batch Loss:     0.277998 Tokens per Sec:     7308, Lr: 0.000200
2020-07-13 01:53:23,765 Epoch  30: total training loss 6.49
2020-07-13 01:53:23,766 EPOCH 31
2020-07-13 01:53:27,558 Epoch  31: total training loss 6.30
2020-07-13 01:53:27,558 EPOCH 32
2020-07-13 01:53:31,252 Epoch  32: total training loss 5.73
2020-07-13 01:53:31,252 EPOCH 33
2020-07-13 01:53:34,936 Epoch  33: total training loss 5.56
2020-07-13 01:53:34,936 EPOCH 34
2020-07-13 01:53:38,648 Epoch  34 Step:  1360700 Batch Loss:     0.264221 Tokens per Sec:     7509, Lr: 0.000200
2020-07-13 01:53:38,649 Epoch  34: total training loss 5.30
2020-07-13 01:53:38,649 EPOCH 35
2020-07-13 01:53:42,445 Epoch  35: total training loss 5.45
2020-07-13 01:53:42,445 EPOCH 36
2020-07-13 01:53:46,239 Epoch  36: total training loss 5.27
2020-07-13 01:53:46,241 EPOCH 37
2020-07-13 01:53:50,072 Epoch  37: total training loss 5.13
2020-07-13 01:53:50,073 EPOCH 38
2020-07-13 01:53:53,736 Epoch  38: total training loss 4.78
2020-07-13 01:53:53,736 EPOCH 39
2020-07-13 01:53:56,727 Epoch  39 Step:  1360800 Batch Loss:     0.255117 Tokens per Sec:     7651, Lr: 0.000200
2020-07-13 01:53:57,543 Epoch  39: total training loss 4.76
2020-07-13 01:53:57,544 EPOCH 40
2020-07-13 01:54:01,320 Epoch  40: total training loss 4.65
2020-07-13 01:54:01,321 EPOCH 41
2020-07-13 01:54:05,083 Epoch  41: total training loss 4.65
2020-07-13 01:54:05,084 EPOCH 42
2020-07-13 01:54:08,886 Epoch  42: total training loss 4.53
2020-07-13 01:54:08,887 EPOCH 43
2020-07-13 01:54:12,660 Epoch  43: total training loss 6.33
2020-07-13 01:54:12,661 EPOCH 44
2020-07-13 01:54:14,797 Epoch  44 Step:  1360900 Batch Loss:     0.308766 Tokens per Sec:     7167, Lr: 0.000200
2020-07-13 01:54:16,434 Epoch  44: total training loss 6.27
2020-07-13 01:54:16,434 EPOCH 45
2020-07-13 01:54:20,238 Epoch  45: total training loss 5.48
2020-07-13 01:54:20,238 EPOCH 46
2020-07-13 01:54:24,030 Epoch  46: total training loss 4.74
2020-07-13 01:54:24,031 EPOCH 47
2020-07-13 01:54:27,815 Epoch  47: total training loss 4.34
2020-07-13 01:54:27,816 EPOCH 48
2020-07-13 01:54:31,501 Epoch  48: total training loss 3.93
2020-07-13 01:54:31,502 EPOCH 49
2020-07-13 01:54:33,027 Epoch  49 Step:  1361000 Batch Loss:     0.155916 Tokens per Sec:     7181, Lr: 0.000200
2020-07-13 01:54:35,291 Epoch  49: total training loss 3.95
2020-07-13 01:54:35,291 EPOCH 50
2020-07-13 01:54:39,071 Epoch  50: total training loss 3.76
2020-07-13 01:54:39,071 EPOCH 51
2020-07-13 01:54:42,861 Epoch  51: total training loss 3.66
2020-07-13 01:54:42,862 EPOCH 52
2020-07-13 01:54:46,653 Epoch  52: total training loss 3.62
2020-07-13 01:54:46,655 EPOCH 53
2020-07-13 01:54:50,472 Epoch  53: total training loss 3.51
2020-07-13 01:54:50,473 EPOCH 54
2020-07-13 01:54:50,957 Epoch  54 Step:  1361100 Batch Loss:     0.180602 Tokens per Sec:     6627, Lr: 0.000200
2020-07-13 01:54:54,283 Epoch  54: total training loss 3.42
2020-07-13 01:54:54,283 EPOCH 55
2020-07-13 01:54:58,089 Epoch  55: total training loss 3.35
2020-07-13 01:54:58,090 EPOCH 56
2020-07-13 01:55:01,902 Epoch  56: total training loss 3.26
2020-07-13 01:55:01,903 EPOCH 57
2020-07-13 01:55:05,728 Epoch  57: total training loss 3.23
2020-07-13 01:55:05,728 EPOCH 58
2020-07-13 01:55:09,209 Epoch  58 Step:  1361200 Batch Loss:     0.155870 Tokens per Sec:     7196, Lr: 0.000200
2020-07-13 01:55:09,578 Epoch  58: total training loss 3.16
2020-07-13 01:55:09,579 EPOCH 59
2020-07-13 01:55:13,391 Epoch  59: total training loss 3.07
2020-07-13 01:55:13,392 EPOCH 60
2020-07-13 01:55:17,191 Epoch  60: total training loss 3.03
2020-07-13 01:55:17,192 EPOCH 61
2020-07-13 01:55:40,795 Example #0
2020-07-13 01:55:40,796 	Raw source:     ['H@@', 'i', 'there', '!', 'How', 'can', 'I', 'help', '?']
2020-07-13 01:55:40,796 	Raw hypothesis: ['Wie', 'kann', 'ich', 'helfen', '?']
2020-07-13 01:55:40,796 	Source:     Hi there ! How can I help ?
2020-07-13 01:55:40,796 	Reference:  Hallo ! Wie kann ich helfen ?
2020-07-13 01:55:40,796 	Hypothesis: Wie kann ich helfen ?
2020-07-13 01:55:40,796 Example #1
2020-07-13 01:55:40,796 	Raw source:     ['H@@', 'ey', 'there', ',', 'I', 'need', 'to', 'take', 'my', 'car', 'to', 'mechan@@', 'ic', 'and', 'I', 'would', 'like', 'to', 'see', 'Intelli@@', 'gent', 'Auto', 'imports', '.']
2020-07-13 01:55:40,796 	Raw hypothesis: ['Und', 'da', 'muss', 'ich', 'mein', 'Auto', 'zum', 'mechan@@', 'ischen', 'Zustand', 'nehmen', ',', 'und', 'ich', 'würde', 'mich', 'auch', 'auf', 'die', 'Intelli@@', 'gent@@', 'er', 'Aut@@', 'ark@@', 'ie', 'aufmerksam', 'sehen', '.']
2020-07-13 01:55:40,796 	Source:     Hey there , I need to take my car to mechanic and I would like to see Intelligent Auto imports .
2020-07-13 01:55:40,796 	Reference:  Hey , ich muss mein Auto zum Mechaniker bringen und ich würde gerne Intelligent Auto Imports besuchen .
2020-07-13 01:55:40,796 	Hypothesis: Und da muss ich mein Auto zum mechanischen Zustand nehmen , und ich würde mich auch auf die Intelligenter Autarkie aufmerksam sehen .
2020-07-13 01:55:40,796 Example #2
2020-07-13 01:55:40,796 	Raw source:     ['S@@', 'ure', '!', 'what', 'type', 'of', 'car', 'is', 'it', '?']
2020-07-13 01:55:40,796 	Raw hypothesis: ['K@@', 'lar', ',', 'was', 'für', 'ein', 'Auto', 'ist', 'es', '?']
2020-07-13 01:55:40,796 	Source:     Sure ! what type of car is it ?
2020-07-13 01:55:40,796 	Reference:  Sicher ! Was für ein Auto ist das ?
2020-07-13 01:55:40,796 	Hypothesis: Klar , was für ein Auto ist es ?
2020-07-13 01:55:40,796 Example #3
2020-07-13 01:55:40,796 	Raw source:     ['about', '2011', 'Ne@@', 'as@@', 'on', 'Road', '.']
2020-07-13 01:55:40,796 	Raw hypothesis: ['über', '2011', 'Ne@@', 'as@@', 'on', 'Road', '.']
2020-07-13 01:55:40,796 	Source:     about 2011 Neason Road .
2020-07-13 01:55:40,797 	Reference:  2011 Neason Road .
2020-07-13 01:55:40,797 	Hypothesis: über 2011 Neason Road .
2020-07-13 01:55:40,797 Validation result (greedy) at epoch  61, step  1361250: bleu:  20.85, loss: 32315.1191, ppl:   6.3502, duration: 22.4841s
2020-07-13 01:55:43,474 Epoch  61: total training loss 2.97
2020-07-13 01:55:43,474 EPOCH 62
2020-07-13 01:55:47,261 Epoch  62: total training loss 2.93
2020-07-13 01:55:47,261 EPOCH 63
2020-07-13 01:55:49,803 Epoch  63 Step:  1361300 Batch Loss:     0.137730 Tokens per Sec:     7204, Lr: 0.000200
2020-07-13 01:55:51,054 Epoch  63: total training loss 2.92
2020-07-13 01:55:51,055 EPOCH 64
2020-07-13 01:55:54,809 Epoch  64: total training loss 2.82
2020-07-13 01:55:54,809 EPOCH 65
2020-07-13 01:55:58,511 Epoch  65: total training loss 2.70
2020-07-13 01:55:58,511 EPOCH 66
2020-07-13 01:56:02,195 Epoch  66: total training loss 2.67
2020-07-13 01:56:02,195 EPOCH 67
2020-07-13 01:56:05,971 Epoch  67: total training loss 2.72
2020-07-13 01:56:05,971 EPOCH 68
2020-07-13 01:56:07,958 Epoch  68 Step:  1361400 Batch Loss:     0.110596 Tokens per Sec:     7069, Lr: 0.000200
2020-07-13 01:56:09,735 Epoch  68: total training loss 2.72
2020-07-13 01:56:09,736 EPOCH 69
2020-07-13 01:56:13,506 Epoch  69: total training loss 2.64
2020-07-13 01:56:13,507 EPOCH 70
2020-07-13 01:56:17,157 Epoch  70: total training loss 2.52
2020-07-13 01:56:17,158 EPOCH 71
2020-07-13 01:56:20,955 Epoch  71: total training loss 2.59
2020-07-13 01:56:20,955 EPOCH 72
2020-07-13 01:56:24,719 Epoch  72: total training loss 2.59
2020-07-13 01:56:24,720 EPOCH 73
2020-07-13 01:56:25,969 Epoch  73 Step:  1361500 Batch Loss:     0.118735 Tokens per Sec:     7968, Lr: 0.000200
2020-07-13 01:56:28,507 Epoch  73: total training loss 2.53
2020-07-13 01:56:28,507 EPOCH 74
2020-07-13 01:56:32,169 Epoch  74: total training loss 2.38
2020-07-13 01:56:32,169 EPOCH 75
2020-07-13 01:56:35,947 Epoch  75: total training loss 2.48
2020-07-13 01:56:35,948 EPOCH 76
2020-07-13 01:56:39,743 Epoch  76: total training loss 2.45
2020-07-13 01:56:39,744 EPOCH 77
2020-07-13 01:56:43,410 Epoch  77: total training loss 2.35
2020-07-13 01:56:43,411 EPOCH 78
2020-07-13 01:56:44,224 Epoch  78 Step:  1361600 Batch Loss:     0.109143 Tokens per Sec:     6554, Lr: 0.000200
2020-07-13 01:56:47,111 Epoch  78: total training loss 2.28
2020-07-13 01:56:47,112 EPOCH 79
2020-07-13 01:56:50,831 Epoch  79: total training loss 2.26
2020-07-13 01:56:50,832 EPOCH 80
2020-07-13 01:56:54,638 Epoch  80: total training loss 2.33
2020-07-13 01:56:54,638 EPOCH 81
2020-07-13 01:56:58,413 Epoch  81: total training loss 2.29
2020-07-13 01:56:58,414 EPOCH 82
2020-07-13 01:57:02,189 Epoch  82: total training loss 2.29
2020-07-13 01:57:02,190 EPOCH 83
2020-07-13 01:57:02,437 Epoch  83 Step:  1361700 Batch Loss:     0.101755 Tokens per Sec:     4422, Lr: 0.000200
2020-07-13 01:57:05,968 Epoch  83: total training loss 2.26
2020-07-13 01:57:05,968 EPOCH 84
2020-07-13 01:57:09,761 Epoch  84: total training loss 2.33
2020-07-13 01:57:09,761 EPOCH 85
2020-07-13 01:57:13,433 Epoch  85: total training loss 2.17
2020-07-13 01:57:13,434 EPOCH 86
2020-07-13 01:57:17,246 Epoch  86: total training loss 2.22
2020-07-13 01:57:17,247 EPOCH 87
2020-07-13 01:57:20,531 Epoch  87 Step:  1361800 Batch Loss:     0.111822 Tokens per Sec:     7744, Lr: 0.000200
2020-07-13 01:57:20,910 Epoch  87: total training loss 2.06
2020-07-13 01:57:20,911 EPOCH 88
2020-07-13 01:57:24,734 Epoch  88: total training loss 2.16
2020-07-13 01:57:24,734 EPOCH 89
2020-07-13 01:57:28,451 Epoch  89: total training loss 2.04
2020-07-13 01:57:28,452 EPOCH 90
2020-07-13 01:57:32,116 Epoch  90: total training loss 2.00
2020-07-13 01:57:32,117 EPOCH 91
2020-07-13 01:57:35,942 Epoch  91: total training loss 2.09
2020-07-13 01:57:35,943 EPOCH 92
2020-07-13 01:57:38,826 Epoch  92 Step:  1361900 Batch Loss:     0.110918 Tokens per Sec:     7096, Lr: 0.000200
2020-07-13 01:57:39,714 Epoch  92: total training loss 2.05
2020-07-13 01:57:39,715 EPOCH 93
2020-07-13 01:57:43,473 Epoch  93: total training loss 2.04
2020-07-13 01:57:43,474 EPOCH 94
2020-07-13 01:57:47,280 Epoch  94: total training loss 2.14
2020-07-13 01:57:47,280 EPOCH 95
2020-07-13 01:57:50,945 Epoch  95: total training loss 1.98
2020-07-13 01:57:50,946 EPOCH 96
2020-07-13 01:57:54,640 Epoch  96: total training loss 1.94
2020-07-13 01:57:54,641 EPOCH 97
2020-07-13 01:57:57,054 Epoch  97 Step:  1362000 Batch Loss:     0.095821 Tokens per Sec:     6834, Lr: 0.000200
2020-07-13 01:58:19,045 Example #0
2020-07-13 01:58:19,045 	Raw source:     ['H@@', 'i', 'there', '!', 'How', 'can', 'I', 'help', '?']
2020-07-13 01:58:19,045 	Raw hypothesis: ['Wie', 'kann', 'ich', 'helfen', '?']
2020-07-13 01:58:19,045 	Source:     Hi there ! How can I help ?
2020-07-13 01:58:19,045 	Reference:  Hallo ! Wie kann ich helfen ?
2020-07-13 01:58:19,045 	Hypothesis: Wie kann ich helfen ?
2020-07-13 01:58:19,045 Example #1
2020-07-13 01:58:19,045 	Raw source:     ['H@@', 'ey', 'there', ',', 'I', 'need', 'to', 'take', 'my', 'car', 'to', 'mechan@@', 'ic', 'and', 'I', 'would', 'like', 'to', 'see', 'Intelli@@', 'gent', 'Auto', 'imports', '.']
2020-07-13 01:58:19,045 	Raw hypothesis: ['H@@', 'ey', ',', 'brau@@', 'che', 'ich', 'mein', 'Autos', 'zum', 'mechan@@', 'ischen', 'Zustand', 'zu', 'bringen', 'und', 'ich', 'würde', 'mir', 'eine', 'Intelli@@', 'gent@@', 'es', 'Aut@@', 'ark@@', 'ie', 'ansehen', '.']
2020-07-13 01:58:19,045 	Source:     Hey there , I need to take my car to mechanic and I would like to see Intelligent Auto imports .
2020-07-13 01:58:19,045 	Reference:  Hey , ich muss mein Auto zum Mechaniker bringen und ich würde gerne Intelligent Auto Imports besuchen .
2020-07-13 01:58:19,045 	Hypothesis: Hey , brauche ich mein Autos zum mechanischen Zustand zu bringen und ich würde mir eine Intelligentes Autarkie ansehen .
2020-07-13 01:58:19,045 Example #2
2020-07-13 01:58:19,045 	Raw source:     ['S@@', 'ure', '!', 'what', 'type', 'of', 'car', 'is', 'it', '?']
2020-07-13 01:58:19,045 	Raw hypothesis: ['Si@@', 'cher', ',', 'welche', 'Art', 'von', 'Autos', 'ist', 'es', '?']
2020-07-13 01:58:19,045 	Source:     Sure ! what type of car is it ?
2020-07-13 01:58:19,045 	Reference:  Sicher ! Was für ein Auto ist das ?
2020-07-13 01:58:19,046 	Hypothesis: Sicher , welche Art von Autos ist es ?
2020-07-13 01:58:19,046 Example #3
2020-07-13 01:58:19,046 	Raw source:     ['about', '2011', 'Ne@@', 'as@@', 'on', 'Road', '.']
2020-07-13 01:58:19,046 	Raw hypothesis: ['über', '2011', 'Ne@@', 'as@@', 'on', 'Street', '.']
2020-07-13 01:58:19,046 	Source:     about 2011 Neason Road .
2020-07-13 01:58:19,046 	Reference:  2011 Neason Road .
2020-07-13 01:58:19,046 	Hypothesis: über 2011 Neason Street .
2020-07-13 01:58:19,046 Validation result (greedy) at epoch  97, step  1362000: bleu:  19.12, loss: 35713.7578, ppl:   7.7128, duration: 21.9906s
2020-07-13 01:58:20,454 Epoch  97: total training loss 2.03
2020-07-13 01:58:20,454 EPOCH 98
2020-07-13 01:58:24,240 Epoch  98: total training loss 2.00
2020-07-13 01:58:24,240 EPOCH 99
2020-07-13 01:58:28,015 Epoch  99: total training loss 1.97
2020-07-13 01:58:28,016 EPOCH 100
2020-07-13 01:58:31,758 Epoch 100: total training loss 1.83
2020-07-13 01:58:31,759 Training ended after 100 epochs.
2020-07-13 01:58:31,759 Best validation result (greedy) at step  1360500:   5.41 ppl.
2020-07-13 01:59:26,703  dev bleu:  24.44 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-13 01:59:26,704 Translations saved to: models/wmt20/domainadaptation/main_tfm_b2048_ende_news_pretrained/01360500.hyps.dev
2020-07-13 01:59:26,706 test bleu:  -1.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-13 01:59:26,706 Translations saved to: models/wmt20/domainadaptation/main_tfm_b2048_ende_news_pretrained/01360500.hyps.test
