2020-06-22 04:23:51,573 Hello! This is Joey-NMT.
2020-06-22 04:24:03,807 Total params: 191296513
2020-06-22 04:24:03,809 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-22 04:24:06,202 cfg.name                           : transformer_multi_enc6x2_hid1024_ende
2020-06-22 04:24:06,202 cfg.data.src                       : en
2020-06-22 04:24:06,202 cfg.data.trg                       : de
2020-06-22 04:24:06,202 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-22 04:24:06,202 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-22 04:24:06,202 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-22 04:24:06,202 cfg.data.level                     : bpe
2020-06-22 04:24:06,202 cfg.data.lowercase                 : True
2020-06-22 04:24:06,202 cfg.data.max_sent_length           : 100
2020-06-22 04:24:06,202 cfg.testing.beam_size              : 5
2020-06-22 04:24:06,202 cfg.testing.alpha                  : 1.0
2020-06-22 04:24:06,202 cfg.training.random_seed           : 42
2020-06-22 04:24:06,202 cfg.training.optimizer             : adam
2020-06-22 04:24:06,203 cfg.training.normalization         : tokens
2020-06-22 04:24:06,203 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-22 04:24:06,203 cfg.training.scheduling            : plateau
2020-06-22 04:24:06,203 cfg.training.patience              : 8
2020-06-22 04:24:06,203 cfg.training.decrease_factor       : 0.7
2020-06-22 04:24:06,203 cfg.training.loss                  : crossentropy
2020-06-22 04:24:06,203 cfg.training.learning_rate         : 0.0002
2020-06-22 04:24:06,203 cfg.training.learning_rate_min     : 1e-08
2020-06-22 04:24:06,203 cfg.training.weight_decay          : 0.0
2020-06-22 04:24:06,203 cfg.training.label_smoothing       : 0.1
2020-06-22 04:24:06,203 cfg.training.batch_size            : 4096
2020-06-22 04:24:06,203 cfg.training.batch_type            : token
2020-06-22 04:24:06,203 cfg.training.eval_batch_size       : 3600
2020-06-22 04:24:06,203 cfg.training.eval_batch_type       : token
2020-06-22 04:24:06,203 cfg.training.batch_multiplier      : 1
2020-06-22 04:24:06,203 cfg.training.early_stopping_metric : ppl
2020-06-22 04:24:06,203 cfg.training.epochs                : 100
2020-06-22 04:24:06,203 cfg.training.validation_freq       : 1000
2020-06-22 04:24:06,203 cfg.training.logging_freq          : 100
2020-06-22 04:24:06,203 cfg.training.eval_metric           : bleu
2020-06-22 04:24:06,203 cfg.training.model_dir             : models/transformer_multi_enc6x2_hid1024_ende
2020-06-22 04:24:06,203 cfg.training.overwrite             : True
2020-06-22 04:24:06,203 cfg.training.shuffle               : True
2020-06-22 04:24:06,203 cfg.training.use_cuda              : True
2020-06-22 04:24:06,203 cfg.training.max_output_length     : 100
2020-06-22 04:24:06,203 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-22 04:24:06,203 cfg.training.keep_last_ckpts       : 3
2020-06-22 04:24:06,203 cfg.model.initializer              : xavier
2020-06-22 04:24:06,203 cfg.model.bias_initializer         : zeros
2020-06-22 04:24:06,203 cfg.model.init_gain                : 1.0
2020-06-22 04:24:06,203 cfg.model.embed_initializer        : xavier
2020-06-22 04:24:06,203 cfg.model.embed_init_gain          : 1.0
2020-06-22 04:24:06,203 cfg.model.tied_embeddings          : False
2020-06-22 04:24:06,203 cfg.model.tied_softmax             : True
2020-06-22 04:24:06,203 cfg.model.encoder.type             : transformer
2020-06-22 04:24:06,203 cfg.model.encoder.num_layers       : 6
2020-06-22 04:24:06,203 cfg.model.encoder.num_heads        : 8
2020-06-22 04:24:06,203 cfg.model.encoder.embeddings.embedding_dim : 1024
2020-06-22 04:24:06,203 cfg.model.encoder.embeddings.scale : True
2020-06-22 04:24:06,203 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-22 04:24:06,204 cfg.model.encoder.hidden_size      : 1024
2020-06-22 04:24:06,204 cfg.model.encoder.ff_size          : 1024
2020-06-22 04:24:06,204 cfg.model.encoder.dropout          : 0.1
2020-06-22 04:24:06,204 cfg.model.encoder.freeze           : False
2020-06-22 04:24:06,204 cfg.model.encoder.multi_encoder    : True
2020-06-22 04:24:06,204 cfg.model.decoder.type             : transformer
2020-06-22 04:24:06,204 cfg.model.decoder.num_layers       : 6
2020-06-22 04:24:06,204 cfg.model.decoder.num_heads        : 8
2020-06-22 04:24:06,204 cfg.model.decoder.embeddings.embedding_dim : 1024
2020-06-22 04:24:06,204 cfg.model.decoder.embeddings.scale : True
2020-06-22 04:24:06,204 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-22 04:24:06,204 cfg.model.decoder.hidden_size      : 1024
2020-06-22 04:24:06,204 cfg.model.decoder.ff_size          : 4096
2020-06-22 04:24:06,204 cfg.model.decoder.dropout          : 0.1
2020-06-22 04:24:06,204 cfg.model.decoder.freeze           : False
2020-06-22 04:24:06,204 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-22 04:24:06,204 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-22 04:24:06,204 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-22 04:24:06,204 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-22 04:24:06,204 Number of Src words (types): 4561
2020-06-22 04:24:06,204 Number of Trg words (types): 5876
2020-06-22 04:24:06,204 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=1024, vocab_size=4561),
	trg_embed=Embeddings(embedding_dim=1024, vocab_size=5876))
2020-06-22 04:24:06,215 EPOCH 1
2020-06-22 04:24:41,477 Epoch   1: total training loss 300.37
2020-06-22 04:24:41,477 EPOCH 2
2020-06-22 04:25:10,327 Epoch   2 Step:      100 Batch Loss:     4.458270 Tokens per Sec:     3675, Lr: 0.000200
2020-06-22 04:25:16,716 Epoch   2: total training loss 265.79
2020-06-22 04:25:16,716 EPOCH 3
2020-06-22 04:25:53,051 Epoch   3: total training loss 242.91
2020-06-22 04:25:53,052 EPOCH 4
2020-06-22 04:26:17,187 Epoch   4 Step:      200 Batch Loss:     1.981594 Tokens per Sec:     3365, Lr: 0.000200
2020-06-22 04:26:30,997 Epoch   4: total training loss 220.25
2020-06-22 04:26:30,998 EPOCH 5
2020-06-22 04:27:09,343 Epoch   5: total training loss 195.26
2020-06-22 04:27:09,344 EPOCH 6
2020-06-22 04:27:26,458 Epoch   6 Step:      300 Batch Loss:     4.874335 Tokens per Sec:     3484, Lr: 0.000200
2020-06-22 04:27:47,351 Epoch   6: total training loss 180.06
2020-06-22 04:27:47,351 EPOCH 7
2020-06-22 04:28:25,374 Epoch   7: total training loss 156.09
2020-06-22 04:28:25,375 EPOCH 8
2020-06-22 04:28:36,336 Epoch   8 Step:      400 Batch Loss:     2.719233 Tokens per Sec:     3378, Lr: 0.000200
2020-06-22 04:29:03,010 Epoch   8: total training loss 143.31
2020-06-22 04:29:03,011 EPOCH 9
2020-06-22 04:29:40,828 Epoch   9: total training loss 132.01
2020-06-22 04:29:40,829 EPOCH 10
2020-06-22 04:29:45,075 Epoch  10 Step:      500 Batch Loss:     2.588534 Tokens per Sec:     4292, Lr: 0.000200
2020-06-22 04:30:19,024 Epoch  10: total training loss 117.76
2020-06-22 04:30:19,025 EPOCH 11
2020-06-22 04:30:55,061 Epoch  11 Step:      600 Batch Loss:     1.455314 Tokens per Sec:     3426, Lr: 0.000200
2020-06-22 04:30:56,904 Epoch  11: total training loss 115.54
2020-06-22 04:30:56,904 EPOCH 12
2020-06-22 04:31:34,485 Epoch  12: total training loss 99.54
2020-06-22 04:31:34,486 EPOCH 13
2020-06-22 04:32:03,779 Epoch  13 Step:      700 Batch Loss:     1.566931 Tokens per Sec:     3549, Lr: 0.000200
2020-06-22 04:32:12,216 Epoch  13: total training loss 88.91
2020-06-22 04:32:12,217 EPOCH 14
2020-06-22 04:32:49,884 Epoch  14: total training loss 84.16
2020-06-22 04:32:49,885 EPOCH 15
2020-06-22 04:33:11,475 Epoch  15 Step:      800 Batch Loss:     1.491505 Tokens per Sec:     3591, Lr: 0.000200
2020-06-22 04:33:28,198 Epoch  15: total training loss 70.69
2020-06-22 04:33:28,198 EPOCH 16
2020-06-22 04:34:05,980 Epoch  16: total training loss 63.94
2020-06-22 04:34:05,980 EPOCH 17
2020-06-22 04:34:21,721 Epoch  17 Step:      900 Batch Loss:     0.983314 Tokens per Sec:     3332, Lr: 0.000200
2020-06-22 04:34:43,744 Epoch  17: total training loss 56.40
2020-06-22 04:34:43,744 EPOCH 18
2020-06-22 04:35:21,750 Epoch  18: total training loss 50.66
2020-06-22 04:35:21,751 EPOCH 19
2020-06-22 04:35:30,842 Epoch  19 Step:     1000 Batch Loss:     0.871720 Tokens per Sec:     3165, Lr: 0.000200
2020-06-22 04:36:13,741 Hooray! New best validation result [ppl]!
2020-06-22 04:36:13,744 Saving new checkpoint.
2020-06-22 04:36:40,627 Example #0
2020-06-22 04:36:40,627 	Raw source:     ['hello', '.']
2020-06-22 04:36:40,627 	Raw hypothesis: ['hallo', '.']
2020-06-22 04:36:40,628 	Source:     hello .
2020-06-22 04:36:40,628 	Reference:  hallo ,
2020-06-22 04:36:40,628 	Hypothesis: hallo .
2020-06-22 04:36:40,628 Example #1
2020-06-22 04:36:40,628 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 04:36:40,628 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 04:36:40,628 	Source:     hi , how can i help you ?
2020-06-22 04:36:40,628 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 04:36:40,628 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 04:36:40,628 Example #2
2020-06-22 04:36:40,628 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 04:36:40,628 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 04:36:40,628 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 04:36:40,628 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 04:36:40,628 	Hypothesis: hallo , ich suche ein restaurant in san francisco , kalifornien .
2020-06-22 04:36:40,628 Example #3
2020-06-22 04:36:40,628 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 04:36:40,628 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 04:36:40,628 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 04:36:40,628 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 04:36:40,628 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 04:36:40,628 Validation result (greedy) at epoch  19, step     1000: bleu:  29.06, loss: 49628.8594, ppl:  10.5168, duration: 69.7854s
2020-06-22 04:37:07,757 Epoch  19: total training loss 45.43
2020-06-22 04:37:07,757 EPOCH 20
2020-06-22 04:37:46,259 Epoch  20: total training loss 37.60
2020-06-22 04:37:46,260 EPOCH 21
2020-06-22 04:37:47,707 Epoch  21 Step:     1100 Batch Loss:     0.704937 Tokens per Sec:     3843, Lr: 0.000200
2020-06-22 04:38:25,517 Epoch  21: total training loss 33.39
2020-06-22 04:38:25,518 EPOCH 22
2020-06-22 04:38:58,532 Epoch  22 Step:     1200 Batch Loss:     0.399927 Tokens per Sec:     3445, Lr: 0.000200
2020-06-22 04:39:03,617 Epoch  22: total training loss 27.46
2020-06-22 04:39:03,618 EPOCH 23
2020-06-22 04:39:41,777 Epoch  23: total training loss 23.53
2020-06-22 04:39:41,778 EPOCH 24
2020-06-22 04:40:08,988 Epoch  24 Step:     1300 Batch Loss:     0.354190 Tokens per Sec:     3410, Lr: 0.000200
2020-06-22 04:40:20,582 Epoch  24: total training loss 21.26
2020-06-22 04:40:20,582 EPOCH 25
2020-06-22 04:40:58,876 Epoch  25: total training loss 18.77
2020-06-22 04:40:58,877 EPOCH 26
2020-06-22 04:41:17,858 Epoch  26 Step:     1400 Batch Loss:     0.292019 Tokens per Sec:     3509, Lr: 0.000200
2020-06-22 04:41:37,172 Epoch  26: total training loss 18.97
2020-06-22 04:41:37,173 EPOCH 27
2020-06-22 04:42:15,415 Epoch  27: total training loss 16.41
2020-06-22 04:42:15,415 EPOCH 28
2020-06-22 04:42:28,216 Epoch  28 Step:     1500 Batch Loss:     0.328239 Tokens per Sec:     3307, Lr: 0.000200
2020-06-22 04:42:53,567 Epoch  28: total training loss 14.08
2020-06-22 04:42:53,568 EPOCH 29
2020-06-22 04:43:31,607 Epoch  29: total training loss 13.56
2020-06-22 04:43:31,608 EPOCH 30
2020-06-22 04:43:36,628 Epoch  30 Step:     1600 Batch Loss:     0.229206 Tokens per Sec:     3956, Lr: 0.000200
2020-06-22 04:44:10,062 Epoch  30: total training loss 12.79
2020-06-22 04:44:10,062 EPOCH 31
2020-06-22 04:44:46,046 Epoch  31 Step:     1700 Batch Loss:     0.222362 Tokens per Sec:     3432, Lr: 0.000200
2020-06-22 04:44:48,116 Epoch  31: total training loss 11.71
2020-06-22 04:44:48,117 EPOCH 32
2020-06-22 04:45:25,740 Epoch  32: total training loss 11.05
2020-06-22 04:45:25,740 EPOCH 33
2020-06-22 04:45:54,248 Epoch  33 Step:     1800 Batch Loss:     0.210776 Tokens per Sec:     3391, Lr: 0.000200
2020-06-22 04:46:03,669 Epoch  33: total training loss 10.79
2020-06-22 04:46:03,670 EPOCH 34
2020-06-22 04:46:41,494 Epoch  34: total training loss 10.20
2020-06-22 04:46:41,495 EPOCH 35
2020-06-22 04:47:02,637 Epoch  35 Step:     1900 Batch Loss:     0.155407 Tokens per Sec:     3479, Lr: 0.000200
2020-06-22 04:47:19,731 Epoch  35: total training loss 9.19
2020-06-22 04:47:19,732 EPOCH 36
2020-06-22 04:47:57,534 Epoch  36: total training loss 9.09
2020-06-22 04:47:57,535 EPOCH 37
2020-06-22 04:48:10,773 Epoch  37 Step:     2000 Batch Loss:     0.164035 Tokens per Sec:     3588, Lr: 0.000200
2020-06-22 04:49:06,961 Example #0
2020-06-22 04:49:06,962 	Raw source:     ['hello', '.']
2020-06-22 04:49:06,962 	Raw hypothesis: ['hallo', '.']
2020-06-22 04:49:06,962 	Source:     hello .
2020-06-22 04:49:06,962 	Reference:  hallo ,
2020-06-22 04:49:06,962 	Hypothesis: hallo .
2020-06-22 04:49:06,962 Example #1
2020-06-22 04:49:06,962 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 04:49:06,962 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 04:49:06,962 	Source:     hi , how can i help you ?
2020-06-22 04:49:06,962 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 04:49:06,962 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 04:49:06,962 Example #2
2020-06-22 04:49:06,962 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 04:49:06,962 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', '.']
2020-06-22 04:49:06,962 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 04:49:06,962 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 04:49:06,962 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall .
2020-06-22 04:49:06,962 Example #3
2020-06-22 04:49:06,962 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 04:49:06,962 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 04:49:06,962 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 04:49:06,962 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 04:49:06,962 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 04:49:06,962 Validation result (greedy) at epoch  37, step     2000: bleu:  33.48, loss: 50395.1133, ppl:  10.9059, duration: 56.1884s
2020-06-22 04:49:31,654 Epoch  37: total training loss 9.28
2020-06-22 04:49:31,654 EPOCH 38
2020-06-22 04:50:09,630 Epoch  38: total training loss 8.25
2020-06-22 04:50:09,631 EPOCH 39
2020-06-22 04:50:15,978 Epoch  39 Step:     2100 Batch Loss:     0.156593 Tokens per Sec:     3739, Lr: 0.000200
2020-06-22 04:50:46,822 Epoch  39: total training loss 8.50
2020-06-22 04:50:46,823 EPOCH 40
2020-06-22 04:51:24,483 Epoch  40: total training loss 7.57
2020-06-22 04:51:24,484 EPOCH 41
2020-06-22 04:51:25,175 Epoch  41 Step:     2200 Batch Loss:     0.121494 Tokens per Sec:     3382, Lr: 0.000200
2020-06-22 04:52:02,168 Epoch  41: total training loss 7.13
2020-06-22 04:52:02,169 EPOCH 42
2020-06-22 04:52:34,882 Epoch  42 Step:     2300 Batch Loss:     0.132068 Tokens per Sec:     3403, Lr: 0.000200
2020-06-22 04:52:40,287 Epoch  42: total training loss 7.09
2020-06-22 04:52:40,287 EPOCH 43
2020-06-22 04:53:17,827 Epoch  43: total training loss 7.36
2020-06-22 04:53:17,827 EPOCH 44
2020-06-22 04:53:41,124 Epoch  44 Step:     2400 Batch Loss:     0.126387 Tokens per Sec:     3776, Lr: 0.000200
2020-06-22 04:53:55,295 Epoch  44: total training loss 7.02
2020-06-22 04:53:55,296 EPOCH 45
2020-06-22 04:54:32,348 Epoch  45: total training loss 7.09
2020-06-22 04:54:32,349 EPOCH 46
2020-06-22 04:54:49,548 Epoch  46 Step:     2500 Batch Loss:     0.115583 Tokens per Sec:     3488, Lr: 0.000200
2020-06-22 04:55:09,706 Epoch  46: total training loss 6.91
2020-06-22 04:55:09,707 EPOCH 47
2020-06-22 04:55:47,106 Epoch  47: total training loss 6.75
2020-06-22 04:55:47,107 EPOCH 48
2020-06-22 04:55:58,105 Epoch  48 Step:     2600 Batch Loss:     0.127439 Tokens per Sec:     3296, Lr: 0.000200
2020-06-22 04:56:24,937 Epoch  48: total training loss 6.76
2020-06-22 04:56:24,938 EPOCH 49
2020-06-22 04:57:02,530 Epoch  49: total training loss 6.68
2020-06-22 04:57:02,531 EPOCH 50
2020-06-22 04:57:06,549 Epoch  50 Step:     2700 Batch Loss:     0.113550 Tokens per Sec:     2662, Lr: 0.000200
2020-06-22 04:57:39,770 Epoch  50: total training loss 6.43
2020-06-22 04:57:39,770 EPOCH 51
2020-06-22 04:58:13,426 Epoch  51 Step:     2800 Batch Loss:     0.130287 Tokens per Sec:     3460, Lr: 0.000200
2020-06-22 04:58:17,563 Epoch  51: total training loss 6.24
2020-06-22 04:58:17,564 EPOCH 52
2020-06-22 04:58:55,641 Epoch  52: total training loss 7.13
2020-06-22 04:58:55,642 EPOCH 53
2020-06-22 04:59:21,493 Epoch  53 Step:     2900 Batch Loss:     0.129486 Tokens per Sec:     3494, Lr: 0.000200
2020-06-22 04:59:33,977 Epoch  53: total training loss 7.99
2020-06-22 04:59:33,978 EPOCH 54
2020-06-22 05:00:11,606 Epoch  54: total training loss 8.07
2020-06-22 05:00:11,611 EPOCH 55
2020-06-22 05:00:31,517 Epoch  55 Step:     3000 Batch Loss:     0.161891 Tokens per Sec:     3365, Lr: 0.000200
2020-06-22 05:01:35,572 Example #0
2020-06-22 05:01:35,573 	Raw source:     ['hello', '.']
2020-06-22 05:01:35,573 	Raw hypothesis: ['hallo', '.']
2020-06-22 05:01:35,573 	Source:     hello .
2020-06-22 05:01:35,573 	Reference:  hallo ,
2020-06-22 05:01:35,573 	Hypothesis: hallo .
2020-06-22 05:01:35,573 Example #1
2020-06-22 05:01:35,573 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 05:01:35,573 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 05:01:35,573 	Source:     hi , how can i help you ?
2020-06-22 05:01:35,573 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 05:01:35,573 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 05:01:35,573 Example #2
2020-06-22 05:01:35,573 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 05:01:35,573 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 05:01:35,573 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 05:01:35,573 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 05:01:35,573 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 05:01:35,573 Example #3
2020-06-22 05:01:35,573 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 05:01:35,573 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 05:01:35,573 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 05:01:35,573 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 05:01:35,573 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 05:01:35,573 Validation result (greedy) at epoch  55, step     3000: bleu:  34.07, loss: 50863.5234, ppl:  11.1508, duration: 64.0549s
2020-06-22 05:01:53,714 Epoch  55: total training loss 7.28
2020-06-22 05:01:53,714 EPOCH 56
2020-06-22 05:02:31,520 Epoch  56: total training loss 6.50
2020-06-22 05:02:31,521 EPOCH 57
2020-06-22 05:02:45,021 Epoch  57 Step:     3100 Batch Loss:     0.125217 Tokens per Sec:     3243, Lr: 0.000200
2020-06-22 05:03:09,647 Epoch  57: total training loss 6.44
2020-06-22 05:03:09,648 EPOCH 58
2020-06-22 05:03:47,431 Epoch  58: total training loss 7.25
2020-06-22 05:03:47,432 EPOCH 59
2020-06-22 05:03:52,233 Epoch  59 Step:     3200 Batch Loss:     0.095967 Tokens per Sec:     3783, Lr: 0.000200
2020-06-22 05:04:24,955 Epoch  59: total training loss 6.25
2020-06-22 05:04:24,956 EPOCH 60
2020-06-22 05:04:59,956 Epoch  60 Step:     3300 Batch Loss:     0.129352 Tokens per Sec:     3463, Lr: 0.000200
2020-06-22 05:05:02,288 Epoch  60: total training loss 6.32
2020-06-22 05:05:02,289 EPOCH 61
2020-06-22 05:05:40,069 Epoch  61: total training loss 5.73
2020-06-22 05:05:40,070 EPOCH 62
2020-06-22 05:06:07,916 Epoch  62 Step:     3400 Batch Loss:     0.091474 Tokens per Sec:     3582, Lr: 0.000200
2020-06-22 05:06:17,602 Epoch  62: total training loss 5.46
2020-06-22 05:06:17,603 EPOCH 63
2020-06-22 05:06:55,825 Epoch  63: total training loss 5.31
2020-06-22 05:06:55,826 EPOCH 64
2020-06-22 05:07:17,759 Epoch  64 Step:     3500 Batch Loss:     0.096960 Tokens per Sec:     3470, Lr: 0.000200
2020-06-22 05:07:33,586 Epoch  64: total training loss 5.21
2020-06-22 05:07:33,587 EPOCH 65
2020-06-22 05:08:11,660 Epoch  65: total training loss 5.39
2020-06-22 05:08:11,661 EPOCH 66
2020-06-22 05:08:27,096 Epoch  66 Step:     3600 Batch Loss:     0.085569 Tokens per Sec:     3413, Lr: 0.000200
2020-06-22 05:08:49,501 Epoch  66: total training loss 5.01
2020-06-22 05:08:49,501 EPOCH 67
2020-06-22 05:09:26,882 Epoch  67: total training loss 5.18
2020-06-22 05:09:26,883 EPOCH 68
2020-06-22 05:09:34,824 Epoch  68 Step:     3700 Batch Loss:     0.100838 Tokens per Sec:     3282, Lr: 0.000200
2020-06-22 05:10:04,688 Epoch  68: total training loss 5.94
2020-06-22 05:10:04,688 EPOCH 69
2020-06-22 05:10:42,247 Epoch  69: total training loss 5.23
2020-06-22 05:10:42,248 EPOCH 70
2020-06-22 05:10:43,328 Epoch  70 Step:     3800 Batch Loss:     0.084152 Tokens per Sec:     5260, Lr: 0.000200
2020-06-22 05:11:19,983 Epoch  70: total training loss 4.97
2020-06-22 05:11:19,984 EPOCH 71
2020-06-22 05:11:52,768 Epoch  71 Step:     3900 Batch Loss:     0.092588 Tokens per Sec:     3442, Lr: 0.000200
2020-06-22 05:11:57,854 Epoch  71: total training loss 4.92
2020-06-22 05:11:57,854 EPOCH 72
2020-06-22 05:12:35,982 Epoch  72: total training loss 4.74
2020-06-22 05:12:35,983 EPOCH 73
2020-06-22 05:13:02,368 Epoch  73 Step:     4000 Batch Loss:     0.090241 Tokens per Sec:     3402, Lr: 0.000200
2020-06-22 05:14:08,901 Example #0
2020-06-22 05:14:08,902 	Raw source:     ['hello', '.']
2020-06-22 05:14:08,902 	Raw hypothesis: ['hallo', '.']
2020-06-22 05:14:08,902 	Source:     hello .
2020-06-22 05:14:08,902 	Reference:  hallo ,
2020-06-22 05:14:08,902 	Hypothesis: hallo .
2020-06-22 05:14:08,902 Example #1
2020-06-22 05:14:08,902 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 05:14:08,902 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 05:14:08,902 	Source:     hi , how can i help you ?
2020-06-22 05:14:08,902 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 05:14:08,902 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 05:14:08,902 Example #2
2020-06-22 05:14:08,902 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 05:14:08,902 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 05:14:08,902 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 05:14:08,902 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 05:14:08,902 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 05:14:08,902 Example #3
2020-06-22 05:14:08,902 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 05:14:08,902 	Raw hypothesis: ['okay', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 05:14:08,902 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 05:14:08,902 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 05:14:08,903 	Hypothesis: okay , nach welcher art von restaurant suchen sie ?
2020-06-22 05:14:08,903 Validation result (greedy) at epoch  73, step     4000: bleu:  33.96, loss: 51246.8203, ppl:  11.3553, duration: 66.5335s
2020-06-22 05:14:20,769 Epoch  73: total training loss 5.12
2020-06-22 05:14:20,770 EPOCH 74
2020-06-22 05:14:58,354 Epoch  74: total training loss 8.93
2020-06-22 05:14:58,355 EPOCH 75
2020-06-22 05:15:17,200 Epoch  75 Step:     4100 Batch Loss:     0.106489 Tokens per Sec:     3497, Lr: 0.000200
2020-06-22 05:15:36,278 Epoch  75: total training loss 6.49
2020-06-22 05:15:36,279 EPOCH 76
2020-06-22 05:16:13,928 Epoch  76: total training loss 6.46
2020-06-22 05:16:13,929 EPOCH 77
2020-06-22 05:16:26,621 Epoch  77 Step:     4200 Batch Loss:     0.109226 Tokens per Sec:     3352, Lr: 0.000200
2020-06-22 05:16:51,805 Epoch  77: total training loss 5.78
2020-06-22 05:16:51,806 EPOCH 78
2020-06-22 05:17:29,595 Epoch  78: total training loss 5.03
2020-06-22 05:17:29,596 EPOCH 79
2020-06-22 05:17:35,591 Epoch  79 Step:     4300 Batch Loss:     0.082055 Tokens per Sec:     3740, Lr: 0.000200
2020-06-22 05:18:07,414 Epoch  79: total training loss 4.96
2020-06-22 05:18:07,415 EPOCH 80
2020-06-22 05:18:44,874 Epoch  80 Step:     4400 Batch Loss:     0.141032 Tokens per Sec:     3410, Lr: 0.000200
2020-06-22 05:18:45,489 Epoch  80: total training loss 6.41
2020-06-22 05:18:45,489 EPOCH 81
2020-06-22 05:19:23,258 Epoch  81: total training loss 6.43
2020-06-22 05:19:23,259 EPOCH 82
2020-06-22 05:19:53,902 Epoch  82 Step:     4500 Batch Loss:     0.114289 Tokens per Sec:     3438, Lr: 0.000200
2020-06-22 05:20:01,535 Epoch  82: total training loss 5.70
2020-06-22 05:20:01,536 EPOCH 83
2020-06-22 05:20:39,369 Epoch  83: total training loss 5.95
2020-06-22 05:20:39,369 EPOCH 84
2020-06-22 05:21:02,847 Epoch  84 Step:     4600 Batch Loss:     0.083416 Tokens per Sec:     3517, Lr: 0.000200
2020-06-22 05:21:17,206 Epoch  84: total training loss 4.84
2020-06-22 05:21:17,207 EPOCH 85
2020-06-22 05:21:54,851 Epoch  85: total training loss 4.59
2020-06-22 05:21:54,852 EPOCH 86
2020-06-22 05:22:11,260 Epoch  86 Step:     4700 Batch Loss:     0.074613 Tokens per Sec:     3399, Lr: 0.000200
2020-06-22 05:22:32,968 Epoch  86: total training loss 4.49
2020-06-22 05:22:32,968 EPOCH 87
2020-06-22 05:23:10,878 Epoch  87: total training loss 4.48
2020-06-22 05:23:10,879 EPOCH 88
2020-06-22 05:23:19,572 Epoch  88 Step:     4800 Batch Loss:     0.080848 Tokens per Sec:     3529, Lr: 0.000200
2020-06-22 05:23:48,532 Epoch  88: total training loss 4.67
2020-06-22 05:23:48,534 EPOCH 89
2020-06-22 05:24:26,487 Epoch  89: total training loss 4.46
2020-06-22 05:24:26,488 EPOCH 90
2020-06-22 05:24:28,927 Epoch  90 Step:     4900 Batch Loss:     0.076167 Tokens per Sec:     2954, Lr: 0.000200
2020-06-22 05:25:04,778 Epoch  90: total training loss 4.41
2020-06-22 05:25:04,779 EPOCH 91
2020-06-22 05:25:36,930 Epoch  91 Step:     5000 Batch Loss:     0.065612 Tokens per Sec:     3490, Lr: 0.000200
2020-06-22 05:27:03,386 Example #0
2020-06-22 05:27:03,387 	Raw source:     ['hello', '.']
2020-06-22 05:27:03,387 	Raw hypothesis: ['hallo', '.']
2020-06-22 05:27:03,387 	Source:     hello .
2020-06-22 05:27:03,387 	Reference:  hallo ,
2020-06-22 05:27:03,387 	Hypothesis: hallo .
2020-06-22 05:27:03,387 Example #1
2020-06-22 05:27:03,387 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 05:27:03,387 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 05:27:03,387 	Source:     hi , how can i help you ?
2020-06-22 05:27:03,387 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 05:27:03,387 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 05:27:03,387 Example #2
2020-06-22 05:27:03,387 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 05:27:03,387 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 05:27:03,387 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 05:27:03,387 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 05:27:03,387 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 05:27:03,387 Example #3
2020-06-22 05:27:03,387 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 05:27:03,387 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 05:27:03,387 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 05:27:03,387 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 05:27:03,387 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 05:27:03,387 Validation result (greedy) at epoch  91, step     5000: bleu:  33.32, loss: 51288.0039, ppl:  11.3774, duration: 86.4560s
2020-06-22 05:27:09,269 Epoch  91: total training loss 4.62
2020-06-22 05:27:09,269 EPOCH 92
2020-06-22 05:27:47,016 Epoch  92: total training loss 4.46
2020-06-22 05:27:47,017 EPOCH 93
2020-06-22 05:28:10,544 Epoch  93 Step:     5100 Batch Loss:     0.067897 Tokens per Sec:     3687, Lr: 0.000200
2020-06-22 05:28:24,624 Epoch  93: total training loss 4.54
2020-06-22 05:28:24,625 EPOCH 94
2020-06-22 05:29:02,182 Epoch  94: total training loss 4.16
2020-06-22 05:29:02,182 EPOCH 95
2020-06-22 05:29:20,714 Epoch  95 Step:     5200 Batch Loss:     0.057438 Tokens per Sec:     3364, Lr: 0.000200
2020-06-22 05:29:40,027 Epoch  95: total training loss 4.06
2020-06-22 05:29:40,028 EPOCH 96
2020-06-22 05:30:17,645 Epoch  96: total training loss 4.18
2020-06-22 05:30:17,646 EPOCH 97
2020-06-22 05:30:28,702 Epoch  97 Step:     5300 Batch Loss:     0.063447 Tokens per Sec:     3636, Lr: 0.000200
2020-06-22 05:30:55,573 Epoch  97: total training loss 4.13
2020-06-22 05:30:55,574 EPOCH 98
2020-06-22 05:31:33,260 Epoch  98: total training loss 4.06
2020-06-22 05:31:33,261 EPOCH 99
2020-06-22 05:31:36,917 Epoch  99 Step:     5400 Batch Loss:     0.069916 Tokens per Sec:     3678, Lr: 0.000200
2020-06-22 05:32:11,113 Epoch  99: total training loss 3.88
2020-06-22 05:32:11,114 EPOCH 100
2020-06-22 05:32:47,455 Epoch 100 Step:     5500 Batch Loss:     0.127940 Tokens per Sec:     3453, Lr: 0.000200
2020-06-22 05:32:48,994 Epoch 100: total training loss 3.86
2020-06-22 05:32:48,995 Training ended after 100 epochs.
2020-06-22 05:32:48,995 Best validation result (greedy) at step     1000:  10.52 ppl.
2020-06-22 05:33:22,383  dev bleu:  29.87 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 05:33:22,387 Translations saved to: models/transformer_multi_enc6x2_hid1024_ende/00001000.hyps.dev
2020-06-22 05:33:46,698 test bleu:  27.29 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 05:33:46,703 Translations saved to: models/transformer_multi_enc6x2_hid1024_ende/00001000.hyps.test
