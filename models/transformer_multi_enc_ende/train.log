2020-06-22 02:55:25,153 Hello! This is Joey-NMT.
2020-06-22 02:55:40,930 Total params: 50537985
2020-06-22 02:55:40,933 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-22 02:55:48,785 cfg.name                           : transformer_multi_enc_head12_ende
2020-06-22 02:55:48,785 cfg.data.src                       : en
2020-06-22 02:55:48,785 cfg.data.trg                       : de
2020-06-22 02:55:48,785 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-22 02:55:48,785 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-22 02:55:48,785 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-22 02:55:48,785 cfg.data.level                     : bpe
2020-06-22 02:55:48,785 cfg.data.lowercase                 : True
2020-06-22 02:55:48,785 cfg.data.max_sent_length           : 100
2020-06-22 02:55:48,785 cfg.testing.beam_size              : 5
2020-06-22 02:55:48,785 cfg.testing.alpha                  : 1.0
2020-06-22 02:55:48,785 cfg.training.random_seed           : 42
2020-06-22 02:55:48,785 cfg.training.optimizer             : adam
2020-06-22 02:55:48,786 cfg.training.normalization         : tokens
2020-06-22 02:55:48,786 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-22 02:55:48,786 cfg.training.scheduling            : plateau
2020-06-22 02:55:48,786 cfg.training.patience              : 8
2020-06-22 02:55:48,786 cfg.training.decrease_factor       : 0.7
2020-06-22 02:55:48,786 cfg.training.loss                  : crossentropy
2020-06-22 02:55:48,786 cfg.training.learning_rate         : 0.0002
2020-06-22 02:55:48,786 cfg.training.learning_rate_min     : 1e-08
2020-06-22 02:55:48,786 cfg.training.weight_decay          : 0.0
2020-06-22 02:55:48,786 cfg.training.label_smoothing       : 0.1
2020-06-22 02:55:48,786 cfg.training.batch_size            : 4096
2020-06-22 02:55:48,786 cfg.training.batch_type            : token
2020-06-22 02:55:48,786 cfg.training.eval_batch_size       : 3600
2020-06-22 02:55:48,786 cfg.training.eval_batch_type       : token
2020-06-22 02:55:48,786 cfg.training.batch_multiplier      : 1
2020-06-22 02:55:48,786 cfg.training.early_stopping_metric : ppl
2020-06-22 02:55:48,786 cfg.training.epochs                : 100
2020-06-22 02:55:48,786 cfg.training.validation_freq       : 1000
2020-06-22 02:55:48,786 cfg.training.logging_freq          : 100
2020-06-22 02:55:48,787 cfg.training.eval_metric           : bleu
2020-06-22 02:55:48,787 cfg.training.model_dir             : models/transformer_multi_enc_ende
2020-06-22 02:55:48,787 cfg.training.overwrite             : True
2020-06-22 02:55:48,787 cfg.training.shuffle               : True
2020-06-22 02:55:48,787 cfg.training.use_cuda              : True
2020-06-22 02:55:48,787 cfg.training.max_output_length     : 100
2020-06-22 02:55:48,787 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-22 02:55:48,787 cfg.training.keep_last_ckpts       : 3
2020-06-22 02:55:48,787 cfg.model.initializer              : xavier
2020-06-22 02:55:48,787 cfg.model.bias_initializer         : zeros
2020-06-22 02:55:48,787 cfg.model.init_gain                : 1.0
2020-06-22 02:55:48,787 cfg.model.embed_initializer        : xavier
2020-06-22 02:55:48,787 cfg.model.embed_init_gain          : 1.0
2020-06-22 02:55:48,787 cfg.model.tied_embeddings          : False
2020-06-22 02:55:48,787 cfg.model.tied_softmax             : True
2020-06-22 02:55:48,787 cfg.model.encoder.type             : transformer
2020-06-22 02:55:48,787 cfg.model.encoder.num_layers       : 3
2020-06-22 02:55:48,787 cfg.model.encoder.num_heads        : 16
2020-06-22 02:55:48,787 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-22 02:55:48,788 cfg.model.encoder.embeddings.scale : True
2020-06-22 02:55:48,788 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-22 02:55:48,788 cfg.model.encoder.hidden_size      : 512
2020-06-22 02:55:48,788 cfg.model.encoder.ff_size          : 2048
2020-06-22 02:55:48,788 cfg.model.encoder.dropout          : 0.1
2020-06-22 02:55:48,788 cfg.model.encoder.freeze           : False
2020-06-22 02:55:48,788 cfg.model.encoder.multi_encoder    : True
2020-06-22 02:55:48,788 cfg.model.decoder.type             : transformer
2020-06-22 02:55:48,788 cfg.model.decoder.num_layers       : 6
2020-06-22 02:55:48,788 cfg.model.decoder.num_heads        : 16
2020-06-22 02:55:48,788 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-22 02:55:48,788 cfg.model.decoder.embeddings.scale : True
2020-06-22 02:55:48,788 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-22 02:55:48,788 cfg.model.decoder.hidden_size      : 512
2020-06-22 02:55:48,788 cfg.model.decoder.ff_size          : 2048
2020-06-22 02:55:48,788 cfg.model.decoder.dropout          : 0.1
2020-06-22 02:55:48,788 cfg.model.decoder.freeze           : False
2020-06-22 02:55:48,788 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-22 02:55:48,788 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-22 02:55:48,789 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-22 02:55:48,789 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-22 02:55:48,789 Number of Src words (types): 4561
2020-06-22 02:55:48,789 Number of Trg words (types): 5876
2020-06-22 02:55:48,789 Model(
	encoder=TransformerEncoder(num_layers=2, num_heads=16),
	decoder=TransformerDecoder(num_layers=6, num_heads=16),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4561),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5876))
2020-06-22 02:55:48,802 EPOCH 1
2020-06-22 02:56:05,501 Epoch   1: total training loss 306.06
2020-06-22 02:56:05,502 EPOCH 2
2020-06-22 02:56:16,947 Epoch   2 Step:      100 Batch Loss:     4.420701 Tokens per Sec:     9265, Lr: 0.000200
2020-06-22 02:56:19,471 Epoch   2: total training loss 260.70
2020-06-22 02:56:19,472 EPOCH 3
2020-06-22 02:56:33,553 Epoch   3: total training loss 242.08
2020-06-22 02:56:33,554 EPOCH 4
2020-06-22 02:56:42,424 Epoch   4 Step:      200 Batch Loss:     1.981732 Tokens per Sec:     9156, Lr: 0.000200
2020-06-22 02:56:47,305 Epoch   4: total training loss 222.27
2020-06-22 02:56:47,306 EPOCH 5
2020-06-22 02:57:01,053 Epoch   5: total training loss 195.04
2020-06-22 02:57:01,054 EPOCH 6
2020-06-22 02:57:07,465 Epoch   6 Step:      300 Batch Loss:     4.995589 Tokens per Sec:     9300, Lr: 0.000200
2020-06-22 02:57:15,115 Epoch   6: total training loss 178.50
2020-06-22 02:57:15,116 EPOCH 7
2020-06-22 02:57:29,634 Epoch   7: total training loss 152.45
2020-06-22 02:57:29,635 EPOCH 8
2020-06-22 02:57:33,855 Epoch   8 Step:      400 Batch Loss:     2.674412 Tokens per Sec:     8775, Lr: 0.000200
2020-06-22 02:57:44,293 Epoch   8: total training loss 141.13
2020-06-22 02:57:44,294 EPOCH 9
2020-06-22 02:57:58,933 Epoch   9: total training loss 130.03
2020-06-22 02:57:58,934 EPOCH 10
2020-06-22 02:58:00,595 Epoch  10 Step:      500 Batch Loss:     2.672927 Tokens per Sec:    10975, Lr: 0.000200
2020-06-22 02:58:14,077 Epoch  10: total training loss 117.55
2020-06-22 02:58:14,077 EPOCH 11
2020-06-22 02:58:28,118 Epoch  11 Step:      600 Batch Loss:     1.296412 Tokens per Sec:     8793, Lr: 0.000200
2020-06-22 02:58:28,817 Epoch  11: total training loss 111.22
2020-06-22 02:58:28,817 EPOCH 12
2020-06-22 02:58:43,710 Epoch  12: total training loss 100.19
2020-06-22 02:58:43,711 EPOCH 13
2020-06-22 02:58:55,047 Epoch  13 Step:      700 Batch Loss:     1.525519 Tokens per Sec:     9171, Lr: 0.000200
2020-06-22 02:58:58,367 Epoch  13: total training loss 91.45
2020-06-22 02:58:58,367 EPOCH 14
2020-06-22 02:59:13,256 Epoch  14: total training loss 84.75
2020-06-22 02:59:13,257 EPOCH 15
2020-06-22 02:59:21,896 Epoch  15 Step:      800 Batch Loss:     1.568726 Tokens per Sec:     8975, Lr: 0.000200
2020-06-22 02:59:28,534 Epoch  15: total training loss 76.03
2020-06-22 02:59:28,534 EPOCH 16
2020-06-22 02:59:43,607 Epoch  16: total training loss 70.99
2020-06-22 02:59:43,608 EPOCH 17
2020-06-22 02:59:49,901 Epoch  17 Step:      900 Batch Loss:     1.140797 Tokens per Sec:     8335, Lr: 0.000200
2020-06-22 02:59:58,702 Epoch  17: total training loss 68.54
2020-06-22 02:59:58,703 EPOCH 18
2020-06-22 03:00:13,675 Epoch  18: total training loss 64.79
2020-06-22 03:00:13,676 EPOCH 19
2020-06-22 03:00:17,190 Epoch  19 Step:     1000 Batch Loss:     1.067479 Tokens per Sec:     8190, Lr: 0.000200
2020-06-22 03:00:51,890 Hooray! New best validation result [ppl]!
2020-06-22 03:00:51,891 Saving new checkpoint.
2020-06-22 03:00:58,845 Example #0
2020-06-22 03:00:58,846 	Raw source:     ['hello', '.']
2020-06-22 03:00:58,846 	Raw hypothesis: ['hallo', '.']
2020-06-22 03:00:58,846 	Source:     hello .
2020-06-22 03:00:58,846 	Reference:  hallo ,
2020-06-22 03:00:58,846 	Hypothesis: hallo .
2020-06-22 03:00:58,846 Example #1
2020-06-22 03:00:58,846 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 03:00:58,846 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 03:00:58,846 	Source:     hi , how can i help you ?
2020-06-22 03:00:58,846 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 03:00:58,846 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 03:00:58,846 Example #2
2020-06-22 03:00:58,846 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 03:00:58,846 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 03:00:58,846 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 03:00:58,846 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 03:00:58,846 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 03:00:58,846 Example #3
2020-06-22 03:00:58,846 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 03:00:58,846 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 03:00:58,846 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 03:00:58,846 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 03:00:58,846 	Hypothesis: ok , welche art von restaurant suchen sie ?
2020-06-22 03:00:58,846 Validation result (greedy) at epoch  19, step     1000: bleu:  32.83, loss: 46266.0039, ppl:   8.9668, duration: 41.6557s
2020-06-22 03:01:09,889 Epoch  19: total training loss 57.18
2020-06-22 03:01:09,889 EPOCH 20
2020-06-22 03:01:24,206 Epoch  20: total training loss 52.62
2020-06-22 03:01:24,206 EPOCH 21
2020-06-22 03:01:24,766 Epoch  21 Step:     1100 Batch Loss:     1.001202 Tokens per Sec:     9944, Lr: 0.000200
2020-06-22 03:01:39,009 Epoch  21: total training loss 48.62
2020-06-22 03:01:39,009 EPOCH 22
2020-06-22 03:01:51,446 Epoch  22 Step:     1200 Batch Loss:     0.495086 Tokens per Sec:     9144, Lr: 0.000200
2020-06-22 03:01:53,364 Epoch  22: total training loss 42.11
2020-06-22 03:01:53,365 EPOCH 23
2020-06-22 03:02:07,986 Epoch  23: total training loss 38.39
2020-06-22 03:02:07,987 EPOCH 24
2020-06-22 03:02:18,777 Epoch  24 Step:     1300 Batch Loss:     0.553010 Tokens per Sec:     8600, Lr: 0.000200
2020-06-22 03:02:23,317 Epoch  24: total training loss 37.84
2020-06-22 03:02:23,317 EPOCH 25
2020-06-22 03:02:38,403 Epoch  25: total training loss 35.00
2020-06-22 03:02:38,404 EPOCH 26
2020-06-22 03:02:45,839 Epoch  26 Step:     1400 Batch Loss:     0.407501 Tokens per Sec:     8958, Lr: 0.000200
2020-06-22 03:02:53,436 Epoch  26: total training loss 33.45
2020-06-22 03:02:53,437 EPOCH 27
2020-06-22 03:03:08,572 Epoch  27: total training loss 29.93
2020-06-22 03:03:08,572 EPOCH 28
2020-06-22 03:03:13,585 Epoch  28 Step:     1500 Batch Loss:     0.803872 Tokens per Sec:     8446, Lr: 0.000200
2020-06-22 03:03:23,602 Epoch  28: total training loss 26.14
2020-06-22 03:03:23,603 EPOCH 29
2020-06-22 03:03:38,804 Epoch  29: total training loss 24.86
2020-06-22 03:03:38,805 EPOCH 30
2020-06-22 03:03:40,743 Epoch  30 Step:     1600 Batch Loss:     0.436031 Tokens per Sec:    10247, Lr: 0.000200
2020-06-22 03:03:53,781 Epoch  30: total training loss 23.11
2020-06-22 03:03:53,782 EPOCH 31
2020-06-22 03:04:07,808 Epoch  31 Step:     1700 Batch Loss:     0.668189 Tokens per Sec:     8805, Lr: 0.000200
2020-06-22 03:04:08,623 Epoch  31: total training loss 21.12
2020-06-22 03:04:08,623 EPOCH 32
2020-06-22 03:04:23,508 Epoch  32: total training loss 20.02
2020-06-22 03:04:23,509 EPOCH 33
2020-06-22 03:04:34,558 Epoch  33 Step:     1800 Batch Loss:     0.264913 Tokens per Sec:     8751, Lr: 0.000200
2020-06-22 03:04:38,323 Epoch  33: total training loss 20.19
2020-06-22 03:04:38,324 EPOCH 34
2020-06-22 03:04:53,480 Epoch  34: total training loss 17.91
2020-06-22 03:04:53,481 EPOCH 35
2020-06-22 03:05:01,894 Epoch  35 Step:     1900 Batch Loss:     0.273154 Tokens per Sec:     8743, Lr: 0.000200
2020-06-22 03:05:08,706 Epoch  35: total training loss 15.31
2020-06-22 03:05:08,706 EPOCH 36
2020-06-22 03:05:23,640 Epoch  36: total training loss 14.50
2020-06-22 03:05:23,641 EPOCH 37
2020-06-22 03:05:28,909 Epoch  37 Step:     2000 Batch Loss:     0.179778 Tokens per Sec:     9016, Lr: 0.000200
2020-06-22 03:05:54,982 Example #0
2020-06-22 03:05:54,982 	Raw source:     ['hello', '.']
2020-06-22 03:05:54,982 	Raw hypothesis: ['hallo', '.']
2020-06-22 03:05:54,982 	Source:     hello .
2020-06-22 03:05:54,982 	Reference:  hallo ,
2020-06-22 03:05:54,982 	Hypothesis: hallo .
2020-06-22 03:05:54,983 Example #1
2020-06-22 03:05:54,983 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 03:05:54,983 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 03:05:54,983 	Source:     hi , how can i help you ?
2020-06-22 03:05:54,983 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 03:05:54,983 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 03:05:54,983 Example #2
2020-06-22 03:05:54,983 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 03:05:54,983 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 03:05:54,983 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 03:05:54,983 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 03:05:54,983 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 03:05:54,983 Example #3
2020-06-22 03:05:54,983 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 03:05:54,983 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 03:05:54,983 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 03:05:54,983 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 03:05:54,983 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 03:05:54,983 Validation result (greedy) at epoch  37, step     2000: bleu:  39.25, loss: 47056.0352, ppl:   9.3091, duration: 26.0732s
2020-06-22 03:06:04,753 Epoch  37: total training loss 13.52
2020-06-22 03:06:04,754 EPOCH 38
2020-06-22 03:06:19,636 Epoch  38: total training loss 12.36
2020-06-22 03:06:19,636 EPOCH 39
2020-06-22 03:06:22,230 Epoch  39 Step:     2100 Batch Loss:     0.189789 Tokens per Sec:     9151, Lr: 0.000200
2020-06-22 03:06:34,736 Epoch  39: total training loss 11.84
2020-06-22 03:06:34,736 EPOCH 40
2020-06-22 03:06:49,684 Epoch  40: total training loss 11.01
2020-06-22 03:06:49,684 EPOCH 41
2020-06-22 03:06:49,955 Epoch  41 Step:     2200 Batch Loss:     0.194192 Tokens per Sec:     8651, Lr: 0.000200
2020-06-22 03:07:04,642 Epoch  41: total training loss 10.71
2020-06-22 03:07:04,642 EPOCH 42
2020-06-22 03:07:17,699 Epoch  42 Step:     2300 Batch Loss:     0.191040 Tokens per Sec:     8526, Lr: 0.000200
2020-06-22 03:07:19,856 Epoch  42: total training loss 10.36
2020-06-22 03:07:19,857 EPOCH 43
2020-06-22 03:07:34,909 Epoch  43: total training loss 10.50
2020-06-22 03:07:34,909 EPOCH 44
2020-06-22 03:07:44,362 Epoch  44 Step:     2400 Batch Loss:     0.203830 Tokens per Sec:     9305, Lr: 0.000200
2020-06-22 03:07:49,992 Epoch  44: total training loss 9.97
2020-06-22 03:07:49,992 EPOCH 45
2020-06-22 03:08:04,898 Epoch  45: total training loss 10.00
2020-06-22 03:08:04,899 EPOCH 46
2020-06-22 03:08:11,846 Epoch  46 Step:     2500 Batch Loss:     0.169228 Tokens per Sec:     8637, Lr: 0.000200
2020-06-22 03:08:19,842 Epoch  46: total training loss 9.83
2020-06-22 03:08:19,843 EPOCH 47
2020-06-22 03:08:34,749 Epoch  47: total training loss 9.40
2020-06-22 03:08:34,749 EPOCH 48
2020-06-22 03:08:38,996 Epoch  48 Step:     2600 Batch Loss:     0.188813 Tokens per Sec:     8537, Lr: 0.000200
2020-06-22 03:08:49,651 Epoch  48: total training loss 9.14
2020-06-22 03:08:49,652 EPOCH 49
2020-06-22 03:09:04,407 Epoch  49: total training loss 8.55
2020-06-22 03:09:04,408 EPOCH 50
2020-06-22 03:09:06,024 Epoch  50 Step:     2700 Batch Loss:     0.133850 Tokens per Sec:     6618, Lr: 0.000200
2020-06-22 03:09:19,373 Epoch  50: total training loss 8.27
2020-06-22 03:09:19,374 EPOCH 51
2020-06-22 03:09:32,873 Epoch  51 Step:     2800 Batch Loss:     0.151116 Tokens per Sec:     8627, Lr: 0.000200
2020-06-22 03:09:34,530 Epoch  51: total training loss 8.22
2020-06-22 03:09:34,530 EPOCH 52
2020-06-22 03:09:49,843 Epoch  52: total training loss 8.66
2020-06-22 03:09:49,844 EPOCH 53
2020-06-22 03:10:00,095 Epoch  53 Step:     2900 Batch Loss:     0.159263 Tokens per Sec:     8812, Lr: 0.000200
2020-06-22 03:10:05,041 Epoch  53: total training loss 9.36
2020-06-22 03:10:05,041 EPOCH 54
2020-06-22 03:10:19,993 Epoch  54: total training loss 9.85
2020-06-22 03:10:19,994 EPOCH 55
2020-06-22 03:10:27,803 Epoch  55 Step:     3000 Batch Loss:     0.208636 Tokens per Sec:     8578, Lr: 0.000200
2020-06-22 03:10:57,327 Example #0
2020-06-22 03:10:57,328 	Raw source:     ['hello', '.']
2020-06-22 03:10:57,328 	Raw hypothesis: ['hallo', ',']
2020-06-22 03:10:57,328 	Source:     hello .
2020-06-22 03:10:57,328 	Reference:  hallo ,
2020-06-22 03:10:57,328 	Hypothesis: hallo ,
2020-06-22 03:10:57,328 Example #1
2020-06-22 03:10:57,328 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 03:10:57,328 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 03:10:57,328 	Source:     hi , how can i help you ?
2020-06-22 03:10:57,328 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 03:10:57,328 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 03:10:57,328 Example #2
2020-06-22 03:10:57,328 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 03:10:57,328 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 03:10:57,328 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 03:10:57,328 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 03:10:57,328 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 03:10:57,328 Example #3
2020-06-22 03:10:57,328 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 03:10:57,328 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 03:10:57,329 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 03:10:57,329 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 03:10:57,329 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 03:10:57,329 Validation result (greedy) at epoch  55, step     3000: bleu:  38.09, loss: 47633.6797, ppl:   9.5675, duration: 29.5245s
2020-06-22 03:11:04,417 Epoch  55: total training loss 8.42
2020-06-22 03:11:04,418 EPOCH 56
2020-06-22 03:11:19,376 Epoch  56: total training loss 7.77
2020-06-22 03:11:19,376 EPOCH 57
2020-06-22 03:11:24,734 Epoch  57 Step:     3100 Batch Loss:     0.164910 Tokens per Sec:     8173, Lr: 0.000200
2020-06-22 03:11:34,554 Epoch  57: total training loss 7.50
2020-06-22 03:11:34,554 EPOCH 58
2020-06-22 03:11:49,023 Epoch  58: total training loss 7.37
2020-06-22 03:11:49,024 EPOCH 59
2020-06-22 03:11:50,899 Epoch  59 Step:     3200 Batch Loss:     0.114032 Tokens per Sec:     9687, Lr: 0.000200
2020-06-22 03:12:03,980 Epoch  59: total training loss 6.91
2020-06-22 03:12:03,980 EPOCH 60
2020-06-22 03:12:18,359 Epoch  60 Step:     3300 Batch Loss:     0.124482 Tokens per Sec:     8430, Lr: 0.000200
2020-06-22 03:12:19,281 Epoch  60: total training loss 6.96
2020-06-22 03:12:19,281 EPOCH 61
2020-06-22 03:12:34,262 Epoch  61: total training loss 6.85
2020-06-22 03:12:34,263 EPOCH 62
2020-06-22 03:12:45,330 Epoch  62 Step:     3400 Batch Loss:     0.114540 Tokens per Sec:     9014, Lr: 0.000200
2020-06-22 03:12:49,149 Epoch  62: total training loss 6.57
2020-06-22 03:12:49,149 EPOCH 63
2020-06-22 03:13:04,408 Epoch  63: total training loss 6.61
2020-06-22 03:13:04,409 EPOCH 64
2020-06-22 03:13:13,117 Epoch  64 Step:     3500 Batch Loss:     0.119993 Tokens per Sec:     8741, Lr: 0.000200
2020-06-22 03:13:19,315 Epoch  64: total training loss 6.31
2020-06-22 03:13:19,316 EPOCH 65
2020-06-22 03:13:34,229 Epoch  65: total training loss 6.17
2020-06-22 03:13:34,229 EPOCH 66
2020-06-22 03:13:40,322 Epoch  66 Step:     3600 Batch Loss:     0.096678 Tokens per Sec:     8649, Lr: 0.000200
2020-06-22 03:13:49,313 Epoch  66: total training loss 6.04
2020-06-22 03:13:49,313 EPOCH 67
2020-06-22 03:14:04,097 Epoch  67: total training loss 6.09
2020-06-22 03:14:04,098 EPOCH 68
2020-06-22 03:14:07,250 Epoch  68 Step:     3700 Batch Loss:     0.100777 Tokens per Sec:     8272, Lr: 0.000200
2020-06-22 03:14:19,243 Epoch  68: total training loss 6.07
2020-06-22 03:14:19,243 EPOCH 69
2020-06-22 03:14:34,239 Epoch  69: total training loss 5.75
2020-06-22 03:14:34,239 EPOCH 70
2020-06-22 03:14:34,689 Epoch  70 Step:     3800 Batch Loss:     0.095562 Tokens per Sec:    12641, Lr: 0.000200
2020-06-22 03:14:49,286 Epoch  70: total training loss 5.74
2020-06-22 03:14:49,286 EPOCH 71
2020-06-22 03:15:02,219 Epoch  71 Step:     3900 Batch Loss:     0.101848 Tokens per Sec:     8725, Lr: 0.000200
2020-06-22 03:15:04,241 Epoch  71: total training loss 5.82
2020-06-22 03:15:04,241 EPOCH 72
2020-06-22 03:15:19,463 Epoch  72: total training loss 5.55
2020-06-22 03:15:19,463 EPOCH 73
2020-06-22 03:15:30,011 Epoch  73 Step:     4000 Batch Loss:     0.122113 Tokens per Sec:     8511, Lr: 0.000200
2020-06-22 03:15:48,485 Example #0
2020-06-22 03:15:48,486 	Raw source:     ['hello', '.']
2020-06-22 03:15:48,486 	Raw hypothesis: ['hallo', ',']
2020-06-22 03:15:48,486 	Source:     hello .
2020-06-22 03:15:48,486 	Reference:  hallo ,
2020-06-22 03:15:48,486 	Hypothesis: hallo ,
2020-06-22 03:15:48,486 Example #1
2020-06-22 03:15:48,486 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 03:15:48,486 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 03:15:48,486 	Source:     hi , how can i help you ?
2020-06-22 03:15:48,486 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 03:15:48,486 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 03:15:48,486 Example #2
2020-06-22 03:15:48,486 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 03:15:48,486 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 03:15:48,486 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 03:15:48,486 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 03:15:48,486 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 03:15:48,486 Example #3
2020-06-22 03:15:48,486 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 03:15:48,486 	Raw hypothesis: ['okay', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 03:15:48,486 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 03:15:48,486 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 03:15:48,486 	Hypothesis: okay , nach welcher art von restaurant suchen sie ?
2020-06-22 03:15:48,486 Validation result (greedy) at epoch  73, step     4000: bleu:  37.98, loss: 48615.9648, ppl:  10.0237, duration: 18.4737s
2020-06-22 03:15:53,035 Epoch  73: total training loss 6.42
2020-06-22 03:15:53,036 EPOCH 74
2020-06-22 03:16:07,541 Epoch  74: total training loss 6.30
2020-06-22 03:16:07,541 EPOCH 75
2020-06-22 03:16:14,953 Epoch  75 Step:     4100 Batch Loss:     0.095608 Tokens per Sec:     8893, Lr: 0.000200
2020-06-22 03:16:22,197 Epoch  75: total training loss 5.77
2020-06-22 03:16:22,197 EPOCH 76
2020-06-22 03:16:37,020 Epoch  76: total training loss 6.04
2020-06-22 03:16:37,021 EPOCH 77
2020-06-22 03:16:41,977 Epoch  77 Step:     4200 Batch Loss:     0.109987 Tokens per Sec:     8586, Lr: 0.000200
2020-06-22 03:16:52,013 Epoch  77: total training loss 6.01
2020-06-22 03:16:52,013 EPOCH 78
2020-06-22 03:17:06,942 Epoch  78: total training loss 5.55
2020-06-22 03:17:06,943 EPOCH 79
2020-06-22 03:17:09,308 Epoch  79 Step:     4300 Batch Loss:     0.096797 Tokens per Sec:     9482, Lr: 0.000200
2020-06-22 03:17:21,788 Epoch  79: total training loss 5.46
2020-06-22 03:17:21,789 EPOCH 80
2020-06-22 03:17:36,405 Epoch  80 Step:     4400 Batch Loss:     0.109885 Tokens per Sec:     8740, Lr: 0.000200
2020-06-22 03:17:36,650 Epoch  80: total training loss 5.30
2020-06-22 03:17:36,651 EPOCH 81
2020-06-22 03:17:51,639 Epoch  81: total training loss 5.18
2020-06-22 03:17:51,640 EPOCH 82
2020-06-22 03:18:03,592 Epoch  82 Step:     4500 Batch Loss:     0.109444 Tokens per Sec:     8814, Lr: 0.000200
2020-06-22 03:18:06,629 Epoch  82: total training loss 5.39
2020-06-22 03:18:06,629 EPOCH 83
2020-06-22 03:18:21,590 Epoch  83: total training loss 6.37
2020-06-22 03:18:21,591 EPOCH 84
2020-06-22 03:18:30,820 Epoch  84 Step:     4600 Batch Loss:     0.092367 Tokens per Sec:     8948, Lr: 0.000200
2020-06-22 03:18:36,273 Epoch  84: total training loss 5.52
2020-06-22 03:18:36,273 EPOCH 85
2020-06-22 03:18:50,940 Epoch  85: total training loss 5.28
2020-06-22 03:18:50,940 EPOCH 86
2020-06-22 03:18:57,480 Epoch  86 Step:     4700 Batch Loss:     0.081786 Tokens per Sec:     8527, Lr: 0.000200
2020-06-22 03:19:06,070 Epoch  86: total training loss 5.41
2020-06-22 03:19:06,070 EPOCH 87
2020-06-22 03:19:21,257 Epoch  87: total training loss 5.26
2020-06-22 03:19:21,258 EPOCH 88
2020-06-22 03:19:24,749 Epoch  88 Step:     4800 Batch Loss:     0.117010 Tokens per Sec:     8790, Lr: 0.000200
2020-06-22 03:19:36,054 Epoch  88: total training loss 6.94
2020-06-22 03:19:36,054 EPOCH 89
2020-06-22 03:19:50,708 Epoch  89: total training loss 6.24
2020-06-22 03:19:50,709 EPOCH 90
2020-06-22 03:19:51,656 Epoch  90 Step:     4900 Batch Loss:     0.092985 Tokens per Sec:     7611, Lr: 0.000200
2020-06-22 03:20:05,648 Epoch  90: total training loss 5.46
2020-06-22 03:20:05,648 EPOCH 91
2020-06-22 03:20:18,387 Epoch  91 Step:     5000 Batch Loss:     0.204307 Tokens per Sec:     8809, Lr: 0.000200
2020-06-22 03:20:55,989 Example #0
2020-06-22 03:20:55,990 	Raw source:     ['hello', '.']
2020-06-22 03:20:55,990 	Raw hypothesis: ['hallo', ',', 'hallo', '.']
2020-06-22 03:20:55,990 	Source:     hello .
2020-06-22 03:20:55,990 	Reference:  hallo ,
2020-06-22 03:20:55,990 	Hypothesis: hallo , hallo .
2020-06-22 03:20:55,990 Example #1
2020-06-22 03:20:55,990 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 03:20:55,990 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 03:20:55,990 	Source:     hi , how can i help you ?
2020-06-22 03:20:55,990 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 03:20:55,990 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 03:20:55,990 Example #2
2020-06-22 03:20:55,990 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 03:20:55,990 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 03:20:55,990 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 03:20:55,990 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 03:20:55,990 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 03:20:55,990 Example #3
2020-06-22 03:20:55,990 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 03:20:55,990 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 03:20:55,990 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 03:20:55,990 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 03:20:55,990 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 03:20:55,990 Validation result (greedy) at epoch  91, step     5000: bleu:  34.63, loss: 48870.9883, ppl:  10.1456, duration: 37.6026s
2020-06-22 03:20:58,282 Epoch  91: total training loss 13.63
2020-06-22 03:20:58,282 EPOCH 92
2020-06-22 03:21:12,868 Epoch  92: total training loss 12.53
2020-06-22 03:21:12,868 EPOCH 93
2020-06-22 03:21:21,965 Epoch  93 Step:     5100 Batch Loss:     0.143439 Tokens per Sec:     9535, Lr: 0.000200
2020-06-22 03:21:27,443 Epoch  93: total training loss 9.08
2020-06-22 03:21:27,443 EPOCH 94
2020-06-22 03:21:42,241 Epoch  94: total training loss 6.17
2020-06-22 03:21:42,242 EPOCH 95
2020-06-22 03:21:49,680 Epoch  95 Step:     5200 Batch Loss:     0.081344 Tokens per Sec:     8382, Lr: 0.000200
2020-06-22 03:21:57,387 Epoch  95: total training loss 5.49
2020-06-22 03:21:57,388 EPOCH 96
2020-06-22 03:22:12,434 Epoch  96: total training loss 5.10
2020-06-22 03:22:12,435 EPOCH 97
2020-06-22 03:22:16,850 Epoch  97 Step:     5300 Batch Loss:     0.078408 Tokens per Sec:     9105, Lr: 0.000200
2020-06-22 03:22:27,614 Epoch  97: total training loss 4.85
2020-06-22 03:22:27,615 EPOCH 98
2020-06-22 03:22:42,371 Epoch  98: total training loss 4.69
2020-06-22 03:22:42,372 EPOCH 99
2020-06-22 03:22:43,861 Epoch  99 Step:     5400 Batch Loss:     0.077533 Tokens per Sec:     9037, Lr: 0.000200
2020-06-22 03:22:56,957 Epoch  99: total training loss 4.40
2020-06-22 03:22:56,958 EPOCH 100
2020-06-22 03:23:11,043 Epoch 100 Step:     5500 Batch Loss:     0.115609 Tokens per Sec:     8910, Lr: 0.000200
2020-06-22 03:23:11,707 Epoch 100: total training loss 4.45
2020-06-22 03:23:11,707 Training ended after 100 epochs.
2020-06-22 03:23:11,707 Best validation result (greedy) at step     1000:   8.97 ppl.
2020-06-22 03:23:31,970  dev bleu:  35.19 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 03:23:31,975 Translations saved to: models/transformer_multi_enc_ende/00001000.hyps.dev
2020-06-22 03:23:46,561 test bleu:  31.87 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 03:23:46,565 Translations saved to: models/transformer_multi_enc_ende/00001000.hyps.test
