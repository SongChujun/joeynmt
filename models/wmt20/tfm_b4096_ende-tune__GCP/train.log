2020-07-08 06:02:25,200 Hello! This is Joey-NMT.
2020-07-08 06:02:25,206 Total params: 62894080
2020-07-08 06:02:25,207 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-07-08 06:02:28,699 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-08 06:02:29,295 Reset optimizer.
2020-07-08 06:02:29,295 Reset scheduler.
2020-07-08 06:02:29,295 Reset tracking of the best checkpoint.
2020-07-08 06:02:29,300 cfg.name                           : transformer
2020-07-08 06:02:29,300 cfg.data.src                       : en
2020-07-08 06:02:29,300 cfg.data.trg                       : de
2020-07-08 06:02:29,300 cfg.data.train                     : chatnmt/official_split/wmt17bpe/train.tags.bpe.wmt-ende-best
2020-07-08 06:02:29,300 cfg.data.dev                       : chatnmt/official_split/wmt17bpe/dev.tags.bpe.wmt-ende-best
2020-07-08 06:02:29,300 cfg.data.test                      : chatnmt/official_split/wmt17bpe/test.tags.bpe.wmt-ende-best
2020-07-08 06:02:29,300 cfg.data.level                     : bpe
2020-07-08 06:02:29,300 cfg.data.lowercase                 : False
2020-07-08 06:02:29,300 cfg.data.max_sent_length           : 100
2020-07-08 06:02:29,300 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-08 06:02:29,300 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-08 06:02:29,300 cfg.testing.beam_size              : 5
2020-07-08 06:02:29,300 cfg.testing.alpha                  : 1.0
2020-07-08 06:02:29,300 cfg.training.random_seed           : 42
2020-07-08 06:02:29,301 cfg.training.optimizer             : adam
2020-07-08 06:02:29,301 cfg.training.normalization         : tokens
2020-07-08 06:02:29,301 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-08 06:02:29,301 cfg.training.scheduling            : plateau
2020-07-08 06:02:29,301 cfg.training.patience              : 8
2020-07-08 06:02:29,301 cfg.training.decrease_factor       : 0.7
2020-07-08 06:02:29,301 cfg.training.loss                  : crossentropy
2020-07-08 06:02:29,301 cfg.training.learning_rate         : 0.0002
2020-07-08 06:02:29,301 cfg.training.learning_rate_min     : 1e-08
2020-07-08 06:02:29,301 cfg.training.weight_decay          : 0.0
2020-07-08 06:02:29,301 cfg.training.label_smoothing       : 0.1
2020-07-08 06:02:29,301 cfg.training.batch_size            : 4096
2020-07-08 06:02:29,301 cfg.training.batch_type            : token
2020-07-08 06:02:29,301 cfg.training.batch_multiplier      : 1
2020-07-08 06:02:29,301 cfg.training.early_stopping_metric : ppl
2020-07-08 06:02:29,301 cfg.training.epochs                : 50
2020-07-08 06:02:29,301 cfg.training.validation_freq       : 1000
2020-07-08 06:02:29,301 cfg.training.logging_freq          : 100
2020-07-08 06:02:29,301 cfg.training.eval_metric           : bleu
2020-07-08 06:02:29,301 cfg.training.model_dir             : models/wmt20/tfm_b4096_ende-tune__GCP
2020-07-08 06:02:29,301 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-08 06:02:29,302 cfg.training.reset_best_ckpt       : True
2020-07-08 06:02:29,302 cfg.training.reset_scheduler       : True
2020-07-08 06:02:29,302 cfg.training.reset_optimizer       : True
2020-07-08 06:02:29,302 cfg.training.overwrite             : False
2020-07-08 06:02:29,302 cfg.training.shuffle               : True
2020-07-08 06:02:29,302 cfg.training.use_cuda              : True
2020-07-08 06:02:29,302 cfg.training.max_output_length     : 100
2020-07-08 06:02:29,302 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-08 06:02:29,302 cfg.training.keep_last_ckpts       : 3
2020-07-08 06:02:29,302 cfg.model.initializer              : xavier
2020-07-08 06:02:29,302 cfg.model.bias_initializer         : zeros
2020-07-08 06:02:29,302 cfg.model.init_gain                : 1.0
2020-07-08 06:02:29,302 cfg.model.embed_initializer        : xavier
2020-07-08 06:02:29,302 cfg.model.embed_init_gain          : 1.0
2020-07-08 06:02:29,302 cfg.model.tied_embeddings          : True
2020-07-08 06:02:29,302 cfg.model.tied_softmax             : True
2020-07-08 06:02:29,302 cfg.model.encoder.type             : transformer
2020-07-08 06:02:29,302 cfg.model.encoder.num_layers       : 6
2020-07-08 06:02:29,302 cfg.model.encoder.num_heads        : 8
2020-07-08 06:02:29,302 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-08 06:02:29,303 cfg.model.encoder.embeddings.scale : True
2020-07-08 06:02:29,303 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-08 06:02:29,303 cfg.model.encoder.hidden_size      : 512
2020-07-08 06:02:29,303 cfg.model.encoder.ff_size          : 2048
2020-07-08 06:02:29,303 cfg.model.encoder.dropout          : 0.1
2020-07-08 06:02:29,303 cfg.model.decoder.type             : transformer
2020-07-08 06:02:29,303 cfg.model.decoder.num_layers       : 6
2020-07-08 06:02:29,303 cfg.model.decoder.num_heads        : 8
2020-07-08 06:02:29,303 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-08 06:02:29,303 cfg.model.decoder.embeddings.scale : True
2020-07-08 06:02:29,303 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-08 06:02:29,303 cfg.model.decoder.hidden_size      : 512
2020-07-08 06:02:29,303 cfg.model.decoder.ff_size          : 2048
2020-07-08 06:02:29,303 cfg.model.decoder.dropout          : 0.1
2020-07-08 06:02:29,303 Data set sizes: 
	train 10939,
	valid 776,
	test 0
2020-07-08 06:02:29,303 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-08 06:02:29,303 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-08 06:02:29,303 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-08 06:02:29,303 Number of Src words (types): 36628
2020-07-08 06:02:29,304 Number of Trg words (types): 36628
2020-07-08 06:02:29,304 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-08 06:02:29,331 EPOCH 1
2020-07-08 06:02:42,052 Epoch   1: total training loss 65.95
2020-07-08 06:02:42,052 EPOCH 2
2020-07-08 06:02:47,964 Epoch   2 Step:  1360100 Batch Loss:     0.807473 Tokens per Sec:    13576, Lr: 0.000200
2020-07-08 06:02:54,605 Epoch   2: total training loss 46.48
2020-07-08 06:02:54,606 EPOCH 3
2020-07-08 06:03:06,337 Epoch   3 Step:  1360200 Batch Loss:     0.807866 Tokens per Sec:    13517, Lr: 0.000200
2020-07-08 06:03:07,284 Epoch   3: total training loss 40.78
2020-07-08 06:03:07,285 EPOCH 4
2020-07-08 06:03:19,930 Epoch   4: total training loss 36.35
2020-07-08 06:03:19,930 EPOCH 5
2020-07-08 06:03:24,779 Epoch   5 Step:  1360300 Batch Loss:     0.507757 Tokens per Sec:    13967, Lr: 0.000200
2020-07-08 06:03:32,687 Epoch   5: total training loss 32.46
2020-07-08 06:03:32,687 EPOCH 6
2020-07-08 06:03:43,362 Epoch   6 Step:  1360400 Batch Loss:     0.469076 Tokens per Sec:    13335, Lr: 0.000200
2020-07-08 06:03:45,456 Epoch   6: total training loss 30.12
2020-07-08 06:03:45,456 EPOCH 7
2020-07-08 06:03:58,238 Epoch   7: total training loss 27.94
2020-07-08 06:03:58,238 EPOCH 8
2020-07-08 06:04:01,940 Epoch   8 Step:  1360500 Batch Loss:     0.452834 Tokens per Sec:    13463, Lr: 0.000200
2020-07-08 06:04:11,173 Epoch   8: total training loss 26.06
2020-07-08 06:04:11,174 EPOCH 9
2020-07-08 06:04:20,666 Epoch   9 Step:  1360600 Batch Loss:     0.372718 Tokens per Sec:    13535, Lr: 0.000200
2020-07-08 06:04:23,931 Epoch   9: total training loss 24.46
2020-07-08 06:04:23,931 EPOCH 10
2020-07-08 06:04:36,837 Epoch  10: total training loss 22.74
2020-07-08 06:04:36,838 EPOCH 11
2020-07-08 06:04:39,366 Epoch  11 Step:  1360700 Batch Loss:     0.342084 Tokens per Sec:    12937, Lr: 0.000200
2020-07-08 06:04:49,805 Epoch  11: total training loss 21.44
2020-07-08 06:04:49,805 EPOCH 12
2020-07-08 06:04:58,091 Epoch  12 Step:  1360800 Batch Loss:     0.324867 Tokens per Sec:    13210, Lr: 0.000200
2020-07-08 06:05:02,696 Epoch  12: total training loss 20.18
2020-07-08 06:05:02,696 EPOCH 13
2020-07-08 06:05:15,456 Epoch  13: total training loss 18.74
2020-07-08 06:05:15,457 EPOCH 14
2020-07-08 06:05:16,760 Epoch  14 Step:  1360900 Batch Loss:     0.264186 Tokens per Sec:    14388, Lr: 0.000200
2020-07-08 06:05:28,379 Epoch  14: total training loss 17.83
2020-07-08 06:05:28,379 EPOCH 15
2020-07-08 06:05:35,801 Epoch  15 Step:  1361000 Batch Loss:     0.226608 Tokens per Sec:    13331, Lr: 0.000200
2020-07-08 06:06:00,853 Hooray! New best validation result [ppl]!
2020-07-08 06:06:00,853 Saving new checkpoint.
2020-07-08 06:06:01,717 Example #0
2020-07-08 06:06:01,717 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-08 06:06:01,717 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-08 06:06:01,718 	Source:     Hi , how can I help you ?
2020-07-08 06:06:01,718 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-08 06:06:01,718 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-08 06:06:01,718 Example #1
2020-07-08 06:06:01,718 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-08 06:06:01,718 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-08 06:06:01,718 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-08 06:06:01,718 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-08 06:06:01,718 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-08 06:06:01,718 Example #2
2020-07-08 06:06:01,718 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-08 06:06:01,718 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'eine', 'Sek@@', 'unde', ',', 'ich', 'werde', 'gleich', 'wieder', 'ein', 'paar', 'Optionen', 'haben', '.']
2020-07-08 06:06:01,718 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-08 06:06:01,718 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-08 06:06:01,718 	Hypothesis: Klar , geben Sie mir eine Sekunde , ich werde gleich wieder ein paar Optionen haben .
2020-07-08 06:06:01,718 Example #3
2020-07-08 06:06:01,718 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-08 06:06:01,718 	Raw hypothesis: ['Ich', 'habe', 'Folgendes', 'gefunden', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'eine', 'wechsel@@', 'nde', 'Speis@@', 'ek@@', 'arte', 'mit', 'sa@@', 'ison@@', 'alen', 'amerikanischen', 'Gerichten', 'zusammen', 'mit', 'internationalen', 'W@@', 'einen', 'in', 'einer', 'geho@@', 'benen', 'Umgebung', '.', 'Ich', 'habe', 'auch', 'Folgendes', 'gefunden', ':', 'P@@', 'lu@@', 'to', "'", 's', ':', 'eine', 'lokale', 'K@@', 'ette', 'mit', 'Gegen@@', 'service', ',', 'die', 'Sal@@', 'ate', '&', 'Sand@@', 'wich@@', 'es', 'in', 'einem', 'lässi@@', 'gen', ',', 'modernen', 'Ambiente', 'anbietet', '.']
2020-07-08 06:06:01,718 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-08 06:06:01,718 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-08 06:06:01,718 	Hypothesis: Ich habe Folgendes gefunden : Seasons 52 , eine wechselnde Speisekarte mit saisonalen amerikanischen Gerichten zusammen mit internationalen Weinen in einer gehobenen Umgebung . Ich habe auch Folgendes gefunden : Pluto ' s : eine lokale Kette mit Gegenservice , die Salate & Sandwiches in einem lässigen , modernen Ambiente anbietet .
2020-07-08 06:06:01,718 Validation result (greedy) at epoch  15, step  1361000: bleu:  58.00, loss: 9462.7285, ppl:   1.8844, duration: 25.9170s
2020-07-08 06:06:07,101 Epoch  15: total training loss 16.74
2020-07-08 06:06:07,101 EPOCH 16
2020-07-08 06:06:19,956 Epoch  16: total training loss 16.08
2020-07-08 06:06:19,956 EPOCH 17
2020-07-08 06:06:20,759 Epoch  17 Step:  1361100 Batch Loss:     0.275619 Tokens per Sec:    13628, Lr: 0.000200
2020-07-08 06:06:32,580 Epoch  17: total training loss 15.37
2020-07-08 06:06:32,580 EPOCH 18
2020-07-08 06:06:39,390 Epoch  18 Step:  1361200 Batch Loss:     0.182392 Tokens per Sec:    13289, Lr: 0.000200
2020-07-08 06:06:45,479 Epoch  18: total training loss 14.72
2020-07-08 06:06:45,479 EPOCH 19
2020-07-08 06:06:58,257 Epoch  19 Step:  1361300 Batch Loss:     0.204218 Tokens per Sec:    13465, Lr: 0.000200
2020-07-08 06:06:58,258 Epoch  19: total training loss 13.89
2020-07-08 06:06:58,258 EPOCH 20
2020-07-08 06:07:10,906 Epoch  20: total training loss 13.36
2020-07-08 06:07:10,907 EPOCH 21
2020-07-08 06:07:16,828 Epoch  21 Step:  1361400 Batch Loss:     0.189883 Tokens per Sec:    13498, Lr: 0.000200
2020-07-08 06:07:23,547 Epoch  21: total training loss 12.88
2020-07-08 06:07:23,547 EPOCH 22
2020-07-08 06:07:35,395 Epoch  22 Step:  1361500 Batch Loss:     0.139980 Tokens per Sec:    13609, Lr: 0.000200
2020-07-08 06:07:36,173 Epoch  22: total training loss 12.41
2020-07-08 06:07:36,173 EPOCH 23
2020-07-08 06:07:48,776 Epoch  23: total training loss 11.73
2020-07-08 06:07:48,776 EPOCH 24
2020-07-08 06:07:53,908 Epoch  24 Step:  1361600 Batch Loss:     0.122124 Tokens per Sec:    13478, Lr: 0.000200
2020-07-08 06:08:01,675 Epoch  24: total training loss 11.41
2020-07-08 06:08:01,675 EPOCH 25
2020-07-08 06:08:12,997 Epoch  25 Step:  1361700 Batch Loss:     0.165273 Tokens per Sec:    13469, Lr: 0.000200
2020-07-08 06:08:14,449 Epoch  25: total training loss 10.88
2020-07-08 06:08:14,449 EPOCH 26
2020-07-08 06:08:27,234 Epoch  26: total training loss 10.63
2020-07-08 06:08:27,234 EPOCH 27
2020-07-08 06:08:31,843 Epoch  27 Step:  1361800 Batch Loss:     0.148354 Tokens per Sec:    13169, Lr: 0.000200
2020-07-08 06:08:40,233 Epoch  27: total training loss 10.23
2020-07-08 06:08:40,233 EPOCH 28
2020-07-08 06:08:50,982 Epoch  28 Step:  1361900 Batch Loss:     0.166245 Tokens per Sec:    13468, Lr: 0.000200
2020-07-08 06:08:53,067 Epoch  28: total training loss 9.79
2020-07-08 06:08:53,067 EPOCH 29
2020-07-08 06:09:05,735 Epoch  29: total training loss 9.53
2020-07-08 06:09:05,736 EPOCH 30
2020-07-08 06:09:09,647 Epoch  30 Step:  1362000 Batch Loss:     0.139757 Tokens per Sec:    13328, Lr: 0.000200
2020-07-08 06:09:34,169 Example #0
2020-07-08 06:09:34,169 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-08 06:09:34,169 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-08 06:09:34,169 	Source:     Hi , how can I help you ?
2020-07-08 06:09:34,169 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-08 06:09:34,169 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-08 06:09:34,169 Example #1
2020-07-08 06:09:34,169 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-08 06:09:34,169 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-08 06:09:34,169 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-08 06:09:34,169 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-08 06:09:34,169 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-08 06:09:34,169 Example #2
2020-07-08 06:09:34,169 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-08 06:09:34,169 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'eine', 'Sek@@', 'unde', ',', 'ich', 'werde', 'gleich', 'ein', 'paar', 'Optionen', 'haben', '.']
2020-07-08 06:09:34,169 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-08 06:09:34,169 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-08 06:09:34,170 	Hypothesis: Klar , geben Sie mir eine Sekunde , ich werde gleich ein paar Optionen haben .
2020-07-08 06:09:34,170 Example #3
2020-07-08 06:09:34,170 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-08 06:09:34,170 	Raw hypothesis: ['Ich', 'habe', 'folgendes', 'gefunden', ':', '"', 'Se@@', 'as@@', 'ons', '52', '"', ',', 'ein', 'wechsel@@', 'n@@', 'des', 'Menü', 'mit', 'sa@@', 'ison@@', 'alen', 'amerikanischen', 'Gerichten', 'sowie', 'internationalen', 'W@@', 'einen', 'in', 'geho@@', 'ben@@', 'em', 'Rahmen', '.', 'Ich', 'habe', 'auch', 'folgendes', 'gefunden', ':', '"', 'P@@', 'lu@@', 'to', "'", 's', '"', ':', 'eine', 'lokale', 'K@@', 'ette', 'zum', 'Hin@@', 'setzen', 'von', 'Sal@@', 'aten', '&', 'Sand@@', 'wich@@', 'es', 'in', 'lässi@@', 'gem', ',', 'moder@@', 'nem', 'Ambiente', '.']
2020-07-08 06:09:34,170 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-08 06:09:34,170 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-08 06:09:34,170 	Hypothesis: Ich habe folgendes gefunden : " Seasons 52 " , ein wechselndes Menü mit saisonalen amerikanischen Gerichten sowie internationalen Weinen in gehobenem Rahmen . Ich habe auch folgendes gefunden : " Pluto ' s " : eine lokale Kette zum Hinsetzen von Salaten & Sandwiches in lässigem , modernem Ambiente .
2020-07-08 06:09:34,170 Validation result (greedy) at epoch  30, step  1362000: bleu:  56.30, loss: 10717.8340, ppl:   2.0496, duration: 24.5218s
2020-07-08 06:09:43,082 Epoch  30: total training loss 9.41
2020-07-08 06:09:43,082 EPOCH 31
2020-07-08 06:09:52,946 Epoch  31 Step:  1362100 Batch Loss:     0.136811 Tokens per Sec:    13270, Lr: 0.000200
2020-07-08 06:09:55,990 Epoch  31: total training loss 9.01
2020-07-08 06:09:55,991 EPOCH 32
2020-07-08 06:10:09,073 Epoch  32: total training loss 9.19
2020-07-08 06:10:09,074 EPOCH 33
2020-07-08 06:10:11,640 Epoch  33 Step:  1362200 Batch Loss:     0.132542 Tokens per Sec:    12991, Lr: 0.000200
2020-07-08 06:10:21,854 Epoch  33: total training loss 8.82
2020-07-08 06:10:21,855 EPOCH 34
2020-07-08 06:10:30,272 Epoch  34 Step:  1362300 Batch Loss:     0.118938 Tokens per Sec:    13743, Lr: 0.000200
2020-07-08 06:10:34,529 Epoch  34: total training loss 8.34
2020-07-08 06:10:34,529 EPOCH 35
2020-07-08 06:10:47,471 Epoch  35: total training loss 8.13
2020-07-08 06:10:47,471 EPOCH 36
2020-07-08 06:10:48,932 Epoch  36 Step:  1362400 Batch Loss:     0.118015 Tokens per Sec:    12621, Lr: 0.000200
2020-07-08 06:11:00,142 Epoch  36: total training loss 7.97
2020-07-08 06:11:00,142 EPOCH 37
2020-07-08 06:11:07,432 Epoch  37 Step:  1362500 Batch Loss:     0.116050 Tokens per Sec:    13786, Lr: 0.000200
2020-07-08 06:11:12,907 Epoch  37: total training loss 7.60
2020-07-08 06:11:12,907 EPOCH 38
2020-07-08 06:11:25,782 Epoch  38: total training loss 7.35
2020-07-08 06:11:25,782 EPOCH 39
2020-07-08 06:11:26,353 Epoch  39 Step:  1362600 Batch Loss:     0.109192 Tokens per Sec:    12699, Lr: 0.000200
2020-07-08 06:11:38,649 Epoch  39: total training loss 7.12
2020-07-08 06:11:38,649 EPOCH 40
2020-07-08 06:11:45,452 Epoch  40 Step:  1362700 Batch Loss:     0.092054 Tokens per Sec:    13583, Lr: 0.000200
2020-07-08 06:11:51,497 Epoch  40: total training loss 7.05
2020-07-08 06:11:51,497 EPOCH 41
2020-07-08 06:12:04,178 Epoch  41 Step:  1362800 Batch Loss:     0.094341 Tokens per Sec:    13374, Lr: 0.000200
2020-07-08 06:12:04,376 Epoch  41: total training loss 7.05
2020-07-08 06:12:04,376 EPOCH 42
2020-07-08 06:12:17,319 Epoch  42: total training loss 6.74
2020-07-08 06:12:17,319 EPOCH 43
2020-07-08 06:12:23,168 Epoch  43 Step:  1362900 Batch Loss:     0.097280 Tokens per Sec:    13314, Lr: 0.000200
2020-07-08 06:12:30,214 Epoch  43: total training loss 6.67
2020-07-08 06:12:30,214 EPOCH 44
2020-07-08 06:12:41,864 Epoch  44 Step:  1363000 Batch Loss:     0.115526 Tokens per Sec:    13358, Lr: 0.000200
2020-07-08 06:13:07,233 Example #0
2020-07-08 06:13:07,234 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-08 06:13:07,234 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-08 06:13:07,234 	Source:     Hi , how can I help you ?
2020-07-08 06:13:07,234 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-08 06:13:07,234 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-08 06:13:07,234 Example #1
2020-07-08 06:13:07,234 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-08 06:13:07,234 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-08 06:13:07,234 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-08 06:13:07,234 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-08 06:13:07,234 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-08 06:13:07,234 Example #2
2020-07-08 06:13:07,234 	Raw source:     ['S@@', 'ure', ',', 'give', 'me', 'a', 'second', ',', 'I', 'will', 'be', 'right', 'back', 'with', 'a', 'couple', 'of', 'options', '.']
2020-07-08 06:13:07,234 	Raw hypothesis: ['K@@', 'lar', ',', 'geben', 'Sie', 'mir', 'eine', 'Sek@@', 'unde', ',', 'ich', 'werde', 'gleich', 'ein', 'paar', 'Optionen', 'haben', '.']
2020-07-08 06:13:07,234 	Source:     Sure , give me a second , I will be right back with a couple of options .
2020-07-08 06:13:07,234 	Reference:  Klar , geben Sie mir eine Sekunde , ich bin gleich zurück mit ein paar Optionen .
2020-07-08 06:13:07,234 	Hypothesis: Klar , geben Sie mir eine Sekunde , ich werde gleich ein paar Optionen haben .
2020-07-08 06:13:07,234 Example #3
2020-07-08 06:13:07,234 	Raw source:     ['I', 'found', ':', 'Se@@', 'as@@', 'ons', '52', ',', 'a', 'Rot@@', 'ating', 'menu', 'of', 'seasonal', 'American', 'dishes', 'alongside', 'International', 'wines', 'in', 'an', 'up@@', 'scale', 'setting', '.', 'I', 'also', 'found', ':', 'P@@', 'lu@@', 'to', "'s", ':', 'a', 'Local', 'counter-@@', 'serve', 'chain', 'featuring', 'buil@@', 'd-@@', 'your@@', '-@@', 'own', 'sal@@', 'ads', '&', 'sand@@', 'wich@@', 'es', 'in', 'a', 'cas@@', 'ual', ',', 'modern', 'setting', '.']
2020-07-08 06:13:07,235 	Raw hypothesis: ['Ich', 'habe', 'folgendes', 'gefunden', ':', '"', 'Se@@', 'as@@', 'ons', '52', '"', ',', 'ein', 'wechsel@@', 'n@@', 'des', 'Menü', 'mit', 'sa@@', 'ison@@', 'alen', 'amerikanischen', 'Gerichten', 'sowie', 'internationalen', 'W@@', 'einen', 'in', 'geho@@', 'ben@@', 'em', 'Rahmen', '.', 'Ich', 'habe', 'auch', 'folgendes', 'gefunden', ':', '"', 'P@@', 'lu@@', 'to', "'", 's', '"', ':', 'eine', 'lokale', 'K@@', 'ette', 'mit', 'Co@@', 'unter@@', '-@@', 'Ser@@', 've-@@', 'Gerichten', ',', 'die', '‖', 'private', 'Sal@@', 'ate', '&', 'Sand@@', 'wich@@', 'es', 'in', 'einem', 'lässi@@', 'gen', ',', 'modernen', 'Ambiente', 'anbietet', '.']
2020-07-08 06:13:07,235 	Source:     I found : Seasons 52 , a Rotating menu of seasonal American dishes alongside International wines in an upscale setting . I also found : Pluto 's : a Local counter-serve chain featuring build-your-own salads & sandwiches in a casual , modern setting .
2020-07-08 06:13:07,235 	Reference:  Ich fand : Seasons 52 , ein wechselndes Menü mit saisonalen amerikanischen Gerichten neben internationalen Weinen in einer gehobenen Umgebung . Ich habe auch : Pluto ' s : eine lokale Counter-Service-Kette , die Salate & Sandwiches zum Selbermachen in einem ungezwungenen , modernen Ambiente anbietet , gefunden .
2020-07-08 06:13:07,235 	Hypothesis: Ich habe folgendes gefunden : " Seasons 52 " , ein wechselndes Menü mit saisonalen amerikanischen Gerichten sowie internationalen Weinen in gehobenem Rahmen . Ich habe auch folgendes gefunden : " Pluto ' s " : eine lokale Kette mit Counter-Serve-Gerichten , die ‖ private Salate & Sandwiches in einem lässigen , modernen Ambiente anbietet .
2020-07-08 06:13:07,235 Validation result (greedy) at epoch  44, step  1363000: bleu:  56.11, loss: 11780.6611, ppl:   2.2007, duration: 25.3701s
2020-07-08 06:13:08,436 Epoch  44: total training loss 6.39
2020-07-08 06:13:08,437 EPOCH 45
2020-07-08 06:13:21,163 Epoch  45: total training loss 6.27
2020-07-08 06:13:21,164 EPOCH 46
2020-07-08 06:13:26,101 Epoch  46 Step:  1363100 Batch Loss:     0.084718 Tokens per Sec:    13709, Lr: 0.000200
2020-07-08 06:13:33,850 Epoch  46: total training loss 6.05
2020-07-08 06:13:33,850 EPOCH 47
2020-07-08 06:13:44,841 Epoch  47 Step:  1363200 Batch Loss:     0.067965 Tokens per Sec:    13349, Lr: 0.000200
2020-07-08 06:13:46,851 Epoch  47: total training loss 6.13
2020-07-08 06:13:46,851 EPOCH 48
2020-07-08 06:13:59,816 Epoch  48: total training loss 5.97
2020-07-08 06:13:59,816 EPOCH 49
2020-07-08 06:14:03,933 Epoch  49 Step:  1363300 Batch Loss:     0.080224 Tokens per Sec:    13054, Lr: 0.000200
2020-07-08 06:14:12,653 Epoch  49: total training loss 5.90
2020-07-08 06:14:12,653 EPOCH 50
2020-07-08 06:14:22,885 Epoch  50 Step:  1363400 Batch Loss:     0.082429 Tokens per Sec:    13464, Lr: 0.000200
2020-07-08 06:14:25,475 Epoch  50: total training loss 5.63
2020-07-08 06:14:25,475 Training ended after  50 epochs.
2020-07-08 06:14:25,475 Best validation result (greedy) at step  1361000:   1.88 ppl.
2020-07-08 06:15:04,587  dev bleu:  57.97 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-08 06:15:04,587 Translations saved to: models/wmt20/tfm_b4096_ende-tune__GCP/01361000.hyps.dev
2020-07-08 06:15:04,589 test bleu:  -1.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-08 06:15:04,589 Translations saved to: models/wmt20/tfm_b4096_ende-tune__GCP/01361000.hyps.test
