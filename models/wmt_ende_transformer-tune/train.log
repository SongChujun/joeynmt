2020-07-02 03:56:22,930 Hello! This is Joey-NMT.
2020-07-02 03:56:38,908 Total params: 62894080
2020-07-02 03:56:38,912 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-07-02 03:56:40,524 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-02 03:56:48,174 Reset optimizer.
2020-07-02 03:56:48,174 Reset scheduler.
2020-07-02 03:56:48,174 Reset tracking of the best checkpoint.
2020-07-02 03:56:48,180 cfg.name                           : transformer
2020-07-02 03:56:48,180 cfg.data.src                       : en
2020-07-02 03:56:48,180 cfg.data.trg                       : de
2020-07-02 03:56:48,180 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-02 03:56:48,180 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-02 03:56:48,180 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-02 03:56:48,180 cfg.data.level                     : bpe
2020-07-02 03:56:48,180 cfg.data.lowercase                 : False
2020-07-02 03:56:48,180 cfg.data.max_sent_length           : 100
2020-07-02 03:56:48,180 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-02 03:56:48,180 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-02 03:56:48,180 cfg.testing.beam_size              : 5
2020-07-02 03:56:48,181 cfg.testing.alpha                  : 1.0
2020-07-02 03:56:48,181 cfg.training.random_seed           : 42
2020-07-02 03:56:48,181 cfg.training.optimizer             : adam
2020-07-02 03:56:48,181 cfg.training.normalization         : tokens
2020-07-02 03:56:48,181 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-02 03:56:48,181 cfg.training.scheduling            : plateau
2020-07-02 03:56:48,181 cfg.training.patience              : 8
2020-07-02 03:56:48,181 cfg.training.decrease_factor       : 0.7
2020-07-02 03:56:48,181 cfg.training.loss                  : crossentropy
2020-07-02 03:56:48,181 cfg.training.learning_rate         : 0.0002
2020-07-02 03:56:48,181 cfg.training.learning_rate_min     : 1e-08
2020-07-02 03:56:48,181 cfg.training.weight_decay          : 0.0
2020-07-02 03:56:48,181 cfg.training.label_smoothing       : 0.1
2020-07-02 03:56:48,181 cfg.training.batch_size            : 4096
2020-07-02 03:56:48,181 cfg.training.batch_type            : token
2020-07-02 03:56:48,181 cfg.training.batch_multiplier      : 1
2020-07-02 03:56:48,181 cfg.training.early_stopping_metric : ppl
2020-07-02 03:56:48,181 cfg.training.epochs                : 100
2020-07-02 03:56:48,181 cfg.training.validation_freq       : 1000
2020-07-02 03:56:48,181 cfg.training.logging_freq          : 100
2020-07-02 03:56:48,181 cfg.training.eval_metric           : bleu
2020-07-02 03:56:48,181 cfg.training.model_dir             : models/wmt_ende_transformer-tune
2020-07-02 03:56:48,181 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-02 03:56:48,181 cfg.training.reset_best_ckpt       : True
2020-07-02 03:56:48,182 cfg.training.reset_scheduler       : True
2020-07-02 03:56:48,182 cfg.training.reset_optimizer       : True
2020-07-02 03:56:48,182 cfg.training.overwrite             : False
2020-07-02 03:56:48,182 cfg.training.shuffle               : True
2020-07-02 03:56:48,182 cfg.training.use_cuda              : True
2020-07-02 03:56:48,182 cfg.training.max_output_length     : 100
2020-07-02 03:56:48,182 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-02 03:56:48,182 cfg.training.keep_last_ckpts       : 3
2020-07-02 03:56:48,182 cfg.model.initializer              : xavier
2020-07-02 03:56:48,182 cfg.model.bias_initializer         : zeros
2020-07-02 03:56:48,182 cfg.model.init_gain                : 1.0
2020-07-02 03:56:48,182 cfg.model.embed_initializer        : xavier
2020-07-02 03:56:48,182 cfg.model.embed_init_gain          : 1.0
2020-07-02 03:56:48,182 cfg.model.tied_embeddings          : True
2020-07-02 03:56:48,182 cfg.model.tied_softmax             : True
2020-07-02 03:56:48,182 cfg.model.encoder.type             : transformer
2020-07-02 03:56:48,182 cfg.model.encoder.num_layers       : 6
2020-07-02 03:56:48,182 cfg.model.encoder.num_heads        : 8
2020-07-02 03:56:48,182 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-02 03:56:48,182 cfg.model.encoder.embeddings.scale : True
2020-07-02 03:56:48,182 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-02 03:56:48,182 cfg.model.encoder.hidden_size      : 512
2020-07-02 03:56:48,182 cfg.model.encoder.ff_size          : 2048
2020-07-02 03:56:48,182 cfg.model.encoder.dropout          : 0.1
2020-07-02 03:56:48,183 cfg.model.decoder.type             : transformer
2020-07-02 03:56:48,183 cfg.model.decoder.num_layers       : 6
2020-07-02 03:56:48,183 cfg.model.decoder.num_heads        : 8
2020-07-02 03:56:48,183 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-02 03:56:48,183 cfg.model.decoder.embeddings.scale : True
2020-07-02 03:56:48,183 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-02 03:56:48,183 cfg.model.decoder.hidden_size      : 512
2020-07-02 03:56:48,183 cfg.model.decoder.ff_size          : 2048
2020-07-02 03:56:48,183 cfg.model.decoder.dropout          : 0.1
2020-07-02 03:56:48,183 Data set sizes: 
	train 9771,
	valid 1525,
	test 1165
2020-07-02 03:56:48,183 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-02 03:56:48,183 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-02 03:56:48,183 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-02 03:56:48,183 Number of Src words (types): 36628
2020-07-02 03:56:48,183 Number of Trg words (types): 36628
2020-07-02 03:56:48,183 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-02 03:56:48,213 EPOCH 1
2020-07-02 03:57:18,853 Epoch   1: total training loss 67.59
2020-07-02 03:57:18,853 EPOCH 2
2020-07-02 03:57:36,074 Epoch   2 Step:  1360100 Batch Loss:     0.631523 Tokens per Sec:     4639, Lr: 0.000200
2020-07-02 03:57:52,584 Epoch   2: total training loss 50.75
2020-07-02 03:57:52,585 EPOCH 3
2020-07-02 03:58:25,948 Epoch   3 Step:  1360200 Batch Loss:     0.615829 Tokens per Sec:     4462, Lr: 0.000200
2020-07-02 03:58:26,965 Epoch   3: total training loss 44.43
2020-07-02 03:58:26,966 EPOCH 4
2020-07-02 03:59:01,075 Epoch   4: total training loss 39.21
2020-07-02 03:59:01,075 EPOCH 5
2020-07-02 03:59:15,835 Epoch   5 Step:  1360300 Batch Loss:     0.351690 Tokens per Sec:     4509, Lr: 0.000200
2020-07-02 03:59:35,346 Epoch   5: total training loss 35.49
2020-07-02 03:59:35,347 EPOCH 6
2020-07-02 04:00:05,976 Epoch   6 Step:  1360400 Batch Loss:     0.405922 Tokens per Sec:     4527, Lr: 0.000200
2020-07-02 04:00:09,296 Epoch   6: total training loss 31.88
2020-07-02 04:00:09,296 EPOCH 7
2020-07-02 04:00:43,563 Epoch   7: total training loss 29.06
2020-07-02 04:00:43,564 EPOCH 8
2020-07-02 04:00:56,406 Epoch   8 Step:  1360500 Batch Loss:     0.412685 Tokens per Sec:     4622, Lr: 0.000200
2020-07-02 04:01:17,897 Epoch   8: total training loss 26.25
2020-07-02 04:01:17,898 EPOCH 9
2020-07-02 04:01:47,155 Epoch   9 Step:  1360600 Batch Loss:     0.368941 Tokens per Sec:     4547, Lr: 0.000200
2020-07-02 04:01:51,639 Epoch   9: total training loss 24.42
2020-07-02 04:01:51,639 EPOCH 10
2020-07-02 04:02:25,564 Epoch  10: total training loss 22.80
2020-07-02 04:02:25,564 EPOCH 11
2020-07-02 04:02:37,916 Epoch  11 Step:  1360700 Batch Loss:     0.269372 Tokens per Sec:     4514, Lr: 0.000200
2020-07-02 04:02:59,591 Epoch  11: total training loss 21.32
2020-07-02 04:02:59,591 EPOCH 12
2020-07-02 04:03:28,117 Epoch  12 Step:  1360800 Batch Loss:     0.303269 Tokens per Sec:     4553, Lr: 0.000200
2020-07-02 04:03:33,624 Epoch  12: total training loss 20.27
2020-07-02 04:03:33,624 EPOCH 13
2020-07-02 04:04:07,535 Epoch  13: total training loss 18.74
2020-07-02 04:04:07,536 EPOCH 14
2020-07-02 04:04:18,570 Epoch  14 Step:  1360900 Batch Loss:     0.261062 Tokens per Sec:     4431, Lr: 0.000200
2020-07-02 04:04:41,747 Epoch  14: total training loss 17.93
2020-07-02 04:04:41,748 EPOCH 15
2020-07-02 04:05:08,943 Epoch  15 Step:  1361000 Batch Loss:     0.201079 Tokens per Sec:     4538, Lr: 0.000200
2020-07-02 04:07:45,307 Hooray! New best validation result [ppl]!
2020-07-02 04:07:45,308 Saving new checkpoint.
2020-07-02 04:07:53,378 Example #0
2020-07-02 04:07:53,379 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:07:53,379 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:07:53,379 	Source:     Hello .
2020-07-02 04:07:53,379 	Reference:  Hallo ,
2020-07-02 04:07:53,379 	Hypothesis: Hallo .
2020-07-02 04:07:53,379 Example #1
2020-07-02 04:07:53,379 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:07:53,379 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:07:53,379 	Source:     Hi , how can I help you ?
2020-07-02 04:07:53,379 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:07:53,379 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:07:53,379 Example #2
2020-07-02 04:07:53,380 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:07:53,380 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:07:53,380 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:07:53,380 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:07:53,380 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:07:53,380 Example #3
2020-07-02 04:07:53,380 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:07:53,380 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:07:53,380 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:07:53,380 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:07:53,380 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:07:53,380 Validation result (greedy) at epoch  15, step  1361000: bleu:  54.87, loss: 16160.0889, ppl:   1.9396, duration: 164.4365s
2020-07-02 04:07:59,935 Epoch  15: total training loss 17.26
2020-07-02 04:07:59,935 EPOCH 16
2020-07-02 04:08:33,679 Epoch  16: total training loss 15.78
2020-07-02 04:08:33,679 EPOCH 17
2020-07-02 04:08:42,769 Epoch  17 Step:  1361100 Batch Loss:     0.214067 Tokens per Sec:     4324, Lr: 0.000200
2020-07-02 04:09:07,422 Epoch  17: total training loss 15.11
2020-07-02 04:09:07,423 EPOCH 18
2020-07-02 04:09:33,485 Epoch  18 Step:  1361200 Batch Loss:     0.206380 Tokens per Sec:     4535, Lr: 0.000200
2020-07-02 04:09:41,710 Epoch  18: total training loss 14.39
2020-07-02 04:09:41,711 EPOCH 19
2020-07-02 04:10:16,093 Epoch  19: total training loss 13.80
2020-07-02 04:10:16,094 EPOCH 20
2020-07-02 04:10:23,433 Epoch  20 Step:  1361300 Batch Loss:     0.182872 Tokens per Sec:     4566, Lr: 0.000200
2020-07-02 04:10:49,971 Epoch  20: total training loss 13.14
2020-07-02 04:10:49,971 EPOCH 21
2020-07-02 04:11:13,929 Epoch  21 Step:  1361400 Batch Loss:     0.202189 Tokens per Sec:     4561, Lr: 0.000200
2020-07-02 04:11:23,970 Epoch  21: total training loss 12.48
2020-07-02 04:11:23,971 EPOCH 22
2020-07-02 04:11:57,823 Epoch  22: total training loss 11.97
2020-07-02 04:11:57,824 EPOCH 23
2020-07-02 04:12:04,342 Epoch  23 Step:  1361500 Batch Loss:     0.174005 Tokens per Sec:     4499, Lr: 0.000200
2020-07-02 04:12:31,851 Epoch  23: total training loss 11.41
2020-07-02 04:12:31,851 EPOCH 24
2020-07-02 04:12:54,894 Epoch  24 Step:  1361600 Batch Loss:     0.156158 Tokens per Sec:     4541, Lr: 0.000200
2020-07-02 04:13:05,901 Epoch  24: total training loss 11.16
2020-07-02 04:13:05,901 EPOCH 25
2020-07-02 04:13:40,154 Epoch  25: total training loss 10.66
2020-07-02 04:13:40,155 EPOCH 26
2020-07-02 04:13:45,697 Epoch  26 Step:  1361700 Batch Loss:     0.145925 Tokens per Sec:     4481, Lr: 0.000200
2020-07-02 04:14:14,320 Epoch  26: total training loss 10.31
2020-07-02 04:14:14,321 EPOCH 27
2020-07-02 04:14:36,099 Epoch  27 Step:  1361800 Batch Loss:     0.152092 Tokens per Sec:     4506, Lr: 0.000200
2020-07-02 04:14:48,326 Epoch  27: total training loss 10.04
2020-07-02 04:14:48,326 EPOCH 28
2020-07-02 04:15:22,576 Epoch  28: total training loss 9.81
2020-07-02 04:15:22,577 EPOCH 29
2020-07-02 04:15:25,585 Epoch  29 Step:  1361900 Batch Loss:     0.124007 Tokens per Sec:     4017, Lr: 0.000200
2020-07-02 04:15:56,836 Epoch  29: total training loss 9.30
2020-07-02 04:15:56,836 EPOCH 30
2020-07-02 04:16:16,082 Epoch  30 Step:  1362000 Batch Loss:     0.131453 Tokens per Sec:     4560, Lr: 0.000200
2020-07-02 04:18:34,878 Example #0
2020-07-02 04:18:34,878 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:18:34,878 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:18:34,878 	Source:     Hello .
2020-07-02 04:18:34,878 	Reference:  Hallo ,
2020-07-02 04:18:34,878 	Hypothesis: Hallo .
2020-07-02 04:18:34,878 Example #1
2020-07-02 04:18:34,879 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:18:34,879 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:18:34,879 	Source:     Hi , how can I help you ?
2020-07-02 04:18:34,879 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:18:34,879 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:18:34,879 Example #2
2020-07-02 04:18:34,879 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:18:34,879 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:18:34,879 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:18:34,879 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:18:34,879 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:18:34,879 Example #3
2020-07-02 04:18:34,879 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:18:34,879 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:18:34,879 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:18:34,879 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:18:34,879 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:18:34,879 Validation result (greedy) at epoch  30, step  1362000: bleu:  53.58, loss: 18284.1055, ppl:   2.1161, duration: 138.7963s
2020-07-02 04:18:49,573 Epoch  30: total training loss 8.90
2020-07-02 04:18:49,573 EPOCH 31
2020-07-02 04:19:23,596 Epoch  31: total training loss 8.77
2020-07-02 04:19:23,596 EPOCH 32
2020-07-02 04:19:24,990 Epoch  32 Step:  1362100 Batch Loss:     0.100476 Tokens per Sec:     4691, Lr: 0.000200
2020-07-02 04:19:57,323 Epoch  32: total training loss 8.39
2020-07-02 04:19:57,323 EPOCH 33
2020-07-02 04:20:15,217 Epoch  33 Step:  1362200 Batch Loss:     0.129307 Tokens per Sec:     4579, Lr: 0.000200
2020-07-02 04:20:31,420 Epoch  33: total training loss 8.25
2020-07-02 04:20:31,420 EPOCH 34
2020-07-02 04:21:05,518 Epoch  34: total training loss 7.89
2020-07-02 04:21:05,519 EPOCH 35
2020-07-02 04:21:06,036 Epoch  35 Step:  1362300 Batch Loss:     0.102948 Tokens per Sec:     3988, Lr: 0.000200
2020-07-02 04:21:39,138 Epoch  35: total training loss 7.54
2020-07-02 04:21:39,138 EPOCH 36
2020-07-02 04:21:56,810 Epoch  36 Step:  1362400 Batch Loss:     0.119056 Tokens per Sec:     4575, Lr: 0.000200
2020-07-02 04:22:13,358 Epoch  36: total training loss 7.61
2020-07-02 04:22:13,358 EPOCH 37
2020-07-02 04:22:46,911 Epoch  37 Step:  1362500 Batch Loss:     0.128481 Tokens per Sec:     4494, Lr: 0.000200
2020-07-02 04:22:47,458 Epoch  37: total training loss 7.27
2020-07-02 04:22:47,458 EPOCH 38
2020-07-02 04:23:22,019 Epoch  38: total training loss 7.18
2020-07-02 04:23:22,020 EPOCH 39
2020-07-02 04:23:37,790 Epoch  39 Step:  1362600 Batch Loss:     0.094387 Tokens per Sec:     4651, Lr: 0.000200
2020-07-02 04:23:56,028 Epoch  39: total training loss 6.96
2020-07-02 04:23:56,029 EPOCH 40
2020-07-02 04:24:28,116 Epoch  40 Step:  1362700 Batch Loss:     0.095096 Tokens per Sec:     4508, Lr: 0.000200
2020-07-02 04:24:30,168 Epoch  40: total training loss 6.78
2020-07-02 04:24:30,168 EPOCH 41
2020-07-02 04:25:04,347 Epoch  41: total training loss 6.63
2020-07-02 04:25:04,347 EPOCH 42
2020-07-02 04:25:18,394 Epoch  42 Step:  1362800 Batch Loss:     0.093023 Tokens per Sec:     4545, Lr: 0.000200
2020-07-02 04:25:38,439 Epoch  42: total training loss 6.55
2020-07-02 04:25:38,439 EPOCH 43
2020-07-02 04:26:08,778 Epoch  43 Step:  1362900 Batch Loss:     0.106824 Tokens per Sec:     4495, Lr: 0.000200
2020-07-02 04:26:12,647 Epoch  43: total training loss 6.43
2020-07-02 04:26:12,647 EPOCH 44
2020-07-02 04:26:46,979 Epoch  44: total training loss 6.13
2020-07-02 04:26:46,980 EPOCH 45
2020-07-02 04:26:59,613 Epoch  45 Step:  1363000 Batch Loss:     0.091682 Tokens per Sec:     4470, Lr: 0.000200
2020-07-02 04:29:11,652 Example #0
2020-07-02 04:29:11,653 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:29:11,653 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:29:11,653 	Source:     Hello .
2020-07-02 04:29:11,653 	Reference:  Hallo ,
2020-07-02 04:29:11,653 	Hypothesis: Hallo .
2020-07-02 04:29:11,653 Example #1
2020-07-02 04:29:11,653 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:29:11,653 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:29:11,653 	Source:     Hi , how can I help you ?
2020-07-02 04:29:11,653 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:29:11,653 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:29:11,653 Example #2
2020-07-02 04:29:11,653 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:29:11,653 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:29:11,653 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:29:11,653 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:29:11,653 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:29:11,653 Example #3
2020-07-02 04:29:11,653 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:29:11,653 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:29:11,653 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:29:11,653 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:29:11,654 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:29:11,654 Validation result (greedy) at epoch  45, step  1363000: bleu:  53.81, loss: 19944.8633, ppl:   2.2652, duration: 132.0394s
2020-07-02 04:29:32,974 Epoch  45: total training loss 5.98
2020-07-02 04:29:32,975 EPOCH 46
2020-07-02 04:30:02,197 Epoch  46 Step:  1363100 Batch Loss:     0.082401 Tokens per Sec:     4502, Lr: 0.000200
2020-07-02 04:30:06,924 Epoch  46: total training loss 5.90
2020-07-02 04:30:06,925 EPOCH 47
2020-07-02 04:30:41,002 Epoch  47: total training loss 6.02
2020-07-02 04:30:41,003 EPOCH 48
2020-07-02 04:30:52,772 Epoch  48 Step:  1363200 Batch Loss:     0.075938 Tokens per Sec:     4408, Lr: 0.000200
2020-07-02 04:31:15,133 Epoch  48: total training loss 5.78
2020-07-02 04:31:15,134 EPOCH 49
2020-07-02 04:31:43,004 Epoch  49 Step:  1363300 Batch Loss:     0.078173 Tokens per Sec:     4598, Lr: 0.000200
2020-07-02 04:31:48,792 Epoch  49: total training loss 5.64
2020-07-02 04:31:48,792 EPOCH 50
2020-07-02 04:32:22,888 Epoch  50: total training loss 5.48
2020-07-02 04:32:22,889 EPOCH 51
2020-07-02 04:32:33,039 Epoch  51 Step:  1363400 Batch Loss:     0.075453 Tokens per Sec:     4501, Lr: 0.000200
2020-07-02 04:32:57,191 Epoch  51: total training loss 5.50
2020-07-02 04:32:57,192 EPOCH 52
2020-07-02 04:33:23,272 Epoch  52 Step:  1363500 Batch Loss:     0.072176 Tokens per Sec:     4477, Lr: 0.000200
2020-07-02 04:33:31,715 Epoch  52: total training loss 5.40
2020-07-02 04:33:31,716 EPOCH 53
2020-07-02 04:34:06,041 Epoch  53: total training loss 5.26
2020-07-02 04:34:06,042 EPOCH 54
2020-07-02 04:34:13,229 Epoch  54 Step:  1363600 Batch Loss:     0.081385 Tokens per Sec:     4322, Lr: 0.000200
2020-07-02 04:34:40,612 Epoch  54: total training loss 5.16
2020-07-02 04:34:40,612 EPOCH 55
2020-07-02 04:35:03,802 Epoch  55 Step:  1363700 Batch Loss:     0.068198 Tokens per Sec:     4501, Lr: 0.000200
2020-07-02 04:35:14,918 Epoch  55: total training loss 5.13
2020-07-02 04:35:14,918 EPOCH 56
2020-07-02 04:35:49,160 Epoch  56: total training loss 5.11
2020-07-02 04:35:49,161 EPOCH 57
2020-07-02 04:35:53,568 Epoch  57 Step:  1363800 Batch Loss:     0.062851 Tokens per Sec:     4652, Lr: 0.000200
2020-07-02 04:36:23,189 Epoch  57: total training loss 4.97
2020-07-02 04:36:23,189 EPOCH 58
2020-07-02 04:36:43,577 Epoch  58 Step:  1363900 Batch Loss:     0.093567 Tokens per Sec:     4542, Lr: 0.000200
2020-07-02 04:36:57,263 Epoch  58: total training loss 4.93
2020-07-02 04:36:57,264 EPOCH 59
2020-07-02 04:37:31,272 Epoch  59: total training loss 4.79
2020-07-02 04:37:31,272 EPOCH 60
2020-07-02 04:37:34,227 Epoch  60 Step:  1364000 Batch Loss:     0.079093 Tokens per Sec:     4422, Lr: 0.000200
2020-07-02 04:39:38,517 Example #0
2020-07-02 04:39:38,518 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:39:38,518 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:39:38,518 	Source:     Hello .
2020-07-02 04:39:38,518 	Reference:  Hallo ,
2020-07-02 04:39:38,518 	Hypothesis: Hallo .
2020-07-02 04:39:38,518 Example #1
2020-07-02 04:39:38,518 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:39:38,518 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:39:38,518 	Source:     Hi , how can I help you ?
2020-07-02 04:39:38,518 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:39:38,518 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:39:38,518 Example #2
2020-07-02 04:39:38,518 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:39:38,518 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:39:38,518 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:39:38,518 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:39:38,518 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:39:38,519 Example #3
2020-07-02 04:39:38,519 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:39:38,519 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:39:38,519 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:39:38,519 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:39:38,519 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:39:38,519 Validation result (greedy) at epoch  60, step  1364000: bleu:  53.53, loss: 21213.2969, ppl:   2.3861, duration: 124.2910s
2020-07-02 04:40:09,780 Epoch  60: total training loss 4.76
2020-07-02 04:40:09,781 EPOCH 61
2020-07-02 04:40:28,964 Epoch  61 Step:  1364100 Batch Loss:     0.055283 Tokens per Sec:     4505, Lr: 0.000200
2020-07-02 04:40:44,076 Epoch  61: total training loss 4.65
2020-07-02 04:40:44,077 EPOCH 62
2020-07-02 04:41:18,210 Epoch  62: total training loss 4.56
2020-07-02 04:41:18,211 EPOCH 63
2020-07-02 04:41:19,187 Epoch  63 Step:  1364200 Batch Loss:     0.071657 Tokens per Sec:     3214, Lr: 0.000200
2020-07-02 04:41:52,190 Epoch  63: total training loss 4.44
2020-07-02 04:41:52,190 EPOCH 64
2020-07-02 04:42:09,459 Epoch  64 Step:  1364300 Batch Loss:     0.075970 Tokens per Sec:     4421, Lr: 0.000200
2020-07-02 04:42:26,072 Epoch  64: total training loss 4.45
2020-07-02 04:42:26,073 EPOCH 65
2020-07-02 04:42:59,683 Epoch  65 Step:  1364400 Batch Loss:     0.057052 Tokens per Sec:     4503, Lr: 0.000200
2020-07-02 04:43:00,199 Epoch  65: total training loss 4.40
2020-07-02 04:43:00,199 EPOCH 66
2020-07-02 04:43:34,354 Epoch  66: total training loss 4.32
2020-07-02 04:43:34,355 EPOCH 67
2020-07-02 04:43:50,139 Epoch  67 Step:  1364500 Batch Loss:     0.052733 Tokens per Sec:     4544, Lr: 0.000200
2020-07-02 04:44:08,570 Epoch  67: total training loss 4.27
2020-07-02 04:44:08,571 EPOCH 68
2020-07-02 04:44:41,424 Epoch  68 Step:  1364600 Batch Loss:     0.064528 Tokens per Sec:     4487, Lr: 0.000200
2020-07-02 04:44:42,830 Epoch  68: total training loss 4.22
2020-07-02 04:44:42,830 EPOCH 69
2020-07-02 04:45:17,096 Epoch  69: total training loss 4.16
2020-07-02 04:45:17,097 EPOCH 70
2020-07-02 04:45:32,342 Epoch  70 Step:  1364700 Batch Loss:     0.050296 Tokens per Sec:     4486, Lr: 0.000200
2020-07-02 04:45:51,235 Epoch  70: total training loss 4.18
2020-07-02 04:45:51,236 EPOCH 71
2020-07-02 04:46:22,519 Epoch  71 Step:  1364800 Batch Loss:     0.057035 Tokens per Sec:     4526, Lr: 0.000200
2020-07-02 04:46:25,132 Epoch  71: total training loss 4.04
2020-07-02 04:46:25,133 EPOCH 72
2020-07-02 04:46:59,441 Epoch  72: total training loss 4.07
2020-07-02 04:46:59,441 EPOCH 73
2020-07-02 04:47:12,967 Epoch  73 Step:  1364900 Batch Loss:     0.050590 Tokens per Sec:     4557, Lr: 0.000200
2020-07-02 04:47:33,467 Epoch  73: total training loss 4.07
2020-07-02 04:47:33,467 EPOCH 74
2020-07-02 04:48:03,290 Epoch  74 Step:  1365000 Batch Loss:     0.056999 Tokens per Sec:     4543, Lr: 0.000200
2020-07-02 04:50:10,780 Example #0
2020-07-02 04:50:10,781 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 04:50:10,781 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 04:50:10,781 	Source:     Hello .
2020-07-02 04:50:10,781 	Reference:  Hallo ,
2020-07-02 04:50:10,781 	Hypothesis: Hallo .
2020-07-02 04:50:10,781 Example #1
2020-07-02 04:50:10,781 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 04:50:10,781 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 04:50:10,781 	Source:     Hi , how can I help you ?
2020-07-02 04:50:10,781 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:50:10,781 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 04:50:10,781 Example #2
2020-07-02 04:50:10,781 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 04:50:10,781 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 04:50:10,781 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 04:50:10,781 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 04:50:10,781 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 04:50:10,781 Example #3
2020-07-02 04:50:10,781 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 04:50:10,781 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 04:50:10,782 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 04:50:10,782 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 04:50:10,782 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 04:50:10,782 Validation result (greedy) at epoch  74, step  1365000: bleu:  53.07, loss: 22097.6289, ppl:   2.4742, duration: 127.4913s
2020-07-02 04:50:14,901 Epoch  74: total training loss 3.93
2020-07-02 04:50:14,902 EPOCH 75
2020-07-02 04:50:49,035 Epoch  75: total training loss 3.86
2020-07-02 04:50:49,036 EPOCH 76
2020-07-02 04:51:01,501 Epoch  76 Step:  1365100 Batch Loss:     0.052027 Tokens per Sec:     4470, Lr: 0.000200
2020-07-02 04:51:22,914 Epoch  76: total training loss 3.81
2020-07-02 04:51:22,914 EPOCH 77
2020-07-02 04:51:51,891 Epoch  77 Step:  1365200 Batch Loss:     0.050851 Tokens per Sec:     4492, Lr: 0.000200
2020-07-02 04:51:57,108 Epoch  77: total training loss 3.86
2020-07-02 04:51:57,108 EPOCH 78
2020-07-02 04:52:31,260 Epoch  78: total training loss 3.84
2020-07-02 04:52:31,260 EPOCH 79
2020-07-02 04:52:42,583 Epoch  79 Step:  1365300 Batch Loss:     0.056547 Tokens per Sec:     4419, Lr: 0.000200
2020-07-02 04:53:05,471 Epoch  79: total training loss 3.82
2020-07-02 04:53:05,472 EPOCH 80
2020-07-02 04:53:32,676 Epoch  80 Step:  1365400 Batch Loss:     0.053720 Tokens per Sec:     4568, Lr: 0.000200
2020-07-02 04:53:39,346 Epoch  80: total training loss 3.73
2020-07-02 04:53:39,347 EPOCH 81
2020-07-02 04:54:13,595 Epoch  81: total training loss 3.70
2020-07-02 04:54:13,596 EPOCH 82
2020-07-02 04:54:23,260 Epoch  82 Step:  1365500 Batch Loss:     0.051127 Tokens per Sec:     4436, Lr: 0.000200
2020-07-02 04:54:47,483 Epoch  82: total training loss 3.67
2020-07-02 04:54:47,484 EPOCH 83
2020-07-02 04:55:13,944 Epoch  83 Step:  1365600 Batch Loss:     0.046391 Tokens per Sec:     4548, Lr: 0.000200
2020-07-02 04:55:21,449 Epoch  83: total training loss 3.62
2020-07-02 04:55:21,449 EPOCH 84
2020-07-02 04:55:55,446 Epoch  84: total training loss 3.61
2020-07-02 04:55:55,446 EPOCH 85
2020-07-02 04:56:04,176 Epoch  85 Step:  1365700 Batch Loss:     0.059276 Tokens per Sec:     4605, Lr: 0.000200
2020-07-02 04:56:29,429 Epoch  85: total training loss 3.54
2020-07-02 04:56:29,430 EPOCH 86
2020-07-02 04:56:54,975 Epoch  86 Step:  1365800 Batch Loss:     0.059159 Tokens per Sec:     4536, Lr: 0.000200
2020-07-02 04:57:03,455 Epoch  86: total training loss 3.51
2020-07-02 04:57:03,456 EPOCH 87
2020-07-02 04:57:37,427 Epoch  87: total training loss 3.50
2020-07-02 04:57:37,428 EPOCH 88
2020-07-02 04:57:45,288 Epoch  88 Step:  1365900 Batch Loss:     0.055591 Tokens per Sec:     4509, Lr: 0.000200
2020-07-02 04:58:11,863 Epoch  88: total training loss 3.54
2020-07-02 04:58:11,864 EPOCH 89
2020-07-02 04:58:36,184 Epoch  89 Step:  1366000 Batch Loss:     0.058145 Tokens per Sec:     4522, Lr: 0.000200
2020-07-02 05:00:36,858 Example #0
2020-07-02 05:00:36,858 	Raw source:     ['H@@', 'ello', '.']
2020-07-02 05:00:36,858 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-02 05:00:36,858 	Source:     Hello .
2020-07-02 05:00:36,858 	Reference:  Hallo ,
2020-07-02 05:00:36,859 	Hypothesis: Hallo .
2020-07-02 05:00:36,859 Example #1
2020-07-02 05:00:36,859 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-02 05:00:36,859 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-02 05:00:36,859 	Source:     Hi , how can I help you ?
2020-07-02 05:00:36,859 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-02 05:00:36,859 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-02 05:00:36,859 Example #2
2020-07-02 05:00:36,859 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-02 05:00:36,859 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-02 05:00:36,859 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-02 05:00:36,859 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-02 05:00:36,859 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-02 05:00:36,859 Example #3
2020-07-02 05:00:36,859 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-02 05:00:36,859 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-02 05:00:36,859 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-02 05:00:36,859 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-02 05:00:36,859 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-02 05:00:36,859 Validation result (greedy) at epoch  89, step  1366000: bleu:  52.84, loss: 22954.7285, ppl:   2.5626, duration: 120.6744s
2020-07-02 05:00:46,769 Epoch  89: total training loss 3.50
2020-07-02 05:00:46,770 EPOCH 90
2020-07-02 05:01:20,692 Epoch  90: total training loss 3.43
2020-07-02 05:01:20,693 EPOCH 91
2020-07-02 05:01:27,364 Epoch  91 Step:  1366100 Batch Loss:     0.043077 Tokens per Sec:     4533, Lr: 0.000200
2020-07-02 05:01:54,801 Epoch  91: total training loss 3.39
2020-07-02 05:01:54,802 EPOCH 92
2020-07-02 05:02:18,196 Epoch  92 Step:  1366200 Batch Loss:     0.042020 Tokens per Sec:     4474, Lr: 0.000200
2020-07-02 05:02:29,009 Epoch  92: total training loss 3.36
2020-07-02 05:02:29,009 EPOCH 93
2020-07-02 05:03:03,409 Epoch  93: total training loss 3.36
2020-07-02 05:03:03,410 EPOCH 94
2020-07-02 05:03:08,937 Epoch  94 Step:  1366300 Batch Loss:     0.044102 Tokens per Sec:     4450, Lr: 0.000200
2020-07-02 05:03:37,723 Epoch  94: total training loss 3.38
2020-07-02 05:03:37,723 EPOCH 95
2020-07-02 05:03:59,354 Epoch  95 Step:  1366400 Batch Loss:     0.045517 Tokens per Sec:     4457, Lr: 0.000200
2020-07-02 05:04:12,364 Epoch  95: total training loss 3.35
2020-07-02 05:04:12,364 EPOCH 96
2020-07-02 05:04:46,304 Epoch  96: total training loss 3.31
2020-07-02 05:04:46,304 EPOCH 97
2020-07-02 05:04:49,912 Epoch  97 Step:  1366500 Batch Loss:     0.046320 Tokens per Sec:     4469, Lr: 0.000200
2020-07-02 05:05:20,504 Epoch  97: total training loss 3.25
2020-07-02 05:05:20,505 EPOCH 98
2020-07-02 05:05:39,913 Epoch  98 Step:  1366600 Batch Loss:     0.037770 Tokens per Sec:     4458, Lr: 0.000200
2020-07-02 05:05:54,654 Epoch  98: total training loss 3.27
2020-07-02 05:05:54,654 EPOCH 99
2020-07-02 05:06:28,787 Epoch  99: total training loss 3.17
2020-07-02 05:06:28,788 EPOCH 100
2020-07-02 05:06:30,799 Epoch 100 Step:  1366700 Batch Loss:     0.078102 Tokens per Sec:     3880, Lr: 0.000200
2020-07-02 05:07:03,178 Epoch 100: total training loss 3.26
2020-07-02 05:07:03,179 Training ended after 100 epochs.
2020-07-02 05:07:03,179 Best validation result (greedy) at step  1361000:   1.94 ppl.
2020-07-02 05:08:33,961  dev bleu:  55.92 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-02 05:08:33,968 Translations saved to: models/wmt_ende_transformer-tune/01361000.hyps.dev
2020-07-02 05:09:33,365 test bleu:  52.01 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-02 05:09:33,370 Translations saved to: models/wmt_ende_transformer-tune/01361000.hyps.test
