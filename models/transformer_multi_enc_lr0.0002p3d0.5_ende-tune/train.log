2020-07-01 14:08:26,642 Hello! This is Joey-NMT.
2020-07-01 14:08:30,891 Total params: 82862081
2020-07-01 14:08:30,893 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight']
2020-07-01 14:08:33,150 Loading model from models/wmt_ende_transformer/best.ckpt
2020-07-01 14:08:33,467 Reset optimizer.
2020-07-01 14:08:33,467 Reset scheduler.
2020-07-01 14:08:33,467 Reset tracking of the best checkpoint.
2020-07-01 14:08:33,477 cfg.name                           : transformer
2020-07-01 14:08:33,477 cfg.data.src                       : en
2020-07-01 14:08:33,477 cfg.data.trg                       : de
2020-07-01 14:08:33,477 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-01 14:08:33,477 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-01 14:08:33,477 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-01 14:08:33,477 cfg.data.level                     : bpe
2020-07-01 14:08:33,478 cfg.data.lowercase                 : False
2020-07-01 14:08:33,478 cfg.data.max_sent_length           : 100
2020-07-01 14:08:33,478 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-07-01 14:08:33,478 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-07-01 14:08:33,478 cfg.testing.beam_size              : 5
2020-07-01 14:08:33,478 cfg.testing.alpha                  : 1.0
2020-07-01 14:08:33,478 cfg.training.random_seed           : 42
2020-07-01 14:08:33,478 cfg.training.optimizer             : adam
2020-07-01 14:08:33,478 cfg.training.normalization         : tokens
2020-07-01 14:08:33,478 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-01 14:08:33,478 cfg.training.scheduling            : plateau
2020-07-01 14:08:33,478 cfg.training.patience              : 3
2020-07-01 14:08:33,478 cfg.training.decrease_factor       : 0.5
2020-07-01 14:08:33,478 cfg.training.loss                  : crossentropy
2020-07-01 14:08:33,478 cfg.training.learning_rate         : 0.0002
2020-07-01 14:08:33,478 cfg.training.learning_rate_min     : 1e-08
2020-07-01 14:08:33,478 cfg.training.weight_decay          : 0.0
2020-07-01 14:08:33,478 cfg.training.label_smoothing       : 0.1
2020-07-01 14:08:33,478 cfg.training.batch_size            : 2048
2020-07-01 14:08:33,478 cfg.training.batch_type            : token
2020-07-01 14:08:33,478 cfg.training.eval_batch_size       : 3600
2020-07-01 14:08:33,478 cfg.training.eval_batch_type       : token
2020-07-01 14:08:33,478 cfg.training.batch_multiplier      : 1
2020-07-01 14:08:33,478 cfg.training.early_stopping_metric : ppl
2020-07-01 14:08:33,478 cfg.training.epochs                : 100
2020-07-01 14:08:33,478 cfg.training.validation_freq       : 1000
2020-07-01 14:08:33,478 cfg.training.logging_freq          : 100
2020-07-01 14:08:33,478 cfg.training.eval_metric           : bleu
2020-07-01 14:08:33,478 cfg.training.model_dir             : models/transformer_multi_enc_lr0.0002p3d0.5_ende-tune
2020-07-01 14:08:33,478 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-07-01 14:08:33,478 cfg.training.reset_best_ckpt       : True
2020-07-01 14:08:33,478 cfg.training.reset_scheduler       : True
2020-07-01 14:08:33,478 cfg.training.reset_optimizer       : True
2020-07-01 14:08:33,478 cfg.training.overwrite             : False
2020-07-01 14:08:33,478 cfg.training.shuffle               : True
2020-07-01 14:08:33,478 cfg.training.use_cuda              : True
2020-07-01 14:08:33,478 cfg.training.max_output_length     : 100
2020-07-01 14:08:33,478 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-01 14:08:33,478 cfg.training.keep_last_ckpts       : 3
2020-07-01 14:08:33,478 cfg.model.initializer              : xavier
2020-07-01 14:08:33,479 cfg.model.bias_initializer         : zeros
2020-07-01 14:08:33,479 cfg.model.init_gain                : 1.0
2020-07-01 14:08:33,479 cfg.model.embed_initializer        : xavier
2020-07-01 14:08:33,479 cfg.model.embed_init_gain          : 1.0
2020-07-01 14:08:33,479 cfg.model.tied_embeddings          : True
2020-07-01 14:08:33,479 cfg.model.tied_softmax             : True
2020-07-01 14:08:33,479 cfg.model.encoder.type             : transformer
2020-07-01 14:08:33,479 cfg.model.encoder.num_layers       : 6
2020-07-01 14:08:33,479 cfg.model.encoder.num_heads        : 8
2020-07-01 14:08:33,479 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-01 14:08:33,479 cfg.model.encoder.embeddings.scale : True
2020-07-01 14:08:33,479 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-01 14:08:33,479 cfg.model.encoder.hidden_size      : 512
2020-07-01 14:08:33,479 cfg.model.encoder.ff_size          : 2048
2020-07-01 14:08:33,479 cfg.model.encoder.dropout          : 0.1
2020-07-01 14:08:33,479 cfg.model.encoder.multi_encoder    : True
2020-07-01 14:08:33,479 cfg.model.decoder.type             : transformer
2020-07-01 14:08:33,479 cfg.model.decoder.num_layers       : 6
2020-07-01 14:08:33,479 cfg.model.decoder.num_heads        : 8
2020-07-01 14:08:33,479 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-01 14:08:33,479 cfg.model.decoder.embeddings.scale : True
2020-07-01 14:08:33,479 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-01 14:08:33,479 cfg.model.decoder.hidden_size      : 512
2020-07-01 14:08:33,479 cfg.model.decoder.ff_size          : 2048
2020-07-01 14:08:33,479 cfg.model.decoder.dropout          : 0.1
2020-07-01 14:08:33,479 Data set sizes: 
	train 9771,
	valid 1522,
	test 1160
2020-07-01 14:08:33,479 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-01 14:08:33,479 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-01 14:08:33,479 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-07-01 14:08:33,479 Number of Src words (types): 36628
2020-07-01 14:08:33,479 Number of Trg words (types): 36628
2020-07-01 14:08:33,480 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-07-01 14:08:33,509 EPOCH 1
2020-07-01 14:08:57,024 Epoch   1 Step:  1360100 Batch Loss:     1.979052 Tokens per Sec:     5225, Lr: 0.000200
2020-07-01 14:09:03,199 Epoch   1: total training loss 464.25
2020-07-01 14:09:03,199 EPOCH 2
2020-07-01 14:09:20,649 Epoch   2 Step:  1360200 Batch Loss:     2.508060 Tokens per Sec:     5131, Lr: 0.000200
2020-07-01 14:09:32,976 Epoch   2: total training loss 187.30
2020-07-01 14:09:32,976 EPOCH 3
2020-07-01 14:09:44,258 Epoch   3 Step:  1360300 Batch Loss:     0.761336 Tokens per Sec:     5140, Lr: 0.000200
2020-07-01 14:10:03,479 Epoch   3: total training loss 142.94
2020-07-01 14:10:03,480 EPOCH 4
2020-07-01 14:10:08,432 Epoch   4 Step:  1360400 Batch Loss:     1.147389 Tokens per Sec:     5125, Lr: 0.000200
2020-07-01 14:10:32,518 Epoch   4 Step:  1360500 Batch Loss:     0.721931 Tokens per Sec:     4954, Lr: 0.000200
2020-07-01 14:10:34,095 Epoch   4: total training loss 120.88
2020-07-01 14:10:34,095 EPOCH 5
2020-07-01 14:10:56,090 Epoch   5 Step:  1360600 Batch Loss:     0.676147 Tokens per Sec:     5073, Lr: 0.000200
2020-07-01 14:11:04,993 Epoch   5: total training loss 108.02
2020-07-01 14:11:04,993 EPOCH 6
2020-07-01 14:11:20,381 Epoch   6 Step:  1360700 Batch Loss:     0.704095 Tokens per Sec:     4954, Lr: 0.000200
2020-07-01 14:11:35,524 Epoch   6: total training loss 94.17
2020-07-01 14:11:35,524 EPOCH 7
2020-07-01 14:11:44,476 Epoch   7 Step:  1360800 Batch Loss:     0.622253 Tokens per Sec:     5170, Lr: 0.000200
2020-07-01 14:12:06,276 Epoch   7: total training loss 87.53
2020-07-01 14:12:06,277 EPOCH 8
2020-07-01 14:12:08,559 Epoch   8 Step:  1360900 Batch Loss:     0.688686 Tokens per Sec:     5336, Lr: 0.000200
2020-07-01 14:12:32,567 Epoch   8 Step:  1361000 Batch Loss:     0.568934 Tokens per Sec:     5000, Lr: 0.000200
2020-07-01 14:13:13,200 Hooray! New best validation result [ppl]!
2020-07-01 14:13:13,200 Saving new checkpoint.
2020-07-01 14:13:24,027 Example #0
2020-07-01 14:13:24,027 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:13:24,028 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:13:24,028 	Source:     Hello .
2020-07-01 14:13:24,028 	Reference:  Hallo ,
2020-07-01 14:13:24,028 	Hypothesis: Hallo .
2020-07-01 14:13:24,028 Example #1
2020-07-01 14:13:24,028 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:13:24,028 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:13:24,028 	Source:     Hi , how can I help you ?
2020-07-01 14:13:24,028 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:13:24,028 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:13:24,028 Example #2
2020-07-01 14:13:24,028 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:13:24,028 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:13:24,028 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:13:24,029 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:13:24,029 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:13:24,029 Example #3
2020-07-01 14:13:24,029 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:13:24,029 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:13:24,029 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:13:24,029 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:13:24,029 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:13:24,029 Validation result (greedy) at epoch   8, step  1361000: bleu:  53.41, loss: 18434.1152, ppl:   2.1512, duration: 51.4614s
2020-07-01 14:13:28,179 Epoch   8: total training loss 81.03
2020-07-01 14:13:28,180 EPOCH 9
2020-07-01 14:13:47,657 Epoch   9 Step:  1361100 Batch Loss:     0.703833 Tokens per Sec:     5069, Lr: 0.000200
2020-07-01 14:13:58,667 Epoch   9: total training loss 76.29
2020-07-01 14:13:58,668 EPOCH 10
2020-07-01 14:14:11,433 Epoch  10 Step:  1361200 Batch Loss:     0.627598 Tokens per Sec:     4986, Lr: 0.000200
2020-07-01 14:14:29,174 Epoch  10: total training loss 69.47
2020-07-01 14:14:29,175 EPOCH 11
2020-07-01 14:14:34,903 Epoch  11 Step:  1361300 Batch Loss:     0.201237 Tokens per Sec:     5057, Lr: 0.000200
2020-07-01 14:14:59,284 Epoch  11 Step:  1361400 Batch Loss:     0.457166 Tokens per Sec:     5002, Lr: 0.000200
2020-07-01 14:14:59,820 Epoch  11: total training loss 65.13
2020-07-01 14:14:59,821 EPOCH 12
2020-07-01 14:15:23,886 Epoch  12 Step:  1361500 Batch Loss:     0.644098 Tokens per Sec:     4898, Lr: 0.000200
2020-07-01 14:15:30,817 Epoch  12: total training loss 61.55
2020-07-01 14:15:30,817 EPOCH 13
2020-07-01 14:15:47,882 Epoch  13 Step:  1361600 Batch Loss:     0.412410 Tokens per Sec:     5023, Lr: 0.000200
2020-07-01 14:16:01,397 Epoch  13: total training loss 56.70
2020-07-01 14:16:01,398 EPOCH 14
2020-07-01 14:16:12,121 Epoch  14 Step:  1361700 Batch Loss:     0.380351 Tokens per Sec:     5101, Lr: 0.000200
2020-07-01 14:16:31,938 Epoch  14: total training loss 52.56
2020-07-01 14:16:31,939 EPOCH 15
2020-07-01 14:16:36,294 Epoch  15 Step:  1361800 Batch Loss:     0.466109 Tokens per Sec:     4844, Lr: 0.000200
2020-07-01 14:17:00,362 Epoch  15 Step:  1361900 Batch Loss:     0.267511 Tokens per Sec:     4995, Lr: 0.000200
2020-07-01 14:17:02,542 Epoch  15: total training loss 49.93
2020-07-01 14:17:02,542 EPOCH 16
2020-07-01 14:17:24,149 Epoch  16 Step:  1362000 Batch Loss:     0.314694 Tokens per Sec:     5049, Lr: 0.000200
2020-07-01 14:18:19,569 Hooray! New best validation result [ppl]!
2020-07-01 14:18:19,570 Saving new checkpoint.
2020-07-01 14:18:30,128 Example #0
2020-07-01 14:18:30,129 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:18:30,129 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:18:30,129 	Source:     Hello .
2020-07-01 14:18:30,129 	Reference:  Hallo ,
2020-07-01 14:18:30,129 	Hypothesis: Hallo .
2020-07-01 14:18:30,129 Example #1
2020-07-01 14:18:30,129 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:18:30,129 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:18:30,129 	Source:     Hi , how can I help you ?
2020-07-01 14:18:30,129 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:18:30,129 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:18:30,129 Example #2
2020-07-01 14:18:30,130 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:18:30,130 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:18:30,130 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:18:30,130 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:18:30,130 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:18:30,130 Example #3
2020-07-01 14:18:30,130 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:18:30,130 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:18:30,130 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:18:30,130 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:18:30,130 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:18:30,130 Validation result (greedy) at epoch  16, step  1362000: bleu:  54.68, loss: 17895.9336, ppl:   2.1036, duration: 65.9801s
2020-07-01 14:18:38,731 Epoch  16: total training loss 47.05
2020-07-01 14:18:38,731 EPOCH 17
2020-07-01 14:18:53,498 Epoch  17 Step:  1362100 Batch Loss:     0.293420 Tokens per Sec:     5179, Lr: 0.000200
2020-07-01 14:19:09,272 Epoch  17: total training loss 45.64
2020-07-01 14:19:09,273 EPOCH 18
2020-07-01 14:19:17,626 Epoch  18 Step:  1362200 Batch Loss:     0.340131 Tokens per Sec:     5078, Lr: 0.000200
2020-07-01 14:19:39,948 Epoch  18: total training loss 42.74
2020-07-01 14:19:39,948 EPOCH 19
2020-07-01 14:19:41,624 Epoch  19 Step:  1362300 Batch Loss:     0.325759 Tokens per Sec:     5117, Lr: 0.000200
2020-07-01 14:20:05,472 Epoch  19 Step:  1362400 Batch Loss:     0.408562 Tokens per Sec:     4990, Lr: 0.000200
2020-07-01 14:20:10,671 Epoch  19: total training loss 40.77
2020-07-01 14:20:10,671 EPOCH 20
2020-07-01 14:20:29,585 Epoch  20 Step:  1362500 Batch Loss:     0.296391 Tokens per Sec:     4943, Lr: 0.000200
2020-07-01 14:20:41,410 Epoch  20: total training loss 38.37
2020-07-01 14:20:41,411 EPOCH 21
2020-07-01 14:20:53,924 Epoch  21 Step:  1362600 Batch Loss:     0.251272 Tokens per Sec:     4979, Lr: 0.000200
2020-07-01 14:21:12,067 Epoch  21: total training loss 35.95
2020-07-01 14:21:12,068 EPOCH 22
2020-07-01 14:21:18,007 Epoch  22 Step:  1362700 Batch Loss:     0.140540 Tokens per Sec:     5166, Lr: 0.000200
2020-07-01 14:21:42,396 Epoch  22 Step:  1362800 Batch Loss:     0.257425 Tokens per Sec:     4936, Lr: 0.000200
2020-07-01 14:21:42,861 Epoch  22: total training loss 34.18
2020-07-01 14:21:42,862 EPOCH 23
2020-07-01 14:22:06,457 Epoch  23 Step:  1362900 Batch Loss:     0.323452 Tokens per Sec:     5044, Lr: 0.000200
2020-07-01 14:22:13,424 Epoch  23: total training loss 33.12
2020-07-01 14:22:13,425 EPOCH 24
2020-07-01 14:22:30,424 Epoch  24 Step:  1363000 Batch Loss:     0.260107 Tokens per Sec:     4892, Lr: 0.000200
2020-07-01 14:23:31,768 Example #0
2020-07-01 14:23:31,769 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:23:31,769 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:23:31,769 	Source:     Hello .
2020-07-01 14:23:31,769 	Reference:  Hallo ,
2020-07-01 14:23:31,769 	Hypothesis: Hallo .
2020-07-01 14:23:31,769 Example #1
2020-07-01 14:23:31,769 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:23:31,769 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:23:31,769 	Source:     Hi , how can I help you ?
2020-07-01 14:23:31,769 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:23:31,769 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:23:31,769 Example #2
2020-07-01 14:23:31,769 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:23:31,769 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:23:31,769 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:23:31,769 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:23:31,769 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:23:31,769 Example #3
2020-07-01 14:23:31,769 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:23:31,769 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:23:31,770 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:23:31,770 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:23:31,770 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:23:31,770 Validation result (greedy) at epoch  24, step  1363000: bleu:  53.66, loss: 18540.4043, ppl:   2.1607, duration: 61.3449s
2020-07-01 14:23:45,448 Epoch  24: total training loss 31.67
2020-07-01 14:23:45,448 EPOCH 25
2020-07-01 14:23:55,829 Epoch  25 Step:  1363100 Batch Loss:     0.249018 Tokens per Sec:     4984, Lr: 0.000200
2020-07-01 14:24:16,160 Epoch  25: total training loss 30.36
2020-07-01 14:24:16,161 EPOCH 26
2020-07-01 14:24:19,836 Epoch  26 Step:  1363200 Batch Loss:     0.237088 Tokens per Sec:     4989, Lr: 0.000200
2020-07-01 14:24:43,576 Epoch  26 Step:  1363300 Batch Loss:     0.221671 Tokens per Sec:     5127, Lr: 0.000200
2020-07-01 14:24:46,565 Epoch  26: total training loss 28.78
2020-07-01 14:24:46,565 EPOCH 27
2020-07-01 14:25:07,430 Epoch  27 Step:  1363400 Batch Loss:     0.253481 Tokens per Sec:     5124, Lr: 0.000200
2020-07-01 14:25:17,057 Epoch  27: total training loss 27.19
2020-07-01 14:25:17,057 EPOCH 28
2020-07-01 14:25:32,047 Epoch  28 Step:  1363500 Batch Loss:     0.226888 Tokens per Sec:     4984, Lr: 0.000200
2020-07-01 14:25:47,303 Epoch  28: total training loss 26.20
2020-07-01 14:25:47,304 EPOCH 29
2020-07-01 14:25:55,853 Epoch  29 Step:  1363600 Batch Loss:     0.217088 Tokens per Sec:     4924, Lr: 0.000200
2020-07-01 14:26:17,362 Epoch  29: total training loss 24.97
2020-07-01 14:26:17,363 EPOCH 30
2020-07-01 14:26:19,235 Epoch  30 Step:  1363700 Batch Loss:     0.181044 Tokens per Sec:     4307, Lr: 0.000200
2020-07-01 14:26:43,458 Epoch  30 Step:  1363800 Batch Loss:     0.168720 Tokens per Sec:     5078, Lr: 0.000200
2020-07-01 14:26:47,908 Epoch  30: total training loss 24.13
2020-07-01 14:26:47,908 EPOCH 31
2020-07-01 14:27:06,794 Epoch  31 Step:  1363900 Batch Loss:     0.196451 Tokens per Sec:     5267, Lr: 0.000200
2020-07-01 14:27:18,105 Epoch  31: total training loss 23.22
2020-07-01 14:27:18,105 EPOCH 32
2020-07-01 14:27:31,117 Epoch  32 Step:  1364000 Batch Loss:     0.185128 Tokens per Sec:     5025, Lr: 0.000200
2020-07-01 14:28:16,319 Example #0
2020-07-01 14:28:16,319 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:28:16,320 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:28:16,320 	Source:     Hello .
2020-07-01 14:28:16,320 	Reference:  Hallo ,
2020-07-01 14:28:16,320 	Hypothesis: Hallo .
2020-07-01 14:28:16,320 Example #1
2020-07-01 14:28:16,320 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:28:16,320 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:28:16,320 	Source:     Hi , how can I help you ?
2020-07-01 14:28:16,320 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:28:16,320 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:28:16,320 Example #2
2020-07-01 14:28:16,320 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:28:16,320 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:28:16,320 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:28:16,320 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:28:16,320 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:28:16,320 Example #3
2020-07-01 14:28:16,320 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:28:16,320 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:28:16,320 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:28:16,320 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:28:16,320 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:28:16,320 Validation result (greedy) at epoch  32, step  1364000: bleu:  54.10, loss: 19603.9727, ppl:   2.2583, duration: 45.2024s
2020-07-01 14:28:33,882 Epoch  32: total training loss 22.18
2020-07-01 14:28:33,883 EPOCH 33
2020-07-01 14:28:40,180 Epoch  33 Step:  1364100 Batch Loss:     0.167281 Tokens per Sec:     5300, Lr: 0.000200
2020-07-01 14:29:03,789 Epoch  33 Step:  1364200 Batch Loss:     0.161340 Tokens per Sec:     5033, Lr: 0.000200
2020-07-01 14:29:04,042 Epoch  33: total training loss 21.64
2020-07-01 14:29:04,042 EPOCH 34
2020-07-01 14:29:28,103 Epoch  34 Step:  1364300 Batch Loss:     0.148350 Tokens per Sec:     4988, Lr: 0.000200
2020-07-01 14:29:34,369 Epoch  34: total training loss 20.60
2020-07-01 14:29:34,369 EPOCH 35
2020-07-01 14:29:51,779 Epoch  35 Step:  1364400 Batch Loss:     0.167208 Tokens per Sec:     5092, Lr: 0.000200
2020-07-01 14:30:04,739 Epoch  35: total training loss 19.91
2020-07-01 14:30:04,740 EPOCH 36
2020-07-01 14:30:15,899 Epoch  36 Step:  1364500 Batch Loss:     0.136851 Tokens per Sec:     5163, Lr: 0.000200
2020-07-01 14:30:35,205 Epoch  36: total training loss 19.54
2020-07-01 14:30:35,205 EPOCH 37
2020-07-01 14:30:39,432 Epoch  37 Step:  1364600 Batch Loss:     0.138929 Tokens per Sec:     5498, Lr: 0.000200
2020-07-01 14:31:03,711 Epoch  37 Step:  1364700 Batch Loss:     0.147825 Tokens per Sec:     4972, Lr: 0.000200
2020-07-01 14:31:06,120 Epoch  37: total training loss 18.98
2020-07-01 14:31:06,121 EPOCH 38
2020-07-01 14:31:27,642 Epoch  38 Step:  1364800 Batch Loss:     0.132697 Tokens per Sec:     5127, Lr: 0.000200
2020-07-01 14:31:36,439 Epoch  38: total training loss 18.21
2020-07-01 14:31:36,440 EPOCH 39
2020-07-01 14:31:52,099 Epoch  39 Step:  1364900 Batch Loss:     0.157548 Tokens per Sec:     4948, Lr: 0.000200
2020-07-01 14:32:07,130 Epoch  39: total training loss 17.92
2020-07-01 14:32:07,131 EPOCH 40
2020-07-01 14:32:15,547 Epoch  40 Step:  1365000 Batch Loss:     0.107585 Tokens per Sec:     5127, Lr: 0.000200
2020-07-01 14:33:03,253 Example #0
2020-07-01 14:33:03,254 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:33:03,254 	Raw hypothesis: ['Hall@@', 'o', '!']
2020-07-01 14:33:03,254 	Source:     Hello .
2020-07-01 14:33:03,254 	Reference:  Hallo ,
2020-07-01 14:33:03,254 	Hypothesis: Hallo !
2020-07-01 14:33:03,254 Example #1
2020-07-01 14:33:03,254 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:33:03,254 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:33:03,254 	Source:     Hi , how can I help you ?
2020-07-01 14:33:03,254 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:33:03,254 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:33:03,254 Example #2
2020-07-01 14:33:03,254 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:33:03,254 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:33:03,254 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:33:03,254 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:33:03,254 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:33:03,254 Example #3
2020-07-01 14:33:03,254 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:33:03,254 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:33:03,254 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:33:03,254 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:33:03,254 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:33:03,254 Validation result (greedy) at epoch  40, step  1365000: bleu:  53.69, loss: 20374.3398, ppl:   2.3318, duration: 47.7054s
2020-07-01 14:33:24,926 Epoch  40: total training loss 17.46
2020-07-01 14:33:24,927 EPOCH 41
2020-07-01 14:33:27,034 Epoch  41 Step:  1365100 Batch Loss:     0.116263 Tokens per Sec:     5061, Lr: 0.000200
2020-07-01 14:33:50,694 Epoch  41 Step:  1365200 Batch Loss:     0.125249 Tokens per Sec:     5099, Lr: 0.000200
2020-07-01 14:33:55,129 Epoch  41: total training loss 16.72
2020-07-01 14:33:55,129 EPOCH 42
2020-07-01 14:34:14,169 Epoch  42 Step:  1365300 Batch Loss:     0.150700 Tokens per Sec:     5026, Lr: 0.000200
2020-07-01 14:34:25,564 Epoch  42: total training loss 16.43
2020-07-01 14:34:25,564 EPOCH 43
2020-07-01 14:34:37,707 Epoch  43 Step:  1365400 Batch Loss:     0.136846 Tokens per Sec:     5250, Lr: 0.000200
2020-07-01 14:34:55,859 Epoch  43: total training loss 15.60
2020-07-01 14:34:55,860 EPOCH 44
2020-07-01 14:35:01,855 Epoch  44 Step:  1365500 Batch Loss:     0.106282 Tokens per Sec:     4953, Lr: 0.000200
2020-07-01 14:35:25,659 Epoch  44 Step:  1365600 Batch Loss:     0.136785 Tokens per Sec:     5054, Lr: 0.000200
2020-07-01 14:35:26,373 Epoch  44: total training loss 15.85
2020-07-01 14:35:26,373 EPOCH 45
2020-07-01 14:35:49,153 Epoch  45 Step:  1365700 Batch Loss:     0.130618 Tokens per Sec:     5118, Lr: 0.000200
2020-07-01 14:35:56,253 Epoch  45: total training loss 15.21
2020-07-01 14:35:56,254 EPOCH 46
2020-07-01 14:36:13,208 Epoch  46 Step:  1365800 Batch Loss:     0.115372 Tokens per Sec:     5093, Lr: 0.000200
2020-07-01 14:36:26,465 Epoch  46: total training loss 14.88
2020-07-01 14:36:26,466 EPOCH 47
2020-07-01 14:36:37,024 Epoch  47 Step:  1365900 Batch Loss:     0.102788 Tokens per Sec:     4929, Lr: 0.000200
2020-07-01 14:36:56,654 Epoch  47: total training loss 14.32
2020-07-01 14:36:56,655 EPOCH 48
2020-07-01 14:37:00,633 Epoch  48 Step:  1366000 Batch Loss:     0.091235 Tokens per Sec:     4533, Lr: 0.000200
2020-07-01 14:37:44,044 Example #0
2020-07-01 14:37:44,044 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:37:44,044 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:37:44,044 	Source:     Hello .
2020-07-01 14:37:44,044 	Reference:  Hallo ,
2020-07-01 14:37:44,044 	Hypothesis: Hallo .
2020-07-01 14:37:44,044 Example #1
2020-07-01 14:37:44,044 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:37:44,044 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:37:44,044 	Source:     Hi , how can I help you ?
2020-07-01 14:37:44,044 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:37:44,044 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:37:44,045 Example #2
2020-07-01 14:37:44,045 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:37:44,045 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:37:44,045 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:37:44,045 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:37:44,045 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:37:44,045 Example #3
2020-07-01 14:37:44,045 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:37:44,045 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:37:44,045 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:37:44,045 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:37:44,045 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:37:44,045 Validation result (greedy) at epoch  48, step  1366000: bleu:  53.83, loss: 20873.7480, ppl:   2.3807, duration: 43.4115s
2020-07-01 14:38:07,906 Epoch  48 Step:  1366100 Batch Loss:     0.113744 Tokens per Sec:     5109, Lr: 0.000100
2020-07-01 14:38:10,364 Epoch  48: total training loss 13.35
2020-07-01 14:38:10,364 EPOCH 49
2020-07-01 14:38:31,955 Epoch  49 Step:  1366200 Batch Loss:     0.100941 Tokens per Sec:     4894, Lr: 0.000100
2020-07-01 14:38:40,824 Epoch  49: total training loss 12.58
2020-07-01 14:38:40,825 EPOCH 50
2020-07-01 14:38:55,781 Epoch  50 Step:  1366300 Batch Loss:     0.088247 Tokens per Sec:     4921, Lr: 0.000100
2020-07-01 14:39:11,300 Epoch  50: total training loss 12.15
2020-07-01 14:39:11,300 EPOCH 51
2020-07-01 14:39:19,299 Epoch  51 Step:  1366400 Batch Loss:     0.102112 Tokens per Sec:     4888, Lr: 0.000100
2020-07-01 14:39:41,313 Epoch  51: total training loss 12.33
2020-07-01 14:39:41,314 EPOCH 52
2020-07-01 14:39:42,367 Epoch  52 Step:  1366500 Batch Loss:     0.088530 Tokens per Sec:     5736, Lr: 0.000100
2020-07-01 14:40:06,395 Epoch  52 Step:  1366600 Batch Loss:     0.086427 Tokens per Sec:     5040, Lr: 0.000100
2020-07-01 14:40:11,619 Epoch  52: total training loss 11.82
2020-07-01 14:40:11,619 EPOCH 53
2020-07-01 14:40:30,179 Epoch  53 Step:  1366700 Batch Loss:     0.078896 Tokens per Sec:     5150, Lr: 0.000100
2020-07-01 14:40:41,617 Epoch  53: total training loss 11.56
2020-07-01 14:40:41,618 EPOCH 54
2020-07-01 14:40:53,723 Epoch  54 Step:  1366800 Batch Loss:     0.090885 Tokens per Sec:     5062, Lr: 0.000100
2020-07-01 14:41:11,550 Epoch  54: total training loss 11.18
2020-07-01 14:41:11,550 EPOCH 55
2020-07-01 14:41:17,235 Epoch  55 Step:  1366900 Batch Loss:     0.080791 Tokens per Sec:     5392, Lr: 0.000100
2020-07-01 14:41:41,142 Epoch  55 Step:  1367000 Batch Loss:     0.091231 Tokens per Sec:     5033, Lr: 0.000100
2020-07-01 14:42:22,971 Example #0
2020-07-01 14:42:22,971 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:42:22,971 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:42:22,971 	Source:     Hello .
2020-07-01 14:42:22,971 	Reference:  Hallo ,
2020-07-01 14:42:22,971 	Hypothesis: Hallo .
2020-07-01 14:42:22,971 Example #1
2020-07-01 14:42:22,972 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:42:22,972 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:42:22,972 	Source:     Hi , how can I help you ?
2020-07-01 14:42:22,972 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:42:22,972 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:42:22,972 Example #2
2020-07-01 14:42:22,972 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:42:22,972 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:42:22,972 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:42:22,972 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:42:22,972 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:42:22,972 Example #3
2020-07-01 14:42:22,972 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:42:22,972 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:42:22,972 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:42:22,972 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:42:22,972 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:42:22,972 Validation result (greedy) at epoch  55, step  1367000: bleu:  54.40, loss: 21357.7012, ppl:   2.4291, duration: 41.8295s
2020-07-01 14:42:23,362 Epoch  55: total training loss 11.12
2020-07-01 14:42:23,362 EPOCH 56
2020-07-01 14:42:46,888 Epoch  56 Step:  1367100 Batch Loss:     0.084641 Tokens per Sec:     5130, Lr: 0.000100
2020-07-01 14:42:53,527 Epoch  56: total training loss 10.80
2020-07-01 14:42:53,528 EPOCH 57
2020-07-01 14:43:10,225 Epoch  57 Step:  1367200 Batch Loss:     0.080084 Tokens per Sec:     5260, Lr: 0.000100
2020-07-01 14:43:23,185 Epoch  57: total training loss 10.67
2020-07-01 14:43:23,185 EPOCH 58
2020-07-01 14:43:33,570 Epoch  58 Step:  1367300 Batch Loss:     0.085242 Tokens per Sec:     5195, Lr: 0.000100
2020-07-01 14:43:53,326 Epoch  58: total training loss 10.74
2020-07-01 14:43:53,326 EPOCH 59
2020-07-01 14:43:57,576 Epoch  59 Step:  1367400 Batch Loss:     0.081625 Tokens per Sec:     5434, Lr: 0.000100
2020-07-01 14:44:21,651 Epoch  59 Step:  1367500 Batch Loss:     0.073738 Tokens per Sec:     4948, Lr: 0.000100
2020-07-01 14:44:23,417 Epoch  59: total training loss 10.67
2020-07-01 14:44:23,417 EPOCH 60
2020-07-01 14:44:44,853 Epoch  60 Step:  1367600 Batch Loss:     0.094733 Tokens per Sec:     5145, Lr: 0.000100
2020-07-01 14:44:53,186 Epoch  60: total training loss 10.38
2020-07-01 14:44:53,187 EPOCH 61
2020-07-01 14:45:08,609 Epoch  61 Step:  1367700 Batch Loss:     0.077860 Tokens per Sec:     5136, Lr: 0.000100
2020-07-01 14:45:22,789 Epoch  61: total training loss 10.18
2020-07-01 14:45:22,789 EPOCH 62
2020-07-01 14:45:31,366 Epoch  62 Step:  1367800 Batch Loss:     0.075688 Tokens per Sec:     5333, Lr: 0.000100
2020-07-01 14:45:52,337 Epoch  62: total training loss 10.25
2020-07-01 14:45:52,337 EPOCH 63
2020-07-01 14:45:54,670 Epoch  63 Step:  1367900 Batch Loss:     0.089465 Tokens per Sec:     5665, Lr: 0.000100
2020-07-01 14:46:18,824 Epoch  63 Step:  1368000 Batch Loss:     0.079001 Tokens per Sec:     5002, Lr: 0.000100
2020-07-01 14:47:01,460 Example #0
2020-07-01 14:47:01,460 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:47:01,460 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:47:01,460 	Source:     Hello .
2020-07-01 14:47:01,460 	Reference:  Hallo ,
2020-07-01 14:47:01,460 	Hypothesis: Hallo .
2020-07-01 14:47:01,460 Example #1
2020-07-01 14:47:01,460 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:47:01,461 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:47:01,461 	Source:     Hi , how can I help you ?
2020-07-01 14:47:01,461 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:47:01,461 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:47:01,461 Example #2
2020-07-01 14:47:01,461 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:47:01,461 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:47:01,461 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:47:01,461 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:47:01,461 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:47:01,461 Example #3
2020-07-01 14:47:01,461 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:47:01,461 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:47:01,461 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:47:01,461 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:47:01,461 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:47:01,461 Validation result (greedy) at epoch  63, step  1368000: bleu:  54.03, loss: 21718.4844, ppl:   2.4657, duration: 42.6356s
2020-07-01 14:47:05,473 Epoch  63: total training loss 10.14
2020-07-01 14:47:05,474 EPOCH 64
2020-07-01 14:47:25,411 Epoch  64 Step:  1368100 Batch Loss:     0.122928 Tokens per Sec:     5022, Lr: 0.000100
2020-07-01 14:47:35,983 Epoch  64: total training loss 9.93
2020-07-01 14:47:35,984 EPOCH 65
2020-07-01 14:47:49,550 Epoch  65 Step:  1368200 Batch Loss:     0.067212 Tokens per Sec:     4947, Lr: 0.000100
2020-07-01 14:48:05,204 Epoch  65: total training loss 9.84
2020-07-01 14:48:05,205 EPOCH 66
2020-07-01 14:48:11,537 Epoch  66 Step:  1368300 Batch Loss:     0.077418 Tokens per Sec:     5551, Lr: 0.000100
2020-07-01 14:48:33,898 Epoch  66: total training loss 9.81
2020-07-01 14:48:33,899 EPOCH 67
2020-07-01 14:48:34,124 Epoch  67 Step:  1368400 Batch Loss:     0.060217 Tokens per Sec:     6231, Lr: 0.000100
2020-07-01 14:48:57,104 Epoch  67 Step:  1368500 Batch Loss:     0.073037 Tokens per Sec:     5211, Lr: 0.000100
2020-07-01 14:49:02,812 Epoch  67: total training loss 9.69
2020-07-01 14:49:02,812 EPOCH 68
2020-07-01 14:49:19,660 Epoch  68 Step:  1368600 Batch Loss:     0.070727 Tokens per Sec:     5331, Lr: 0.000100
2020-07-01 14:49:31,650 Epoch  68: total training loss 9.43
2020-07-01 14:49:31,651 EPOCH 69
2020-07-01 14:49:42,335 Epoch  69 Step:  1368700 Batch Loss:     0.069599 Tokens per Sec:     5200, Lr: 0.000100
2020-07-01 14:50:00,406 Epoch  69: total training loss 9.47
2020-07-01 14:50:00,407 EPOCH 70
2020-07-01 14:50:04,936 Epoch  70 Step:  1368800 Batch Loss:     0.063122 Tokens per Sec:     4933, Lr: 0.000100
2020-07-01 14:50:27,789 Epoch  70 Step:  1368900 Batch Loss:     0.090847 Tokens per Sec:     5351, Lr: 0.000100
2020-07-01 14:50:29,341 Epoch  70: total training loss 9.48
2020-07-01 14:50:29,341 EPOCH 71
2020-07-01 14:50:50,366 Epoch  71 Step:  1369000 Batch Loss:     0.065760 Tokens per Sec:     5267, Lr: 0.000100
2020-07-01 14:51:33,161 Example #0
2020-07-01 14:51:33,162 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:51:33,162 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:51:33,162 	Source:     Hello .
2020-07-01 14:51:33,162 	Reference:  Hallo ,
2020-07-01 14:51:33,162 	Hypothesis: Hallo .
2020-07-01 14:51:33,162 Example #1
2020-07-01 14:51:33,162 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:51:33,162 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:51:33,162 	Source:     Hi , how can I help you ?
2020-07-01 14:51:33,162 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:51:33,162 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:51:33,162 Example #2
2020-07-01 14:51:33,162 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:51:33,162 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:51:33,162 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:51:33,162 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:51:33,162 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:51:33,163 Example #3
2020-07-01 14:51:33,163 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:51:33,163 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:51:33,163 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:51:33,163 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:51:33,163 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:51:33,163 Validation result (greedy) at epoch  71, step  1369000: bleu:  54.18, loss: 21954.9316, ppl:   2.4901, duration: 42.7956s
2020-07-01 14:51:41,292 Epoch  71: total training loss 9.18
2020-07-01 14:51:41,293 EPOCH 72
2020-07-01 14:51:56,534 Epoch  72 Step:  1369100 Batch Loss:     0.068315 Tokens per Sec:     5126, Lr: 0.000100
2020-07-01 14:52:11,326 Epoch  72: total training loss 9.19
2020-07-01 14:52:11,327 EPOCH 73
2020-07-01 14:52:19,740 Epoch  73 Step:  1369200 Batch Loss:     0.097613 Tokens per Sec:     5124, Lr: 0.000100
2020-07-01 14:52:41,266 Epoch  73: total training loss 9.15
2020-07-01 14:52:41,267 EPOCH 74
2020-07-01 14:52:43,236 Epoch  74 Step:  1369300 Batch Loss:     0.056711 Tokens per Sec:     5135, Lr: 0.000100
2020-07-01 14:53:07,120 Epoch  74 Step:  1369400 Batch Loss:     0.082597 Tokens per Sec:     5152, Lr: 0.000100
2020-07-01 14:53:11,103 Epoch  74: total training loss 8.77
2020-07-01 14:53:11,103 EPOCH 75
2020-07-01 14:53:31,222 Epoch  75 Step:  1369500 Batch Loss:     0.064905 Tokens per Sec:     5012, Lr: 0.000100
2020-07-01 14:53:41,233 Epoch  75: total training loss 8.83
2020-07-01 14:53:41,234 EPOCH 76
2020-07-01 14:53:54,799 Epoch  76 Step:  1369600 Batch Loss:     0.099902 Tokens per Sec:     5049, Lr: 0.000100
2020-07-01 14:54:11,576 Epoch  76: total training loss 8.88
2020-07-01 14:54:11,577 EPOCH 77
2020-07-01 14:54:18,974 Epoch  77 Step:  1369700 Batch Loss:     0.059652 Tokens per Sec:     4890, Lr: 0.000100
2020-07-01 14:54:41,996 Epoch  77: total training loss 8.66
2020-07-01 14:54:41,997 EPOCH 78
2020-07-01 14:54:42,462 Epoch  78 Step:  1369800 Batch Loss:     0.060443 Tokens per Sec:     5460, Lr: 0.000100
2020-07-01 14:55:05,909 Epoch  78 Step:  1369900 Batch Loss:     0.058130 Tokens per Sec:     5078, Lr: 0.000100
2020-07-01 14:55:12,124 Epoch  78: total training loss 8.68
2020-07-01 14:55:12,124 EPOCH 79
2020-07-01 14:55:30,120 Epoch  79 Step:  1370000 Batch Loss:     0.069951 Tokens per Sec:     4954, Lr: 0.000100
2020-07-01 14:56:11,402 Example #0
2020-07-01 14:56:11,402 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 14:56:11,403 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 14:56:11,403 	Source:     Hello .
2020-07-01 14:56:11,403 	Reference:  Hallo ,
2020-07-01 14:56:11,403 	Hypothesis: Hallo .
2020-07-01 14:56:11,403 Example #1
2020-07-01 14:56:11,403 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 14:56:11,403 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 14:56:11,403 	Source:     Hi , how can I help you ?
2020-07-01 14:56:11,403 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:56:11,403 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 14:56:11,403 Example #2
2020-07-01 14:56:11,403 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 14:56:11,403 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 14:56:11,403 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 14:56:11,403 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 14:56:11,403 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 14:56:11,403 Example #3
2020-07-01 14:56:11,403 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 14:56:11,404 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 14:56:11,404 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 14:56:11,404 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 14:56:11,404 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 14:56:11,404 Validation result (greedy) at epoch  79, step  1370000: bleu:  54.10, loss: 22356.7324, ppl:   2.5320, duration: 41.2830s
2020-07-01 14:56:24,121 Epoch  79: total training loss 8.47
2020-07-01 14:56:24,121 EPOCH 80
2020-07-01 14:56:35,439 Epoch  80 Step:  1370100 Batch Loss:     0.054834 Tokens per Sec:     5099, Lr: 0.000050
2020-07-01 14:56:54,442 Epoch  80: total training loss 8.01
2020-07-01 14:56:54,443 EPOCH 81
2020-07-01 14:56:59,128 Epoch  81 Step:  1370200 Batch Loss:     0.057600 Tokens per Sec:     5255, Lr: 0.000050
2020-07-01 14:57:22,870 Epoch  81 Step:  1370300 Batch Loss:     0.080341 Tokens per Sec:     4985, Lr: 0.000050
2020-07-01 14:57:24,567 Epoch  81: total training loss 8.01
2020-07-01 14:57:24,567 EPOCH 82
2020-07-01 14:57:46,510 Epoch  82 Step:  1370400 Batch Loss:     0.085081 Tokens per Sec:     5008, Lr: 0.000050
2020-07-01 14:57:54,555 Epoch  82: total training loss 7.83
2020-07-01 14:57:54,555 EPOCH 83
2020-07-01 14:58:09,950 Epoch  83 Step:  1370500 Batch Loss:     0.070134 Tokens per Sec:     5112, Lr: 0.000050
2020-07-01 14:58:25,251 Epoch  83: total training loss 7.83
2020-07-01 14:58:25,251 EPOCH 84
2020-07-01 14:58:34,041 Epoch  84 Step:  1370600 Batch Loss:     0.059652 Tokens per Sec:     5095, Lr: 0.000050
2020-07-01 14:58:55,602 Epoch  84: total training loss 7.64
2020-07-01 14:58:55,603 EPOCH 85
2020-07-01 14:58:58,301 Epoch  85 Step:  1370700 Batch Loss:     0.052108 Tokens per Sec:     4983, Lr: 0.000050
2020-07-01 14:59:22,052 Epoch  85 Step:  1370800 Batch Loss:     0.051181 Tokens per Sec:     5172, Lr: 0.000050
2020-07-01 14:59:25,794 Epoch  85: total training loss 7.51
2020-07-01 14:59:25,794 EPOCH 86
2020-07-01 14:59:45,793 Epoch  86 Step:  1370900 Batch Loss:     0.051160 Tokens per Sec:     5077, Lr: 0.000050
2020-07-01 14:59:56,098 Epoch  86: total training loss 7.65
2020-07-01 14:59:56,099 EPOCH 87
2020-07-01 15:00:09,139 Epoch  87 Step:  1371000 Batch Loss:     0.058467 Tokens per Sec:     5146, Lr: 0.000050
2020-07-01 15:00:50,978 Example #0
2020-07-01 15:00:50,979 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:00:50,979 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:00:50,979 	Source:     Hello .
2020-07-01 15:00:50,979 	Reference:  Hallo ,
2020-07-01 15:00:50,979 	Hypothesis: Hallo .
2020-07-01 15:00:50,979 Example #1
2020-07-01 15:00:50,980 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:00:50,980 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:00:50,980 	Source:     Hi , how can I help you ?
2020-07-01 15:00:50,980 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:00:50,980 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:00:50,980 Example #2
2020-07-01 15:00:50,980 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:00:50,980 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:00:50,980 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:00:50,980 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:00:50,980 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:00:50,980 Example #3
2020-07-01 15:00:50,980 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:00:50,980 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:00:50,980 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:00:50,980 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:00:50,980 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:00:50,980 Validation result (greedy) at epoch  87, step  1371000: bleu:  54.26, loss: 22343.3750, ppl:   2.5306, duration: 41.8400s
2020-07-01 15:01:08,245 Epoch  87: total training loss 7.60
2020-07-01 15:01:08,245 EPOCH 88
2020-07-01 15:01:15,231 Epoch  88 Step:  1371100 Batch Loss:     0.072044 Tokens per Sec:     5072, Lr: 0.000050
2020-07-01 15:01:38,476 Epoch  88: total training loss 7.51
2020-07-01 15:01:38,477 EPOCH 89
2020-07-01 15:01:38,875 Epoch  89 Step:  1371200 Batch Loss:     0.047436 Tokens per Sec:     6024, Lr: 0.000050
2020-07-01 15:02:02,661 Epoch  89 Step:  1371300 Batch Loss:     0.056937 Tokens per Sec:     5087, Lr: 0.000050
2020-07-01 15:02:08,593 Epoch  89: total training loss 7.47
2020-07-01 15:02:08,594 EPOCH 90
2020-07-01 15:02:26,336 Epoch  90 Step:  1371400 Batch Loss:     0.067146 Tokens per Sec:     5126, Lr: 0.000050
2020-07-01 15:02:38,918 Epoch  90: total training loss 7.48
2020-07-01 15:02:38,918 EPOCH 91
2020-07-01 15:02:50,088 Epoch  91 Step:  1371500 Batch Loss:     0.052672 Tokens per Sec:     5025, Lr: 0.000050
2020-07-01 15:03:09,070 Epoch  91: total training loss 7.49
2020-07-01 15:03:09,070 EPOCH 92
2020-07-01 15:03:13,467 Epoch  92 Step:  1371600 Batch Loss:     0.061797 Tokens per Sec:     4762, Lr: 0.000050
2020-07-01 15:03:37,100 Epoch  92 Step:  1371700 Batch Loss:     0.063679 Tokens per Sec:     5145, Lr: 0.000050
2020-07-01 15:03:39,220 Epoch  92: total training loss 7.30
2020-07-01 15:03:39,220 EPOCH 93
2020-07-01 15:04:00,227 Epoch  93 Step:  1371800 Batch Loss:     0.050986 Tokens per Sec:     5174, Lr: 0.000050
2020-07-01 15:04:09,447 Epoch  93: total training loss 7.37
2020-07-01 15:04:09,447 EPOCH 94
2020-07-01 15:04:24,218 Epoch  94 Step:  1371900 Batch Loss:     0.048004 Tokens per Sec:     4936, Lr: 0.000050
2020-07-01 15:04:40,087 Epoch  94: total training loss 7.38
2020-07-01 15:04:40,088 EPOCH 95
2020-07-01 15:04:48,302 Epoch  95 Step:  1372000 Batch Loss:     0.044992 Tokens per Sec:     4948, Lr: 0.000050
2020-07-01 15:05:30,351 Example #0
2020-07-01 15:05:30,351 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 15:05:30,351 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 15:05:30,351 	Source:     Hello .
2020-07-01 15:05:30,351 	Reference:  Hallo ,
2020-07-01 15:05:30,351 	Hypothesis: Hallo .
2020-07-01 15:05:30,351 Example #1
2020-07-01 15:05:30,351 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 15:05:30,352 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 15:05:30,352 	Source:     Hi , how can I help you ?
2020-07-01 15:05:30,352 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:05:30,352 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 15:05:30,352 Example #2
2020-07-01 15:05:30,352 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 15:05:30,352 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 15:05:30,352 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 15:05:30,352 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 15:05:30,352 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 15:05:30,352 Example #3
2020-07-01 15:05:30,352 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 15:05:30,352 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 15:05:30,352 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 15:05:30,352 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 15:05:30,352 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 15:05:30,352 Validation result (greedy) at epoch  95, step  1372000: bleu:  54.09, loss: 22465.6387, ppl:   2.5435, duration: 42.0494s
2020-07-01 15:05:52,381 Epoch  95: total training loss 7.13
2020-07-01 15:05:52,382 EPOCH 96
2020-07-01 15:05:54,007 Epoch  96 Step:  1372100 Batch Loss:     0.052969 Tokens per Sec:     6018, Lr: 0.000050
2020-07-01 15:06:18,047 Epoch  96 Step:  1372200 Batch Loss:     0.052452 Tokens per Sec:     5009, Lr: 0.000050
2020-07-01 15:06:22,766 Epoch  96: total training loss 7.15
2020-07-01 15:06:22,766 EPOCH 97
2020-07-01 15:06:41,835 Epoch  97 Step:  1372300 Batch Loss:     0.048435 Tokens per Sec:     5107, Lr: 0.000050
2020-07-01 15:06:52,925 Epoch  97: total training loss 7.13
2020-07-01 15:06:52,925 EPOCH 98
2020-07-01 15:07:05,684 Epoch  98 Step:  1372400 Batch Loss:     0.051528 Tokens per Sec:     5319, Lr: 0.000050
2020-07-01 15:07:23,133 Epoch  98: total training loss 7.09
2020-07-01 15:07:23,134 EPOCH 99
2020-07-01 15:07:29,121 Epoch  99 Step:  1372500 Batch Loss:     0.044636 Tokens per Sec:     5132, Lr: 0.000050
2020-07-01 15:07:53,205 Epoch  99 Step:  1372600 Batch Loss:     0.053254 Tokens per Sec:     5049, Lr: 0.000050
2020-07-01 15:07:53,486 Epoch  99: total training loss 7.14
2020-07-01 15:07:53,486 EPOCH 100
2020-07-01 15:08:17,438 Epoch 100 Step:  1372700 Batch Loss:     0.054214 Tokens per Sec:     5028, Lr: 0.000050
2020-07-01 15:08:23,956 Epoch 100: total training loss 7.01
2020-07-01 15:08:23,956 Training ended after 100 epochs.
2020-07-01 15:08:23,956 Best validation result (greedy) at step  1362000:   2.10 ppl.
2020-07-01 15:09:20,610  dev bleu:  55.44 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 15:09:20,613 Translations saved to: models/transformer_multi_enc_lr0.0002p3d0.5_ende-tune/01362000.hyps.dev
2020-07-01 15:09:44,948 test bleu:  51.44 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 15:09:44,952 Translations saved to: models/transformer_multi_enc_lr0.0002p3d0.5_ende-tune/01362000.hyps.test
