2020-05-27 18:01:05,030 Hello! This is Joey-NMT.
2020-05-27 18:01:12,758 Total params: 62894080
2020-05-27 18:01:12,760 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2020-05-27 18:01:14,952 Loading model from models/wmt_ende_transformer/best.ckpt
2020-05-27 18:01:15,268 Reset optimizer.
2020-05-27 18:01:15,354 Reset scheduler.
2020-05-27 18:01:15,354 Reset tracking of the best checkpoint.
2020-05-27 18:01:15,362 cfg.name                           : transformer
2020-05-27 18:01:15,362 cfg.data.src                       : en
2020-05-27 18:01:15,362 cfg.data.trg                       : de
2020-05-27 18:01:15,362 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-05-27 18:01:15,362 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-05-27 18:01:15,362 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-05-27 18:01:15,362 cfg.data.level                     : bpe
2020-05-27 18:01:15,363 cfg.data.lowercase                 : False
2020-05-27 18:01:15,363 cfg.data.max_sent_length           : 100
2020-05-27 18:01:15,363 cfg.data.src_vocab                 : models/wmt_ende_transformer/src_vocab.txt
2020-05-27 18:01:15,363 cfg.data.trg_vocab                 : models/wmt_ende_transformer/trg_vocab.txt
2020-05-27 18:01:15,363 cfg.testing.beam_size              : 5
2020-05-27 18:01:15,363 cfg.testing.alpha                  : 1.0
2020-05-27 18:01:15,363 cfg.training.random_seed           : 42
2020-05-27 18:01:15,363 cfg.training.optimizer             : adam
2020-05-27 18:01:15,363 cfg.training.normalization         : tokens
2020-05-27 18:01:15,363 cfg.training.adam_betas            : [0.9, 0.999]
2020-05-27 18:01:15,363 cfg.training.scheduling            : plateau
2020-05-27 18:01:15,363 cfg.training.patience              : 8
2020-05-27 18:01:15,363 cfg.training.decrease_factor       : 0.7
2020-05-27 18:01:15,363 cfg.training.loss                  : crossentropy
2020-05-27 18:01:15,363 cfg.training.learning_rate         : 0.0002
2020-05-27 18:01:15,363 cfg.training.learning_rate_min     : 1e-08
2020-05-27 18:01:15,364 cfg.training.weight_decay          : 0.0
2020-05-27 18:01:15,364 cfg.training.label_smoothing       : 0.1
2020-05-27 18:01:15,364 cfg.training.batch_size            : 4096
2020-05-27 18:01:15,364 cfg.training.batch_type            : token
2020-05-27 18:01:15,364 cfg.training.eval_batch_size       : 3600
2020-05-27 18:01:15,364 cfg.training.eval_batch_type       : token
2020-05-27 18:01:15,364 cfg.training.batch_multiplier      : 1
2020-05-27 18:01:15,364 cfg.training.early_stopping_metric : ppl
2020-05-27 18:01:15,364 cfg.training.epochs                : 100
2020-05-27 18:01:15,364 cfg.training.validation_freq       : 1000
2020-05-27 18:01:15,364 cfg.training.logging_freq          : 100
2020-05-27 18:01:15,364 cfg.training.eval_metric           : bleu
2020-05-27 18:01:15,364 cfg.training.model_dir             : models/wmt_ende_transformer-tune
2020-05-27 18:01:15,364 cfg.training.load_model            : models/wmt_ende_transformer/best.ckpt
2020-05-27 18:01:15,364 cfg.training.reset_best_ckpt       : True
2020-05-27 18:01:15,364 cfg.training.reset_scheduler       : True
2020-05-27 18:01:15,365 cfg.training.reset_optimizer       : True
2020-05-27 18:01:15,365 cfg.training.overwrite             : False
2020-05-27 18:01:15,365 cfg.training.shuffle               : True
2020-05-27 18:01:15,365 cfg.training.use_cuda              : True
2020-05-27 18:01:15,365 cfg.training.max_output_length     : 100
2020-05-27 18:01:15,365 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-05-27 18:01:15,365 cfg.training.keep_last_ckpts       : 3
2020-05-27 18:01:15,365 cfg.model.initializer              : xavier
2020-05-27 18:01:15,365 cfg.model.bias_initializer         : zeros
2020-05-27 18:01:15,365 cfg.model.init_gain                : 1.0
2020-05-27 18:01:15,365 cfg.model.embed_initializer        : xavier
2020-05-27 18:01:15,365 cfg.model.embed_init_gain          : 1.0
2020-05-27 18:01:15,365 cfg.model.tied_embeddings          : True
2020-05-27 18:01:15,365 cfg.model.tied_softmax             : True
2020-05-27 18:01:15,365 cfg.model.encoder.type             : transformer
2020-05-27 18:01:15,365 cfg.model.encoder.num_layers       : 6
2020-05-27 18:01:15,365 cfg.model.encoder.num_heads        : 8
2020-05-27 18:01:15,366 cfg.model.encoder.embeddings.embedding_dim : 512
2020-05-27 18:01:15,366 cfg.model.encoder.embeddings.scale : True
2020-05-27 18:01:15,366 cfg.model.encoder.embeddings.dropout : 0.0
2020-05-27 18:01:15,366 cfg.model.encoder.hidden_size      : 512
2020-05-27 18:01:15,366 cfg.model.encoder.ff_size          : 2048
2020-05-27 18:01:15,366 cfg.model.encoder.dropout          : 0.1
2020-05-27 18:01:15,366 cfg.model.decoder.type             : transformer
2020-05-27 18:01:15,366 cfg.model.decoder.num_layers       : 6
2020-05-27 18:01:15,366 cfg.model.decoder.num_heads        : 8
2020-05-27 18:01:15,366 cfg.model.decoder.embeddings.embedding_dim : 512
2020-05-27 18:01:15,366 cfg.model.decoder.embeddings.scale : True
2020-05-27 18:01:15,366 cfg.model.decoder.embeddings.dropout : 0.0
2020-05-27 18:01:15,366 cfg.model.decoder.hidden_size      : 512
2020-05-27 18:01:15,366 cfg.model.decoder.ff_size          : 2048
2020-05-27 18:01:15,366 cfg.model.decoder.dropout          : 0.1
2020-05-27 18:01:15,366 Data set sizes: 
	train 9747,
	valid 1527,
	test 1192
2020-05-27 18:01:15,367 First training example:
	[SRC] H@@ i there@@ ! How can I hel@@ p@@ ?
	[TRG] Hal@@ lo@@ ! Wie kann ich hel@@ fen@@ ?
2020-05-27 18:01:15,367 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-05-27 18:01:15,367 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) of (9) der
2020-05-27 18:01:15,367 Number of Src words (types): 36628
2020-05-27 18:01:15,367 Number of Trg words (types): 36628
2020-05-27 18:01:15,367 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=36628),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=36628))
2020-05-27 18:01:15,407 EPOCH 1
2020-05-27 18:01:29,071 Epoch   1: total training loss 107.84
2020-05-27 18:01:29,072 EPOCH 2
2020-05-27 18:01:36,153 Epoch   2 Step:  1360100 Batch Loss:     1.255984 Tokens per Sec:    12278, Lr: 0.000200
2020-05-27 18:01:42,309 Epoch   2: total training loss 68.12
2020-05-27 18:01:42,309 EPOCH 3
2020-05-27 18:01:55,513 Epoch   3: total training loss 54.94
2020-05-27 18:01:55,514 EPOCH 4
2020-05-27 18:01:56,452 Epoch   4 Step:  1360200 Batch Loss:     0.594936 Tokens per Sec:    12722, Lr: 0.000200
2020-05-27 18:02:08,730 Epoch   4: total training loss 47.32
2020-05-27 18:02:08,731 EPOCH 5
2020-05-27 18:02:16,908 Epoch   5 Step:  1360300 Batch Loss:     0.717999 Tokens per Sec:    12427, Lr: 0.000200
2020-05-27 18:02:22,062 Epoch   5: total training loss 41.88
2020-05-27 18:02:22,062 EPOCH 6
2020-05-27 18:02:35,634 Epoch   6: total training loss 37.48
2020-05-27 18:02:35,635 EPOCH 7
2020-05-27 18:02:37,751 Epoch   7 Step:  1360400 Batch Loss:     0.616799 Tokens per Sec:    12190, Lr: 0.000200
2020-05-27 18:02:49,197 Epoch   7: total training loss 34.89
2020-05-27 18:02:49,198 EPOCH 8
2020-05-27 18:02:58,236 Epoch   8 Step:  1360500 Batch Loss:     0.555660 Tokens per Sec:    12150, Lr: 0.000200
2020-05-27 18:03:02,725 Epoch   8: total training loss 31.79
2020-05-27 18:03:02,726 EPOCH 9
2020-05-27 18:03:16,347 Epoch   9: total training loss 29.40
2020-05-27 18:03:16,347 EPOCH 10
2020-05-27 18:03:18,822 Epoch  10 Step:  1360600 Batch Loss:     0.352512 Tokens per Sec:    12306, Lr: 0.000200
2020-05-27 18:03:29,775 Epoch  10: total training loss 27.07
2020-05-27 18:03:29,775 EPOCH 11
2020-05-27 18:03:39,387 Epoch  11 Step:  1360700 Batch Loss:     0.449429 Tokens per Sec:    12334, Lr: 0.000200
2020-05-27 18:03:43,129 Epoch  11: total training loss 25.02
2020-05-27 18:03:43,129 EPOCH 12
2020-05-27 18:03:56,495 Epoch  12: total training loss 23.52
2020-05-27 18:03:56,496 EPOCH 13
2020-05-27 18:03:59,885 Epoch  13 Step:  1360800 Batch Loss:     0.335785 Tokens per Sec:    11743, Lr: 0.000200
2020-05-27 18:04:10,107 Epoch  13: total training loss 22.73
2020-05-27 18:04:10,108 EPOCH 14
2020-05-27 18:04:20,386 Epoch  14 Step:  1360900 Batch Loss:     0.356996 Tokens per Sec:    12010, Lr: 0.000200
2020-05-27 18:04:23,680 Epoch  14: total training loss 21.18
2020-05-27 18:04:23,680 EPOCH 15
2020-05-27 18:04:37,131 Epoch  15: total training loss 19.61
2020-05-27 18:04:37,131 EPOCH 16
2020-05-27 18:04:41,000 Epoch  16 Step:  1361000 Batch Loss:     0.343070 Tokens per Sec:    12151, Lr: 0.000200
2020-05-27 18:05:29,694 Hooray! New best validation result [ppl]!
2020-05-27 18:05:29,695 Saving new checkpoint.
2020-05-27 18:05:38,251 Example #0
2020-05-27 18:05:38,252 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-05-27 18:05:38,252 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-05-27 18:05:38,252 	Source:     Hello.
2020-05-27 18:05:38,252 	Reference:  Hallo,
2020-05-27 18:05:38,252 	Hypothesis: Hallo.
2020-05-27 18:05:38,252 Example #1
2020-05-27 18:05:38,252 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-05-27 18:05:38,252 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-05-27 18:05:38,252 	Source:     Hi, how can I help you?
2020-05-27 18:05:38,252 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:05:38,252 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:05:38,252 Example #2
2020-05-27 18:05:38,252 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-05-27 18:05:38,252 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-05-27 18:05:38,252 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-05-27 18:05:38,252 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-05-27 18:05:38,252 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-05-27 18:05:38,252 Example #3
2020-05-27 18:05:38,252 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-05-27 18:05:38,252 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-05-27 18:05:38,252 	Source:     Ok, what type of restaurant are you looking for?
2020-05-27 18:05:38,252 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-05-27 18:05:38,252 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-05-27 18:05:38,252 Validation result (greedy) at epoch  16, step  1361000: bleu:  45.92, loss: 19161.7891, ppl:   2.0826, duration: 57.2514s
2020-05-27 18:05:47,963 Epoch  16: total training loss 18.50
2020-05-27 18:05:47,963 EPOCH 17
2020-05-27 18:05:59,269 Epoch  17 Step:  1361100 Batch Loss:     0.275671 Tokens per Sec:    12097, Lr: 0.000200
2020-05-27 18:06:01,515 Epoch  17: total training loss 17.54
2020-05-27 18:06:01,516 EPOCH 18
2020-05-27 18:06:15,072 Epoch  18: total training loss 16.47
2020-05-27 18:06:15,073 EPOCH 19
2020-05-27 18:06:20,379 Epoch  19 Step:  1361200 Batch Loss:     0.267189 Tokens per Sec:    12138, Lr: 0.000200
2020-05-27 18:06:28,635 Epoch  19: total training loss 15.89
2020-05-27 18:06:28,636 EPOCH 20
2020-05-27 18:06:41,172 Epoch  20 Step:  1361300 Batch Loss:     0.263757 Tokens per Sec:    11951, Lr: 0.000200
2020-05-27 18:06:42,239 Epoch  20: total training loss 15.12
2020-05-27 18:06:42,239 EPOCH 21
2020-05-27 18:06:55,917 Epoch  21: total training loss 14.75
2020-05-27 18:06:55,918 EPOCH 22
2020-05-27 18:07:02,006 Epoch  22 Step:  1361400 Batch Loss:     0.227591 Tokens per Sec:    12148, Lr: 0.000200
2020-05-27 18:07:09,505 Epoch  22: total training loss 13.97
2020-05-27 18:07:09,506 EPOCH 23
2020-05-27 18:07:22,890 Epoch  23 Step:  1361500 Batch Loss:     0.234691 Tokens per Sec:    11754, Lr: 0.000200
2020-05-27 18:07:23,331 Epoch  23: total training loss 13.56
2020-05-27 18:07:23,331 EPOCH 24
2020-05-27 18:07:37,095 Epoch  24: total training loss 12.98
2020-05-27 18:07:37,095 EPOCH 25
2020-05-27 18:07:43,857 Epoch  25 Step:  1361600 Batch Loss:     0.185041 Tokens per Sec:    11822, Lr: 0.000200
2020-05-27 18:07:50,812 Epoch  25: total training loss 12.31
2020-05-27 18:07:50,812 EPOCH 26
2020-05-27 18:08:04,550 Epoch  26: total training loss 11.75
2020-05-27 18:08:04,551 EPOCH 27
2020-05-27 18:08:04,982 Epoch  27 Step:  1361700 Batch Loss:     0.163343 Tokens per Sec:     9839, Lr: 0.000200
2020-05-27 18:08:18,347 Epoch  27: total training loss 11.61
2020-05-27 18:08:18,348 EPOCH 28
2020-05-27 18:08:25,838 Epoch  28 Step:  1361800 Batch Loss:     0.184955 Tokens per Sec:    11845, Lr: 0.000200
2020-05-27 18:08:32,119 Epoch  28: total training loss 11.16
2020-05-27 18:08:32,119 EPOCH 29
2020-05-27 18:08:45,810 Epoch  29: total training loss 10.71
2020-05-27 18:08:45,810 EPOCH 30
2020-05-27 18:08:46,882 Epoch  30 Step:  1361900 Batch Loss:     0.169113 Tokens per Sec:    12051, Lr: 0.000200
2020-05-27 18:08:59,635 Epoch  30: total training loss 10.46
2020-05-27 18:08:59,636 EPOCH 31
2020-05-27 18:09:07,893 Epoch  31 Step:  1362000 Batch Loss:     0.171017 Tokens per Sec:    11742, Lr: 0.000200
2020-05-27 18:09:55,434 Example #0
2020-05-27 18:09:55,434 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-05-27 18:09:55,434 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-05-27 18:09:55,434 	Source:     Hello.
2020-05-27 18:09:55,434 	Reference:  Hallo,
2020-05-27 18:09:55,434 	Hypothesis: Hallo.
2020-05-27 18:09:55,434 Example #1
2020-05-27 18:09:55,434 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-05-27 18:09:55,434 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-05-27 18:09:55,434 	Source:     Hi, how can I help you?
2020-05-27 18:09:55,434 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:09:55,434 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:09:55,434 Example #2
2020-05-27 18:09:55,434 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-05-27 18:09:55,434 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-05-27 18:09:55,434 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-05-27 18:09:55,434 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-05-27 18:09:55,435 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-05-27 18:09:55,435 Example #3
2020-05-27 18:09:55,435 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-05-27 18:09:55,435 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-05-27 18:09:55,435 	Source:     Ok, what type of restaurant are you looking for?
2020-05-27 18:09:55,435 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-05-27 18:09:55,435 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-05-27 18:09:55,435 Validation result (greedy) at epoch  31, step  1362000: bleu:  45.71, loss: 20835.5254, ppl:   2.2204, duration: 47.5408s
2020-05-27 18:10:01,064 Epoch  31: total training loss 10.18
2020-05-27 18:10:01,064 EPOCH 32
2020-05-27 18:10:14,817 Epoch  32: total training loss 9.54
2020-05-27 18:10:14,817 EPOCH 33
2020-05-27 18:10:16,748 Epoch  33 Step:  1362100 Batch Loss:     0.146897 Tokens per Sec:    11409, Lr: 0.000200
2020-05-27 18:10:28,638 Epoch  33: total training loss 9.56
2020-05-27 18:10:28,638 EPOCH 34
2020-05-27 18:10:37,562 Epoch  34 Step:  1362200 Batch Loss:     0.142573 Tokens per Sec:    11797, Lr: 0.000200
2020-05-27 18:10:42,480 Epoch  34: total training loss 9.30
2020-05-27 18:10:42,480 EPOCH 35
2020-05-27 18:10:56,282 Epoch  35: total training loss 8.91
2020-05-27 18:10:56,283 EPOCH 36
2020-05-27 18:10:58,849 Epoch  36 Step:  1362300 Batch Loss:     0.127627 Tokens per Sec:    11163, Lr: 0.000200
2020-05-27 18:11:10,215 Epoch  36: total training loss 8.77
2020-05-27 18:11:10,215 EPOCH 37
2020-05-27 18:11:19,809 Epoch  37 Step:  1362400 Batch Loss:     0.131664 Tokens per Sec:    11855, Lr: 0.000200
2020-05-27 18:11:24,056 Epoch  37: total training loss 8.50
2020-05-27 18:11:24,057 EPOCH 38
2020-05-27 18:11:37,797 Epoch  38: total training loss 8.17
2020-05-27 18:11:37,798 EPOCH 39
2020-05-27 18:11:40,990 Epoch  39 Step:  1362500 Batch Loss:     0.117013 Tokens per Sec:    12117, Lr: 0.000200
2020-05-27 18:11:51,634 Epoch  39: total training loss 8.06
2020-05-27 18:11:51,634 EPOCH 40
2020-05-27 18:12:02,038 Epoch  40 Step:  1362600 Batch Loss:     0.132330 Tokens per Sec:    12026, Lr: 0.000200
2020-05-27 18:12:05,389 Epoch  40: total training loss 7.79
2020-05-27 18:12:05,389 EPOCH 41
2020-05-27 18:12:19,093 Epoch  41: total training loss 7.59
2020-05-27 18:12:19,094 EPOCH 42
2020-05-27 18:12:23,150 Epoch  42 Step:  1362700 Batch Loss:     0.112604 Tokens per Sec:    11766, Lr: 0.000200
2020-05-27 18:12:32,828 Epoch  42: total training loss 7.43
2020-05-27 18:12:32,829 EPOCH 43
2020-05-27 18:12:44,264 Epoch  43 Step:  1362800 Batch Loss:     0.114779 Tokens per Sec:    11814, Lr: 0.000200
2020-05-27 18:12:46,672 Epoch  43: total training loss 7.29
2020-05-27 18:12:46,673 EPOCH 44
2020-05-27 18:13:00,454 Epoch  44: total training loss 7.04
2020-05-27 18:13:00,455 EPOCH 45
2020-05-27 18:13:05,539 Epoch  45 Step:  1362900 Batch Loss:     0.111430 Tokens per Sec:    11888, Lr: 0.000200
2020-05-27 18:13:14,325 Epoch  45: total training loss 7.06
2020-05-27 18:13:14,325 EPOCH 46
2020-05-27 18:13:26,555 Epoch  46 Step:  1363000 Batch Loss:     0.110797 Tokens per Sec:    11930, Lr: 0.000200
2020-05-27 18:14:12,166 Example #0
2020-05-27 18:14:12,166 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-05-27 18:14:12,167 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-05-27 18:14:12,167 	Source:     Hello.
2020-05-27 18:14:12,167 	Reference:  Hallo,
2020-05-27 18:14:12,167 	Hypothesis: Hallo.
2020-05-27 18:14:12,167 Example #1
2020-05-27 18:14:12,167 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-05-27 18:14:12,167 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-05-27 18:14:12,167 	Source:     Hi, how can I help you?
2020-05-27 18:14:12,167 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:14:12,167 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:14:12,167 Example #2
2020-05-27 18:14:12,167 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-05-27 18:14:12,167 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-05-27 18:14:12,167 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-05-27 18:14:12,167 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-05-27 18:14:12,167 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-05-27 18:14:12,167 Example #3
2020-05-27 18:14:12,167 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-05-27 18:14:12,167 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-05-27 18:14:12,167 	Source:     Ok, what type of restaurant are you looking for?
2020-05-27 18:14:12,167 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-05-27 18:14:12,167 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-05-27 18:14:12,167 Validation result (greedy) at epoch  46, step  1363000: bleu:  45.47, loss: 22336.6504, ppl:   2.3517, duration: 45.6111s
2020-05-27 18:14:13,686 Epoch  46: total training loss 6.77
2020-05-27 18:14:13,686 EPOCH 47
2020-05-27 18:14:27,527 Epoch  47: total training loss 6.60
2020-05-27 18:14:27,527 EPOCH 48
2020-05-27 18:14:33,415 Epoch  48 Step:  1363100 Batch Loss:     0.095282 Tokens per Sec:    11976, Lr: 0.000200
2020-05-27 18:14:41,270 Epoch  48: total training loss 6.60
2020-05-27 18:14:41,271 EPOCH 49
2020-05-27 18:14:54,253 Epoch  49 Step:  1363200 Batch Loss:     0.101020 Tokens per Sec:    11911, Lr: 0.000200
2020-05-27 18:14:54,977 Epoch  49: total training loss 6.73
2020-05-27 18:14:54,977 EPOCH 50
2020-05-27 18:15:08,725 Epoch  50: total training loss 6.37
2020-05-27 18:15:08,726 EPOCH 51
2020-05-27 18:15:15,277 Epoch  51 Step:  1363300 Batch Loss:     0.102826 Tokens per Sec:    11880, Lr: 0.000200
2020-05-27 18:15:22,381 Epoch  51: total training loss 6.16
2020-05-27 18:15:22,381 EPOCH 52
2020-05-27 18:15:36,199 Epoch  52: total training loss 6.04
2020-05-27 18:15:36,199 EPOCH 53
2020-05-27 18:15:36,432 Epoch  53 Step:  1363400 Batch Loss:     0.087998 Tokens per Sec:    12698, Lr: 0.000200
2020-05-27 18:15:50,097 Epoch  53: total training loss 6.01
2020-05-27 18:15:50,098 EPOCH 54
2020-05-27 18:15:57,476 Epoch  54 Step:  1363500 Batch Loss:     0.093407 Tokens per Sec:    11819, Lr: 0.000200
2020-05-27 18:16:03,990 Epoch  54: total training loss 6.09
2020-05-27 18:16:03,990 EPOCH 55
2020-05-27 18:16:17,711 Epoch  55: total training loss 5.64
2020-05-27 18:16:17,712 EPOCH 56
2020-05-27 18:16:18,812 Epoch  56 Step:  1363600 Batch Loss:     0.090588 Tokens per Sec:    11566, Lr: 0.000200
2020-05-27 18:16:31,504 Epoch  56: total training loss 5.62
2020-05-27 18:16:31,505 EPOCH 57
2020-05-27 18:16:40,194 Epoch  57 Step:  1363700 Batch Loss:     0.084774 Tokens per Sec:    11905, Lr: 0.000200
2020-05-27 18:16:45,269 Epoch  57: total training loss 5.46
2020-05-27 18:16:45,270 EPOCH 58
2020-05-27 18:16:59,161 Epoch  58: total training loss 5.52
2020-05-27 18:16:59,161 EPOCH 59
2020-05-27 18:17:01,234 Epoch  59 Step:  1363800 Batch Loss:     0.083541 Tokens per Sec:    11582, Lr: 0.000200
2020-05-27 18:17:13,043 Epoch  59: total training loss 5.39
2020-05-27 18:17:13,043 EPOCH 60
2020-05-27 18:17:22,429 Epoch  60 Step:  1363900 Batch Loss:     0.079901 Tokens per Sec:    11848, Lr: 0.000200
2020-05-27 18:17:26,914 Epoch  60: total training loss 5.24
2020-05-27 18:17:26,914 EPOCH 61
2020-05-27 18:17:40,807 Epoch  61: total training loss 5.14
2020-05-27 18:17:40,808 EPOCH 62
2020-05-27 18:17:43,833 Epoch  62 Step:  1364000 Batch Loss:     0.076743 Tokens per Sec:    12186, Lr: 0.000200
2020-05-27 18:18:29,883 Example #0
2020-05-27 18:18:29,883 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-05-27 18:18:29,883 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-05-27 18:18:29,883 	Source:     Hello.
2020-05-27 18:18:29,883 	Reference:  Hallo,
2020-05-27 18:18:29,883 	Hypothesis: Hallo.
2020-05-27 18:18:29,883 Example #1
2020-05-27 18:18:29,883 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-05-27 18:18:29,883 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-05-27 18:18:29,883 	Source:     Hi, how can I help you?
2020-05-27 18:18:29,883 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:18:29,883 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:18:29,883 Example #2
2020-05-27 18:18:29,883 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-05-27 18:18:29,883 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-05-27 18:18:29,883 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-05-27 18:18:29,883 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-05-27 18:18:29,883 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-05-27 18:18:29,883 Example #3
2020-05-27 18:18:29,883 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-05-27 18:18:29,883 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-05-27 18:18:29,883 	Source:     Ok, what type of restaurant are you looking for?
2020-05-27 18:18:29,883 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-05-27 18:18:29,884 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-05-27 18:18:29,884 Validation result (greedy) at epoch  62, step  1364000: bleu:  45.26, loss: 23633.9160, ppl:   2.4715, duration: 46.0499s
2020-05-27 18:18:40,649 Epoch  62: total training loss 5.11
2020-05-27 18:18:40,649 EPOCH 63
2020-05-27 18:18:51,097 Epoch  63 Step:  1364100 Batch Loss:     0.078961 Tokens per Sec:    11885, Lr: 0.000200
2020-05-27 18:18:54,457 Epoch  63: total training loss 5.03
2020-05-27 18:18:54,457 EPOCH 64
2020-05-27 18:19:08,183 Epoch  64: total training loss 4.90
2020-05-27 18:19:08,184 EPOCH 65
2020-05-27 18:19:12,383 Epoch  65 Step:  1364200 Batch Loss:     0.077376 Tokens per Sec:    11855, Lr: 0.000200
2020-05-27 18:19:22,038 Epoch  65: total training loss 4.95
2020-05-27 18:19:22,039 EPOCH 66
2020-05-27 18:19:33,423 Epoch  66 Step:  1364300 Batch Loss:     0.071266 Tokens per Sec:    12013, Lr: 0.000200
2020-05-27 18:19:35,693 Epoch  66: total training loss 4.85
2020-05-27 18:19:35,694 EPOCH 67
2020-05-27 18:19:49,370 Epoch  67: total training loss 4.73
2020-05-27 18:19:49,373 EPOCH 68
2020-05-27 18:19:54,306 Epoch  68 Step:  1364400 Batch Loss:     0.081164 Tokens per Sec:    11627, Lr: 0.000200
2020-05-27 18:20:03,117 Epoch  68: total training loss 4.76
2020-05-27 18:20:03,117 EPOCH 69
2020-05-27 18:20:15,317 Epoch  69 Step:  1364500 Batch Loss:     0.075695 Tokens per Sec:    11785, Lr: 0.000200
2020-05-27 18:20:16,861 Epoch  69: total training loss 4.66
2020-05-27 18:20:16,861 EPOCH 70
2020-05-27 18:20:30,617 Epoch  70: total training loss 4.65
2020-05-27 18:20:30,618 EPOCH 71
2020-05-27 18:20:36,284 Epoch  71 Step:  1364600 Batch Loss:     0.061214 Tokens per Sec:    11957, Lr: 0.000200
2020-05-27 18:20:44,356 Epoch  71: total training loss 4.59
2020-05-27 18:20:44,357 EPOCH 72
2020-05-27 18:20:57,291 Epoch  72 Step:  1364700 Batch Loss:     0.064282 Tokens per Sec:    12014, Lr: 0.000200
2020-05-27 18:20:58,027 Epoch  72: total training loss 4.52
2020-05-27 18:20:58,027 EPOCH 73
2020-05-27 18:21:11,739 Epoch  73: total training loss 4.46
2020-05-27 18:21:11,739 EPOCH 74
2020-05-27 18:21:18,250 Epoch  74 Step:  1364800 Batch Loss:     0.062611 Tokens per Sec:    11869, Lr: 0.000200
2020-05-27 18:21:25,538 Epoch  74: total training loss 4.46
2020-05-27 18:21:25,538 EPOCH 75
2020-05-27 18:21:39,183 Epoch  75 Step:  1364900 Batch Loss:     0.066459 Tokens per Sec:    11943, Lr: 0.000200
2020-05-27 18:21:39,184 Epoch  75: total training loss 4.37
2020-05-27 18:21:39,184 EPOCH 76
2020-05-27 18:21:52,786 Epoch  76: total training loss 4.21
2020-05-27 18:21:52,787 EPOCH 77
2020-05-27 18:22:00,254 Epoch  77 Step:  1365000 Batch Loss:     0.060358 Tokens per Sec:    11913, Lr: 0.000200
2020-05-27 18:22:46,898 Example #0
2020-05-27 18:22:46,899 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-05-27 18:22:46,899 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-05-27 18:22:46,899 	Source:     Hello.
2020-05-27 18:22:46,899 	Reference:  Hallo,
2020-05-27 18:22:46,899 	Hypothesis: Hallo.
2020-05-27 18:22:46,899 Example #1
2020-05-27 18:22:46,899 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-05-27 18:22:46,899 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-05-27 18:22:46,899 	Source:     Hi, how can I help you?
2020-05-27 18:22:46,899 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:22:46,899 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:22:46,899 Example #2
2020-05-27 18:22:46,899 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-05-27 18:22:46,899 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-05-27 18:22:46,899 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-05-27 18:22:46,899 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-05-27 18:22:46,899 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-05-27 18:22:46,899 Example #3
2020-05-27 18:22:46,899 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-05-27 18:22:46,899 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-05-27 18:22:46,899 	Source:     Ok, what type of restaurant are you looking for?
2020-05-27 18:22:46,899 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-05-27 18:22:46,899 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-05-27 18:22:46,899 Validation result (greedy) at epoch  77, step  1365000: bleu:  45.51, loss: 24454.4375, ppl:   2.5504, duration: 46.6438s
2020-05-27 18:22:53,277 Epoch  77: total training loss 4.29
2020-05-27 18:22:53,278 EPOCH 78
2020-05-27 18:23:07,007 Epoch  78: total training loss 4.28
2020-05-27 18:23:07,007 EPOCH 79
2020-05-27 18:23:07,901 Epoch  79 Step:  1365100 Batch Loss:     0.060392 Tokens per Sec:    12537, Lr: 0.000200
2020-05-27 18:23:20,806 Epoch  79: total training loss 4.24
2020-05-27 18:23:20,806 EPOCH 80
2020-05-27 18:23:28,898 Epoch  80 Step:  1365200 Batch Loss:     0.058586 Tokens per Sec:    11926, Lr: 0.000200
2020-05-27 18:23:34,491 Epoch  80: total training loss 4.11
2020-05-27 18:23:34,491 EPOCH 81
2020-05-27 18:23:48,338 Epoch  81: total training loss 4.04
2020-05-27 18:23:48,339 EPOCH 82
2020-05-27 18:23:50,042 Epoch  82 Step:  1365300 Batch Loss:     0.052812 Tokens per Sec:    11578, Lr: 0.000200
2020-05-27 18:24:02,126 Epoch  82: total training loss 4.00
2020-05-27 18:24:02,127 EPOCH 83
2020-05-27 18:24:11,264 Epoch  83 Step:  1365400 Batch Loss:     0.056028 Tokens per Sec:    11815, Lr: 0.000200
2020-05-27 18:24:15,817 Epoch  83: total training loss 3.96
2020-05-27 18:24:15,817 EPOCH 84
2020-05-27 18:24:29,439 Epoch  84: total training loss 3.98
2020-05-27 18:24:29,440 EPOCH 85
2020-05-27 18:24:32,249 Epoch  85 Step:  1365500 Batch Loss:     0.054257 Tokens per Sec:    12528, Lr: 0.000200
2020-05-27 18:24:43,098 Epoch  85: total training loss 3.95
2020-05-27 18:24:43,099 EPOCH 86
2020-05-27 18:24:53,169 Epoch  86 Step:  1365600 Batch Loss:     0.060913 Tokens per Sec:    11901, Lr: 0.000200
2020-05-27 18:24:56,725 Epoch  86: total training loss 3.93
2020-05-27 18:24:56,725 EPOCH 87
2020-05-27 18:25:10,428 Epoch  87: total training loss 3.88
2020-05-27 18:25:10,428 EPOCH 88
2020-05-27 18:25:14,160 Epoch  88 Step:  1365700 Batch Loss:     0.061722 Tokens per Sec:    11485, Lr: 0.000200
2020-05-27 18:25:24,106 Epoch  88: total training loss 3.83
2020-05-27 18:25:24,106 EPOCH 89
2020-05-27 18:25:34,814 Epoch  89 Step:  1365800 Batch Loss:     0.047914 Tokens per Sec:    12127, Lr: 0.000200
2020-05-27 18:25:37,528 Epoch  89: total training loss 3.92
2020-05-27 18:25:37,528 EPOCH 90
2020-05-27 18:25:50,442 Epoch  90: total training loss 3.66
2020-05-27 18:25:50,442 EPOCH 91
2020-05-27 18:25:54,886 Epoch  91 Step:  1365900 Batch Loss:     0.050984 Tokens per Sec:    12673, Lr: 0.000200
2020-05-27 18:26:03,386 Epoch  91: total training loss 3.71
2020-05-27 18:26:03,386 EPOCH 92
2020-05-27 18:26:14,940 Epoch  92 Step:  1366000 Batch Loss:     0.055722 Tokens per Sec:    12709, Lr: 0.000200
2020-05-27 18:27:02,586 Example #0
2020-05-27 18:27:02,587 	Raw source:     ['Hel@@', 'lo@@', '.']
2020-05-27 18:27:02,587 	Raw hypothesis: ['Hal@@', 'lo@@', '.']
2020-05-27 18:27:02,587 	Source:     Hello.
2020-05-27 18:27:02,587 	Reference:  Hallo,
2020-05-27 18:27:02,587 	Hypothesis: Hallo.
2020-05-27 18:27:02,587 Example #1
2020-05-27 18:27:02,587 	Raw source:     ['Hi@@', ',', 'how', 'can', 'I', 'help', 'you@@', '?']
2020-05-27 18:27:02,587 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'wie', 'kann', 'ich', 'Ihnen', 'hel@@', 'fen@@', '?']
2020-05-27 18:27:02,587 	Source:     Hi, how can I help you?
2020-05-27 18:27:02,587 	Reference:  Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:27:02,587 	Hypothesis: Hallo, wie kann ich Ihnen helfen?
2020-05-27 18:27:02,587 Example #2
2020-05-27 18:27:02,587 	Raw source:     ['Hi@@', ',', 'I@@', "'m", 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Califor@@', 'ni@@', 'a.']
2020-05-27 18:27:02,587 	Raw hypothesis: ['Hal@@', 'lo@@', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'Mall@@', ',', 'in', 'San', 'Franc@@', 'is@@', 'co@@', ',', 'Kalifor@@', 'ni@@', 'en@@', '.']
2020-05-27 18:27:02,587 	Source:     Hi, I'm looking for a restaurant inside the Arden Fair mall in San Francisco, California.
2020-05-27 18:27:02,587 	Reference:  Hallo, ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco, Kalifornien.
2020-05-27 18:27:02,587 	Hypothesis: Hallo, ich suche ein Restaurant in der Arden Fair Mall, in San Francisco, Kalifornien.
2020-05-27 18:27:02,587 Example #3
2020-05-27 18:27:02,587 	Raw source:     ['Ok@@', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for@@', '?']
2020-05-27 18:27:02,587 	Raw hypothesis: ['Ok@@', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie@@', '?']
2020-05-27 18:27:02,587 	Source:     Ok, what type of restaurant are you looking for?
2020-05-27 18:27:02,587 	Reference:  Ok. Welche Art von Restaurant suchen Sie denn genau?
2020-05-27 18:27:02,587 	Hypothesis: Ok, nach welcher Art von Restaurant suchen Sie?
2020-05-27 18:27:02,587 Validation result (greedy) at epoch  92, step  1366000: bleu:  44.63, loss: 25336.2188, ppl:   2.6379, duration: 47.6462s
2020-05-27 18:27:04,014 Epoch  92: total training loss 3.59
2020-05-27 18:27:04,014 EPOCH 93
2020-05-27 18:27:17,054 Epoch  93: total training loss 3.69
2020-05-27 18:27:17,054 EPOCH 94
2020-05-27 18:27:22,462 Epoch  94 Step:  1366100 Batch Loss:     0.050260 Tokens per Sec:    12751, Lr: 0.000200
2020-05-27 18:27:30,009 Epoch  94: total training loss 3.61
2020-05-27 18:27:30,010 EPOCH 95
2020-05-27 18:27:42,436 Epoch  95 Step:  1366200 Batch Loss:     0.081000 Tokens per Sec:    12548, Lr: 0.000200
2020-05-27 18:27:43,023 Epoch  95: total training loss 3.63
2020-05-27 18:27:43,023 EPOCH 96
2020-05-27 18:27:56,070 Epoch  96: total training loss 3.64
2020-05-27 18:27:56,071 EPOCH 97
2020-05-27 18:28:02,099 Epoch  97 Step:  1366300 Batch Loss:     0.049306 Tokens per Sec:    12547, Lr: 0.000200
2020-05-27 18:28:09,086 Epoch  97: total training loss 3.56
2020-05-27 18:28:09,086 EPOCH 98
2020-05-27 18:28:21,999 Epoch  98 Step:  1366400 Batch Loss:     0.047993 Tokens per Sec:    12620, Lr: 0.000200
2020-05-27 18:28:22,000 Epoch  98: total training loss 3.48
2020-05-27 18:28:22,000 EPOCH 99
2020-05-27 18:28:34,908 Epoch  99: total training loss 3.48
2020-05-27 18:28:34,908 EPOCH 100
2020-05-27 18:28:41,849 Epoch 100 Step:  1366500 Batch Loss:     0.046100 Tokens per Sec:    12600, Lr: 0.000200
2020-05-27 18:28:47,900 Epoch 100: total training loss 3.45
2020-05-27 18:28:47,900 Training ended after 100 epochs.
2020-05-27 18:28:47,900 Best validation result (greedy) at step  1361000:   2.08 ppl.
2020-05-27 18:29:35,666  dev bleu:  45.26 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-05-27 18:29:35,670 Translations saved to: models/wmt_ende_transformer-tune/01361000.hyps.dev
2020-05-27 18:30:02,945 test bleu:  42.56 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-05-27 18:30:02,951 Translations saved to: models/wmt_ende_transformer-tune/01361000.hyps.test
