2020-06-22 01:17:58,453 Hello! This is Joey-NMT.
2020-06-22 01:18:05,646 Total params: 13996801
2020-06-22 01:18:05,649 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-22 01:18:07,776 cfg.name                           : transformer_multi_enc_hid256_ende
2020-06-22 01:18:07,777 cfg.data.src                       : en
2020-06-22 01:18:07,777 cfg.data.trg                       : de
2020-06-22 01:18:07,777 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-22 01:18:07,777 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-22 01:18:07,777 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-22 01:18:07,777 cfg.data.level                     : bpe
2020-06-22 01:18:07,777 cfg.data.lowercase                 : True
2020-06-22 01:18:07,777 cfg.data.max_sent_length           : 100
2020-06-22 01:18:07,777 cfg.testing.beam_size              : 5
2020-06-22 01:18:07,777 cfg.testing.alpha                  : 1.0
2020-06-22 01:18:07,777 cfg.training.random_seed           : 42
2020-06-22 01:18:07,777 cfg.training.optimizer             : adam
2020-06-22 01:18:07,777 cfg.training.normalization         : tokens
2020-06-22 01:18:07,777 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-22 01:18:07,778 cfg.training.scheduling            : plateau
2020-06-22 01:18:07,778 cfg.training.patience              : 8
2020-06-22 01:18:07,778 cfg.training.decrease_factor       : 0.7
2020-06-22 01:18:07,778 cfg.training.loss                  : crossentropy
2020-06-22 01:18:07,778 cfg.training.learning_rate         : 0.0002
2020-06-22 01:18:07,778 cfg.training.learning_rate_min     : 1e-08
2020-06-22 01:18:07,778 cfg.training.weight_decay          : 0.0
2020-06-22 01:18:07,778 cfg.training.label_smoothing       : 0.1
2020-06-22 01:18:07,778 cfg.training.batch_size            : 4096
2020-06-22 01:18:07,778 cfg.training.batch_type            : token
2020-06-22 01:18:07,778 cfg.training.eval_batch_size       : 3600
2020-06-22 01:18:07,778 cfg.training.eval_batch_type       : token
2020-06-22 01:18:07,778 cfg.training.batch_multiplier      : 1
2020-06-22 01:18:07,778 cfg.training.early_stopping_metric : ppl
2020-06-22 01:18:07,778 cfg.training.epochs                : 100
2020-06-22 01:18:07,778 cfg.training.validation_freq       : 1000
2020-06-22 01:18:07,779 cfg.training.logging_freq          : 100
2020-06-22 01:18:07,779 cfg.training.eval_metric           : bleu
2020-06-22 01:18:07,779 cfg.training.model_dir             : models/transformer_multi_enc_hid256_ende
2020-06-22 01:18:07,779 cfg.training.overwrite             : True
2020-06-22 01:18:07,779 cfg.training.shuffle               : True
2020-06-22 01:18:07,779 cfg.training.use_cuda              : True
2020-06-22 01:18:07,779 cfg.training.max_output_length     : 100
2020-06-22 01:18:07,779 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-22 01:18:07,779 cfg.training.keep_last_ckpts       : 3
2020-06-22 01:18:07,779 cfg.model.initializer              : xavier
2020-06-22 01:18:07,779 cfg.model.bias_initializer         : zeros
2020-06-22 01:18:07,779 cfg.model.init_gain                : 1.0
2020-06-22 01:18:07,779 cfg.model.embed_initializer        : xavier
2020-06-22 01:18:07,779 cfg.model.embed_init_gain          : 1.0
2020-06-22 01:18:07,779 cfg.model.tied_embeddings          : False
2020-06-22 01:18:07,779 cfg.model.tied_softmax             : True
2020-06-22 01:18:07,779 cfg.model.encoder.type             : transformer
2020-06-22 01:18:07,780 cfg.model.encoder.num_layers       : 3
2020-06-22 01:18:07,780 cfg.model.encoder.num_heads        : 8
2020-06-22 01:18:07,780 cfg.model.encoder.embeddings.embedding_dim : 256
2020-06-22 01:18:07,780 cfg.model.encoder.embeddings.scale : True
2020-06-22 01:18:07,780 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-22 01:18:07,780 cfg.model.encoder.hidden_size      : 256
2020-06-22 01:18:07,780 cfg.model.encoder.ff_size          : 1024
2020-06-22 01:18:07,780 cfg.model.encoder.dropout          : 0.1
2020-06-22 01:18:07,780 cfg.model.encoder.freeze           : False
2020-06-22 01:18:07,780 cfg.model.encoder.multi_encoder    : True
2020-06-22 01:18:07,780 cfg.model.decoder.type             : transformer
2020-06-22 01:18:07,780 cfg.model.decoder.num_layers       : 6
2020-06-22 01:18:07,780 cfg.model.decoder.num_heads        : 8
2020-06-22 01:18:07,780 cfg.model.decoder.embeddings.embedding_dim : 256
2020-06-22 01:18:07,780 cfg.model.decoder.embeddings.scale : True
2020-06-22 01:18:07,780 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-22 01:18:07,781 cfg.model.decoder.hidden_size      : 256
2020-06-22 01:18:07,781 cfg.model.decoder.ff_size          : 1024
2020-06-22 01:18:07,781 cfg.model.decoder.dropout          : 0.1
2020-06-22 01:18:07,781 cfg.model.decoder.freeze           : False
2020-06-22 01:18:07,781 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-22 01:18:07,781 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-22 01:18:07,781 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-22 01:18:07,781 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-22 01:18:07,781 Number of Src words (types): 4561
2020-06-22 01:18:07,781 Number of Trg words (types): 5876
2020-06-22 01:18:07,782 Model(
	encoder=TransformerEncoder(num_layers=2, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4561),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=5876))
2020-06-22 01:18:07,795 EPOCH 1
2020-06-22 01:18:15,209 Epoch   1: total training loss 339.28
2020-06-22 01:18:15,209 EPOCH 2
2020-06-22 01:18:20,513 Epoch   2 Step:      100 Batch Loss:     4.657632 Tokens per Sec:    19993, Lr: 0.000200
2020-06-22 01:18:21,688 Epoch   2: total training loss 272.78
2020-06-22 01:18:21,688 EPOCH 3
2020-06-22 01:18:28,215 Epoch   3: total training loss 255.59
2020-06-22 01:18:28,216 EPOCH 4
2020-06-22 01:18:32,385 Epoch   4 Step:      200 Batch Loss:     2.413611 Tokens per Sec:    19485, Lr: 0.000200
2020-06-22 01:18:34,755 Epoch   4: total training loss 241.98
2020-06-22 01:18:34,756 EPOCH 5
2020-06-22 01:18:41,416 Epoch   5: total training loss 220.67
2020-06-22 01:18:41,417 EPOCH 6
2020-06-22 01:18:44,591 Epoch   6 Step:      300 Batch Loss:     5.460202 Tokens per Sec:    18785, Lr: 0.000200
2020-06-22 01:18:48,526 Epoch   6: total training loss 213.27
2020-06-22 01:18:48,526 EPOCH 7
2020-06-22 01:18:55,814 Epoch   7: total training loss 194.14
2020-06-22 01:18:55,815 EPOCH 8
2020-06-22 01:18:57,938 Epoch   8 Step:      400 Batch Loss:     3.474155 Tokens per Sec:    17439, Lr: 0.000200
2020-06-22 01:19:02,625 Epoch   8: total training loss 181.35
2020-06-22 01:19:02,626 EPOCH 9
2020-06-22 01:19:09,629 Epoch   9: total training loss 174.54
2020-06-22 01:19:09,630 EPOCH 10
2020-06-22 01:19:10,455 Epoch  10 Step:      500 Batch Loss:     3.589429 Tokens per Sec:    22118, Lr: 0.000200
2020-06-22 01:19:16,473 Epoch  10: total training loss 162.87
2020-06-22 01:19:16,473 EPOCH 11
2020-06-22 01:19:22,812 Epoch  11 Step:      600 Batch Loss:     2.015480 Tokens per Sec:    19475, Lr: 0.000200
2020-06-22 01:19:23,156 Epoch  11: total training loss 157.51
2020-06-22 01:19:23,156 EPOCH 12
2020-06-22 01:19:29,988 Epoch  12: total training loss 146.16
2020-06-22 01:19:29,988 EPOCH 13
2020-06-22 01:19:35,327 Epoch  13 Step:      700 Batch Loss:     2.467652 Tokens per Sec:    19474, Lr: 0.000200
2020-06-22 01:19:37,025 Epoch  13: total training loss 137.69
2020-06-22 01:19:37,025 EPOCH 14
2020-06-22 01:19:44,299 Epoch  14: total training loss 128.64
2020-06-22 01:19:44,299 EPOCH 15
2020-06-22 01:19:48,476 Epoch  15 Step:      800 Batch Loss:     2.561312 Tokens per Sec:    18569, Lr: 0.000200
2020-06-22 01:19:51,613 Epoch  15: total training loss 121.78
2020-06-22 01:19:51,613 EPOCH 16
2020-06-22 01:19:58,827 Epoch  16: total training loss 117.19
2020-06-22 01:19:58,827 EPOCH 17
2020-06-22 01:20:01,735 Epoch  17 Step:      900 Batch Loss:     2.021830 Tokens per Sec:    18040, Lr: 0.000200
2020-06-22 01:20:05,868 Epoch  17: total training loss 113.97
2020-06-22 01:20:05,868 EPOCH 18
2020-06-22 01:20:13,035 Epoch  18: total training loss 109.17
2020-06-22 01:20:13,035 EPOCH 19
2020-06-22 01:20:14,715 Epoch  19 Step:     1000 Batch Loss:     2.012605 Tokens per Sec:    17140, Lr: 0.000200
2020-06-22 01:20:26,908 Hooray! New best validation result [ppl]!
2020-06-22 01:20:26,909 Saving new checkpoint.
2020-06-22 01:20:28,885 Example #0
2020-06-22 01:20:28,886 	Raw source:     ['hello', '.']
2020-06-22 01:20:28,886 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:20:28,886 	Source:     hello .
2020-06-22 01:20:28,886 	Reference:  hallo ,
2020-06-22 01:20:28,886 	Hypothesis: hallo .
2020-06-22 01:20:28,886 Example #1
2020-06-22 01:20:28,886 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:20:28,886 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:20:28,886 	Source:     hi , how can i help you ?
2020-06-22 01:20:28,886 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:20:28,886 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:20:28,886 Example #2
2020-06-22 01:20:28,886 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:20:28,886 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:20:28,886 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:20:28,886 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:20:28,886 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:20:28,886 Example #3
2020-06-22 01:20:28,886 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:20:28,886 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'möchten', 'sie', '?']
2020-06-22 01:20:28,886 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:20:28,886 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:20:28,887 	Hypothesis: ok , welche art von restaurant möchten sie ?
2020-06-22 01:20:28,887 Validation result (greedy) at epoch  19, step     1000: bleu:  24.93, loss: 53694.8438, ppl:  12.7527, duration: 14.1715s
2020-06-22 01:20:34,384 Epoch  19: total training loss 101.76
2020-06-22 01:20:34,385 EPOCH 20
2020-06-22 01:20:41,553 Epoch  20: total training loss 99.79
2020-06-22 01:20:41,554 EPOCH 21
2020-06-22 01:20:41,836 Epoch  21 Step:     1100 Batch Loss:     2.033697 Tokens per Sec:    19741, Lr: 0.000200
2020-06-22 01:20:48,770 Epoch  21: total training loss 95.46
2020-06-22 01:20:48,770 EPOCH 22
2020-06-22 01:20:54,872 Epoch  22 Step:     1200 Batch Loss:     1.028600 Tokens per Sec:    18638, Lr: 0.000200
2020-06-22 01:20:55,811 Epoch  22: total training loss 86.45
2020-06-22 01:20:55,811 EPOCH 23
2020-06-22 01:21:02,987 Epoch  23: total training loss 83.38
2020-06-22 01:21:02,987 EPOCH 24
2020-06-22 01:21:08,182 Epoch  24 Step:     1300 Batch Loss:     1.301962 Tokens per Sec:    17864, Lr: 0.000200
2020-06-22 01:21:10,491 Epoch  24: total training loss 82.91
2020-06-22 01:21:10,491 EPOCH 25
2020-06-22 01:21:21,197 Epoch  25: total training loss 83.26
2020-06-22 01:21:21,198 EPOCH 26
2020-06-22 01:21:26,595 Epoch  26 Step:     1400 Batch Loss:     0.957663 Tokens per Sec:    12341, Lr: 0.000200
2020-06-22 01:21:32,001 Epoch  26: total training loss 79.40
2020-06-22 01:21:32,001 EPOCH 27
2020-06-22 01:21:39,992 Epoch  27: total training loss 74.47
2020-06-22 01:21:39,995 EPOCH 28
2020-06-22 01:21:42,450 Epoch  28 Step:     1500 Batch Loss:     2.060584 Tokens per Sec:    17251, Lr: 0.000200
2020-06-22 01:21:47,271 Epoch  28: total training loss 70.07
2020-06-22 01:21:47,272 EPOCH 29
2020-06-22 01:21:54,723 Epoch  29: total training loss 72.10
2020-06-22 01:21:54,723 EPOCH 30
2020-06-22 01:21:55,709 Epoch  30 Step:     1600 Batch Loss:     1.357449 Tokens per Sec:    20152, Lr: 0.000200
2020-06-22 01:22:01,943 Epoch  30: total training loss 67.05
2020-06-22 01:22:01,944 EPOCH 31
2020-06-22 01:22:08,543 Epoch  31 Step:     1700 Batch Loss:     2.435219 Tokens per Sec:    18715, Lr: 0.000200
2020-06-22 01:22:08,916 Epoch  31: total training loss 65.78
2020-06-22 01:22:08,917 EPOCH 32
2020-06-22 01:22:15,830 Epoch  32: total training loss 64.12
2020-06-22 01:22:15,830 EPOCH 33
2020-06-22 01:22:21,081 Epoch  33 Step:     1800 Batch Loss:     0.530132 Tokens per Sec:    18415, Lr: 0.000200
2020-06-22 01:22:22,819 Epoch  33: total training loss 58.94
2020-06-22 01:22:22,820 EPOCH 34
2020-06-22 01:22:29,869 Epoch  34: total training loss 60.09
2020-06-22 01:22:29,869 EPOCH 35
2020-06-22 01:22:33,757 Epoch  35 Step:     1900 Batch Loss:     1.125067 Tokens per Sec:    18922, Lr: 0.000200
2020-06-22 01:22:36,948 Epoch  35: total training loss 54.43
2020-06-22 01:22:36,949 EPOCH 36
2020-06-22 01:22:43,941 Epoch  36: total training loss 54.73
2020-06-22 01:22:43,942 EPOCH 37
2020-06-22 01:22:46,414 Epoch  37 Step:     2000 Batch Loss:     0.432503 Tokens per Sec:    19215, Lr: 0.000200
2020-06-22 01:22:58,572 Hooray! New best validation result [ppl]!
2020-06-22 01:22:58,572 Saving new checkpoint.
2020-06-22 01:23:00,643 Example #0
2020-06-22 01:23:00,643 	Raw source:     ['hello', '.']
2020-06-22 01:23:00,643 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:23:00,644 	Source:     hello .
2020-06-22 01:23:00,644 	Reference:  hallo ,
2020-06-22 01:23:00,644 	Hypothesis: hallo .
2020-06-22 01:23:00,644 Example #1
2020-06-22 01:23:00,644 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:23:00,644 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:23:00,644 	Source:     hi , how can i help you ?
2020-06-22 01:23:00,644 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:23:00,644 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:23:00,644 Example #2
2020-06-22 01:23:00,644 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:23:00,644 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:23:00,644 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:23:00,644 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:23:00,644 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:23:00,644 Example #3
2020-06-22 01:23:00,644 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:23:00,644 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:23:00,645 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:23:00,645 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:23:00,645 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:23:00,645 Validation result (greedy) at epoch  37, step     2000: bleu:  33.97, loss: 48752.4414, ppl:  10.0887, duration: 14.2306s
2020-06-22 01:23:05,465 Epoch  37: total training loss 52.32
2020-06-22 01:23:05,465 EPOCH 38
2020-06-22 01:23:13,111 Epoch  38: total training loss 48.37
2020-06-22 01:23:13,111 EPOCH 39
2020-06-22 01:23:14,404 Epoch  39 Step:     2100 Batch Loss:     0.710942 Tokens per Sec:    18361, Lr: 0.000200
2020-06-22 01:23:20,578 Epoch  39: total training loss 47.22
2020-06-22 01:23:20,579 EPOCH 40
2020-06-22 01:23:27,841 Epoch  40: total training loss 43.37
2020-06-22 01:23:27,842 EPOCH 41
2020-06-22 01:23:27,977 Epoch  41 Step:     2200 Batch Loss:     0.775920 Tokens per Sec:    17324, Lr: 0.000200
2020-06-22 01:23:34,889 Epoch  41: total training loss 43.14
2020-06-22 01:23:34,890 EPOCH 42
2020-06-22 01:23:40,946 Epoch  42 Step:     2300 Batch Loss:     0.614578 Tokens per Sec:    18384, Lr: 0.000200
2020-06-22 01:23:41,935 Epoch  42: total training loss 41.14
2020-06-22 01:23:41,935 EPOCH 43
2020-06-22 01:23:49,018 Epoch  43: total training loss 42.27
2020-06-22 01:23:49,018 EPOCH 44
2020-06-22 01:23:53,471 Epoch  44 Step:     2400 Batch Loss:     0.919315 Tokens per Sec:    19756, Lr: 0.000200
2020-06-22 01:23:56,070 Epoch  44: total training loss 39.20
2020-06-22 01:23:56,071 EPOCH 45
2020-06-22 01:24:03,115 Epoch  45: total training loss 39.27
2020-06-22 01:24:03,118 EPOCH 46
2020-06-22 01:24:06,685 Epoch  46 Step:     2500 Batch Loss:     0.582753 Tokens per Sec:    16825, Lr: 0.000200
2020-06-22 01:24:10,684 Epoch  46: total training loss 35.94
2020-06-22 01:24:10,685 EPOCH 47
2020-06-22 01:24:17,814 Epoch  47: total training loss 35.54
2020-06-22 01:24:17,814 EPOCH 48
2020-06-22 01:24:19,835 Epoch  48 Step:     2600 Batch Loss:     0.772490 Tokens per Sec:    17940, Lr: 0.000200
2020-06-22 01:24:24,823 Epoch  48: total training loss 34.28
2020-06-22 01:24:24,824 EPOCH 49
2020-06-22 01:24:31,911 Epoch  49: total training loss 34.10
2020-06-22 01:24:31,911 EPOCH 50
2020-06-22 01:24:32,662 Epoch  50 Step:     2700 Batch Loss:     0.386671 Tokens per Sec:    14268, Lr: 0.000200
2020-06-22 01:24:38,996 Epoch  50: total training loss 32.88
2020-06-22 01:24:38,996 EPOCH 51
2020-06-22 01:24:45,463 Epoch  51 Step:     2800 Batch Loss:     0.828440 Tokens per Sec:    18009, Lr: 0.000200
2020-06-22 01:24:46,241 Epoch  51: total training loss 31.39
2020-06-22 01:24:46,241 EPOCH 52
2020-06-22 01:24:53,413 Epoch  52: total training loss 30.01
2020-06-22 01:24:53,413 EPOCH 53
2020-06-22 01:24:58,338 Epoch  53 Step:     2900 Batch Loss:     0.582132 Tokens per Sec:    18344, Lr: 0.000200
2020-06-22 01:25:00,676 Epoch  53: total training loss 29.76
2020-06-22 01:25:00,676 EPOCH 54
2020-06-22 01:25:07,691 Epoch  54: total training loss 27.00
2020-06-22 01:25:07,692 EPOCH 55
2020-06-22 01:25:11,409 Epoch  55 Step:     3000 Batch Loss:     0.834480 Tokens per Sec:    18020, Lr: 0.000200
2020-06-22 01:25:24,398 Example #0
2020-06-22 01:25:24,399 	Raw source:     ['hello', '.']
2020-06-22 01:25:24,399 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:25:24,399 	Source:     hello .
2020-06-22 01:25:24,399 	Reference:  hallo ,
2020-06-22 01:25:24,399 	Hypothesis: hallo .
2020-06-22 01:25:24,399 Example #1
2020-06-22 01:25:24,399 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:25:24,399 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:25:24,399 	Source:     hi , how can i help you ?
2020-06-22 01:25:24,399 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:25:24,399 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:25:24,399 Example #2
2020-06-22 01:25:24,399 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:25:24,399 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:25:24,399 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:25:24,399 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:25:24,399 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:25:24,399 Example #3
2020-06-22 01:25:24,399 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:25:24,400 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:25:24,400 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:25:24,400 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:25:24,400 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:25:24,400 Validation result (greedy) at epoch  55, step     3000: bleu:  36.75, loss: 48971.3008, ppl:  10.1940, duration: 12.9894s
2020-06-22 01:25:27,759 Epoch  55: total training loss 25.84
2020-06-22 01:25:27,759 EPOCH 56
2020-06-22 01:25:34,814 Epoch  56: total training loss 24.63
2020-06-22 01:25:34,814 EPOCH 57
2020-06-22 01:25:37,249 Epoch  57 Step:     3100 Batch Loss:     0.742244 Tokens per Sec:    17982, Lr: 0.000200
2020-06-22 01:25:41,898 Epoch  57: total training loss 24.14
2020-06-22 01:25:41,899 EPOCH 58
2020-06-22 01:25:48,965 Epoch  58: total training loss 24.25
2020-06-22 01:25:48,966 EPOCH 59
2020-06-22 01:25:49,883 Epoch  59 Step:     3200 Batch Loss:     0.340191 Tokens per Sec:    19808, Lr: 0.000200
2020-06-22 01:25:56,137 Epoch  59: total training loss 23.00
2020-06-22 01:25:56,137 EPOCH 60
2020-06-22 01:26:02,815 Epoch  60 Step:     3300 Batch Loss:     0.622251 Tokens per Sec:    18153, Lr: 0.000200
2020-06-22 01:26:03,257 Epoch  60: total training loss 23.27
2020-06-22 01:26:03,257 EPOCH 61
2020-06-22 01:26:10,295 Epoch  61: total training loss 21.95
2020-06-22 01:26:10,295 EPOCH 62
2020-06-22 01:26:15,470 Epoch  62 Step:     3400 Batch Loss:     0.396255 Tokens per Sec:    19279, Lr: 0.000200
2020-06-22 01:26:17,267 Epoch  62: total training loss 20.96
2020-06-22 01:26:17,267 EPOCH 63
2020-06-22 01:26:24,419 Epoch  63: total training loss 21.70
2020-06-22 01:26:24,420 EPOCH 64
2020-06-22 01:26:28,473 Epoch  64 Step:     3500 Batch Loss:     0.325981 Tokens per Sec:    18786, Lr: 0.000200
2020-06-22 01:26:31,453 Epoch  64: total training loss 20.17
2020-06-22 01:26:31,454 EPOCH 65
2020-06-22 01:26:38,558 Epoch  65: total training loss 19.30
2020-06-22 01:26:38,559 EPOCH 66
2020-06-22 01:26:41,422 Epoch  66 Step:     3600 Batch Loss:     0.292556 Tokens per Sec:    18408, Lr: 0.000200
2020-06-22 01:26:45,602 Epoch  66: total training loss 18.36
2020-06-22 01:26:45,603 EPOCH 67
2020-06-22 01:26:52,551 Epoch  67: total training loss 17.84
2020-06-22 01:26:52,551 EPOCH 68
2020-06-22 01:26:54,014 Epoch  68 Step:     3700 Batch Loss:     0.261365 Tokens per Sec:    17828, Lr: 0.000200
2020-06-22 01:26:59,623 Epoch  68: total training loss 17.65
2020-06-22 01:26:59,624 EPOCH 69
2020-06-22 01:27:06,574 Epoch  69: total training loss 16.61
2020-06-22 01:27:06,574 EPOCH 70
2020-06-22 01:27:06,795 Epoch  70 Step:     3800 Batch Loss:     0.317995 Tokens per Sec:    25815, Lr: 0.000200
2020-06-22 01:27:13,596 Epoch  70: total training loss 16.39
2020-06-22 01:27:13,596 EPOCH 71
2020-06-22 01:27:19,656 Epoch  71 Step:     3900 Batch Loss:     0.249432 Tokens per Sec:    18621, Lr: 0.000200
2020-06-22 01:27:20,612 Epoch  71: total training loss 16.38
2020-06-22 01:27:20,613 EPOCH 72
2020-06-22 01:27:27,715 Epoch  72: total training loss 15.53
2020-06-22 01:27:27,715 EPOCH 73
2020-06-22 01:27:32,620 Epoch  73 Step:     4000 Batch Loss:     0.232921 Tokens per Sec:    18309, Lr: 0.000200
2020-06-22 01:27:44,102 Example #0
2020-06-22 01:27:44,102 	Raw source:     ['hello', '.']
2020-06-22 01:27:44,102 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:27:44,102 	Source:     hello .
2020-06-22 01:27:44,102 	Reference:  hallo ,
2020-06-22 01:27:44,102 	Hypothesis: hallo .
2020-06-22 01:27:44,102 Example #1
2020-06-22 01:27:44,102 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:27:44,102 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:27:44,102 	Source:     hi , how can i help you ?
2020-06-22 01:27:44,102 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:27:44,102 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:27:44,102 Example #2
2020-06-22 01:27:44,102 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:27:44,102 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:27:44,102 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:27:44,102 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:27:44,102 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:27:44,102 Example #3
2020-06-22 01:27:44,102 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:27:44,102 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:27:44,102 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:27:44,102 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:27:44,102 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:27:44,102 Validation result (greedy) at epoch  73, step     4000: bleu:  37.47, loss: 50392.4180, ppl:  10.9045, duration: 11.4820s
2020-06-22 01:27:46,323 Epoch  73: total training loss 15.86
2020-06-22 01:27:46,323 EPOCH 74
2020-06-22 01:27:53,356 Epoch  74: total training loss 15.37
2020-06-22 01:27:53,357 EPOCH 75
2020-06-22 01:27:56,871 Epoch  75 Step:     4100 Batch Loss:     0.248032 Tokens per Sec:    18761, Lr: 0.000200
2020-06-22 01:28:00,383 Epoch  75: total training loss 14.94
2020-06-22 01:28:00,383 EPOCH 76
2020-06-22 01:28:07,383 Epoch  76: total training loss 15.04
2020-06-22 01:28:07,383 EPOCH 77
2020-06-22 01:28:09,712 Epoch  77 Step:     4200 Batch Loss:     0.250176 Tokens per Sec:    18272, Lr: 0.000200
2020-06-22 01:28:14,358 Epoch  77: total training loss 14.63
2020-06-22 01:28:14,358 EPOCH 78
2020-06-22 01:28:21,325 Epoch  78: total training loss 13.81
2020-06-22 01:28:21,326 EPOCH 79
2020-06-22 01:28:22,434 Epoch  79 Step:     4300 Batch Loss:     0.237083 Tokens per Sec:    20236, Lr: 0.000200
2020-06-22 01:28:28,339 Epoch  79: total training loss 13.59
2020-06-22 01:28:28,340 EPOCH 80
2020-06-22 01:28:35,249 Epoch  80 Step:     4400 Batch Loss:     0.209532 Tokens per Sec:    18492, Lr: 0.000200
2020-06-22 01:28:35,368 Epoch  80: total training loss 13.49
2020-06-22 01:28:35,368 EPOCH 81
2020-06-22 01:28:42,432 Epoch  81: total training loss 13.25
2020-06-22 01:28:42,432 EPOCH 82
2020-06-22 01:28:48,065 Epoch  82 Step:     4500 Batch Loss:     0.154354 Tokens per Sec:    18705, Lr: 0.000200
2020-06-22 01:28:49,505 Epoch  82: total training loss 13.48
2020-06-22 01:28:49,506 EPOCH 83
2020-06-22 01:28:56,924 Epoch  83: total training loss 13.75
2020-06-22 01:28:56,925 EPOCH 84
2020-06-22 01:29:01,489 Epoch  84 Step:     4600 Batch Loss:     0.242092 Tokens per Sec:    18094, Lr: 0.000200
2020-06-22 01:29:04,315 Epoch  84: total training loss 12.58
2020-06-22 01:29:04,316 EPOCH 85
2020-06-22 01:29:11,745 Epoch  85: total training loss 12.36
2020-06-22 01:29:11,746 EPOCH 86
2020-06-22 01:29:14,815 Epoch  86 Step:     4700 Batch Loss:     0.221460 Tokens per Sec:    18169, Lr: 0.000200
2020-06-22 01:29:18,851 Epoch  86: total training loss 12.40
2020-06-22 01:29:18,851 EPOCH 87
2020-06-22 01:29:26,045 Epoch  87: total training loss 12.50
2020-06-22 01:29:26,046 EPOCH 88
2020-06-22 01:29:27,808 Epoch  88 Step:     4800 Batch Loss:     0.257213 Tokens per Sec:    17419, Lr: 0.000200
2020-06-22 01:29:33,417 Epoch  88: total training loss 12.09
2020-06-22 01:29:33,417 EPOCH 89
2020-06-22 01:29:40,422 Epoch  89: total training loss 11.65
2020-06-22 01:29:40,423 EPOCH 90
2020-06-22 01:29:40,880 Epoch  90 Step:     4900 Batch Loss:     0.190111 Tokens per Sec:    15776, Lr: 0.000200
2020-06-22 01:29:47,648 Epoch  90: total training loss 11.77
2020-06-22 01:29:47,649 EPOCH 91
2020-06-22 01:29:53,882 Epoch  91 Step:     5000 Batch Loss:     0.196022 Tokens per Sec:    18004, Lr: 0.000200
2020-06-22 01:30:06,354 Example #0
2020-06-22 01:30:06,354 	Raw source:     ['hello', '.']
2020-06-22 01:30:06,354 	Raw hypothesis: ['hallo', '.']
2020-06-22 01:30:06,354 	Source:     hello .
2020-06-22 01:30:06,354 	Reference:  hallo ,
2020-06-22 01:30:06,354 	Hypothesis: hallo .
2020-06-22 01:30:06,354 Example #1
2020-06-22 01:30:06,354 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 01:30:06,354 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 01:30:06,354 	Source:     hi , how can i help you ?
2020-06-22 01:30:06,354 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 01:30:06,354 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 01:30:06,355 Example #2
2020-06-22 01:30:06,355 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 01:30:06,355 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 01:30:06,355 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 01:30:06,355 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 01:30:06,355 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 01:30:06,355 Example #3
2020-06-22 01:30:06,355 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 01:30:06,355 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 01:30:06,355 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 01:30:06,355 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 01:30:06,355 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 01:30:06,355 Validation result (greedy) at epoch  91, step     5000: bleu:  37.08, loss: 50809.9414, ppl:  11.1225, duration: 12.4720s
2020-06-22 01:30:07,524 Epoch  91: total training loss 11.67
2020-06-22 01:30:07,524 EPOCH 92
2020-06-22 01:30:15,193 Epoch  92: total training loss 11.47
2020-06-22 01:30:15,194 EPOCH 93
2020-06-22 01:30:19,984 Epoch  93 Step:     5100 Batch Loss:     0.204003 Tokens per Sec:    18109, Lr: 0.000200
2020-06-22 01:30:22,838 Epoch  93: total training loss 11.22
2020-06-22 01:30:22,839 EPOCH 94
2020-06-22 01:30:30,359 Epoch  94: total training loss 10.59
2020-06-22 01:30:30,360 EPOCH 95
2020-06-22 01:30:34,114 Epoch  95 Step:     5200 Batch Loss:     0.184775 Tokens per Sec:    16612, Lr: 0.000200
2020-06-22 01:30:37,912 Epoch  95: total training loss 10.88
2020-06-22 01:30:37,913 EPOCH 96
2020-06-22 01:30:45,534 Epoch  96: total training loss 10.59
2020-06-22 01:30:45,535 EPOCH 97
2020-06-22 01:30:47,751 Epoch  97 Step:     5300 Batch Loss:     0.184710 Tokens per Sec:    18145, Lr: 0.000200
2020-06-22 01:30:53,158 Epoch  97: total training loss 10.48
2020-06-22 01:30:53,159 EPOCH 98
2020-06-22 01:31:00,577 Epoch  98: total training loss 10.68
2020-06-22 01:31:00,578 EPOCH 99
2020-06-22 01:31:01,357 Epoch  99 Step:     5400 Batch Loss:     0.253575 Tokens per Sec:    17275, Lr: 0.000200
2020-06-22 01:31:08,212 Epoch  99: total training loss 11.08
2020-06-22 01:31:08,212 EPOCH 100
2020-06-22 01:31:15,496 Epoch 100 Step:     5500 Batch Loss:     0.178592 Tokens per Sec:    17232, Lr: 0.000200
2020-06-22 01:31:15,845 Epoch 100: total training loss 10.20
2020-06-22 01:31:15,845 Training ended after 100 epochs.
2020-06-22 01:31:15,845 Best validation result (greedy) at step     2000:  10.09 ppl.
2020-06-22 01:31:27,850  dev bleu:  35.15 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 01:31:27,855 Translations saved to: models/transformer_multi_enc_hid256_ende/00002000.hyps.dev
2020-06-22 01:31:36,848 test bleu:  32.24 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 01:31:36,853 Translations saved to: models/transformer_multi_enc_hid256_ende/00002000.hyps.test
