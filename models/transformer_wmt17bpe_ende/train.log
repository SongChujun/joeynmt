2020-07-01 10:42:18,443 Hello! This is Joey-NMT.
2020-07-01 10:42:31,790 Total params: 49382400
2020-07-01 10:42:31,792 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-07-01 10:42:34,084 cfg.name                           : transformer
2020-07-01 10:42:34,084 cfg.data.src                       : en
2020-07-01 10:42:34,084 cfg.data.trg                       : de
2020-07-01 10:42:34,084 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-01 10:42:34,084 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-01 10:42:34,084 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-01 10:42:34,084 cfg.data.level                     : bpe
2020-07-01 10:42:34,084 cfg.data.lowercase                 : False
2020-07-01 10:42:34,084 cfg.data.max_sent_length           : 100
2020-07-01 10:42:34,084 cfg.testing.beam_size              : 5
2020-07-01 10:42:34,085 cfg.testing.alpha                  : 1.0
2020-07-01 10:42:34,085 cfg.training.random_seed           : 42
2020-07-01 10:42:34,085 cfg.training.optimizer             : adam
2020-07-01 10:42:34,085 cfg.training.normalization         : tokens
2020-07-01 10:42:34,085 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-01 10:42:34,085 cfg.training.scheduling            : plateau
2020-07-01 10:42:34,085 cfg.training.patience              : 8
2020-07-01 10:42:34,085 cfg.training.decrease_factor       : 0.7
2020-07-01 10:42:34,085 cfg.training.loss                  : crossentropy
2020-07-01 10:42:34,085 cfg.training.learning_rate         : 0.0002
2020-07-01 10:42:34,085 cfg.training.learning_rate_min     : 1e-08
2020-07-01 10:42:34,085 cfg.training.weight_decay          : 0.0
2020-07-01 10:42:34,085 cfg.training.label_smoothing       : 0.1
2020-07-01 10:42:34,085 cfg.training.batch_size            : 4096
2020-07-01 10:42:34,085 cfg.training.batch_type            : token
2020-07-01 10:42:34,085 cfg.training.batch_multiplier      : 1
2020-07-01 10:42:34,085 cfg.training.early_stopping_metric : ppl
2020-07-01 10:42:34,085 cfg.training.epochs                : 100
2020-07-01 10:42:34,085 cfg.training.validation_freq       : 1000
2020-07-01 10:42:34,085 cfg.training.logging_freq          : 100
2020-07-01 10:42:34,085 cfg.training.eval_metric           : bleu
2020-07-01 10:42:34,085 cfg.training.model_dir             : models/transformer_wmt17bpe_ende
2020-07-01 10:42:34,085 cfg.training.overwrite             : True
2020-07-01 10:42:34,085 cfg.training.shuffle               : True
2020-07-01 10:42:34,085 cfg.training.use_cuda              : True
2020-07-01 10:42:34,085 cfg.training.max_output_length     : 100
2020-07-01 10:42:34,085 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-01 10:42:34,085 cfg.training.keep_last_ckpts       : 3
2020-07-01 10:42:34,085 cfg.model.initializer              : xavier
2020-07-01 10:42:34,085 cfg.model.bias_initializer         : zeros
2020-07-01 10:42:34,085 cfg.model.init_gain                : 1.0
2020-07-01 10:42:34,085 cfg.model.embed_initializer        : xavier
2020-07-01 10:42:34,085 cfg.model.embed_init_gain          : 1.0
2020-07-01 10:42:34,085 cfg.model.tied_embeddings          : False
2020-07-01 10:42:34,085 cfg.model.tied_softmax             : True
2020-07-01 10:42:34,085 cfg.model.encoder.type             : transformer
2020-07-01 10:42:34,085 cfg.model.encoder.num_layers       : 6
2020-07-01 10:42:34,085 cfg.model.encoder.num_heads        : 8
2020-07-01 10:42:34,085 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-01 10:42:34,085 cfg.model.encoder.embeddings.scale : True
2020-07-01 10:42:34,085 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-01 10:42:34,086 cfg.model.encoder.hidden_size      : 512
2020-07-01 10:42:34,086 cfg.model.encoder.ff_size          : 2048
2020-07-01 10:42:34,086 cfg.model.encoder.freeze           : False
2020-07-01 10:42:34,086 cfg.model.encoder.dropout          : 0.1
2020-07-01 10:42:34,086 cfg.model.decoder.type             : transformer
2020-07-01 10:42:34,086 cfg.model.decoder.num_layers       : 6
2020-07-01 10:42:34,086 cfg.model.decoder.num_heads        : 8
2020-07-01 10:42:34,086 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-01 10:42:34,086 cfg.model.decoder.embeddings.scale : True
2020-07-01 10:42:34,086 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-01 10:42:34,086 cfg.model.decoder.hidden_size      : 512
2020-07-01 10:42:34,086 cfg.model.decoder.ff_size          : 2048
2020-07-01 10:42:34,086 cfg.model.decoder.freeze           : False
2020-07-01 10:42:34,086 cfg.model.decoder.dropout          : 0.1
2020-07-01 10:42:34,086 Data set sizes: 
	train 9771,
	valid 1525,
	test 1165
2020-07-01 10:42:34,086 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-01 10:42:34,086 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) &@@ (9) a@@
2020-07-01 10:42:34,086 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) Sie (8) ich (9) das
2020-07-01 10:42:34,086 Number of Src words (types): 4442
2020-07-01 10:42:34,086 Number of Trg words (types): 5796
2020-07-01 10:42:34,086 Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4442),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5796))
2020-07-01 10:42:34,094 EPOCH 1
2020-07-01 10:42:45,412 Epoch   1: total training loss 374.95
2020-07-01 10:42:45,413 EPOCH 2
2020-07-01 10:42:50,669 Epoch   2 Step:      100 Batch Loss:     4.258930 Tokens per Sec:    15199, Lr: 0.000200
2020-07-01 10:42:55,870 Epoch   2: total training loss 332.04
2020-07-01 10:42:55,871 EPOCH 3
2020-07-01 10:43:06,386 Epoch   3 Step:      200 Batch Loss:     3.972482 Tokens per Sec:    14158, Lr: 0.000200
2020-07-01 10:43:06,709 Epoch   3: total training loss 313.31
2020-07-01 10:43:06,709 EPOCH 4
2020-07-01 10:43:17,688 Epoch   4: total training loss 273.08
2020-07-01 10:43:17,688 EPOCH 5
2020-07-01 10:43:22,216 Epoch   5 Step:      300 Batch Loss:     2.299614 Tokens per Sec:    14701, Lr: 0.000200
2020-07-01 10:43:28,553 Epoch   5: total training loss 243.18
2020-07-01 10:43:28,554 EPOCH 6
2020-07-01 10:43:38,577 Epoch   6 Step:      400 Batch Loss:     2.680868 Tokens per Sec:    13835, Lr: 0.000200
2020-07-01 10:43:39,684 Epoch   6: total training loss 211.94
2020-07-01 10:43:39,684 EPOCH 7
2020-07-01 10:43:51,181 Epoch   7: total training loss 186.42
2020-07-01 10:43:51,181 EPOCH 8
2020-07-01 10:43:55,672 Epoch   8 Step:      500 Batch Loss:     2.226015 Tokens per Sec:    13219, Lr: 0.000200
2020-07-01 10:44:03,272 Epoch   8: total training loss 160.80
2020-07-01 10:44:03,273 EPOCH 9
2020-07-01 10:44:13,401 Epoch   9 Step:      600 Batch Loss:     2.469547 Tokens per Sec:    13135, Lr: 0.000200
2020-07-01 10:44:15,030 Epoch   9: total training loss 145.25
2020-07-01 10:44:15,030 EPOCH 10
2020-07-01 10:44:26,790 Epoch  10: total training loss 127.12
2020-07-01 10:44:26,791 EPOCH 11
2020-07-01 10:44:31,002 Epoch  11 Step:      700 Batch Loss:     1.103807 Tokens per Sec:    13241, Lr: 0.000200
2020-07-01 10:44:38,386 Epoch  11: total training loss 117.49
2020-07-01 10:44:38,387 EPOCH 12
2020-07-01 10:44:48,618 Epoch  12 Step:      800 Batch Loss:     1.730232 Tokens per Sec:    12693, Lr: 0.000200
2020-07-01 10:44:50,614 Epoch  12: total training loss 106.89
2020-07-01 10:44:50,614 EPOCH 13
2020-07-01 10:45:02,709 Epoch  13: total training loss 96.58
2020-07-01 10:45:02,710 EPOCH 14
2020-07-01 10:45:06,615 Epoch  14 Step:      900 Batch Loss:     0.960881 Tokens per Sec:    12524, Lr: 0.000200
2020-07-01 10:45:14,864 Epoch  14: total training loss 88.53
2020-07-01 10:45:14,864 EPOCH 15
2020-07-01 10:45:24,485 Epoch  15 Step:     1000 Batch Loss:     0.997561 Tokens per Sec:    12828, Lr: 0.000200
2020-07-01 10:46:15,212 Hooray! New best validation result [ppl]!
2020-07-01 10:46:15,213 Saving new checkpoint.
2020-07-01 10:46:21,581 Example #0
2020-07-01 10:46:21,581 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 10:46:21,581 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 10:46:21,581 	Source:     Hello .
2020-07-01 10:46:21,581 	Reference:  Hallo ,
2020-07-01 10:46:21,581 	Hypothesis: Hallo .
2020-07-01 10:46:21,581 Example #1
2020-07-01 10:46:21,581 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 10:46:21,581 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 10:46:21,581 	Source:     Hi , how can I help you ?
2020-07-01 10:46:21,581 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:46:21,581 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:46:21,581 Example #2
2020-07-01 10:46:21,581 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 10:46:21,581 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 10:46:21,582 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 10:46:21,582 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 10:46:21,582 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 10:46:21,582 Example #3
2020-07-01 10:46:21,582 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 10:46:21,582 	Raw hypothesis: ['O@@', 'k', ',', 'welche', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 10:46:21,582 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 10:46:21,582 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 10:46:21,582 	Hypothesis: Ok , welche Art von Restaurant suchen Sie ?
2020-07-01 10:46:21,582 Validation result (greedy) at epoch  15, step     1000: bleu:  29.26, loss: 48002.0195, ppl:   7.1554, duration: 57.0962s
2020-07-01 10:46:24,049 Epoch  15: total training loss 83.49
2020-07-01 10:46:24,049 EPOCH 16
2020-07-01 10:46:35,938 Epoch  16: total training loss 71.01
2020-07-01 10:46:35,938 EPOCH 17
2020-07-01 10:46:39,153 Epoch  17 Step:     1100 Batch Loss:     0.561791 Tokens per Sec:    12226, Lr: 0.000200
2020-07-01 10:46:47,936 Epoch  17: total training loss 65.15
2020-07-01 10:46:47,937 EPOCH 18
2020-07-01 10:46:57,194 Epoch  18 Step:     1200 Batch Loss:     0.682022 Tokens per Sec:    12768, Lr: 0.000200
2020-07-01 10:47:00,163 Epoch  18: total training loss 58.19
2020-07-01 10:47:00,163 EPOCH 19
2020-07-01 10:47:12,337 Epoch  19: total training loss 53.25
2020-07-01 10:47:12,338 EPOCH 20
2020-07-01 10:47:14,844 Epoch  20 Step:     1300 Batch Loss:     0.529475 Tokens per Sec:    13375, Lr: 0.000200
2020-07-01 10:47:23,832 Epoch  20: total training loss 48.35
2020-07-01 10:47:23,833 EPOCH 21
2020-07-01 10:47:31,904 Epoch  21 Step:     1400 Batch Loss:     0.569776 Tokens per Sec:    13540, Lr: 0.000200
2020-07-01 10:47:35,298 Epoch  21: total training loss 42.07
2020-07-01 10:47:35,299 EPOCH 22
2020-07-01 10:47:46,767 Epoch  22: total training loss 37.39
2020-07-01 10:47:46,768 EPOCH 23
2020-07-01 10:47:48,961 Epoch  23 Step:     1500 Batch Loss:     0.420339 Tokens per Sec:    13378, Lr: 0.000200
2020-07-01 10:47:58,256 Epoch  23: total training loss 34.78
2020-07-01 10:47:58,257 EPOCH 24
2020-07-01 10:48:06,060 Epoch  24 Step:     1600 Batch Loss:     0.785037 Tokens per Sec:    13409, Lr: 0.000200
2020-07-01 10:48:09,789 Epoch  24: total training loss 32.73
2020-07-01 10:48:09,789 EPOCH 25
2020-07-01 10:48:21,623 Epoch  25: total training loss 28.95
2020-07-01 10:48:21,624 EPOCH 26
2020-07-01 10:48:23,647 Epoch  26 Step:     1700 Batch Loss:     0.377543 Tokens per Sec:    12281, Lr: 0.000200
2020-07-01 10:48:33,770 Epoch  26: total training loss 25.94
2020-07-01 10:48:33,770 EPOCH 27
2020-07-01 10:48:41,135 Epoch  27 Step:     1800 Batch Loss:     0.369301 Tokens per Sec:    13324, Lr: 0.000200
2020-07-01 10:48:45,304 Epoch  27: total training loss 24.24
2020-07-01 10:48:45,304 EPOCH 28
2020-07-01 10:48:57,424 Epoch  28: total training loss 22.70
2020-07-01 10:48:57,425 EPOCH 29
2020-07-01 10:48:58,474 Epoch  29 Step:     1900 Batch Loss:     0.290457 Tokens per Sec:    11527, Lr: 0.000200
2020-07-01 10:49:09,585 Epoch  29: total training loss 20.51
2020-07-01 10:49:09,586 EPOCH 30
2020-07-01 10:49:16,374 Epoch  30 Step:     2000 Batch Loss:     0.207537 Tokens per Sec:    12931, Lr: 0.000200
2020-07-01 10:49:52,856 Hooray! New best validation result [ppl]!
2020-07-01 10:49:52,857 Saving new checkpoint.
2020-07-01 10:49:59,157 Example #0
2020-07-01 10:49:59,158 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 10:49:59,158 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 10:49:59,158 	Source:     Hello .
2020-07-01 10:49:59,158 	Reference:  Hallo ,
2020-07-01 10:49:59,158 	Hypothesis: Hallo .
2020-07-01 10:49:59,158 Example #1
2020-07-01 10:49:59,158 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 10:49:59,158 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 10:49:59,158 	Source:     Hi , how can I help you ?
2020-07-01 10:49:59,158 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:49:59,158 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:49:59,158 Example #2
2020-07-01 10:49:59,158 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 10:49:59,158 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 10:49:59,159 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 10:49:59,159 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 10:49:59,159 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 10:49:59,159 Example #3
2020-07-01 10:49:59,159 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 10:49:59,159 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 10:49:59,159 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 10:49:59,159 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 10:49:59,159 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 10:49:59,159 Validation result (greedy) at epoch  30, step     2000: bleu:  35.90, loss: 45902.0469, ppl:   6.5651, duration: 42.7845s
2020-07-01 10:50:04,117 Epoch  30: total training loss 18.19
2020-07-01 10:50:04,118 EPOCH 31
2020-07-01 10:50:16,003 Epoch  31: total training loss 17.23
2020-07-01 10:50:16,003 EPOCH 32
2020-07-01 10:50:16,515 Epoch  32 Step:     2100 Batch Loss:     0.410908 Tokens per Sec:    12776, Lr: 0.000200
2020-07-01 10:50:27,899 Epoch  32: total training loss 16.92
2020-07-01 10:50:27,899 EPOCH 33
2020-07-01 10:50:34,156 Epoch  33 Step:     2200 Batch Loss:     0.240130 Tokens per Sec:    13096, Lr: 0.000200
2020-07-01 10:50:39,879 Epoch  33: total training loss 15.13
2020-07-01 10:50:39,879 EPOCH 34
2020-07-01 10:50:51,934 Epoch  34: total training loss 13.87
2020-07-01 10:50:51,935 EPOCH 35
2020-07-01 10:50:52,127 Epoch  35 Step:     2300 Batch Loss:     0.159630 Tokens per Sec:    10776, Lr: 0.000200
2020-07-01 10:51:03,708 Epoch  35: total training loss 12.96
2020-07-01 10:51:03,709 EPOCH 36
2020-07-01 10:51:09,962 Epoch  36 Step:     2400 Batch Loss:     0.194476 Tokens per Sec:    12929, Lr: 0.000200
2020-07-01 10:51:15,894 Epoch  36: total training loss 12.79
2020-07-01 10:51:15,894 EPOCH 37
2020-07-01 10:51:27,757 Epoch  37 Step:     2500 Batch Loss:     0.179915 Tokens per Sec:    12713, Lr: 0.000200
2020-07-01 10:51:27,950 Epoch  37: total training loss 12.16
2020-07-01 10:51:27,951 EPOCH 38
2020-07-01 10:51:40,162 Epoch  38: total training loss 11.80
2020-07-01 10:51:40,164 EPOCH 39
2020-07-01 10:51:45,747 Epoch  39 Step:     2600 Batch Loss:     0.172886 Tokens per Sec:    13139, Lr: 0.000200
2020-07-01 10:51:52,255 Epoch  39: total training loss 11.29
2020-07-01 10:51:52,256 EPOCH 40
2020-07-01 10:52:03,654 Epoch  40 Step:     2700 Batch Loss:     0.158736 Tokens per Sec:    12692, Lr: 0.000200
2020-07-01 10:52:04,354 Epoch  40: total training loss 11.04
2020-07-01 10:52:04,355 EPOCH 41
2020-07-01 10:52:16,482 Epoch  41: total training loss 10.69
2020-07-01 10:52:16,482 EPOCH 42
2020-07-01 10:52:21,378 Epoch  42 Step:     2800 Batch Loss:     0.138495 Tokens per Sec:    13043, Lr: 0.000200
2020-07-01 10:52:28,471 Epoch  42: total training loss 11.43
2020-07-01 10:52:28,471 EPOCH 43
2020-07-01 10:52:39,216 Epoch  43 Step:     2900 Batch Loss:     0.146443 Tokens per Sec:    12693, Lr: 0.000200
2020-07-01 10:52:40,590 Epoch  43: total training loss 10.31
2020-07-01 10:52:40,590 EPOCH 44
2020-07-01 10:52:52,706 Epoch  44: total training loss 9.63
2020-07-01 10:52:52,706 EPOCH 45
2020-07-01 10:52:57,172 Epoch  45 Step:     3000 Batch Loss:     0.132686 Tokens per Sec:    12648, Lr: 0.000200
2020-07-01 10:53:28,848 Example #0
2020-07-01 10:53:28,849 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 10:53:28,849 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 10:53:28,849 	Source:     Hello .
2020-07-01 10:53:28,849 	Reference:  Hallo ,
2020-07-01 10:53:28,849 	Hypothesis: Hallo .
2020-07-01 10:53:28,849 Example #1
2020-07-01 10:53:28,849 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 10:53:28,849 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 10:53:28,849 	Source:     Hi , how can I help you ?
2020-07-01 10:53:28,849 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:53:28,849 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:53:28,849 Example #2
2020-07-01 10:53:28,849 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 10:53:28,849 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'zum', 'Mittagessen', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', '.']
2020-07-01 10:53:28,849 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 10:53:28,849 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 10:53:28,849 	Hypothesis: Hallo , ich suche ein Restaurant zum Mittagessen in der Arden Fair Mall .
2020-07-01 10:53:28,849 Example #3
2020-07-01 10:53:28,849 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 10:53:28,849 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 10:53:28,849 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 10:53:28,849 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 10:53:28,850 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 10:53:28,850 Validation result (greedy) at epoch  45, step     3000: bleu:  36.09, loss: 46506.7617, ppl:   6.7299, duration: 31.6769s
2020-07-01 10:53:36,265 Epoch  45: total training loss 9.26
2020-07-01 10:53:36,266 EPOCH 46
2020-07-01 10:53:46,339 Epoch  46 Step:     3100 Batch Loss:     0.149557 Tokens per Sec:    13061, Lr: 0.000200
2020-07-01 10:53:48,021 Epoch  46: total training loss 8.99
2020-07-01 10:53:48,021 EPOCH 47
2020-07-01 10:54:00,031 Epoch  47: total training loss 8.84
2020-07-01 10:54:00,032 EPOCH 48
2020-07-01 10:54:04,196 Epoch  48 Step:     3200 Batch Loss:     0.144879 Tokens per Sec:    12461, Lr: 0.000200
2020-07-01 10:54:11,833 Epoch  48: total training loss 8.75
2020-07-01 10:54:11,833 EPOCH 49
2020-07-01 10:54:21,750 Epoch  49 Step:     3300 Batch Loss:     0.119986 Tokens per Sec:    12922, Lr: 0.000200
2020-07-01 10:54:23,849 Epoch  49: total training loss 8.62
2020-07-01 10:54:23,849 EPOCH 50
2020-07-01 10:54:35,845 Epoch  50: total training loss 8.78
2020-07-01 10:54:35,845 EPOCH 51
2020-07-01 10:54:39,409 Epoch  51 Step:     3400 Batch Loss:     0.128916 Tokens per Sec:    12820, Lr: 0.000200
2020-07-01 10:54:47,795 Epoch  51: total training loss 8.77
2020-07-01 10:54:47,795 EPOCH 52
2020-07-01 10:54:57,038 Epoch  52 Step:     3500 Batch Loss:     0.107267 Tokens per Sec:    12633, Lr: 0.000200
2020-07-01 10:54:59,982 Epoch  52: total training loss 8.46
2020-07-01 10:54:59,982 EPOCH 53
2020-07-01 10:55:12,110 Epoch  53: total training loss 9.31
2020-07-01 10:55:12,111 EPOCH 54
2020-07-01 10:55:14,629 Epoch  54 Step:     3600 Batch Loss:     0.133221 Tokens per Sec:    12338, Lr: 0.000200
2020-07-01 10:55:24,236 Epoch  54: total training loss 9.47
2020-07-01 10:55:24,236 EPOCH 55
2020-07-01 10:55:32,406 Epoch  55 Step:     3700 Batch Loss:     0.117282 Tokens per Sec:    12777, Lr: 0.000200
2020-07-01 10:55:36,305 Epoch  55: total training loss 8.47
2020-07-01 10:55:36,305 EPOCH 56
2020-07-01 10:55:48,452 Epoch  56: total training loss 8.18
2020-07-01 10:55:48,453 EPOCH 57
2020-07-01 10:55:50,016 Epoch  57 Step:     3800 Batch Loss:     0.115264 Tokens per Sec:    13119, Lr: 0.000200
2020-07-01 10:56:00,427 Epoch  57: total training loss 9.73
2020-07-01 10:56:00,427 EPOCH 58
2020-07-01 10:56:07,615 Epoch  58 Step:     3900 Batch Loss:     0.319093 Tokens per Sec:    12885, Lr: 0.000200
2020-07-01 10:56:12,476 Epoch  58: total training loss 9.88
2020-07-01 10:56:12,476 EPOCH 59
2020-07-01 10:56:24,383 Epoch  59: total training loss 8.41
2020-07-01 10:56:24,384 EPOCH 60
2020-07-01 10:56:25,407 Epoch  60 Step:     4000 Batch Loss:     0.099835 Tokens per Sec:    12775, Lr: 0.000200
2020-07-01 10:57:05,402 Example #0
2020-07-01 10:57:05,403 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 10:57:05,403 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 10:57:05,403 	Source:     Hello .
2020-07-01 10:57:05,403 	Reference:  Hallo ,
2020-07-01 10:57:05,403 	Hypothesis: Hallo .
2020-07-01 10:57:05,403 Example #1
2020-07-01 10:57:05,403 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 10:57:05,403 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 10:57:05,403 	Source:     Hi , how can I help you ?
2020-07-01 10:57:05,403 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:57:05,403 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 10:57:05,403 Example #2
2020-07-01 10:57:05,403 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 10:57:05,403 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 10:57:05,403 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 10:57:05,403 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 10:57:05,403 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 10:57:05,403 Example #3
2020-07-01 10:57:05,403 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 10:57:05,403 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 10:57:05,403 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 10:57:05,403 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 10:57:05,403 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 10:57:05,404 Validation result (greedy) at epoch  60, step     4000: bleu:  36.61, loss: 46811.0781, ppl:   6.8144, duration: 39.9958s
2020-07-01 10:57:15,918 Epoch  60: total training loss 7.66
2020-07-01 10:57:15,919 EPOCH 61
2020-07-01 10:57:22,531 Epoch  61 Step:     4100 Batch Loss:     0.096561 Tokens per Sec:    13070, Lr: 0.000200
2020-07-01 10:57:27,813 Epoch  61: total training loss 7.19
2020-07-01 10:57:27,814 EPOCH 62
2020-07-01 10:57:39,893 Epoch  62: total training loss 7.00
2020-07-01 10:57:39,894 EPOCH 63
2020-07-01 10:57:40,255 Epoch  63 Step:     4200 Batch Loss:     0.092920 Tokens per Sec:     8711, Lr: 0.000200
2020-07-01 10:57:51,870 Epoch  63: total training loss 6.69
2020-07-01 10:57:51,870 EPOCH 64
2020-07-01 10:57:57,861 Epoch  64 Step:     4300 Batch Loss:     0.103629 Tokens per Sec:    12744, Lr: 0.000200
2020-07-01 10:58:03,778 Epoch  64: total training loss 6.61
2020-07-01 10:58:03,778 EPOCH 65
2020-07-01 10:58:15,803 Epoch  65 Step:     4400 Batch Loss:     0.098396 Tokens per Sec:    12587, Lr: 0.000200
2020-07-01 10:58:15,986 Epoch  65: total training loss 6.55
2020-07-01 10:58:15,986 EPOCH 66
2020-07-01 10:58:27,715 Epoch  66: total training loss 6.63
2020-07-01 10:58:27,715 EPOCH 67
2020-07-01 10:58:33,347 Epoch  67 Step:     4500 Batch Loss:     0.081634 Tokens per Sec:    12737, Lr: 0.000200
2020-07-01 10:58:39,805 Epoch  67: total training loss 6.44
2020-07-01 10:58:39,806 EPOCH 68
2020-07-01 10:58:51,021 Epoch  68 Step:     4600 Batch Loss:     0.095412 Tokens per Sec:    13145, Lr: 0.000200
2020-07-01 10:58:51,520 Epoch  68: total training loss 6.35
2020-07-01 10:58:51,521 EPOCH 69
2020-07-01 10:59:03,654 Epoch  69: total training loss 6.26
2020-07-01 10:59:03,654 EPOCH 70
2020-07-01 10:59:09,054 Epoch  70 Step:     4700 Batch Loss:     0.085367 Tokens per Sec:    12666, Lr: 0.000200
2020-07-01 10:59:15,846 Epoch  70: total training loss 6.47
2020-07-01 10:59:15,847 EPOCH 71
2020-07-01 10:59:26,931 Epoch  71 Step:     4800 Batch Loss:     0.105401 Tokens per Sec:    12773, Lr: 0.000200
2020-07-01 10:59:27,853 Epoch  71: total training loss 7.18
2020-07-01 10:59:27,853 EPOCH 72
2020-07-01 10:59:39,985 Epoch  72: total training loss 6.87
2020-07-01 10:59:39,986 EPOCH 73
2020-07-01 10:59:44,734 Epoch  73 Step:     4900 Batch Loss:     0.102586 Tokens per Sec:    12981, Lr: 0.000200
2020-07-01 10:59:51,943 Epoch  73: total training loss 7.01
2020-07-01 10:59:51,943 EPOCH 74
2020-07-01 11:00:02,477 Epoch  74 Step:     5000 Batch Loss:     0.108240 Tokens per Sec:    12862, Lr: 0.000200
2020-07-01 11:00:38,165 Example #0
2020-07-01 11:00:38,166 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:00:38,166 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:00:38,166 	Source:     Hello .
2020-07-01 11:00:38,166 	Reference:  Hallo ,
2020-07-01 11:00:38,166 	Hypothesis: Hallo .
2020-07-01 11:00:38,166 Example #1
2020-07-01 11:00:38,166 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:00:38,166 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:00:38,166 	Source:     Hi , how can I help you ?
2020-07-01 11:00:38,166 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:00:38,166 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:00:38,166 Example #2
2020-07-01 11:00:38,166 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:00:38,166 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:00:38,167 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:00:38,167 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:00:38,167 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:00:38,167 Example #3
2020-07-01 11:00:38,167 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:00:38,167 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:00:38,167 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:00:38,167 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:00:38,167 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:00:38,167 Validation result (greedy) at epoch  74, step     5000: bleu:  36.83, loss: 47227.5273, ppl:   6.9317, duration: 35.6894s
2020-07-01 11:00:39,555 Epoch  74: total training loss 6.27
2020-07-01 11:00:39,555 EPOCH 75
2020-07-01 11:00:51,629 Epoch  75: total training loss 6.45
2020-07-01 11:00:51,629 EPOCH 76
2020-07-01 11:00:56,091 Epoch  76 Step:     5100 Batch Loss:     0.094548 Tokens per Sec:    12487, Lr: 0.000200
2020-07-01 11:01:03,668 Epoch  76: total training loss 6.10
2020-07-01 11:01:03,669 EPOCH 77
2020-07-01 11:01:13,909 Epoch  77 Step:     5200 Batch Loss:     0.090251 Tokens per Sec:    12710, Lr: 0.000200
2020-07-01 11:01:15,707 Epoch  77: total training loss 5.85
2020-07-01 11:01:15,708 EPOCH 78
2020-07-01 11:01:27,168 Epoch  78: total training loss 5.86
2020-07-01 11:01:27,169 EPOCH 79
2020-07-01 11:01:30,949 Epoch  79 Step:     5300 Batch Loss:     0.076799 Tokens per Sec:    13242, Lr: 0.000200
2020-07-01 11:01:38,636 Epoch  79: total training loss 5.74
2020-07-01 11:01:38,637 EPOCH 80
2020-07-01 11:01:48,302 Epoch  80 Step:     5400 Batch Loss:     0.077362 Tokens per Sec:    12858, Lr: 0.000200
2020-07-01 11:01:50,689 Epoch  80: total training loss 5.73
2020-07-01 11:01:50,690 EPOCH 81
2020-07-01 11:02:02,770 Epoch  81: total training loss 5.55
2020-07-01 11:02:02,770 EPOCH 82
2020-07-01 11:02:06,225 Epoch  82 Step:     5500 Batch Loss:     0.086043 Tokens per Sec:    12412, Lr: 0.000200
2020-07-01 11:02:14,731 Epoch  82: total training loss 5.60
2020-07-01 11:02:14,731 EPOCH 83
2020-07-01 11:02:24,040 Epoch  83 Step:     5600 Batch Loss:     0.081553 Tokens per Sec:    12927, Lr: 0.000200
2020-07-01 11:02:26,747 Epoch  83: total training loss 5.71
2020-07-01 11:02:26,748 EPOCH 84
2020-07-01 11:02:38,794 Epoch  84: total training loss 5.75
2020-07-01 11:02:38,794 EPOCH 85
2020-07-01 11:02:41,914 Epoch  85 Step:     5700 Batch Loss:     0.078707 Tokens per Sec:    12888, Lr: 0.000200
2020-07-01 11:02:50,783 Epoch  85: total training loss 5.93
2020-07-01 11:02:50,783 EPOCH 86
2020-07-01 11:02:59,699 Epoch  86 Step:     5800 Batch Loss:     0.104048 Tokens per Sec:    12997, Lr: 0.000200
2020-07-01 11:03:02,721 Epoch  86: total training loss 6.59
2020-07-01 11:03:02,722 EPOCH 87
2020-07-01 11:03:14,476 Epoch  87: total training loss 6.05
2020-07-01 11:03:14,477 EPOCH 88
2020-07-01 11:03:17,226 Epoch  88 Step:     5900 Batch Loss:     0.085752 Tokens per Sec:    12896, Lr: 0.000200
2020-07-01 11:03:26,295 Epoch  88: total training loss 5.73
2020-07-01 11:03:26,295 EPOCH 89
2020-07-01 11:03:34,880 Epoch  89 Step:     6000 Batch Loss:     0.081895 Tokens per Sec:    12810, Lr: 0.000200
2020-07-01 11:04:09,631 Example #0
2020-07-01 11:04:09,631 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:04:09,632 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:04:09,632 	Source:     Hello .
2020-07-01 11:04:09,632 	Reference:  Hallo ,
2020-07-01 11:04:09,632 	Hypothesis: Hallo .
2020-07-01 11:04:09,632 Example #1
2020-07-01 11:04:09,632 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:04:09,632 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:04:09,632 	Source:     Hi , how can I help you ?
2020-07-01 11:04:09,632 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:04:09,632 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:04:09,632 Example #2
2020-07-01 11:04:09,632 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:04:09,632 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:04:09,632 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:04:09,632 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:04:09,632 	Hypothesis: Hallo , ich suche ein Restaurant in San Francisco , Kalifornien .
2020-07-01 11:04:09,632 Example #3
2020-07-01 11:04:09,632 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:04:09,632 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:04:09,632 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:04:09,632 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:04:09,632 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:04:09,632 Validation result (greedy) at epoch  89, step     6000: bleu:  37.28, loss: 47391.9766, ppl:   6.9786, duration: 34.7507s
2020-07-01 11:04:13,176 Epoch  89: total training loss 5.65
2020-07-01 11:04:13,176 EPOCH 90
2020-07-01 11:04:25,088 Epoch  90: total training loss 5.45
2020-07-01 11:04:25,089 EPOCH 91
2020-07-01 11:04:27,348 Epoch  91 Step:     6100 Batch Loss:     0.072603 Tokens per Sec:    13386, Lr: 0.000200
2020-07-01 11:04:36,582 Epoch  91: total training loss 5.28
2020-07-01 11:04:36,582 EPOCH 92
2020-07-01 11:04:44,430 Epoch  92 Step:     6200 Batch Loss:     0.069705 Tokens per Sec:    13336, Lr: 0.000200
2020-07-01 11:04:48,050 Epoch  92: total training loss 5.30
2020-07-01 11:04:48,051 EPOCH 93
2020-07-01 11:04:59,704 Epoch  93: total training loss 5.23
2020-07-01 11:04:59,704 EPOCH 94
2020-07-01 11:05:01,650 Epoch  94 Step:     6300 Batch Loss:     0.071744 Tokens per Sec:    12651, Lr: 0.000200
2020-07-01 11:05:11,894 Epoch  94: total training loss 5.44
2020-07-01 11:05:11,894 EPOCH 95
2020-07-01 11:05:19,463 Epoch  95 Step:     6400 Batch Loss:     0.093001 Tokens per Sec:    12737, Lr: 0.000200
2020-07-01 11:05:24,044 Epoch  95: total training loss 5.41
2020-07-01 11:05:24,044 EPOCH 96
2020-07-01 11:05:36,031 Epoch  96: total training loss 5.31
2020-07-01 11:05:36,031 EPOCH 97
2020-07-01 11:05:37,287 Epoch  97 Step:     6500 Batch Loss:     0.069688 Tokens per Sec:    12844, Lr: 0.000200
2020-07-01 11:05:48,138 Epoch  97: total training loss 5.30
2020-07-01 11:05:48,138 EPOCH 98
2020-07-01 11:05:54,892 Epoch  98 Step:     6600 Batch Loss:     0.072321 Tokens per Sec:    12811, Lr: 0.000200
2020-07-01 11:06:00,011 Epoch  98: total training loss 5.15
2020-07-01 11:06:00,011 EPOCH 99
2020-07-01 11:06:12,080 Epoch  99: total training loss 4.90
2020-07-01 11:06:12,081 EPOCH 100
2020-07-01 11:06:12,742 Epoch 100 Step:     6700 Batch Loss:     0.097044 Tokens per Sec:    11809, Lr: 0.000200
2020-07-01 11:06:24,197 Epoch 100: total training loss 5.08
2020-07-01 11:06:24,198 Training ended after 100 epochs.
2020-07-01 11:06:24,198 Best validation result (greedy) at step     2000:   6.57 ppl.
2020-07-01 11:06:52,236  dev bleu:  37.33 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 11:06:52,240 Translations saved to: models/transformer_wmt17bpe_ende/00002000.hyps.dev
2020-07-01 11:07:12,428 test bleu:  34.27 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 11:07:12,509 Translations saved to: models/transformer_wmt17bpe_ende/00002000.hyps.test
