2020-06-17 09:59:05,503 Hello! This is Joey-NMT.
2020-06-17 09:59:12,321 Total params: 69452289
2020-06-17 09:59:12,323 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-17 09:59:14,689 cfg.name                           : transformer_multi_enc6x2_deen
2020-06-17 09:59:14,689 cfg.data.src                       : de
2020-06-17 09:59:14,689 cfg.data.trg                       : en
2020-06-17 09:59:14,689 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-17 09:59:14,689 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-17 09:59:14,689 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-17 09:59:14,689 cfg.data.level                     : bpe
2020-06-17 09:59:14,689 cfg.data.lowercase                 : True
2020-06-17 09:59:14,690 cfg.data.max_sent_length           : 100
2020-06-17 09:59:14,690 cfg.testing.beam_size              : 5
2020-06-17 09:59:14,690 cfg.testing.alpha                  : 1.0
2020-06-17 09:59:14,690 cfg.training.random_seed           : 42
2020-06-17 09:59:14,690 cfg.training.optimizer             : adam
2020-06-17 09:59:14,690 cfg.training.normalization         : tokens
2020-06-17 09:59:14,690 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-17 09:59:14,690 cfg.training.scheduling            : plateau
2020-06-17 09:59:14,690 cfg.training.patience              : 8
2020-06-17 09:59:14,690 cfg.training.decrease_factor       : 0.7
2020-06-17 09:59:14,690 cfg.training.loss                  : crossentropy
2020-06-17 09:59:14,690 cfg.training.learning_rate         : 0.0002
2020-06-17 09:59:14,690 cfg.training.learning_rate_min     : 1e-08
2020-06-17 09:59:14,690 cfg.training.weight_decay          : 0.0
2020-06-17 09:59:14,690 cfg.training.label_smoothing       : 0.1
2020-06-17 09:59:14,690 cfg.training.batch_size            : 4096
2020-06-17 09:59:14,690 cfg.training.batch_type            : token
2020-06-17 09:59:14,690 cfg.training.eval_batch_size       : 3600
2020-06-17 09:59:14,690 cfg.training.eval_batch_type       : token
2020-06-17 09:59:14,690 cfg.training.batch_multiplier      : 1
2020-06-17 09:59:14,690 cfg.training.early_stopping_metric : ppl
2020-06-17 09:59:14,690 cfg.training.epochs                : 100
2020-06-17 09:59:14,690 cfg.training.validation_freq       : 1000
2020-06-17 09:59:14,690 cfg.training.logging_freq          : 100
2020-06-17 09:59:14,690 cfg.training.eval_metric           : bleu
2020-06-17 09:59:14,690 cfg.training.model_dir             : models/transformer_multi_enc6x2_deen
2020-06-17 09:59:14,690 cfg.training.overwrite             : True
2020-06-17 09:59:14,690 cfg.training.shuffle               : True
2020-06-17 09:59:14,690 cfg.training.use_cuda              : True
2020-06-17 09:59:14,690 cfg.training.max_output_length     : 100
2020-06-17 09:59:14,690 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-17 09:59:14,690 cfg.training.keep_last_ckpts       : 3
2020-06-17 09:59:14,690 cfg.model.initializer              : xavier
2020-06-17 09:59:14,690 cfg.model.bias_initializer         : zeros
2020-06-17 09:59:14,690 cfg.model.init_gain                : 1.0
2020-06-17 09:59:14,690 cfg.model.embed_initializer        : xavier
2020-06-17 09:59:14,690 cfg.model.embed_init_gain          : 1.0
2020-06-17 09:59:14,690 cfg.model.tied_embeddings          : False
2020-06-17 09:59:14,690 cfg.model.tied_softmax             : True
2020-06-17 09:59:14,691 cfg.model.encoder.type             : transformer
2020-06-17 09:59:14,691 cfg.model.encoder.num_layers       : 6
2020-06-17 09:59:14,691 cfg.model.encoder.num_heads        : 8
2020-06-17 09:59:14,691 cfg.model.encoder.embeddings.embedding_dim : 512
2020-06-17 09:59:14,691 cfg.model.encoder.embeddings.scale : True
2020-06-17 09:59:14,691 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-17 09:59:14,691 cfg.model.encoder.hidden_size      : 512
2020-06-17 09:59:14,691 cfg.model.encoder.ff_size          : 2048
2020-06-17 09:59:14,691 cfg.model.encoder.dropout          : 0.1
2020-06-17 09:59:14,691 cfg.model.encoder.freeze           : False
2020-06-17 09:59:14,691 cfg.model.encoder.multi_encoder    : True
2020-06-17 09:59:14,691 cfg.model.decoder.type             : transformer
2020-06-17 09:59:14,691 cfg.model.decoder.num_layers       : 6
2020-06-17 09:59:14,691 cfg.model.decoder.num_heads        : 8
2020-06-17 09:59:14,691 cfg.model.decoder.embeddings.embedding_dim : 512
2020-06-17 09:59:14,691 cfg.model.decoder.embeddings.scale : True
2020-06-17 09:59:14,691 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-17 09:59:14,691 cfg.model.decoder.hidden_size      : 512
2020-06-17 09:59:14,691 cfg.model.decoder.ff_size          : 2048
2020-06-17 09:59:14,691 cfg.model.decoder.dropout          : 0.1
2020-06-17 09:59:14,691 cfg.model.decoder.freeze           : False
2020-06-17 09:59:14,691 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-17 09:59:14,691 First training example:
	[SRC] hallo ! wie kann ich helfen ?
	[TRG] hi there ! how can i help ?
2020-06-17 09:59:14,691 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-17 09:59:14,691 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-17 09:59:14,691 Number of Src words (types): 5876
2020-06-17 09:59:14,691 Number of Trg words (types): 4561
2020-06-17 09:59:14,691 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=5876),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4561))
2020-06-17 09:59:14,702 EPOCH 1
2020-06-17 09:59:35,603 Epoch   1: total training loss 285.67
2020-06-17 09:59:35,604 EPOCH 2
2020-06-17 09:59:53,780 Epoch   2 Step:      100 Batch Loss:     4.774674 Tokens per Sec:     6375, Lr: 0.000200
2020-06-17 09:59:56,016 Epoch   2: total training loss 243.59
2020-06-17 09:59:56,016 EPOCH 3
2020-06-17 10:00:17,879 Epoch   3: total training loss 231.24
2020-06-17 10:00:17,880 EPOCH 4
2020-06-17 10:00:34,723 Epoch   4 Step:      200 Batch Loss:     4.598469 Tokens per Sec:     5707, Lr: 0.000200
2020-06-17 10:00:40,704 Epoch   4: total training loss 211.77
2020-06-17 10:00:40,704 EPOCH 5
2020-06-17 10:01:03,752 Epoch   5: total training loss 191.85
2020-06-17 10:01:03,753 EPOCH 6
2020-06-17 10:01:17,066 Epoch   6 Step:      300 Batch Loss:     3.588804 Tokens per Sec:     5697, Lr: 0.000200
2020-06-17 10:01:26,710 Epoch   6: total training loss 170.64
2020-06-17 10:01:26,711 EPOCH 7
2020-06-17 10:01:50,151 Epoch   7: total training loss 160.50
2020-06-17 10:01:50,152 EPOCH 8
2020-06-17 10:02:00,139 Epoch   8 Step:      400 Batch Loss:     2.012864 Tokens per Sec:     5750, Lr: 0.000200
2020-06-17 10:02:13,515 Epoch   8: total training loss 150.73
2020-06-17 10:02:13,515 EPOCH 9
2020-06-17 10:02:36,849 Epoch   9: total training loss 134.76
2020-06-17 10:02:36,850 EPOCH 10
2020-06-17 10:02:44,200 Epoch  10 Step:      500 Batch Loss:     3.014272 Tokens per Sec:     5724, Lr: 0.000200
2020-06-17 10:03:00,263 Epoch  10: total training loss 133.53
2020-06-17 10:03:00,264 EPOCH 11
2020-06-17 10:03:23,526 Epoch  11: total training loss 121.25
2020-06-17 10:03:23,527 EPOCH 12
2020-06-17 10:03:27,549 Epoch  12 Step:      600 Batch Loss:     1.623368 Tokens per Sec:     5652, Lr: 0.000200
2020-06-17 10:03:46,364 Epoch  12: total training loss 110.63
2020-06-17 10:03:46,364 EPOCH 13
2020-06-17 10:04:09,277 Epoch  13: total training loss 106.71
2020-06-17 10:04:09,278 EPOCH 14
2020-06-17 10:04:10,223 Epoch  14 Step:      700 Batch Loss:     2.132871 Tokens per Sec:     4686, Lr: 0.000200
2020-06-17 10:04:32,158 Epoch  14: total training loss 98.18
2020-06-17 10:04:32,159 EPOCH 15
2020-06-17 10:04:53,129 Epoch  15 Step:      800 Batch Loss:     1.596275 Tokens per Sec:     5758, Lr: 0.000200
2020-06-17 10:04:55,273 Epoch  15: total training loss 87.82
2020-06-17 10:04:55,274 EPOCH 16
2020-06-17 10:05:18,635 Epoch  16: total training loss 84.79
2020-06-17 10:05:18,635 EPOCH 17
2020-06-17 10:05:37,236 Epoch  17 Step:      900 Batch Loss:     1.845878 Tokens per Sec:     5636, Lr: 0.000200
2020-06-17 10:05:41,494 Epoch  17: total training loss 75.70
2020-06-17 10:05:41,494 EPOCH 18
2020-06-17 10:06:04,263 Epoch  18: total training loss 72.92
2020-06-17 10:06:04,264 EPOCH 19
2020-06-17 10:06:18,490 Epoch  19 Step:     1000 Batch Loss:     0.901210 Tokens per Sec:     5959, Lr: 0.000200
2020-06-17 10:06:56,860 Hooray! New best validation result [ppl]!
2020-06-17 10:06:56,860 Saving new checkpoint.
2020-06-17 10:07:06,098 Example #0
2020-06-17 10:07:06,098 	Raw source:     ['hallo', ',']
2020-06-17 10:07:06,098 	Raw hypothesis: ['hello', '?']
2020-06-17 10:07:06,098 	Source:     hallo ,
2020-06-17 10:07:06,099 	Reference:  hello .
2020-06-17 10:07:06,099 	Hypothesis: hello ?
2020-06-17 10:07:06,099 Example #1
2020-06-17 10:07:06,099 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:07:06,099 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:07:06,099 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:07:06,099 	Reference:  hi , how can i help you ?
2020-06-17 10:07:06,099 	Hypothesis: hi , how can i help you ?
2020-06-17 10:07:06,099 Example #2
2020-06-17 10:07:06,099 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:07:06,099 	Raw hypothesis: ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'place', 'to', 'eat', 'lunch', 'in', 'sacramento', ',', 'california', '.']
2020-06-17 10:07:06,099 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:07:06,099 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:07:06,099 	Hypothesis: hi , i &apos;m looking for a place to eat lunch in sacramento , california .
2020-06-17 10:07:06,099 Example #3
2020-06-17 10:07:06,099 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:07:06,099 	Raw hypothesis: ['ok', ',', 'what', 'kind', 'of', 'food', 'do', 'you', 'like', '?']
2020-06-17 10:07:06,099 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:07:06,099 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:07:06,099 	Hypothesis: ok , what kind of food do you like ?
2020-06-17 10:07:06,099 Validation result (greedy) at epoch  19, step     1000: bleu:  25.07, loss: 45455.9766, ppl:   8.7127, duration: 47.6088s
2020-06-17 10:07:14,325 Epoch  19: total training loss 66.45
2020-06-17 10:07:14,325 EPOCH 20
2020-06-17 10:07:36,810 Epoch  20: total training loss 63.37
2020-06-17 10:07:36,811 EPOCH 21
2020-06-17 10:07:48,388 Epoch  21 Step:     1100 Batch Loss:     0.708328 Tokens per Sec:     5420, Lr: 0.000200
2020-06-17 10:08:00,042 Epoch  21: total training loss 62.45
2020-06-17 10:08:00,042 EPOCH 22
2020-06-17 10:08:22,445 Epoch  22: total training loss 53.59
2020-06-17 10:08:22,445 EPOCH 23
2020-06-17 10:08:30,027 Epoch  23 Step:     1200 Batch Loss:     1.594894 Tokens per Sec:     5787, Lr: 0.000200
2020-06-17 10:08:44,903 Epoch  23: total training loss 50.99
2020-06-17 10:08:44,904 EPOCH 24
2020-06-17 10:09:07,061 Epoch  24: total training loss 46.58
2020-06-17 10:09:07,062 EPOCH 25
2020-06-17 10:09:11,210 Epoch  25 Step:     1300 Batch Loss:     0.542132 Tokens per Sec:     5972, Lr: 0.000200
2020-06-17 10:09:29,043 Epoch  25: total training loss 44.76
2020-06-17 10:09:29,044 EPOCH 26
2020-06-17 10:09:51,053 Epoch  26: total training loss 39.04
2020-06-17 10:09:51,056 EPOCH 27
2020-06-17 10:09:51,502 Epoch  27 Step:     1400 Batch Loss:     0.468444 Tokens per Sec:     6059, Lr: 0.000200
2020-06-17 10:10:12,838 Epoch  27: total training loss 35.57
2020-06-17 10:10:12,842 EPOCH 28
2020-06-17 10:10:31,221 Epoch  28 Step:     1500 Batch Loss:     0.402072 Tokens per Sec:     6246, Lr: 0.000200
2020-06-17 10:10:34,085 Epoch  28: total training loss 33.15
2020-06-17 10:10:34,086 EPOCH 29
2020-06-17 10:10:55,471 Epoch  29: total training loss 31.23
2020-06-17 10:10:55,471 EPOCH 30
2020-06-17 10:11:11,361 Epoch  30 Step:     1600 Batch Loss:     0.493430 Tokens per Sec:     6197, Lr: 0.000200
2020-06-17 10:11:16,835 Epoch  30: total training loss 27.37
2020-06-17 10:11:16,835 EPOCH 31
2020-06-17 10:11:37,983 Epoch  31: total training loss 24.51
2020-06-17 10:11:37,983 EPOCH 32
2020-06-17 10:11:51,181 Epoch  32 Step:     1700 Batch Loss:     0.302680 Tokens per Sec:     6172, Lr: 0.000200
2020-06-17 10:11:59,149 Epoch  32: total training loss 22.67
2020-06-17 10:11:59,150 EPOCH 33
2020-06-17 10:12:20,470 Epoch  33: total training loss 21.69
2020-06-17 10:12:20,471 EPOCH 34
2020-06-17 10:12:30,332 Epoch  34 Step:     1800 Batch Loss:     0.288851 Tokens per Sec:     6149, Lr: 0.000200
2020-06-17 10:12:41,662 Epoch  34: total training loss 20.35
2020-06-17 10:12:41,662 EPOCH 35
2020-06-17 10:13:03,216 Epoch  35: total training loss 19.02
2020-06-17 10:13:03,217 EPOCH 36
2020-06-17 10:13:09,131 Epoch  36 Step:     1900 Batch Loss:     0.269563 Tokens per Sec:     6773, Lr: 0.000200
2020-06-17 10:13:24,044 Epoch  36: total training loss 18.20
2020-06-17 10:13:24,045 EPOCH 37
2020-06-17 10:13:44,527 Epoch  37: total training loss 16.01
2020-06-17 10:13:44,528 EPOCH 38
2020-06-17 10:13:47,766 Epoch  38 Step:     2000 Batch Loss:     0.354468 Tokens per Sec:     6539, Lr: 0.000200
2020-06-17 10:14:12,759 Hooray! New best validation result [ppl]!
2020-06-17 10:14:12,760 Saving new checkpoint.
2020-06-17 10:14:21,879 Example #0
2020-06-17 10:14:21,881 	Raw source:     ['hallo', ',']
2020-06-17 10:14:21,881 	Raw hypothesis: ['hello', '?']
2020-06-17 10:14:21,881 	Source:     hallo ,
2020-06-17 10:14:21,881 	Reference:  hello .
2020-06-17 10:14:21,881 	Hypothesis: hello ?
2020-06-17 10:14:21,881 Example #1
2020-06-17 10:14:21,881 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:14:21,881 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:14:21,881 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:14:21,881 	Reference:  hi , how can i help you ?
2020-06-17 10:14:21,881 	Hypothesis: hi , how can i help you ?
2020-06-17 10:14:21,881 Example #2
2020-06-17 10:14:21,881 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:14:21,881 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:14:21,881 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:14:21,881 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:14:21,882 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:14:21,882 Example #3
2020-06-17 10:14:21,882 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:14:21,882 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:14:21,882 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:14:21,882 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:14:21,882 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 10:14:21,882 Validation result (greedy) at epoch  38, step     2000: bleu:  34.95, loss: 43901.7227, ppl:   8.0910, duration: 34.1153s
2020-06-17 10:14:39,039 Epoch  38: total training loss 15.43
2020-06-17 10:14:39,040 EPOCH 39
2020-06-17 10:15:00,455 Epoch  39: total training loss 13.95
2020-06-17 10:15:00,456 EPOCH 40
2020-06-17 10:15:00,803 Epoch  40 Step:     2100 Batch Loss:     0.236814 Tokens per Sec:     7409, Lr: 0.000200
2020-06-17 10:15:21,947 Epoch  40: total training loss 13.07
2020-06-17 10:15:21,948 EPOCH 41
2020-06-17 10:15:41,153 Epoch  41 Step:     2200 Batch Loss:     0.183347 Tokens per Sec:     6061, Lr: 0.000200
2020-06-17 10:15:43,761 Epoch  41: total training loss 12.54
2020-06-17 10:15:43,762 EPOCH 42
2020-06-17 10:16:06,236 Epoch  42: total training loss 12.05
2020-06-17 10:16:06,236 EPOCH 43
2020-06-17 10:16:21,697 Epoch  43 Step:     2300 Batch Loss:     0.258310 Tokens per Sec:     6167, Lr: 0.000200
2020-06-17 10:16:28,417 Epoch  43: total training loss 11.16
2020-06-17 10:16:28,418 EPOCH 44
2020-06-17 10:16:50,401 Epoch  44: total training loss 10.80
2020-06-17 10:16:50,401 EPOCH 45
2020-06-17 10:17:03,731 Epoch  45 Step:     2400 Batch Loss:     0.175603 Tokens per Sec:     5992, Lr: 0.000200
2020-06-17 10:17:12,446 Epoch  45: total training loss 10.38
2020-06-17 10:17:12,446 EPOCH 46
2020-06-17 10:17:35,072 Epoch  46: total training loss 9.87
2020-06-17 10:17:35,073 EPOCH 47
2020-06-17 10:17:46,100 Epoch  47 Step:     2500 Batch Loss:     0.169510 Tokens per Sec:     5575, Lr: 0.000200
2020-06-17 10:17:57,577 Epoch  47: total training loss 9.60
2020-06-17 10:17:57,578 EPOCH 48
2020-06-17 10:18:20,126 Epoch  48: total training loss 9.47
2020-06-17 10:18:20,127 EPOCH 49
2020-06-17 10:18:27,407 Epoch  49 Step:     2600 Batch Loss:     0.155014 Tokens per Sec:     5594, Lr: 0.000200
2020-06-17 10:18:42,983 Epoch  49: total training loss 9.45
2020-06-17 10:18:42,984 EPOCH 50
2020-06-17 10:19:05,903 Epoch  50: total training loss 9.24
2020-06-17 10:19:05,903 EPOCH 51
2020-06-17 10:19:09,504 Epoch  51 Step:     2700 Batch Loss:     0.162993 Tokens per Sec:     5706, Lr: 0.000200
2020-06-17 10:19:28,887 Epoch  51: total training loss 9.36
2020-06-17 10:19:28,888 EPOCH 52
2020-06-17 10:19:51,320 Epoch  52 Step:     2800 Batch Loss:     0.155373 Tokens per Sec:     5744, Lr: 0.000200
2020-06-17 10:19:51,800 Epoch  52: total training loss 8.83
2020-06-17 10:19:51,800 EPOCH 53
2020-06-17 10:20:14,717 Epoch  53: total training loss 8.28
2020-06-17 10:20:14,718 EPOCH 54
2020-06-17 10:20:33,942 Epoch  54 Step:     2900 Batch Loss:     0.137821 Tokens per Sec:     5617, Lr: 0.000200
2020-06-17 10:20:37,519 Epoch  54: total training loss 8.32
2020-06-17 10:20:37,519 EPOCH 55
2020-06-17 10:21:00,879 Epoch  55: total training loss 7.82
2020-06-17 10:21:00,879 EPOCH 56
2020-06-17 10:21:16,646 Epoch  56 Step:     3000 Batch Loss:     0.132764 Tokens per Sec:     5783, Lr: 0.000200
2020-06-17 10:21:44,674 Example #0
2020-06-17 10:21:44,674 	Raw source:     ['hallo', ',']
2020-06-17 10:21:44,674 	Raw hypothesis: ['hello', '.']
2020-06-17 10:21:44,674 	Source:     hallo ,
2020-06-17 10:21:44,674 	Reference:  hello .
2020-06-17 10:21:44,674 	Hypothesis: hello .
2020-06-17 10:21:44,674 Example #1
2020-06-17 10:21:44,674 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:21:44,674 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:21:44,675 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:21:44,675 	Reference:  hi , how can i help you ?
2020-06-17 10:21:44,675 	Hypothesis: hi , how can i help you ?
2020-06-17 10:21:44,675 Example #2
2020-06-17 10:21:44,675 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:21:44,675 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-17 10:21:44,675 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:21:44,675 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:21:44,675 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in san francisco , california .
2020-06-17 10:21:44,675 Example #3
2020-06-17 10:21:44,675 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:21:44,675 	Raw hypothesis: ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:21:44,675 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:21:44,675 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:21:44,675 	Hypothesis: ok , what type of restaurant are you looking for ?
2020-06-17 10:21:44,675 Validation result (greedy) at epoch  56, step     3000: bleu:  37.42, loss: 43956.8320, ppl:   8.1123, duration: 28.0282s
2020-06-17 10:21:51,682 Epoch  56: total training loss 7.50
2020-06-17 10:21:51,682 EPOCH 57
2020-06-17 10:22:14,401 Epoch  57: total training loss 7.25
2020-06-17 10:22:14,401 EPOCH 58
2020-06-17 10:22:26,996 Epoch  58 Step:     3100 Batch Loss:     0.122059 Tokens per Sec:     5747, Lr: 0.000200
2020-06-17 10:22:37,545 Epoch  58: total training loss 7.08
2020-06-17 10:22:37,546 EPOCH 59
2020-06-17 10:23:00,699 Epoch  59: total training loss 7.21
2020-06-17 10:23:00,700 EPOCH 60
2020-06-17 10:23:10,354 Epoch  60 Step:     3200 Batch Loss:     0.157135 Tokens per Sec:     5591, Lr: 0.000200
2020-06-17 10:23:23,966 Epoch  60: total training loss 7.29
2020-06-17 10:23:23,967 EPOCH 61
2020-06-17 10:23:47,076 Epoch  61: total training loss 7.20
2020-06-17 10:23:47,077 EPOCH 62
2020-06-17 10:23:52,894 Epoch  62 Step:     3300 Batch Loss:     0.132605 Tokens per Sec:     6184, Lr: 0.000200
2020-06-17 10:24:09,847 Epoch  62: total training loss 7.39
2020-06-17 10:24:09,848 EPOCH 63
2020-06-17 10:24:32,768 Epoch  63: total training loss 6.87
2020-06-17 10:24:32,769 EPOCH 64
2020-06-17 10:24:35,220 Epoch  64 Step:     3400 Batch Loss:     0.105953 Tokens per Sec:     7215, Lr: 0.000200
2020-06-17 10:24:55,559 Epoch  64: total training loss 6.73
2020-06-17 10:24:55,560 EPOCH 65
2020-06-17 10:25:18,766 Epoch  65: total training loss 6.48
2020-06-17 10:25:18,766 EPOCH 66
2020-06-17 10:25:19,038 Epoch  66 Step:     3500 Batch Loss:     0.122041 Tokens per Sec:    10046, Lr: 0.000200
2020-06-17 10:25:41,728 Epoch  66: total training loss 6.35
2020-06-17 10:25:41,729 EPOCH 67
2020-06-17 10:26:02,166 Epoch  67 Step:     3600 Batch Loss:     0.099254 Tokens per Sec:     5718, Lr: 0.000200
2020-06-17 10:26:04,801 Epoch  67: total training loss 6.29
2020-06-17 10:26:04,801 EPOCH 68
2020-06-17 10:26:28,261 Epoch  68: total training loss 5.97
2020-06-17 10:26:28,262 EPOCH 69
2020-06-17 10:26:45,329 Epoch  69 Step:     3700 Batch Loss:     0.102913 Tokens per Sec:     5831, Lr: 0.000200
2020-06-17 10:26:50,859 Epoch  69: total training loss 6.00
2020-06-17 10:26:50,860 EPOCH 70
2020-06-17 10:27:14,117 Epoch  70: total training loss 5.89
2020-06-17 10:27:14,118 EPOCH 71
2020-06-17 10:27:27,303 Epoch  71 Step:     3800 Batch Loss:     0.112244 Tokens per Sec:     6231, Lr: 0.000200
2020-06-17 10:27:37,290 Epoch  71: total training loss 5.74
2020-06-17 10:27:37,291 EPOCH 72
2020-06-17 10:28:00,217 Epoch  72: total training loss 5.87
2020-06-17 10:28:00,218 EPOCH 73
2020-06-17 10:28:11,040 Epoch  73 Step:     3900 Batch Loss:     0.131868 Tokens per Sec:     5643, Lr: 0.000200
2020-06-17 10:28:23,443 Epoch  73: total training loss 6.62
2020-06-17 10:28:23,444 EPOCH 74
2020-06-17 10:28:46,937 Epoch  74: total training loss 6.29
2020-06-17 10:28:46,937 EPOCH 75
2020-06-17 10:28:54,045 Epoch  75 Step:     4000 Batch Loss:     0.116942 Tokens per Sec:     6118, Lr: 0.000200
2020-06-17 10:29:20,043 Example #0
2020-06-17 10:29:20,043 	Raw source:     ['hallo', ',']
2020-06-17 10:29:20,043 	Raw hypothesis: ['hello', '?']
2020-06-17 10:29:20,043 	Source:     hallo ,
2020-06-17 10:29:20,043 	Reference:  hello .
2020-06-17 10:29:20,043 	Hypothesis: hello ?
2020-06-17 10:29:20,043 Example #1
2020-06-17 10:29:20,043 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:29:20,043 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:29:20,043 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:29:20,043 	Reference:  hi , how can i help you ?
2020-06-17 10:29:20,043 	Hypothesis: hi , how can i help you ?
2020-06-17 10:29:20,043 Example #2
2020-06-17 10:29:20,044 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:29:20,044 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', ',', 'california', 'area', '.']
2020-06-17 10:29:20,044 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:29:20,044 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:29:20,044 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall , california area .
2020-06-17 10:29:20,044 Example #3
2020-06-17 10:29:20,044 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:29:20,044 	Raw hypothesis: ['ok', '.', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:29:20,044 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:29:20,044 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:29:20,044 	Hypothesis: ok . what type of restaurant are you looking for ?
2020-06-17 10:29:20,044 Validation result (greedy) at epoch  75, step     4000: bleu:  36.80, loss: 44792.9844, ppl:   8.4419, duration: 25.9974s
2020-06-17 10:29:35,787 Epoch  75: total training loss 6.03
2020-06-17 10:29:35,788 EPOCH 76
2020-06-17 10:29:58,983 Epoch  76: total training loss 5.81
2020-06-17 10:29:58,983 EPOCH 77
2020-06-17 10:30:03,390 Epoch  77 Step:     4100 Batch Loss:     0.084311 Tokens per Sec:     5694, Lr: 0.000200
2020-06-17 10:30:22,225 Epoch  77: total training loss 5.78
2020-06-17 10:30:22,225 EPOCH 78
2020-06-17 10:30:45,209 Epoch  78: total training loss 5.64
2020-06-17 10:30:45,210 EPOCH 79
2020-06-17 10:30:46,402 Epoch  79 Step:     4200 Batch Loss:     0.126858 Tokens per Sec:     6221, Lr: 0.000200
2020-06-17 10:31:08,530 Epoch  79: total training loss 6.10
2020-06-17 10:31:08,530 EPOCH 80
2020-06-17 10:31:30,445 Epoch  80 Step:     4300 Batch Loss:     0.107991 Tokens per Sec:     5468, Lr: 0.000200
2020-06-17 10:31:32,501 Epoch  80: total training loss 5.90
2020-06-17 10:31:32,501 EPOCH 81
2020-06-17 10:31:55,571 Epoch  81: total training loss 5.66
2020-06-17 10:31:55,571 EPOCH 82
2020-06-17 10:32:14,050 Epoch  82 Step:     4400 Batch Loss:     0.092542 Tokens per Sec:     5613, Lr: 0.000200
2020-06-17 10:32:18,660 Epoch  82: total training loss 5.37
2020-06-17 10:32:18,660 EPOCH 83
2020-06-17 10:32:41,667 Epoch  83: total training loss 5.18
2020-06-17 10:32:41,668 EPOCH 84
2020-06-17 10:32:56,097 Epoch  84 Step:     4500 Batch Loss:     0.086238 Tokens per Sec:     5793, Lr: 0.000200
2020-06-17 10:33:04,683 Epoch  84: total training loss 5.01
2020-06-17 10:33:04,684 EPOCH 85
2020-06-17 10:33:27,976 Epoch  85: total training loss 4.82
2020-06-17 10:33:27,977 EPOCH 86
2020-06-17 10:33:40,004 Epoch  86 Step:     4600 Batch Loss:     0.079935 Tokens per Sec:     5621, Lr: 0.000200
2020-06-17 10:33:51,288 Epoch  86: total training loss 4.91
2020-06-17 10:33:51,289 EPOCH 87
2020-06-17 10:34:14,348 Epoch  87: total training loss 5.44
2020-06-17 10:34:14,348 EPOCH 88
2020-06-17 10:34:21,823 Epoch  88 Step:     4700 Batch Loss:     0.154508 Tokens per Sec:     5948, Lr: 0.000200
2020-06-17 10:34:37,333 Epoch  88: total training loss 5.68
2020-06-17 10:34:37,334 EPOCH 89
2020-06-17 10:35:00,359 Epoch  89: total training loss 5.45
2020-06-17 10:35:00,360 EPOCH 90
2020-06-17 10:35:06,096 Epoch  90 Step:     4800 Batch Loss:     0.101660 Tokens per Sec:     4577, Lr: 0.000200
2020-06-17 10:35:23,278 Epoch  90: total training loss 5.03
2020-06-17 10:35:23,279 EPOCH 91
2020-06-17 10:35:46,713 Epoch  91: total training loss 5.11
2020-06-17 10:35:46,714 EPOCH 92
2020-06-17 10:35:48,755 Epoch  92 Step:     4900 Batch Loss:     0.086162 Tokens per Sec:     3136, Lr: 0.000200
2020-06-17 10:36:10,144 Epoch  92: total training loss 5.25
2020-06-17 10:36:10,145 EPOCH 93
2020-06-17 10:36:30,804 Epoch  93 Step:     5000 Batch Loss:     0.111433 Tokens per Sec:     5826, Lr: 0.000200
2020-06-17 10:37:00,359 Example #0
2020-06-17 10:37:00,360 	Raw source:     ['hallo', ',']
2020-06-17 10:37:00,360 	Raw hypothesis: ['hello', '?']
2020-06-17 10:37:00,360 	Source:     hallo ,
2020-06-17 10:37:00,360 	Reference:  hello .
2020-06-17 10:37:00,360 	Hypothesis: hello ?
2020-06-17 10:37:00,360 Example #1
2020-06-17 10:37:00,360 	Raw source:     ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-17 10:37:00,360 	Raw hypothesis: ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-17 10:37:00,360 	Source:     hallo , wie kann ich ihnen helfen ?
2020-06-17 10:37:00,360 	Reference:  hi , how can i help you ?
2020-06-17 10:37:00,360 	Hypothesis: hi , how can i help you ?
2020-06-17 10:37:00,360 Example #2
2020-06-17 10:37:00,360 	Raw source:     ['hallo', ',', 'ich', 'bin', 'auf', 'der', 'suche', 'nach', 'einem', 'restaurant', 'im', 'arden', 'fair', 'einkaufszentrum', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-17 10:37:00,360 	Raw hypothesis: ['hi', ',', 'i', 'was', 'looking', 'for', 'a', 'restaurant', 'in', 'the', 'arden', 'fair', 'mall', 'in', 'sacramento', ',', 'california', 'area', '.']
2020-06-17 10:37:00,360 	Source:     hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-17 10:37:00,360 	Reference:  hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-17 10:37:00,360 	Hypothesis: hi , i was looking for a restaurant in the arden fair mall in sacramento , california area .
2020-06-17 10:37:00,360 Example #3
2020-06-17 10:37:00,360 	Raw source:     ['ok', '.', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', 'denn', 'genau', '?']
2020-06-17 10:37:00,360 	Raw hypothesis: ['ok', '.', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-17 10:37:00,360 	Source:     ok . welche art von restaurant suchen sie denn genau ?
2020-06-17 10:37:00,360 	Reference:  ok , what type of restaurant are you looking for ?
2020-06-17 10:37:00,360 	Hypothesis: ok . what type of restaurant are you looking for ?
2020-06-17 10:37:00,360 Validation result (greedy) at epoch  93, step     5000: bleu:  37.41, loss: 44450.0078, ppl:   8.3051, duration: 29.5550s
2020-06-17 10:37:02,320 Epoch  93: total training loss 4.81
2020-06-17 10:37:02,320 EPOCH 94
2020-06-17 10:37:25,230 Epoch  94: total training loss 4.74
2020-06-17 10:37:25,231 EPOCH 95
2020-06-17 10:37:43,808 Epoch  95 Step:     5100 Batch Loss:     0.108335 Tokens per Sec:     5432, Lr: 0.000200
2020-06-17 10:37:48,397 Epoch  95: total training loss 5.79
2020-06-17 10:37:48,398 EPOCH 96
2020-06-17 10:38:11,591 Epoch  96: total training loss 5.99
2020-06-17 10:38:11,592 EPOCH 97
2020-06-17 10:38:25,584 Epoch  97 Step:     5200 Batch Loss:     0.083185 Tokens per Sec:     5738, Lr: 0.000200
2020-06-17 10:38:34,869 Epoch  97: total training loss 5.07
2020-06-17 10:38:34,870 EPOCH 98
2020-06-17 10:38:58,057 Epoch  98: total training loss 4.84
2020-06-17 10:38:58,057 EPOCH 99
2020-06-17 10:39:09,254 Epoch  99 Step:     5300 Batch Loss:     0.072604 Tokens per Sec:     5835, Lr: 0.000200
2020-06-17 10:39:21,340 Epoch  99: total training loss 4.60
2020-06-17 10:39:21,341 EPOCH 100
2020-06-17 10:39:44,137 Epoch 100: total training loss 4.42
2020-06-17 10:39:44,138 Training ended after 100 epochs.
2020-06-17 10:39:44,138 Best validation result (greedy) at step     2000:   8.09 ppl.
2020-06-17 10:40:06,594  dev bleu:  39.14 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 10:40:06,598 Translations saved to: models/transformer_multi_enc6x2_deen/00002000.hyps.dev
2020-06-17 10:40:23,817 test bleu:  35.78 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-17 10:40:23,821 Translations saved to: models/transformer_multi_enc6x2_deen/00002000.hyps.test
