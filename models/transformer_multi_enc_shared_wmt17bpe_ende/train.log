2020-07-01 11:21:34,398 Hello! This is Joey-NMT.
2020-07-01 11:21:39,981 Total params: 53588481
2020-07-01 11:21:39,984 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-07-01 11:21:42,269 cfg.name                           : transformer_multi_enc_ende
2020-07-01 11:21:42,269 cfg.data.src                       : en
2020-07-01 11:21:42,269 cfg.data.trg                       : de
2020-07-01 11:21:42,269 cfg.data.train                     : chatnmt/prep/train.tags.bpe.wmt_ende_best
2020-07-01 11:21:42,269 cfg.data.dev                       : chatnmt/prep/dev.tags.bpe.wmt_ende_best
2020-07-01 11:21:42,269 cfg.data.test                      : chatnmt/prep/test.tags.bpe.wmt_ende_best
2020-07-01 11:21:42,269 cfg.data.level                     : bpe
2020-07-01 11:21:42,269 cfg.data.lowercase                 : False
2020-07-01 11:21:42,269 cfg.data.max_sent_length           : 100
2020-07-01 11:21:42,269 cfg.testing.beam_size              : 5
2020-07-01 11:21:42,269 cfg.testing.alpha                  : 1.0
2020-07-01 11:21:42,269 cfg.training.random_seed           : 42
2020-07-01 11:21:42,269 cfg.training.optimizer             : adam
2020-07-01 11:21:42,269 cfg.training.normalization         : tokens
2020-07-01 11:21:42,269 cfg.training.adam_betas            : [0.9, 0.999]
2020-07-01 11:21:42,269 cfg.training.scheduling            : plateau
2020-07-01 11:21:42,269 cfg.training.patience              : 8
2020-07-01 11:21:42,269 cfg.training.decrease_factor       : 0.7
2020-07-01 11:21:42,269 cfg.training.loss                  : crossentropy
2020-07-01 11:21:42,269 cfg.training.learning_rate         : 0.0002
2020-07-01 11:21:42,270 cfg.training.learning_rate_min     : 1e-08
2020-07-01 11:21:42,270 cfg.training.weight_decay          : 0.0
2020-07-01 11:21:42,270 cfg.training.label_smoothing       : 0.1
2020-07-01 11:21:42,270 cfg.training.batch_size            : 4096
2020-07-01 11:21:42,270 cfg.training.batch_type            : token
2020-07-01 11:21:42,270 cfg.training.eval_batch_size       : 3600
2020-07-01 11:21:42,270 cfg.training.eval_batch_type       : token
2020-07-01 11:21:42,270 cfg.training.batch_multiplier      : 1
2020-07-01 11:21:42,270 cfg.training.early_stopping_metric : ppl
2020-07-01 11:21:42,270 cfg.training.epochs                : 100
2020-07-01 11:21:42,270 cfg.training.validation_freq       : 1000
2020-07-01 11:21:42,270 cfg.training.logging_freq          : 100
2020-07-01 11:21:42,270 cfg.training.eval_metric           : bleu
2020-07-01 11:21:42,270 cfg.training.model_dir             : models/transformer_multi_enc_shared_wmt17bpe_ende
2020-07-01 11:21:42,270 cfg.training.overwrite             : True
2020-07-01 11:21:42,270 cfg.training.shuffle               : True
2020-07-01 11:21:42,270 cfg.training.use_cuda              : True
2020-07-01 11:21:42,270 cfg.training.max_output_length     : 100
2020-07-01 11:21:42,270 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-07-01 11:21:42,270 cfg.training.keep_last_ckpts       : 3
2020-07-01 11:21:42,270 cfg.model.initializer              : xavier
2020-07-01 11:21:42,270 cfg.model.bias_initializer         : zeros
2020-07-01 11:21:42,270 cfg.model.init_gain                : 1.0
2020-07-01 11:21:42,270 cfg.model.embed_initializer        : xavier
2020-07-01 11:21:42,270 cfg.model.embed_init_gain          : 1.0
2020-07-01 11:21:42,270 cfg.model.tied_embeddings          : False
2020-07-01 11:21:42,270 cfg.model.tied_softmax             : True
2020-07-01 11:21:42,270 cfg.model.encoder.type             : transformer
2020-07-01 11:21:42,270 cfg.model.encoder.num_layers       : 6
2020-07-01 11:21:42,270 cfg.model.encoder.num_heads        : 8
2020-07-01 11:21:42,270 cfg.model.encoder.embeddings.embedding_dim : 512
2020-07-01 11:21:42,270 cfg.model.encoder.embeddings.scale : True
2020-07-01 11:21:42,270 cfg.model.encoder.embeddings.dropout : 0.0
2020-07-01 11:21:42,270 cfg.model.encoder.hidden_size      : 512
2020-07-01 11:21:42,270 cfg.model.encoder.ff_size          : 2048
2020-07-01 11:21:42,270 cfg.model.encoder.dropout          : 0.1
2020-07-01 11:21:42,270 cfg.model.encoder.freeze           : False
2020-07-01 11:21:42,270 cfg.model.encoder.multi_encoder    : True
2020-07-01 11:21:42,270 cfg.model.encoder.share_encoder    : True
2020-07-01 11:21:42,270 cfg.model.decoder.type             : transformer
2020-07-01 11:21:42,270 cfg.model.decoder.num_layers       : 6
2020-07-01 11:21:42,270 cfg.model.decoder.num_heads        : 8
2020-07-01 11:21:42,270 cfg.model.decoder.embeddings.embedding_dim : 512
2020-07-01 11:21:42,271 cfg.model.decoder.embeddings.scale : True
2020-07-01 11:21:42,271 cfg.model.decoder.embeddings.dropout : 0.0
2020-07-01 11:21:42,271 cfg.model.decoder.hidden_size      : 512
2020-07-01 11:21:42,271 cfg.model.decoder.ff_size          : 2048
2020-07-01 11:21:42,271 cfg.model.decoder.dropout          : 0.1
2020-07-01 11:21:42,271 cfg.model.decoder.freeze           : False
2020-07-01 11:21:42,271 Data set sizes: 
	train 9771,
	valid 1522,
	test 1160
2020-07-01 11:21:42,271 First training example:
	[SRC] H@@ i there ! How can I help ?
	[TRG] Hall@@ o ! Wie kann ich helfen ?
2020-07-01 11:21:42,271 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) &@@ (9) a@@
2020-07-01 11:21:42,271 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) Sie (8) ich (9) das
2020-07-01 11:21:42,271 Number of Src words (types): 4442
2020-07-01 11:21:42,271 Number of Trg words (types): 5796
2020-07-01 11:21:42,271 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4442),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=5796))
2020-07-01 11:21:42,279 EPOCH 1
2020-07-01 11:22:07,140 Epoch   1: total training loss 375.50
2020-07-01 11:22:07,140 EPOCH 2
2020-07-01 11:22:19,902 Epoch   2 Step:      100 Batch Loss:     4.351603 Tokens per Sec:     6260, Lr: 0.000200
2020-07-01 11:22:32,211 Epoch   2: total training loss 335.25
2020-07-01 11:22:32,212 EPOCH 3
2020-07-01 11:22:56,607 Epoch   3 Step:      200 Batch Loss:     4.069482 Tokens per Sec:     6102, Lr: 0.000200
2020-07-01 11:22:57,215 Epoch   3: total training loss 315.71
2020-07-01 11:22:57,215 EPOCH 4
2020-07-01 11:23:23,084 Epoch   4: total training loss 279.36
2020-07-01 11:23:23,085 EPOCH 5
2020-07-01 11:23:34,556 Epoch   5 Step:      300 Batch Loss:     2.398865 Tokens per Sec:     5801, Lr: 0.000200
2020-07-01 11:23:49,350 Epoch   5: total training loss 250.43
2020-07-01 11:23:49,350 EPOCH 6
2020-07-01 11:24:13,729 Epoch   6 Step:      400 Batch Loss:     2.814831 Tokens per Sec:     5688, Lr: 0.000200
2020-07-01 11:24:15,996 Epoch   6: total training loss 217.07
2020-07-01 11:24:15,996 EPOCH 7
2020-07-01 11:24:42,928 Epoch   7: total training loss 198.39
2020-07-01 11:24:42,929 EPOCH 8
2020-07-01 11:24:53,128 Epoch   8 Step:      500 Batch Loss:     2.507015 Tokens per Sec:     5820, Lr: 0.000200
2020-07-01 11:25:09,464 Epoch   8: total training loss 177.64
2020-07-01 11:25:09,465 EPOCH 9
2020-07-01 11:25:32,825 Epoch   9 Step:      600 Batch Loss:     2.754969 Tokens per Sec:     5695, Lr: 0.000200
2020-07-01 11:25:35,844 Epoch   9: total training loss 165.74
2020-07-01 11:25:35,844 EPOCH 10
2020-07-01 11:26:02,411 Epoch  10: total training loss 149.33
2020-07-01 11:26:02,411 EPOCH 11
2020-07-01 11:26:12,029 Epoch  11 Step:      700 Batch Loss:     1.491813 Tokens per Sec:     5797, Lr: 0.000200
2020-07-01 11:26:28,967 Epoch  11: total training loss 139.87
2020-07-01 11:26:28,967 EPOCH 12
2020-07-01 11:26:50,585 Epoch  12 Step:      800 Batch Loss:     2.084319 Tokens per Sec:     6008, Lr: 0.000200
2020-07-01 11:26:54,931 Epoch  12: total training loss 131.98
2020-07-01 11:26:54,931 EPOCH 13
2020-07-01 11:27:21,333 Epoch  13: total training loss 118.00
2020-07-01 11:27:21,334 EPOCH 14
2020-07-01 11:27:29,783 Epoch  14 Step:      900 Batch Loss:     1.407780 Tokens per Sec:     5787, Lr: 0.000200
2020-07-01 11:27:47,095 Epoch  14: total training loss 115.54
2020-07-01 11:27:47,095 EPOCH 15
2020-07-01 11:28:07,471 Epoch  15 Step:     1000 Batch Loss:     1.447165 Tokens per Sec:     6056, Lr: 0.000200
2020-07-01 11:28:54,688 Hooray! New best validation result [ppl]!
2020-07-01 11:28:54,689 Saving new checkpoint.
2020-07-01 11:29:01,605 Example #0
2020-07-01 11:29:01,605 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:29:01,605 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:29:01,605 	Source:     Hello .
2020-07-01 11:29:01,605 	Reference:  Hallo ,
2020-07-01 11:29:01,605 	Hypothesis: Hallo .
2020-07-01 11:29:01,605 Example #1
2020-07-01 11:29:01,605 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:29:01,606 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:29:01,606 	Source:     Hi , how can I help you ?
2020-07-01 11:29:01,606 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:29:01,606 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:29:01,606 Example #2
2020-07-01 11:29:01,606 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:29:01,606 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'möchte', 'ein', 'paar', 'Kin@@', 'ok@@', 'arten', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:29:01,606 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:29:01,606 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:29:01,606 	Hypothesis: Hallo , ich möchte ein paar Kinokarten in San Francisco , Kalifornien .
2020-07-01 11:29:01,606 Example #3
2020-07-01 11:29:01,606 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:29:01,606 	Raw hypothesis: ['O@@', 'k', ',', 'welche', 'Art', 'von', 'Essen', 'mögen', 'Sie', '?']
2020-07-01 11:29:01,606 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:29:01,606 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:29:01,606 	Hypothesis: Ok , welche Art von Essen mögen Sie ?
2020-07-01 11:29:01,606 Validation result (greedy) at epoch  15, step     1000: bleu:  16.36, loss: 55117.5664, ppl:   9.8785, duration: 54.1341s
2020-07-01 11:29:07,272 Epoch  15: total training loss 108.49
2020-07-01 11:29:07,272 EPOCH 16
2020-07-01 11:29:32,956 Epoch  16: total training loss 95.41
2020-07-01 11:29:32,957 EPOCH 17
2020-07-01 11:29:40,506 Epoch  17 Step:     1100 Batch Loss:     0.853147 Tokens per Sec:     5207, Lr: 0.000200
2020-07-01 11:29:58,855 Epoch  17: total training loss 87.62
2020-07-01 11:29:58,856 EPOCH 18
2020-07-01 11:30:18,975 Epoch  18 Step:     1200 Batch Loss:     1.061170 Tokens per Sec:     5875, Lr: 0.000200
2020-07-01 11:30:25,166 Epoch  18: total training loss 81.28
2020-07-01 11:30:25,167 EPOCH 19
2020-07-01 11:30:51,365 Epoch  19: total training loss 74.02
2020-07-01 11:30:51,365 EPOCH 20
2020-07-01 11:30:56,653 Epoch  20 Step:     1300 Batch Loss:     0.840324 Tokens per Sec:     6339, Lr: 0.000200
2020-07-01 11:31:17,434 Epoch  20: total training loss 69.95
2020-07-01 11:31:17,434 EPOCH 21
2020-07-01 11:31:35,667 Epoch  21 Step:     1400 Batch Loss:     0.917786 Tokens per Sec:     5993, Lr: 0.000200
2020-07-01 11:31:43,544 Epoch  21: total training loss 62.78
2020-07-01 11:31:43,545 EPOCH 22
2020-07-01 11:32:09,601 Epoch  22: total training loss 55.62
2020-07-01 11:32:09,601 EPOCH 23
2020-07-01 11:32:14,088 Epoch  23 Step:     1500 Batch Loss:     0.695147 Tokens per Sec:     6536, Lr: 0.000200
2020-07-01 11:32:35,757 Epoch  23: total training loss 52.24
2020-07-01 11:32:35,757 EPOCH 24
2020-07-01 11:32:53,234 Epoch  24 Step:     1600 Batch Loss:     0.995500 Tokens per Sec:     5987, Lr: 0.000200
2020-07-01 11:33:02,051 Epoch  24: total training loss 48.84
2020-07-01 11:33:02,052 EPOCH 25
2020-07-01 11:33:28,395 Epoch  25: total training loss 42.63
2020-07-01 11:33:28,396 EPOCH 26
2020-07-01 11:33:32,334 Epoch  26 Step:     1700 Batch Loss:     0.640194 Tokens per Sec:     6307, Lr: 0.000200
2020-07-01 11:33:54,575 Epoch  26: total training loss 39.38
2020-07-01 11:33:54,575 EPOCH 27
2020-07-01 11:34:11,334 Epoch  27 Step:     1800 Batch Loss:     0.601753 Tokens per Sec:     5855, Lr: 0.000200
2020-07-01 11:34:20,786 Epoch  27: total training loss 36.93
2020-07-01 11:34:20,787 EPOCH 28
2020-07-01 11:34:46,436 Epoch  28: total training loss 34.13
2020-07-01 11:34:46,437 EPOCH 29
2020-07-01 11:34:49,175 Epoch  29 Step:     1900 Batch Loss:     0.433715 Tokens per Sec:     4416, Lr: 0.000200
2020-07-01 11:35:12,425 Epoch  29: total training loss 30.60
2020-07-01 11:35:12,425 EPOCH 30
2020-07-01 11:35:27,551 Epoch  30 Step:     2000 Batch Loss:     0.276048 Tokens per Sec:     5803, Lr: 0.000200
2020-07-01 11:35:54,924 Hooray! New best validation result [ppl]!
2020-07-01 11:35:54,925 Saving new checkpoint.
2020-07-01 11:36:01,894 Example #0
2020-07-01 11:36:01,895 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:36:01,895 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:36:01,895 	Source:     Hello .
2020-07-01 11:36:01,895 	Reference:  Hallo ,
2020-07-01 11:36:01,895 	Hypothesis: Hallo .
2020-07-01 11:36:01,895 Example #1
2020-07-01 11:36:01,895 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:36:01,895 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:36:01,895 	Source:     Hi , how can I help you ?
2020-07-01 11:36:01,895 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:36:01,895 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:36:01,895 Example #2
2020-07-01 11:36:01,896 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:36:01,896 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:36:01,896 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:36:01,896 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:36:01,896 	Hypothesis: Hallo , ich suche ein Restaurant in San Francisco , Kalifornien .
2020-07-01 11:36:01,896 Example #3
2020-07-01 11:36:01,896 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:36:01,896 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:36:01,896 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:36:01,896 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:36:01,896 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:36:01,896 Validation result (greedy) at epoch  30, step     2000: bleu:  28.27, loss: 51092.8086, ppl:   8.3571, duration: 34.3439s
2020-07-01 11:36:12,460 Epoch  30: total training loss 27.04
2020-07-01 11:36:12,461 EPOCH 31
2020-07-01 11:36:38,194 Epoch  31: total training loss 25.93
2020-07-01 11:36:38,194 EPOCH 32
2020-07-01 11:36:39,005 Epoch  32 Step:     2100 Batch Loss:     0.342267 Tokens per Sec:     8071, Lr: 0.000200
2020-07-01 11:37:04,148 Epoch  32: total training loss 23.94
2020-07-01 11:37:04,149 EPOCH 33
2020-07-01 11:37:17,463 Epoch  33 Step:     2200 Batch Loss:     0.370328 Tokens per Sec:     6154, Lr: 0.000200
2020-07-01 11:37:30,249 Epoch  33: total training loss 21.80
2020-07-01 11:37:30,250 EPOCH 34
2020-07-01 11:37:56,286 Epoch  34: total training loss 19.79
2020-07-01 11:37:56,287 EPOCH 35
2020-07-01 11:37:56,877 Epoch  35 Step:     2300 Batch Loss:     0.217071 Tokens per Sec:     3492, Lr: 0.000200
2020-07-01 11:38:22,393 Epoch  35: total training loss 18.05
2020-07-01 11:38:22,393 EPOCH 36
2020-07-01 11:38:35,702 Epoch  36 Step:     2400 Batch Loss:     0.273920 Tokens per Sec:     6074, Lr: 0.000200
2020-07-01 11:38:48,200 Epoch  36: total training loss 17.65
2020-07-01 11:38:48,201 EPOCH 37
2020-07-01 11:39:13,773 Epoch  37 Step:     2500 Batch Loss:     0.236512 Tokens per Sec:     5897, Lr: 0.000200
2020-07-01 11:39:14,180 Epoch  37: total training loss 16.71
2020-07-01 11:39:14,180 EPOCH 38
2020-07-01 11:39:40,179 Epoch  38: total training loss 15.76
2020-07-01 11:39:40,179 EPOCH 39
2020-07-01 11:39:52,756 Epoch  39 Step:     2600 Batch Loss:     0.234452 Tokens per Sec:     5832, Lr: 0.000200
2020-07-01 11:40:06,505 Epoch  39: total training loss 14.80
2020-07-01 11:40:06,506 EPOCH 40
2020-07-01 11:40:30,825 Epoch  40 Step:     2700 Batch Loss:     0.218335 Tokens per Sec:     5948, Lr: 0.000200
2020-07-01 11:40:32,783 Epoch  40: total training loss 14.40
2020-07-01 11:40:32,784 EPOCH 41
2020-07-01 11:40:58,599 Epoch  41: total training loss 13.62
2020-07-01 11:40:58,600 EPOCH 42
2020-07-01 11:41:09,110 Epoch  42 Step:     2800 Batch Loss:     0.183013 Tokens per Sec:     6075, Lr: 0.000200
2020-07-01 11:41:24,564 Epoch  42: total training loss 13.44
2020-07-01 11:41:24,565 EPOCH 43
2020-07-01 11:41:48,594 Epoch  43 Step:     2900 Batch Loss:     0.185536 Tokens per Sec:     5676, Lr: 0.000200
2020-07-01 11:41:51,129 Epoch  43: total training loss 12.90
2020-07-01 11:41:51,129 EPOCH 44
2020-07-01 11:42:16,962 Epoch  44: total training loss 12.17
2020-07-01 11:42:16,963 EPOCH 45
2020-07-01 11:42:26,235 Epoch  45 Step:     3000 Batch Loss:     0.170594 Tokens per Sec:     6090, Lr: 0.000200
2020-07-01 11:42:54,133 Example #0
2020-07-01 11:42:54,134 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:42:54,134 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:42:54,134 	Source:     Hello .
2020-07-01 11:42:54,134 	Reference:  Hallo ,
2020-07-01 11:42:54,134 	Hypothesis: Hallo .
2020-07-01 11:42:54,134 Example #1
2020-07-01 11:42:54,134 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:42:54,134 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:42:54,134 	Source:     Hi , how can I help you ?
2020-07-01 11:42:54,134 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:42:54,134 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:42:54,135 Example #2
2020-07-01 11:42:54,135 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:42:54,135 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:42:54,135 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:42:54,135 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:42:54,135 	Hypothesis: Hallo , ich suche ein Restaurant in San Francisco , Kalifornien .
2020-07-01 11:42:54,135 Example #3
2020-07-01 11:42:54,135 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:42:54,135 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:42:54,135 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:42:54,135 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:42:54,135 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:42:54,135 Validation result (greedy) at epoch  45, step     3000: bleu:  30.05, loss: 51974.5547, ppl:   8.6690, duration: 27.8993s
2020-07-01 11:43:10,348 Epoch  45: total training loss 11.89
2020-07-01 11:43:10,349 EPOCH 46
2020-07-01 11:43:32,635 Epoch  46 Step:     3100 Batch Loss:     0.188971 Tokens per Sec:     5904, Lr: 0.000200
2020-07-01 11:43:36,267 Epoch  46: total training loss 11.61
2020-07-01 11:43:36,267 EPOCH 47
2020-07-01 11:44:02,201 Epoch  47: total training loss 11.30
2020-07-01 11:44:02,201 EPOCH 48
2020-07-01 11:44:11,461 Epoch  48 Step:     3200 Batch Loss:     0.170332 Tokens per Sec:     5603, Lr: 0.000200
2020-07-01 11:44:28,027 Epoch  48: total training loss 11.60
2020-07-01 11:44:28,028 EPOCH 49
2020-07-01 11:44:49,669 Epoch  49 Step:     3300 Batch Loss:     0.158866 Tokens per Sec:     5922, Lr: 0.000200
2020-07-01 11:44:54,198 Epoch  49: total training loss 11.13
2020-07-01 11:44:54,199 EPOCH 50
2020-07-01 11:45:20,322 Epoch  50: total training loss 10.89
2020-07-01 11:45:20,322 EPOCH 51
2020-07-01 11:45:28,823 Epoch  51 Step:     3400 Batch Loss:     0.173123 Tokens per Sec:     5374, Lr: 0.000200
2020-07-01 11:45:46,286 Epoch  51: total training loss 10.94
2020-07-01 11:45:46,287 EPOCH 52
2020-07-01 11:46:05,873 Epoch  52 Step:     3500 Batch Loss:     0.140156 Tokens per Sec:     5961, Lr: 0.000200
2020-07-01 11:46:12,401 Epoch  52: total training loss 10.42
2020-07-01 11:46:12,402 EPOCH 53
2020-07-01 11:46:38,169 Epoch  53: total training loss 10.64
2020-07-01 11:46:38,169 EPOCH 54
2020-07-01 11:46:43,631 Epoch  54 Step:     3600 Batch Loss:     0.143264 Tokens per Sec:     5687, Lr: 0.000200
2020-07-01 11:47:04,295 Epoch  54: total training loss 10.26
2020-07-01 11:47:04,296 EPOCH 55
2020-07-01 11:47:21,519 Epoch  55 Step:     3700 Batch Loss:     0.143147 Tokens per Sec:     6061, Lr: 0.000200
2020-07-01 11:47:30,132 Epoch  55: total training loss 9.98
2020-07-01 11:47:30,133 EPOCH 56
2020-07-01 11:47:56,684 Epoch  56: total training loss 9.79
2020-07-01 11:47:56,684 EPOCH 57
2020-07-01 11:48:00,397 Epoch  57 Step:     3800 Batch Loss:     0.130403 Tokens per Sec:     5522, Lr: 0.000200
2020-07-01 11:48:22,833 Epoch  57: total training loss 10.50
2020-07-01 11:48:22,833 EPOCH 58
2020-07-01 11:48:38,465 Epoch  58 Step:     3900 Batch Loss:     0.491107 Tokens per Sec:     5925, Lr: 0.000200
2020-07-01 11:48:49,287 Epoch  58: total training loss 12.59
2020-07-01 11:48:49,288 EPOCH 59
2020-07-01 11:49:15,377 Epoch  59: total training loss 11.24
2020-07-01 11:49:15,377 EPOCH 60
2020-07-01 11:49:17,690 Epoch  60 Step:     4000 Batch Loss:     0.114233 Tokens per Sec:     5652, Lr: 0.000200
2020-07-01 11:49:51,627 Example #0
2020-07-01 11:49:51,627 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:49:51,628 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:49:51,628 	Source:     Hello .
2020-07-01 11:49:51,628 	Reference:  Hallo ,
2020-07-01 11:49:51,628 	Hypothesis: Hallo .
2020-07-01 11:49:51,628 Example #1
2020-07-01 11:49:51,628 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:49:51,628 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:49:51,628 	Source:     Hi , how can I help you ?
2020-07-01 11:49:51,628 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:49:51,628 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:49:51,628 Example #2
2020-07-01 11:49:51,628 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:49:51,628 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', '.']
2020-07-01 11:49:51,628 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:49:51,628 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:49:51,629 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall .
2020-07-01 11:49:51,629 Example #3
2020-07-01 11:49:51,629 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:49:51,629 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:49:51,629 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:49:51,629 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:49:51,629 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:49:51,629 Validation result (greedy) at epoch  60, step     4000: bleu:  30.06, loss: 52527.0586, ppl:   8.8704, duration: 33.9379s
2020-07-01 11:50:15,592 Epoch  60: total training loss 9.30
2020-07-01 11:50:15,593 EPOCH 61
2020-07-01 11:50:30,234 Epoch  61 Step:     4100 Batch Loss:     0.111488 Tokens per Sec:     5903, Lr: 0.000200
2020-07-01 11:50:42,063 Epoch  61: total training loss 8.58
2020-07-01 11:50:42,063 EPOCH 62
2020-07-01 11:51:08,411 Epoch  62: total training loss 8.34
2020-07-01 11:51:08,412 EPOCH 63
2020-07-01 11:51:09,658 Epoch  63 Step:     4200 Batch Loss:     0.114206 Tokens per Sec:     2518, Lr: 0.000200
2020-07-01 11:51:34,457 Epoch  63: total training loss 7.99
2020-07-01 11:51:34,458 EPOCH 64
2020-07-01 11:51:48,378 Epoch  64 Step:     4300 Batch Loss:     0.115093 Tokens per Sec:     5484, Lr: 0.000200
2020-07-01 11:52:00,392 Epoch  64: total training loss 7.91
2020-07-01 11:52:00,393 EPOCH 65
2020-07-01 11:52:26,316 Epoch  65 Step:     4400 Batch Loss:     0.112651 Tokens per Sec:     5838, Lr: 0.000200
2020-07-01 11:52:26,779 Epoch  65: total training loss 7.74
2020-07-01 11:52:26,779 EPOCH 66
2020-07-01 11:52:52,913 Epoch  66: total training loss 7.65
2020-07-01 11:52:52,914 EPOCH 67
2020-07-01 11:53:04,810 Epoch  67 Step:     4500 Batch Loss:     0.100388 Tokens per Sec:     6030, Lr: 0.000200
2020-07-01 11:53:18,562 Epoch  67: total training loss 7.45
2020-07-01 11:53:18,563 EPOCH 68
2020-07-01 11:53:43,606 Epoch  68 Step:     4600 Batch Loss:     0.114446 Tokens per Sec:     5886, Lr: 0.000200
2020-07-01 11:53:44,861 Epoch  68: total training loss 7.38
2020-07-01 11:53:44,861 EPOCH 69
2020-07-01 11:54:11,145 Epoch  69: total training loss 7.28
2020-07-01 11:54:11,146 EPOCH 70
2020-07-01 11:54:22,420 Epoch  70 Step:     4700 Batch Loss:     0.095837 Tokens per Sec:     6067, Lr: 0.000200
2020-07-01 11:54:37,425 Epoch  70: total training loss 7.48
2020-07-01 11:54:37,425 EPOCH 71
2020-07-01 11:55:01,511 Epoch  71 Step:     4800 Batch Loss:     0.118366 Tokens per Sec:     5878, Lr: 0.000200
2020-07-01 11:55:03,601 Epoch  71: total training loss 7.30
2020-07-01 11:55:03,601 EPOCH 72
2020-07-01 11:55:29,467 Epoch  72: total training loss 7.33
2020-07-01 11:55:29,468 EPOCH 73
2020-07-01 11:55:39,735 Epoch  73 Step:     4900 Batch Loss:     0.112588 Tokens per Sec:     6003, Lr: 0.000200
2020-07-01 11:55:55,642 Epoch  73: total training loss 7.32
2020-07-01 11:55:55,642 EPOCH 74
2020-07-01 11:56:18,635 Epoch  74 Step:     5000 Batch Loss:     0.118488 Tokens per Sec:     5893, Lr: 0.000200
2020-07-01 11:56:53,113 Example #0
2020-07-01 11:56:53,114 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 11:56:53,114 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 11:56:53,114 	Source:     Hello .
2020-07-01 11:56:53,114 	Reference:  Hallo ,
2020-07-01 11:56:53,114 	Hypothesis: Hallo .
2020-07-01 11:56:53,114 Example #1
2020-07-01 11:56:53,114 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 11:56:53,114 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 11:56:53,114 	Source:     Hi , how can I help you ?
2020-07-01 11:56:53,114 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:56:53,114 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 11:56:53,114 Example #2
2020-07-01 11:56:53,114 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 11:56:53,114 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 11:56:53,114 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 11:56:53,114 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 11:56:53,114 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 11:56:53,114 Example #3
2020-07-01 11:56:53,114 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 11:56:53,114 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 11:56:53,114 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 11:56:53,114 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 11:56:53,114 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 11:56:53,115 Validation result (greedy) at epoch  74, step     5000: bleu:  31.65, loss: 51618.9375, ppl:   8.5419, duration: 34.4791s
2020-07-01 11:56:56,523 Epoch  74: total training loss 7.07
2020-07-01 11:56:56,524 EPOCH 75
2020-07-01 11:57:22,591 Epoch  75: total training loss 7.03
2020-07-01 11:57:22,592 EPOCH 76
2020-07-01 11:57:32,331 Epoch  76 Step:     5100 Batch Loss:     0.116008 Tokens per Sec:     5721, Lr: 0.000200
2020-07-01 11:57:48,657 Epoch  76: total training loss 7.10
2020-07-01 11:57:48,658 EPOCH 77
2020-07-01 11:58:10,455 Epoch  77 Step:     5200 Batch Loss:     0.115004 Tokens per Sec:     5971, Lr: 0.000200
2020-07-01 11:58:15,013 Epoch  77: total training loss 7.06
2020-07-01 11:58:15,014 EPOCH 78
2020-07-01 11:58:41,067 Epoch  78: total training loss 7.14
2020-07-01 11:58:41,068 EPOCH 79
2020-07-01 11:58:49,634 Epoch  79 Step:     5300 Batch Loss:     0.099723 Tokens per Sec:     5842, Lr: 0.000200
2020-07-01 11:59:07,348 Epoch  79: total training loss 7.06
2020-07-01 11:59:07,348 EPOCH 80
2020-07-01 11:59:28,376 Epoch  80 Step:     5400 Batch Loss:     0.103355 Tokens per Sec:     5910, Lr: 0.000200
2020-07-01 11:59:33,543 Epoch  80: total training loss 7.02
2020-07-01 11:59:33,543 EPOCH 81
2020-07-01 11:59:59,721 Epoch  81: total training loss 7.97
2020-07-01 11:59:59,722 EPOCH 82
2020-07-01 12:00:07,214 Epoch  82 Step:     5500 Batch Loss:     0.116199 Tokens per Sec:     5723, Lr: 0.000200
2020-07-01 12:00:25,741 Epoch  82: total training loss 7.40
2020-07-01 12:00:25,741 EPOCH 83
2020-07-01 12:00:46,100 Epoch  83 Step:     5600 Batch Loss:     0.098598 Tokens per Sec:     5911, Lr: 0.000200
2020-07-01 12:00:51,831 Epoch  83: total training loss 6.94
2020-07-01 12:00:51,831 EPOCH 84
2020-07-01 12:01:17,727 Epoch  84: total training loss 6.76
2020-07-01 12:01:17,727 EPOCH 85
2020-07-01 12:01:24,445 Epoch  85 Step:     5700 Batch Loss:     0.096344 Tokens per Sec:     5985, Lr: 0.000200
2020-07-01 12:01:43,498 Epoch  85: total training loss 6.54
2020-07-01 12:01:43,499 EPOCH 86
2020-07-01 12:02:02,816 Epoch  86 Step:     5800 Batch Loss:     0.102491 Tokens per Sec:     5998, Lr: 0.000200
2020-07-01 12:02:09,215 Epoch  86: total training loss 6.68
2020-07-01 12:02:09,216 EPOCH 87
2020-07-01 12:02:34,604 Epoch  87: total training loss 6.38
2020-07-01 12:02:34,604 EPOCH 88
2020-07-01 12:02:39,937 Epoch  88 Step:     5900 Batch Loss:     0.106905 Tokens per Sec:     6647, Lr: 0.000200
2020-07-01 12:03:00,346 Epoch  88: total training loss 10.83
2020-07-01 12:03:00,347 EPOCH 89
2020-07-01 12:03:18,615 Epoch  89 Step:     6000 Batch Loss:     0.154477 Tokens per Sec:     6020, Lr: 0.000200
2020-07-01 12:03:43,106 Example #0
2020-07-01 12:03:43,106 	Raw source:     ['H@@', 'ello', '.']
2020-07-01 12:03:43,106 	Raw hypothesis: ['Hall@@', 'o', '.']
2020-07-01 12:03:43,106 	Source:     Hello .
2020-07-01 12:03:43,106 	Reference:  Hallo ,
2020-07-01 12:03:43,107 	Hypothesis: Hallo .
2020-07-01 12:03:43,107 Example #1
2020-07-01 12:03:43,107 	Raw source:     ['H@@', 'i', ',', 'how', 'can', 'I', 'help', 'you', '?']
2020-07-01 12:03:43,107 	Raw hypothesis: ['Hall@@', 'o', ',', 'wie', 'kann', 'ich', 'Ihnen', 'helfen', '?']
2020-07-01 12:03:43,107 	Source:     Hi , how can I help you ?
2020-07-01 12:03:43,107 	Reference:  Hallo , wie kann ich Ihnen helfen ?
2020-07-01 12:03:43,107 	Hypothesis: Hallo , wie kann ich Ihnen helfen ?
2020-07-01 12:03:43,107 Example #2
2020-07-01 12:03:43,107 	Raw source:     ['H@@', 'i', ',', 'I', '&@@', 'a@@', 'pos@@', ';@@', 'm', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'Ar@@', 'den', 'Fair', 'm@@', 'all', 'in', 'San', 'Francisco', ',', 'California', '.']
2020-07-01 12:03:43,107 	Raw hypothesis: ['Hall@@', 'o', ',', 'ich', 'suche', 'ein', 'Restaurant', 'in', 'der', 'Ar@@', 'den', 'Fair', 'M@@', 'all', 'in', 'San', 'Francisco', ',', 'Kalifor@@', 'nien', '.']
2020-07-01 12:03:43,107 	Source:     Hi , I &apos;m looking for a restaurant inside the Arden Fair mall in San Francisco , California .
2020-07-01 12:03:43,107 	Reference:  Hallo , ich bin auf der Suche nach einem Restaurant im Arden Fair Einkaufszentrum in San Francisco , Kalifornien .
2020-07-01 12:03:43,107 	Hypothesis: Hallo , ich suche ein Restaurant in der Arden Fair Mall in San Francisco , Kalifornien .
2020-07-01 12:03:43,107 Example #3
2020-07-01 12:03:43,107 	Raw source:     ['O@@', 'k', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-07-01 12:03:43,107 	Raw hypothesis: ['O@@', 'k', ',', 'nach', 'welcher', 'Art', 'von', 'Restaurant', 'suchen', 'Sie', '?']
2020-07-01 12:03:43,107 	Source:     Ok , what type of restaurant are you looking for ?
2020-07-01 12:03:43,107 	Reference:  Ok . Welche Art von Restaurant suchen Sie denn genau ?
2020-07-01 12:03:43,107 	Hypothesis: Ok , nach welcher Art von Restaurant suchen Sie ?
2020-07-01 12:03:43,107 Validation result (greedy) at epoch  89, step     6000: bleu:  29.84, loss: 53110.8906, ppl:   9.0882, duration: 24.4904s
2020-07-01 12:03:51,156 Epoch  89: total training loss 12.39
2020-07-01 12:03:51,157 EPOCH 90
2020-07-01 12:04:17,611 Epoch  90: total training loss 7.98
2020-07-01 12:04:17,611 EPOCH 91
2020-07-01 12:04:22,293 Epoch  91 Step:     6100 Batch Loss:     0.103650 Tokens per Sec:     6459, Lr: 0.000200
2020-07-01 12:04:43,391 Epoch  91: total training loss 6.83
2020-07-01 12:04:43,392 EPOCH 92
2020-07-01 12:05:01,798 Epoch  92 Step:     6200 Batch Loss:     0.094640 Tokens per Sec:     5686, Lr: 0.000200
2020-07-01 12:05:08,883 Epoch  92: total training loss 6.37
2020-07-01 12:05:08,884 EPOCH 93
2020-07-01 12:05:34,288 Epoch  93: total training loss 6.14
2020-07-01 12:05:34,288 EPOCH 94
2020-07-01 12:05:38,313 Epoch  94 Step:     6300 Batch Loss:     0.085418 Tokens per Sec:     6114, Lr: 0.000200
2020-07-01 12:05:59,639 Epoch  94: total training loss 6.03
2020-07-01 12:05:59,640 EPOCH 95
2020-07-01 12:06:15,588 Epoch  95 Step:     6400 Batch Loss:     0.096009 Tokens per Sec:     6045, Lr: 0.000200
2020-07-01 12:06:25,037 Epoch  95: total training loss 5.79
2020-07-01 12:06:25,038 EPOCH 96
2020-07-01 12:06:50,193 Epoch  96: total training loss 5.68
2020-07-01 12:06:50,194 EPOCH 97
2020-07-01 12:06:53,074 Epoch  97 Step:     6500 Batch Loss:     0.073549 Tokens per Sec:     5598, Lr: 0.000200
2020-07-01 12:07:15,569 Epoch  97: total training loss 5.56
2020-07-01 12:07:15,569 EPOCH 98
2020-07-01 12:07:29,994 Epoch  98 Step:     6600 Batch Loss:     0.073362 Tokens per Sec:     5998, Lr: 0.000200
2020-07-01 12:07:41,097 Epoch  98: total training loss 5.53
2020-07-01 12:07:41,098 EPOCH 99
2020-07-01 12:08:06,376 Epoch  99: total training loss 5.35
2020-07-01 12:08:06,377 EPOCH 100
2020-07-01 12:08:08,155 Epoch 100 Step:     6700 Batch Loss:     0.100838 Tokens per Sec:     4390, Lr: 0.000200
2020-07-01 12:08:31,712 Epoch 100: total training loss 5.43
2020-07-01 12:08:31,713 Training ended after 100 epochs.
2020-07-01 12:08:31,713 Best validation result (greedy) at step     2000:   8.36 ppl.
2020-07-01 12:08:55,126  dev bleu:  29.22 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 12:08:55,131 Translations saved to: models/transformer_multi_enc_shared_wmt17bpe_ende/00002000.hyps.dev
2020-07-01 12:09:10,179 test bleu:  27.07 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-07-01 12:09:10,184 Translations saved to: models/transformer_multi_enc_shared_wmt17bpe_ende/00002000.hyps.test
