2020-06-22 02:15:06,554 Hello! This is Joey-NMT.
2020-06-22 02:15:10,957 Total params: 5369985
2020-06-22 02:15:10,960 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder_2.layer_norm.bias', 'encoder_2.layer_norm.weight', 'encoder_2.layers.0.feed_forward.layer_norm.bias', 'encoder_2.layers.0.feed_forward.layer_norm.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.0.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.0.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.0.layer_norm.bias', 'encoder_2.layers.0.layer_norm.weight', 'encoder_2.layers.0.src_src_att.k_layer.bias', 'encoder_2.layers.0.src_src_att.k_layer.weight', 'encoder_2.layers.0.src_src_att.output_layer.bias', 'encoder_2.layers.0.src_src_att.output_layer.weight', 'encoder_2.layers.0.src_src_att.q_layer.bias', 'encoder_2.layers.0.src_src_att.q_layer.weight', 'encoder_2.layers.0.src_src_att.v_layer.bias', 'encoder_2.layers.0.src_src_att.v_layer.weight', 'encoder_2.layers.1.feed_forward.layer_norm.bias', 'encoder_2.layers.1.feed_forward.layer_norm.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.1.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.1.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.1.layer_norm.bias', 'encoder_2.layers.1.layer_norm.weight', 'encoder_2.layers.1.src_src_att.k_layer.bias', 'encoder_2.layers.1.src_src_att.k_layer.weight', 'encoder_2.layers.1.src_src_att.output_layer.bias', 'encoder_2.layers.1.src_src_att.output_layer.weight', 'encoder_2.layers.1.src_src_att.q_layer.bias', 'encoder_2.layers.1.src_src_att.q_layer.weight', 'encoder_2.layers.1.src_src_att.v_layer.bias', 'encoder_2.layers.1.src_src_att.v_layer.weight', 'encoder_2.layers.2.feed_forward.layer_norm.bias', 'encoder_2.layers.2.feed_forward.layer_norm.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.2.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.2.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.2.layer_norm.bias', 'encoder_2.layers.2.layer_norm.weight', 'encoder_2.layers.2.src_src_att.k_layer.bias', 'encoder_2.layers.2.src_src_att.k_layer.weight', 'encoder_2.layers.2.src_src_att.output_layer.bias', 'encoder_2.layers.2.src_src_att.output_layer.weight', 'encoder_2.layers.2.src_src_att.q_layer.bias', 'encoder_2.layers.2.src_src_att.q_layer.weight', 'encoder_2.layers.2.src_src_att.v_layer.bias', 'encoder_2.layers.2.src_src_att.v_layer.weight', 'encoder_2.layers.3.feed_forward.layer_norm.bias', 'encoder_2.layers.3.feed_forward.layer_norm.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.3.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.3.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.3.layer_norm.bias', 'encoder_2.layers.3.layer_norm.weight', 'encoder_2.layers.3.src_src_att.k_layer.bias', 'encoder_2.layers.3.src_src_att.k_layer.weight', 'encoder_2.layers.3.src_src_att.output_layer.bias', 'encoder_2.layers.3.src_src_att.output_layer.weight', 'encoder_2.layers.3.src_src_att.q_layer.bias', 'encoder_2.layers.3.src_src_att.q_layer.weight', 'encoder_2.layers.3.src_src_att.v_layer.bias', 'encoder_2.layers.3.src_src_att.v_layer.weight', 'encoder_2.layers.4.feed_forward.layer_norm.bias', 'encoder_2.layers.4.feed_forward.layer_norm.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.4.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.4.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.4.layer_norm.bias', 'encoder_2.layers.4.layer_norm.weight', 'encoder_2.layers.4.src_src_att.k_layer.bias', 'encoder_2.layers.4.src_src_att.k_layer.weight', 'encoder_2.layers.4.src_src_att.output_layer.bias', 'encoder_2.layers.4.src_src_att.output_layer.weight', 'encoder_2.layers.4.src_src_att.q_layer.bias', 'encoder_2.layers.4.src_src_att.q_layer.weight', 'encoder_2.layers.4.src_src_att.v_layer.bias', 'encoder_2.layers.4.src_src_att.v_layer.weight', 'encoder_2.layers.5.feed_forward.layer_norm.bias', 'encoder_2.layers.5.feed_forward.layer_norm.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.0.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.0.weight', 'encoder_2.layers.5.feed_forward.pwff_layer.3.bias', 'encoder_2.layers.5.feed_forward.pwff_layer.3.weight', 'encoder_2.layers.5.layer_norm.bias', 'encoder_2.layers.5.layer_norm.weight', 'encoder_2.layers.5.src_src_att.k_layer.bias', 'encoder_2.layers.5.src_src_att.k_layer.weight', 'encoder_2.layers.5.src_src_att.output_layer.bias', 'encoder_2.layers.5.src_src_att.output_layer.weight', 'encoder_2.layers.5.src_src_att.q_layer.bias', 'encoder_2.layers.5.src_src_att.q_layer.weight', 'encoder_2.layers.5.src_src_att.v_layer.bias', 'encoder_2.layers.5.src_src_att.v_layer.weight', 'last_layer.W_g.weight', 'last_layer.b_g', 'last_layer.feed_forward.layer_norm.bias', 'last_layer.feed_forward.layer_norm.weight', 'last_layer.feed_forward.pwff_layer.0.bias', 'last_layer.feed_forward.pwff_layer.0.weight', 'last_layer.feed_forward.pwff_layer.3.bias', 'last_layer.feed_forward.pwff_layer.3.weight', 'last_layer.layer_norm.bias', 'last_layer.layer_norm.weight', 'last_layer.src2_src_att.k_layer.bias', 'last_layer.src2_src_att.k_layer.weight', 'last_layer.src2_src_att.output_layer.bias', 'last_layer.src2_src_att.output_layer.weight', 'last_layer.src2_src_att.q_layer.bias', 'last_layer.src2_src_att.q_layer.weight', 'last_layer.src2_src_att.v_layer.bias', 'last_layer.src2_src_att.v_layer.weight', 'last_layer.src_src_att.k_layer.bias', 'last_layer.src_src_att.k_layer.weight', 'last_layer.src_src_att.output_layer.bias', 'last_layer.src_src_att.output_layer.weight', 'last_layer.src_src_att.q_layer.bias', 'last_layer.src_src_att.q_layer.weight', 'last_layer.src_src_att.v_layer.bias', 'last_layer.src_src_att.v_layer.weight', 'last_layer_norm.bias', 'last_layer_norm.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-06-22 02:15:13,070 cfg.name                           : transformer_multi_enc6x2_hid128_ende
2020-06-22 02:15:13,070 cfg.data.src                       : en
2020-06-22 02:15:13,070 cfg.data.trg                       : de
2020-06-22 02:15:13,070 cfg.data.train                     : chatnmt/multi_encoder/train.tags.bpe.10000
2020-06-22 02:15:13,070 cfg.data.dev                       : chatnmt/multi_encoder/dev.tags.bpe.10000
2020-06-22 02:15:13,070 cfg.data.test                      : chatnmt/multi_encoder/test.tags.bpe.10000
2020-06-22 02:15:13,070 cfg.data.level                     : bpe
2020-06-22 02:15:13,070 cfg.data.lowercase                 : True
2020-06-22 02:15:13,070 cfg.data.max_sent_length           : 100
2020-06-22 02:15:13,071 cfg.testing.beam_size              : 5
2020-06-22 02:15:13,071 cfg.testing.alpha                  : 1.0
2020-06-22 02:15:13,071 cfg.training.random_seed           : 42
2020-06-22 02:15:13,071 cfg.training.optimizer             : adam
2020-06-22 02:15:13,071 cfg.training.normalization         : tokens
2020-06-22 02:15:13,071 cfg.training.adam_betas            : [0.9, 0.999]
2020-06-22 02:15:13,071 cfg.training.scheduling            : plateau
2020-06-22 02:15:13,071 cfg.training.patience              : 8
2020-06-22 02:15:13,071 cfg.training.decrease_factor       : 0.7
2020-06-22 02:15:13,071 cfg.training.loss                  : crossentropy
2020-06-22 02:15:13,071 cfg.training.learning_rate         : 0.0002
2020-06-22 02:15:13,071 cfg.training.learning_rate_min     : 1e-08
2020-06-22 02:15:13,071 cfg.training.weight_decay          : 0.0
2020-06-22 02:15:13,071 cfg.training.label_smoothing       : 0.1
2020-06-22 02:15:13,071 cfg.training.batch_size            : 4096
2020-06-22 02:15:13,071 cfg.training.batch_type            : token
2020-06-22 02:15:13,072 cfg.training.eval_batch_size       : 3600
2020-06-22 02:15:13,072 cfg.training.eval_batch_type       : token
2020-06-22 02:15:13,072 cfg.training.batch_multiplier      : 1
2020-06-22 02:15:13,072 cfg.training.early_stopping_metric : ppl
2020-06-22 02:15:13,072 cfg.training.epochs                : 100
2020-06-22 02:15:13,072 cfg.training.validation_freq       : 1000
2020-06-22 02:15:13,072 cfg.training.logging_freq          : 100
2020-06-22 02:15:13,072 cfg.training.eval_metric           : bleu
2020-06-22 02:15:13,072 cfg.training.model_dir             : models/transformer_multi_enc6x2_hid128_ende
2020-06-22 02:15:13,072 cfg.training.overwrite             : True
2020-06-22 02:15:13,072 cfg.training.shuffle               : True
2020-06-22 02:15:13,072 cfg.training.use_cuda              : True
2020-06-22 02:15:13,072 cfg.training.max_output_length     : 100
2020-06-22 02:15:13,072 cfg.training.print_valid_sents     : [0, 1, 2, 3]
2020-06-22 02:15:13,072 cfg.training.keep_last_ckpts       : 3
2020-06-22 02:15:13,072 cfg.model.initializer              : xavier
2020-06-22 02:15:13,073 cfg.model.bias_initializer         : zeros
2020-06-22 02:15:13,073 cfg.model.init_gain                : 1.0
2020-06-22 02:15:13,073 cfg.model.embed_initializer        : xavier
2020-06-22 02:15:13,073 cfg.model.embed_init_gain          : 1.0
2020-06-22 02:15:13,073 cfg.model.tied_embeddings          : False
2020-06-22 02:15:13,073 cfg.model.tied_softmax             : True
2020-06-22 02:15:13,073 cfg.model.encoder.type             : transformer
2020-06-22 02:15:13,073 cfg.model.encoder.num_layers       : 6
2020-06-22 02:15:13,073 cfg.model.encoder.num_heads        : 8
2020-06-22 02:15:13,073 cfg.model.encoder.embeddings.embedding_dim : 128
2020-06-22 02:15:13,073 cfg.model.encoder.embeddings.scale : True
2020-06-22 02:15:13,073 cfg.model.encoder.embeddings.dropout : 0.0
2020-06-22 02:15:13,073 cfg.model.encoder.hidden_size      : 128
2020-06-22 02:15:13,073 cfg.model.encoder.ff_size          : 512
2020-06-22 02:15:13,073 cfg.model.encoder.dropout          : 0.1
2020-06-22 02:15:13,073 cfg.model.encoder.freeze           : False
2020-06-22 02:15:13,074 cfg.model.encoder.multi_encoder    : True
2020-06-22 02:15:13,074 cfg.model.decoder.type             : transformer
2020-06-22 02:15:13,074 cfg.model.decoder.num_layers       : 6
2020-06-22 02:15:13,074 cfg.model.decoder.num_heads        : 8
2020-06-22 02:15:13,074 cfg.model.decoder.embeddings.embedding_dim : 128
2020-06-22 02:15:13,074 cfg.model.decoder.embeddings.scale : True
2020-06-22 02:15:13,074 cfg.model.decoder.embeddings.dropout : 0.0
2020-06-22 02:15:13,074 cfg.model.decoder.hidden_size      : 128
2020-06-22 02:15:13,074 cfg.model.decoder.ff_size          : 512
2020-06-22 02:15:13,074 cfg.model.decoder.dropout          : 0.1
2020-06-22 02:15:13,074 cfg.model.decoder.freeze           : False
2020-06-22 02:15:13,074 Data set sizes: 
	train 9790,
	valid 1524,
	test 1165
2020-06-22 02:15:13,074 First training example:
	[SRC] hi there ! how can i help ?
	[TRG] hallo ! wie kann ich helfen ?
2020-06-22 02:15:13,074 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) you (8) i (9) the
2020-06-22 02:15:13,075 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) ? (7) sie (8) ich (9) das
2020-06-22 02:15:13,075 Number of Src words (types): 4561
2020-06-22 02:15:13,075 Number of Trg words (types): 5876
2020-06-22 02:15:13,075 Model(
	encoder=TransformerEncoder(num_layers=5, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=128, vocab_size=4561),
	trg_embed=Embeddings(embedding_dim=128, vocab_size=5876))
2020-06-22 02:15:13,088 EPOCH 1
2020-06-22 02:15:20,355 Epoch   1: total training loss 376.17
2020-06-22 02:15:20,356 EPOCH 2
2020-06-22 02:15:25,651 Epoch   2 Step:      100 Batch Loss:     5.316026 Tokens per Sec:    20027, Lr: 0.000200
2020-06-22 02:15:26,818 Epoch   2: total training loss 311.36
2020-06-22 02:15:26,818 EPOCH 3
2020-06-22 02:15:33,243 Epoch   3: total training loss 283.19
2020-06-22 02:15:33,243 EPOCH 4
2020-06-22 02:15:37,360 Epoch   4 Step:      200 Batch Loss:     3.570513 Tokens per Sec:    19737, Lr: 0.000200
2020-06-22 02:15:39,872 Epoch   4: total training loss 272.46
2020-06-22 02:15:39,873 EPOCH 5
2020-06-22 02:15:46,556 Epoch   5: total training loss 257.47
2020-06-22 02:15:46,557 EPOCH 6
2020-06-22 02:15:49,687 Epoch   6 Step:      300 Batch Loss:     5.860322 Tokens per Sec:    19057, Lr: 0.000200
2020-06-22 02:15:53,472 Epoch   6: total training loss 253.87
2020-06-22 02:15:53,472 EPOCH 7
2020-06-22 02:16:00,082 Epoch   7: total training loss 238.88
2020-06-22 02:16:00,083 EPOCH 8
2020-06-22 02:16:01,984 Epoch   8 Step:      400 Batch Loss:     4.385202 Tokens per Sec:    19475, Lr: 0.000200
2020-06-22 02:16:06,523 Epoch   8: total training loss 231.14
2020-06-22 02:16:06,524 EPOCH 9
2020-06-22 02:16:12,970 Epoch   9: total training loss 227.53
2020-06-22 02:16:12,970 EPOCH 10
2020-06-22 02:16:13,711 Epoch  10 Step:      500 Batch Loss:     4.584021 Tokens per Sec:    24630, Lr: 0.000200
2020-06-22 02:16:19,478 Epoch  10: total training loss 219.93
2020-06-22 02:16:19,478 EPOCH 11
2020-06-22 02:16:26,064 Epoch  11 Step:      600 Batch Loss:     3.048901 Tokens per Sec:    18746, Lr: 0.000200
2020-06-22 02:16:26,379 Epoch  11: total training loss 213.32
2020-06-22 02:16:26,379 EPOCH 12
2020-06-22 02:16:32,823 Epoch  12: total training loss 204.83
2020-06-22 02:16:32,824 EPOCH 13
2020-06-22 02:16:37,778 Epoch  13 Step:      700 Batch Loss:     3.676323 Tokens per Sec:    20989, Lr: 0.000200
2020-06-22 02:16:39,290 Epoch  13: total training loss 198.82
2020-06-22 02:16:39,290 EPOCH 14
2020-06-22 02:16:45,771 Epoch  14: total training loss 191.33
2020-06-22 02:16:45,771 EPOCH 15
2020-06-22 02:16:49,465 Epoch  15 Step:      800 Batch Loss:     3.791680 Tokens per Sec:    20997, Lr: 0.000200
2020-06-22 02:16:52,301 Epoch  15: total training loss 185.42
2020-06-22 02:16:52,302 EPOCH 16
2020-06-22 02:16:58,763 Epoch  16: total training loss 181.45
2020-06-22 02:16:58,764 EPOCH 17
2020-06-22 02:17:01,450 Epoch  17 Step:      900 Batch Loss:     3.265895 Tokens per Sec:    19532, Lr: 0.000200
2020-06-22 02:17:05,258 Epoch  17: total training loss 175.71
2020-06-22 02:17:05,258 EPOCH 18
2020-06-22 02:17:11,824 Epoch  18: total training loss 171.79
2020-06-22 02:17:11,824 EPOCH 19
2020-06-22 02:17:13,367 Epoch  19 Step:     1000 Batch Loss:     3.364158 Tokens per Sec:    18667, Lr: 0.000200
2020-06-22 02:17:27,442 Hooray! New best validation result [ppl]!
2020-06-22 02:17:27,442 Saving new checkpoint.
2020-06-22 02:17:28,231 Example #0
2020-06-22 02:17:28,231 	Raw source:     ['hello', '.']
2020-06-22 02:17:28,231 	Raw hypothesis: ['hallo', '.']
2020-06-22 02:17:28,231 	Source:     hello .
2020-06-22 02:17:28,231 	Reference:  hallo ,
2020-06-22 02:17:28,231 	Hypothesis: hallo .
2020-06-22 02:17:28,231 Example #1
2020-06-22 02:17:28,231 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 02:17:28,231 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 02:17:28,232 	Source:     hi , how can i help you ?
2020-06-22 02:17:28,232 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 02:17:28,232 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 02:17:28,232 Example #2
2020-06-22 02:17:28,232 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 02:17:28,232 	Raw hypothesis: ['hallo', ',', 'ich', 'möchte', 'in', 'der', 'der', 'san', 'francisco', ',', 'der', 'der', 'san', 'francisco', ',', 'der', 'san', 'francisco', ',', 'der', 'san', 'francisco', ',', 'kalifornien', ',', 'kalifornien', '.']
2020-06-22 02:17:28,232 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 02:17:28,232 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 02:17:28,232 	Hypothesis: hallo , ich möchte in der der san francisco , der der san francisco , der san francisco , der san francisco , kalifornien , kalifornien .
2020-06-22 02:17:28,232 Example #3
2020-06-22 02:17:28,232 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 02:17:28,232 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'essen', '?']
2020-06-22 02:17:28,232 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 02:17:28,232 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 02:17:28,232 	Hypothesis: ok , welche art von essen ?
2020-06-22 02:17:28,232 Validation result (greedy) at epoch  19, step     1000: bleu:   9.21, loss: 72286.1719, ppl:  30.7898, duration: 14.8650s
2020-06-22 02:17:33,218 Epoch  19: total training loss 165.11
2020-06-22 02:17:33,219 EPOCH 20
2020-06-22 02:17:39,770 Epoch  20: total training loss 161.88
2020-06-22 02:17:39,771 EPOCH 21
2020-06-22 02:17:40,029 Epoch  21 Step:     1100 Batch Loss:     3.344696 Tokens per Sec:    21619, Lr: 0.000200
2020-06-22 02:17:46,348 Epoch  21: total training loss 158.16
2020-06-22 02:17:46,348 EPOCH 22
2020-06-22 02:17:51,945 Epoch  22 Step:     1200 Batch Loss:     2.013207 Tokens per Sec:    20322, Lr: 0.000200
2020-06-22 02:17:52,794 Epoch  22: total training loss 148.55
2020-06-22 02:17:52,794 EPOCH 23
2020-06-22 02:17:59,297 Epoch  23: total training loss 145.54
2020-06-22 02:17:59,298 EPOCH 24
2020-06-22 02:18:03,958 Epoch  24 Step:     1300 Batch Loss:     2.455742 Tokens per Sec:    19914, Lr: 0.000200
2020-06-22 02:18:05,945 Epoch  24: total training loss 144.59
2020-06-22 02:18:05,945 EPOCH 25
2020-06-22 02:18:12,613 Epoch  25: total training loss 145.00
2020-06-22 02:18:12,614 EPOCH 26
2020-06-22 02:18:15,898 Epoch  26 Step:     1400 Batch Loss:     1.807691 Tokens per Sec:    20289, Lr: 0.000200
2020-06-22 02:18:19,218 Epoch  26: total training loss 137.98
2020-06-22 02:18:19,218 EPOCH 27
2020-06-22 02:18:25,826 Epoch  27: total training loss 135.66
2020-06-22 02:18:25,827 EPOCH 28
2020-06-22 02:18:28,128 Epoch  28 Step:     1500 Batch Loss:     3.339963 Tokens per Sec:    18403, Lr: 0.000200
2020-06-22 02:18:32,857 Epoch  28: total training loss 128.32
2020-06-22 02:18:32,857 EPOCH 29
2020-06-22 02:18:40,183 Epoch  29: total training loss 133.36
2020-06-22 02:18:40,183 EPOCH 30
2020-06-22 02:18:41,167 Epoch  30 Step:     1600 Batch Loss:     2.620404 Tokens per Sec:    20194, Lr: 0.000200
2020-06-22 02:18:47,179 Epoch  30: total training loss 127.74
2020-06-22 02:18:47,179 EPOCH 31
2020-06-22 02:18:53,538 Epoch  31 Step:     1700 Batch Loss:     3.860049 Tokens per Sec:    19424, Lr: 0.000200
2020-06-22 02:18:53,896 Epoch  31: total training loss 127.66
2020-06-22 02:18:53,897 EPOCH 32
2020-06-22 02:19:00,543 Epoch  32: total training loss 125.60
2020-06-22 02:19:00,543 EPOCH 33
2020-06-22 02:19:05,561 Epoch  33 Step:     1800 Batch Loss:     1.174440 Tokens per Sec:    19272, Lr: 0.000200
2020-06-22 02:19:07,216 Epoch  33: total training loss 118.87
2020-06-22 02:19:07,216 EPOCH 34
2020-06-22 02:19:13,943 Epoch  34: total training loss 121.10
2020-06-22 02:19:13,944 EPOCH 35
2020-06-22 02:19:17,688 Epoch  35 Step:     1900 Batch Loss:     2.439761 Tokens per Sec:    19647, Lr: 0.000200
2020-06-22 02:19:20,866 Epoch  35: total training loss 113.67
2020-06-22 02:19:20,866 EPOCH 36
2020-06-22 02:19:27,867 Epoch  36: total training loss 112.69
2020-06-22 02:19:27,868 EPOCH 37
2020-06-22 02:19:30,366 Epoch  37 Step:     2000 Batch Loss:     1.072925 Tokens per Sec:    19017, Lr: 0.000200
2020-06-22 02:19:42,211 Hooray! New best validation result [ppl]!
2020-06-22 02:19:42,211 Saving new checkpoint.
2020-06-22 02:19:42,978 Example #0
2020-06-22 02:19:42,979 	Raw source:     ['hello', '.']
2020-06-22 02:19:42,979 	Raw hypothesis: ['hallo', '.']
2020-06-22 02:19:42,979 	Source:     hello .
2020-06-22 02:19:42,979 	Reference:  hallo ,
2020-06-22 02:19:42,979 	Hypothesis: hallo .
2020-06-22 02:19:42,979 Example #1
2020-06-22 02:19:42,979 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 02:19:42,979 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 02:19:42,979 	Source:     hi , how can i help you ?
2020-06-22 02:19:42,979 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 02:19:42,979 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 02:19:42,979 Example #2
2020-06-22 02:19:42,979 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 02:19:42,979 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'nach', 'einem', 'restaurant', 'in', 'der', 'suche', 'nach', 'einem', 'restaurant', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 02:19:42,980 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 02:19:42,980 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 02:19:42,980 	Hypothesis: hallo , ich suche nach einem restaurant in der suche nach einem restaurant in san francisco , kalifornien .
2020-06-22 02:19:42,980 Example #3
2020-06-22 02:19:42,980 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 02:19:42,980 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'haben', 'sie', '?']
2020-06-22 02:19:42,980 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 02:19:42,980 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 02:19:42,980 	Hypothesis: ok , welche art von restaurant haben sie ?
2020-06-22 02:19:42,980 Validation result (greedy) at epoch  37, step     2000: bleu:  20.12, loss: 58646.0234, ppl:  16.1269, duration: 12.6138s
2020-06-22 02:19:47,355 Epoch  37: total training loss 111.43
2020-06-22 02:19:47,355 EPOCH 38
2020-06-22 02:19:54,039 Epoch  38: total training loss 107.60
2020-06-22 02:19:54,040 EPOCH 39
2020-06-22 02:19:55,187 Epoch  39 Step:     2100 Batch Loss:     1.743930 Tokens per Sec:    20708, Lr: 0.000200
2020-06-22 02:20:00,692 Epoch  39: total training loss 106.20
2020-06-22 02:20:00,692 EPOCH 40
2020-06-22 02:20:07,272 Epoch  40: total training loss 100.85
2020-06-22 02:20:07,273 EPOCH 41
2020-06-22 02:20:07,407 Epoch  41 Step:     2200 Batch Loss:     1.938115 Tokens per Sec:    17574, Lr: 0.000200
2020-06-22 02:20:13,935 Epoch  41: total training loss 102.74
2020-06-22 02:20:13,936 EPOCH 42
2020-06-22 02:20:19,684 Epoch  42 Step:     2300 Batch Loss:     1.518030 Tokens per Sec:    19369, Lr: 0.000200
2020-06-22 02:20:20,624 Epoch  42: total training loss 98.24
2020-06-22 02:20:20,624 EPOCH 43
2020-06-22 02:20:27,326 Epoch  43: total training loss 100.96
2020-06-22 02:20:27,327 EPOCH 44
2020-06-22 02:20:31,537 Epoch  44 Step:     2400 Batch Loss:     2.291461 Tokens per Sec:    20897, Lr: 0.000200
2020-06-22 02:20:33,990 Epoch  44: total training loss 96.69
2020-06-22 02:20:33,990 EPOCH 45
2020-06-22 02:20:40,676 Epoch  45: total training loss 98.47
2020-06-22 02:20:40,676 EPOCH 46
2020-06-22 02:20:43,730 Epoch  46 Step:     2500 Batch Loss:     1.570233 Tokens per Sec:    19646, Lr: 0.000200
2020-06-22 02:20:47,290 Epoch  46: total training loss 91.48
2020-06-22 02:20:47,290 EPOCH 47
2020-06-22 02:20:53,992 Epoch  47: total training loss 94.18
2020-06-22 02:20:53,992 EPOCH 48
2020-06-22 02:20:55,912 Epoch  48 Step:     2600 Batch Loss:     2.182824 Tokens per Sec:    18891, Lr: 0.000200
2020-06-22 02:21:00,719 Epoch  48: total training loss 91.09
2020-06-22 02:21:00,719 EPOCH 49
2020-06-22 02:21:07,376 Epoch  49: total training loss 89.51
2020-06-22 02:21:07,376 EPOCH 50
2020-06-22 02:21:08,087 Epoch  50 Step:     2700 Batch Loss:     0.944691 Tokens per Sec:    15057, Lr: 0.000200
2020-06-22 02:21:14,058 Epoch  50: total training loss 88.23
2020-06-22 02:21:14,058 EPOCH 51
2020-06-22 02:21:20,114 Epoch  51 Step:     2800 Batch Loss:     2.325161 Tokens per Sec:    19229, Lr: 0.000200
2020-06-22 02:21:20,828 Epoch  51: total training loss 90.58
2020-06-22 02:21:20,829 EPOCH 52
2020-06-22 02:21:27,638 Epoch  52: total training loss 88.26
2020-06-22 02:21:27,639 EPOCH 53
2020-06-22 02:21:32,284 Epoch  53 Step:     2900 Batch Loss:     1.904295 Tokens per Sec:    19449, Lr: 0.000200
2020-06-22 02:21:34,581 Epoch  53: total training loss 87.97
2020-06-22 02:21:34,581 EPOCH 54
2020-06-22 02:21:41,191 Epoch  54: total training loss 81.17
2020-06-22 02:21:41,192 EPOCH 55
2020-06-22 02:21:44,690 Epoch  55 Step:     3000 Batch Loss:     2.550902 Tokens per Sec:    19151, Lr: 0.000200
2020-06-22 02:21:54,423 Hooray! New best validation result [ppl]!
2020-06-22 02:21:54,424 Saving new checkpoint.
2020-06-22 02:21:55,221 Example #0
2020-06-22 02:21:55,221 	Raw source:     ['hello', '.']
2020-06-22 02:21:55,221 	Raw hypothesis: ['hallo', '.']
2020-06-22 02:21:55,221 	Source:     hello .
2020-06-22 02:21:55,221 	Reference:  hallo ,
2020-06-22 02:21:55,221 	Hypothesis: hallo .
2020-06-22 02:21:55,221 Example #1
2020-06-22 02:21:55,221 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 02:21:55,221 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 02:21:55,221 	Source:     hi , how can i help you ?
2020-06-22 02:21:55,221 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 02:21:55,221 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 02:21:55,221 Example #2
2020-06-22 02:21:55,221 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 02:21:55,221 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'nach', 'einem', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 02:21:55,222 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 02:21:55,222 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 02:21:55,222 	Hypothesis: hallo , ich suche nach einem restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 02:21:55,222 Example #3
2020-06-22 02:21:55,222 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 02:21:55,222 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 02:21:55,222 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 02:21:55,222 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 02:21:55,222 	Hypothesis: ok , welche art von restaurant suchen sie ?
2020-06-22 02:21:55,222 Validation result (greedy) at epoch  55, step     3000: bleu:  26.31, loss: 54490.2930, ppl:  13.2428, duration: 10.5314s
2020-06-22 02:21:58,462 Epoch  55: total training loss 80.99
2020-06-22 02:21:58,462 EPOCH 56
2020-06-22 02:22:05,199 Epoch  56: total training loss 79.31
2020-06-22 02:22:05,200 EPOCH 57
2020-06-22 02:22:07,714 Epoch  57 Step:     3100 Batch Loss:     2.461338 Tokens per Sec:    17424, Lr: 0.000200
2020-06-22 02:22:12,654 Epoch  57: total training loss 79.48
2020-06-22 02:22:12,654 EPOCH 58
2020-06-22 02:22:19,786 Epoch  58: total training loss 77.68
2020-06-22 02:22:19,786 EPOCH 59
2020-06-22 02:22:20,728 Epoch  59 Step:     3200 Batch Loss:     1.173947 Tokens per Sec:    19305, Lr: 0.000200
2020-06-22 02:22:27,108 Epoch  59: total training loss 75.93
2020-06-22 02:22:27,109 EPOCH 60
2020-06-22 02:22:33,897 Epoch  60 Step:     3300 Batch Loss:     2.559353 Tokens per Sec:    17860, Lr: 0.000200
2020-06-22 02:22:34,342 Epoch  60: total training loss 77.79
2020-06-22 02:22:34,342 EPOCH 61
2020-06-22 02:22:41,480 Epoch  61: total training loss 73.58
2020-06-22 02:22:41,481 EPOCH 62
2020-06-22 02:22:46,754 Epoch  62 Step:     3400 Batch Loss:     1.374224 Tokens per Sec:    18921, Lr: 0.000200
2020-06-22 02:22:48,574 Epoch  62: total training loss 71.02
2020-06-22 02:22:48,575 EPOCH 63
2020-06-22 02:22:55,833 Epoch  63: total training loss 74.50
2020-06-22 02:22:55,834 EPOCH 64
2020-06-22 02:22:59,971 Epoch  64 Step:     3500 Batch Loss:     0.954745 Tokens per Sec:    18402, Lr: 0.000200
2020-06-22 02:23:03,158 Epoch  64: total training loss 71.17
2020-06-22 02:23:03,158 EPOCH 65
2020-06-22 02:23:10,453 Epoch  65: total training loss 69.42
2020-06-22 02:23:10,454 EPOCH 66
2020-06-22 02:23:13,388 Epoch  66 Step:     3600 Batch Loss:     1.134968 Tokens per Sec:    17963, Lr: 0.000200
2020-06-22 02:23:17,580 Epoch  66: total training loss 68.24
2020-06-22 02:23:17,580 EPOCH 67
2020-06-22 02:23:24,416 Epoch  67: total training loss 66.91
2020-06-22 02:23:24,417 EPOCH 68
2020-06-22 02:23:25,899 Epoch  68 Step:     3700 Batch Loss:     0.777428 Tokens per Sec:    17591, Lr: 0.000200
2020-06-22 02:23:31,439 Epoch  68: total training loss 66.30
2020-06-22 02:23:31,439 EPOCH 69
2020-06-22 02:23:38,052 Epoch  69: total training loss 63.33
2020-06-22 02:23:38,052 EPOCH 70
2020-06-22 02:23:38,261 Epoch  70 Step:     3800 Batch Loss:     1.387038 Tokens per Sec:    27281, Lr: 0.000200
2020-06-22 02:23:44,751 Epoch  70: total training loss 64.35
2020-06-22 02:23:44,752 EPOCH 71
2020-06-22 02:23:50,533 Epoch  71 Step:     3900 Batch Loss:     0.758508 Tokens per Sec:    19521, Lr: 0.000200
2020-06-22 02:23:51,458 Epoch  71: total training loss 64.50
2020-06-22 02:23:51,458 EPOCH 72
2020-06-22 02:23:58,214 Epoch  72: total training loss 61.10
2020-06-22 02:23:58,215 EPOCH 73
2020-06-22 02:24:02,921 Epoch  73 Step:     4000 Batch Loss:     0.622235 Tokens per Sec:    19079, Lr: 0.000200
2020-06-22 02:24:12,345 Example #0
2020-06-22 02:24:12,346 	Raw source:     ['hello', '.']
2020-06-22 02:24:12,346 	Raw hypothesis: ['hallo', '.']
2020-06-22 02:24:12,346 	Source:     hello .
2020-06-22 02:24:12,346 	Reference:  hallo ,
2020-06-22 02:24:12,347 	Hypothesis: hallo .
2020-06-22 02:24:12,347 Example #1
2020-06-22 02:24:12,347 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 02:24:12,347 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 02:24:12,347 	Source:     hi , how can i help you ?
2020-06-22 02:24:12,347 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 02:24:12,347 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 02:24:12,347 Example #2
2020-06-22 02:24:12,347 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 02:24:12,347 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 02:24:12,347 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 02:24:12,347 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 02:24:12,347 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 02:24:12,347 Example #3
2020-06-22 02:24:12,347 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 02:24:12,347 	Raw hypothesis: ['ok', ',', 'welche', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 02:24:12,347 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 02:24:12,347 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 02:24:12,347 	Hypothesis: ok , welche art von restaurant suchen sie ?
2020-06-22 02:24:12,347 Validation result (greedy) at epoch  73, step     4000: bleu:  27.81, loss: 54812.5938, ppl:  13.4468, duration: 9.4258s
2020-06-22 02:24:14,488 Epoch  73: total training loss 63.63
2020-06-22 02:24:14,488 EPOCH 74
2020-06-22 02:24:21,224 Epoch  74: total training loss 61.32
2020-06-22 02:24:21,225 EPOCH 75
2020-06-22 02:24:24,711 Epoch  75 Step:     4100 Batch Loss:     0.898628 Tokens per Sec:    18908, Lr: 0.000200
2020-06-22 02:24:28,285 Epoch  75: total training loss 59.99
2020-06-22 02:24:28,285 EPOCH 76
2020-06-22 02:24:35,328 Epoch  76: total training loss 60.07
2020-06-22 02:24:35,328 EPOCH 77
2020-06-22 02:24:37,540 Epoch  77 Step:     4200 Batch Loss:     0.759393 Tokens per Sec:    19243, Lr: 0.000200
2020-06-22 02:24:41,994 Epoch  77: total training loss 57.26
2020-06-22 02:24:41,995 EPOCH 78
2020-06-22 02:24:48,667 Epoch  78: total training loss 54.95
2020-06-22 02:24:48,667 EPOCH 79
2020-06-22 02:24:49,726 Epoch  79 Step:     4300 Batch Loss:     0.981347 Tokens per Sec:    21188, Lr: 0.000200
2020-06-22 02:24:55,400 Epoch  79: total training loss 55.60
2020-06-22 02:24:55,400 EPOCH 80
2020-06-22 02:25:02,054 Epoch  80 Step:     4400 Batch Loss:     0.572333 Tokens per Sec:    19202, Lr: 0.000200
2020-06-22 02:25:02,161 Epoch  80: total training loss 55.57
2020-06-22 02:25:02,162 EPOCH 81
2020-06-22 02:25:09,011 Epoch  81: total training loss 54.81
2020-06-22 02:25:09,011 EPOCH 82
2020-06-22 02:25:14,465 Epoch  82 Step:     4500 Batch Loss:     0.260049 Tokens per Sec:    19318, Lr: 0.000200
2020-06-22 02:25:15,894 Epoch  82: total training loss 55.65
2020-06-22 02:25:15,895 EPOCH 83
2020-06-22 02:25:22,603 Epoch  83: total training loss 53.55
2020-06-22 02:25:22,604 EPOCH 84
2020-06-22 02:25:26,702 Epoch  84 Step:     4600 Batch Loss:     1.156370 Tokens per Sec:    20150, Lr: 0.000200
2020-06-22 02:25:29,253 Epoch  84: total training loss 50.61
2020-06-22 02:25:29,253 EPOCH 85
2020-06-22 02:25:35,916 Epoch  85: total training loss 50.55
2020-06-22 02:25:35,916 EPOCH 86
2020-06-22 02:25:38,857 Epoch  86 Step:     4700 Batch Loss:     0.757768 Tokens per Sec:    18967, Lr: 0.000200
2020-06-22 02:25:42,733 Epoch  86: total training loss 52.26
2020-06-22 02:25:42,733 EPOCH 87
2020-06-22 02:25:49,510 Epoch  87: total training loss 51.34
2020-06-22 02:25:49,510 EPOCH 88
2020-06-22 02:25:51,097 Epoch  88 Step:     4800 Batch Loss:     1.193533 Tokens per Sec:    19340, Lr: 0.000200
2020-06-22 02:25:56,363 Epoch  88: total training loss 49.03
2020-06-22 02:25:56,363 EPOCH 89
2020-06-22 02:26:03,592 Epoch  89: total training loss 48.38
2020-06-22 02:26:03,592 EPOCH 90
2020-06-22 02:26:04,037 Epoch  90 Step:     4900 Batch Loss:     0.588017 Tokens per Sec:    16211, Lr: 0.000200
2020-06-22 02:26:10,709 Epoch  90: total training loss 48.93
2020-06-22 02:26:10,709 EPOCH 91
2020-06-22 02:26:16,616 Epoch  91 Step:     5000 Batch Loss:     0.757539 Tokens per Sec:    18999, Lr: 0.000200
2020-06-22 02:26:27,608 Example #0
2020-06-22 02:26:27,608 	Raw source:     ['hello', '.']
2020-06-22 02:26:27,608 	Raw hypothesis: ['hallo', '.']
2020-06-22 02:26:27,608 	Source:     hello .
2020-06-22 02:26:27,608 	Reference:  hallo ,
2020-06-22 02:26:27,608 	Hypothesis: hallo .
2020-06-22 02:26:27,609 Example #1
2020-06-22 02:26:27,609 	Raw source:     ['hi', ',', 'how', 'can', 'i', 'help', 'you', '?']
2020-06-22 02:26:27,609 	Raw hypothesis: ['hallo', ',', 'wie', 'kann', 'ich', 'ihnen', 'helfen', '?']
2020-06-22 02:26:27,609 	Source:     hi , how can i help you ?
2020-06-22 02:26:27,609 	Reference:  hallo , wie kann ich ihnen helfen ?
2020-06-22 02:26:27,609 	Hypothesis: hallo , wie kann ich ihnen helfen ?
2020-06-22 02:26:27,609 Example #2
2020-06-22 02:26:27,609 	Raw source:     ['hi', ',', 'i', '&apos;m', 'looking', 'for', 'a', 'restaurant', 'inside', 'the', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'california', '.']
2020-06-22 02:26:27,609 	Raw hypothesis: ['hallo', ',', 'ich', 'suche', 'ein', 'restaurant', 'in', 'der', 'arden', 'fair', 'mall', 'in', 'san', 'francisco', ',', 'kalifornien', '.']
2020-06-22 02:26:27,609 	Source:     hi , i &apos;m looking for a restaurant inside the arden fair mall in san francisco , california .
2020-06-22 02:26:27,609 	Reference:  hallo , ich bin auf der suche nach einem restaurant im arden fair einkaufszentrum in san francisco , kalifornien .
2020-06-22 02:26:27,609 	Hypothesis: hallo , ich suche ein restaurant in der arden fair mall in san francisco , kalifornien .
2020-06-22 02:26:27,609 Example #3
2020-06-22 02:26:27,609 	Raw source:     ['ok', ',', 'what', 'type', 'of', 'restaurant', 'are', 'you', 'looking', 'for', '?']
2020-06-22 02:26:27,609 	Raw hypothesis: ['ok', ',', 'nach', 'welcher', 'art', 'von', 'restaurant', 'suchen', 'sie', '?']
2020-06-22 02:26:27,609 	Source:     ok , what type of restaurant are you looking for ?
2020-06-22 02:26:27,609 	Reference:  ok . welche art von restaurant suchen sie denn genau ?
2020-06-22 02:26:27,609 	Hypothesis: ok , nach welcher art von restaurant suchen sie ?
2020-06-22 02:26:27,609 Validation result (greedy) at epoch  91, step     5000: bleu:  29.34, loss: 55111.7383, ppl:  13.6388, duration: 10.9923s
2020-06-22 02:26:28,706 Epoch  91: total training loss 48.36
2020-06-22 02:26:28,706 EPOCH 92
2020-06-22 02:26:35,862 Epoch  92: total training loss 47.44
2020-06-22 02:26:35,863 EPOCH 93
2020-06-22 02:26:40,419 Epoch  93 Step:     5100 Batch Loss:     0.971923 Tokens per Sec:    19043, Lr: 0.000200
2020-06-22 02:26:43,143 Epoch  93: total training loss 46.13
2020-06-22 02:26:43,143 EPOCH 94
2020-06-22 02:26:50,275 Epoch  94: total training loss 44.15
2020-06-22 02:26:50,275 EPOCH 95
2020-06-22 02:26:53,845 Epoch  95 Step:     5200 Batch Loss:     0.633372 Tokens per Sec:    17469, Lr: 0.000200
2020-06-22 02:26:57,811 Epoch  95: total training loss 44.87
2020-06-22 02:26:57,812 EPOCH 96
2020-06-22 02:27:06,007 Epoch  96: total training loss 44.05
2020-06-22 02:27:06,008 EPOCH 97
2020-06-22 02:27:08,376 Epoch  97 Step:     5300 Batch Loss:     0.654623 Tokens per Sec:    16984, Lr: 0.000200
2020-06-22 02:27:14,126 Epoch  97: total training loss 42.98
2020-06-22 02:27:14,127 EPOCH 98
2020-06-22 02:27:21,125 Epoch  98: total training loss 44.00
2020-06-22 02:27:21,126 EPOCH 99
2020-06-22 02:27:21,820 Epoch  99 Step:     5400 Batch Loss:     0.973931 Tokens per Sec:    19400, Lr: 0.000200
2020-06-22 02:27:28,914 Epoch  99: total training loss 41.63
2020-06-22 02:27:28,915 EPOCH 100
2020-06-22 02:27:36,824 Epoch 100 Step:     5500 Batch Loss:     0.435505 Tokens per Sec:    15872, Lr: 0.000200
2020-06-22 02:27:37,239 Epoch 100: total training loss 40.35
2020-06-22 02:27:37,239 Training ended after 100 epochs.
2020-06-22 02:27:37,240 Best validation result (greedy) at step     3000:  13.24 ppl.
2020-06-22 02:27:54,945  dev bleu:  27.44 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 02:27:54,949 Translations saved to: models/transformer_multi_enc6x2_hid128_ende/00003000.hyps.dev
2020-06-22 02:28:04,393 test bleu:  25.66 [Beam search decoding with beam size = 5 and alpha = 1.0]
2020-06-22 02:28:04,397 Translations saved to: models/transformer_multi_enc6x2_hid128_ende/00003000.hyps.test
